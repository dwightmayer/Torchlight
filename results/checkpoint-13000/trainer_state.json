{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 94.8905109489051,
  "eval_steps": 500,
  "global_step": 13000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.072992700729927,
      "grad_norm": 7.779646396636963,
      "learning_rate": 4.996350364963504e-05,
      "loss": 5.3102,
      "step": 10
    },
    {
      "epoch": 0.145985401459854,
      "grad_norm": 8.160876274108887,
      "learning_rate": 4.992700729927007e-05,
      "loss": 5.1245,
      "step": 20
    },
    {
      "epoch": 0.21897810218978103,
      "grad_norm": 7.367201328277588,
      "learning_rate": 4.989051094890511e-05,
      "loss": 4.7619,
      "step": 30
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 7.570680618286133,
      "learning_rate": 4.985401459854015e-05,
      "loss": 5.2921,
      "step": 40
    },
    {
      "epoch": 0.36496350364963503,
      "grad_norm": 8.184865951538086,
      "learning_rate": 4.981751824817518e-05,
      "loss": 5.1328,
      "step": 50
    },
    {
      "epoch": 0.43795620437956206,
      "grad_norm": 7.597873210906982,
      "learning_rate": 4.978102189781022e-05,
      "loss": 5.1363,
      "step": 60
    },
    {
      "epoch": 0.5109489051094891,
      "grad_norm": 8.181844711303711,
      "learning_rate": 4.974452554744526e-05,
      "loss": 5.4716,
      "step": 70
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 7.822225093841553,
      "learning_rate": 4.97080291970803e-05,
      "loss": 5.1213,
      "step": 80
    },
    {
      "epoch": 0.656934306569343,
      "grad_norm": 7.779703617095947,
      "learning_rate": 4.967153284671533e-05,
      "loss": 5.1979,
      "step": 90
    },
    {
      "epoch": 0.7299270072992701,
      "grad_norm": 8.025532722473145,
      "learning_rate": 4.963503649635037e-05,
      "loss": 5.2621,
      "step": 100
    },
    {
      "epoch": 0.8029197080291971,
      "grad_norm": 7.586985111236572,
      "learning_rate": 4.959854014598541e-05,
      "loss": 5.2388,
      "step": 110
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 7.3722147941589355,
      "learning_rate": 4.956204379562044e-05,
      "loss": 5.0637,
      "step": 120
    },
    {
      "epoch": 0.948905109489051,
      "grad_norm": 7.123636245727539,
      "learning_rate": 4.952554744525548e-05,
      "loss": 5.0949,
      "step": 130
    },
    {
      "epoch": 1.0218978102189782,
      "grad_norm": 7.682340621948242,
      "learning_rate": 4.948905109489051e-05,
      "loss": 4.8805,
      "step": 140
    },
    {
      "epoch": 1.094890510948905,
      "grad_norm": 7.73562479019165,
      "learning_rate": 4.945255474452555e-05,
      "loss": 5.0014,
      "step": 150
    },
    {
      "epoch": 1.167883211678832,
      "grad_norm": 7.479316234588623,
      "learning_rate": 4.941605839416058e-05,
      "loss": 5.2415,
      "step": 160
    },
    {
      "epoch": 1.2408759124087592,
      "grad_norm": 7.5931477546691895,
      "learning_rate": 4.937956204379562e-05,
      "loss": 5.3764,
      "step": 170
    },
    {
      "epoch": 1.313868613138686,
      "grad_norm": 7.9356160163879395,
      "learning_rate": 4.9343065693430654e-05,
      "loss": 4.9954,
      "step": 180
    },
    {
      "epoch": 1.3868613138686132,
      "grad_norm": 7.314726829528809,
      "learning_rate": 4.930656934306569e-05,
      "loss": 4.9815,
      "step": 190
    },
    {
      "epoch": 1.4598540145985401,
      "grad_norm": 7.720823287963867,
      "learning_rate": 4.927007299270073e-05,
      "loss": 4.9378,
      "step": 200
    },
    {
      "epoch": 1.5328467153284673,
      "grad_norm": 7.932222366333008,
      "learning_rate": 4.923357664233577e-05,
      "loss": 5.1459,
      "step": 210
    },
    {
      "epoch": 1.6058394160583942,
      "grad_norm": 7.258546829223633,
      "learning_rate": 4.91970802919708e-05,
      "loss": 5.072,
      "step": 220
    },
    {
      "epoch": 1.6788321167883211,
      "grad_norm": 8.04538345336914,
      "learning_rate": 4.916058394160584e-05,
      "loss": 4.9421,
      "step": 230
    },
    {
      "epoch": 1.7518248175182483,
      "grad_norm": 7.0679121017456055,
      "learning_rate": 4.912408759124088e-05,
      "loss": 4.9349,
      "step": 240
    },
    {
      "epoch": 1.8248175182481752,
      "grad_norm": 7.736692428588867,
      "learning_rate": 4.908759124087591e-05,
      "loss": 5.0272,
      "step": 250
    },
    {
      "epoch": 1.897810218978102,
      "grad_norm": 7.48113489151001,
      "learning_rate": 4.905109489051095e-05,
      "loss": 4.8434,
      "step": 260
    },
    {
      "epoch": 1.9708029197080292,
      "grad_norm": 7.668025016784668,
      "learning_rate": 4.901459854014599e-05,
      "loss": 5.2279,
      "step": 270
    },
    {
      "epoch": 2.0437956204379564,
      "grad_norm": 8.072490692138672,
      "learning_rate": 4.897810218978102e-05,
      "loss": 4.8029,
      "step": 280
    },
    {
      "epoch": 2.116788321167883,
      "grad_norm": 7.892195224761963,
      "learning_rate": 4.894160583941606e-05,
      "loss": 4.8459,
      "step": 290
    },
    {
      "epoch": 2.18978102189781,
      "grad_norm": 7.652929782867432,
      "learning_rate": 4.89051094890511e-05,
      "loss": 5.1124,
      "step": 300
    },
    {
      "epoch": 2.2627737226277373,
      "grad_norm": 7.523911476135254,
      "learning_rate": 4.886861313868613e-05,
      "loss": 4.7465,
      "step": 310
    },
    {
      "epoch": 2.335766423357664,
      "grad_norm": 7.255472183227539,
      "learning_rate": 4.883211678832117e-05,
      "loss": 4.892,
      "step": 320
    },
    {
      "epoch": 2.408759124087591,
      "grad_norm": 7.5723748207092285,
      "learning_rate": 4.879562043795621e-05,
      "loss": 4.9207,
      "step": 330
    },
    {
      "epoch": 2.4817518248175183,
      "grad_norm": 7.521195411682129,
      "learning_rate": 4.875912408759125e-05,
      "loss": 4.9722,
      "step": 340
    },
    {
      "epoch": 2.554744525547445,
      "grad_norm": 8.44996452331543,
      "learning_rate": 4.872262773722628e-05,
      "loss": 5.1186,
      "step": 350
    },
    {
      "epoch": 2.627737226277372,
      "grad_norm": 7.696522235870361,
      "learning_rate": 4.868613138686132e-05,
      "loss": 4.978,
      "step": 360
    },
    {
      "epoch": 2.7007299270072993,
      "grad_norm": 8.393426895141602,
      "learning_rate": 4.864963503649636e-05,
      "loss": 5.2009,
      "step": 370
    },
    {
      "epoch": 2.7737226277372264,
      "grad_norm": 7.042917728424072,
      "learning_rate": 4.861313868613139e-05,
      "loss": 4.8158,
      "step": 380
    },
    {
      "epoch": 2.846715328467153,
      "grad_norm": 9.69466495513916,
      "learning_rate": 4.857664233576643e-05,
      "loss": 5.0019,
      "step": 390
    },
    {
      "epoch": 2.9197080291970803,
      "grad_norm": 7.302897930145264,
      "learning_rate": 4.854014598540147e-05,
      "loss": 4.9022,
      "step": 400
    },
    {
      "epoch": 2.9927007299270074,
      "grad_norm": 7.4515790939331055,
      "learning_rate": 4.85036496350365e-05,
      "loss": 5.0171,
      "step": 410
    },
    {
      "epoch": 3.065693430656934,
      "grad_norm": 7.22563362121582,
      "learning_rate": 4.846715328467154e-05,
      "loss": 5.0258,
      "step": 420
    },
    {
      "epoch": 3.1386861313868613,
      "grad_norm": 7.913239002227783,
      "learning_rate": 4.843065693430657e-05,
      "loss": 4.7085,
      "step": 430
    },
    {
      "epoch": 3.2116788321167884,
      "grad_norm": 7.312809944152832,
      "learning_rate": 4.839416058394161e-05,
      "loss": 5.0824,
      "step": 440
    },
    {
      "epoch": 3.2846715328467155,
      "grad_norm": 7.843894004821777,
      "learning_rate": 4.835766423357664e-05,
      "loss": 4.7597,
      "step": 450
    },
    {
      "epoch": 3.3576642335766422,
      "grad_norm": 7.4024434089660645,
      "learning_rate": 4.832116788321168e-05,
      "loss": 4.9392,
      "step": 460
    },
    {
      "epoch": 3.4306569343065694,
      "grad_norm": 7.531661510467529,
      "learning_rate": 4.828467153284672e-05,
      "loss": 4.9676,
      "step": 470
    },
    {
      "epoch": 3.5036496350364965,
      "grad_norm": 7.291359901428223,
      "learning_rate": 4.824817518248175e-05,
      "loss": 4.6672,
      "step": 480
    },
    {
      "epoch": 3.576642335766423,
      "grad_norm": 7.6296706199646,
      "learning_rate": 4.821167883211679e-05,
      "loss": 5.0109,
      "step": 490
    },
    {
      "epoch": 3.6496350364963503,
      "grad_norm": 7.298050880432129,
      "learning_rate": 4.817518248175183e-05,
      "loss": 5.0672,
      "step": 500
    },
    {
      "epoch": 3.7226277372262775,
      "grad_norm": 7.649781227111816,
      "learning_rate": 4.813868613138686e-05,
      "loss": 4.8704,
      "step": 510
    },
    {
      "epoch": 3.795620437956204,
      "grad_norm": 7.55747652053833,
      "learning_rate": 4.81021897810219e-05,
      "loss": 4.8366,
      "step": 520
    },
    {
      "epoch": 3.8686131386861313,
      "grad_norm": 7.560015678405762,
      "learning_rate": 4.806569343065694e-05,
      "loss": 4.7219,
      "step": 530
    },
    {
      "epoch": 3.9416058394160585,
      "grad_norm": 7.472229957580566,
      "learning_rate": 4.802919708029197e-05,
      "loss": 4.8132,
      "step": 540
    },
    {
      "epoch": 4.014598540145985,
      "grad_norm": 7.416871547698975,
      "learning_rate": 4.799270072992701e-05,
      "loss": 4.9347,
      "step": 550
    },
    {
      "epoch": 4.087591240875913,
      "grad_norm": 7.513606071472168,
      "learning_rate": 4.795620437956205e-05,
      "loss": 4.7723,
      "step": 560
    },
    {
      "epoch": 4.160583941605839,
      "grad_norm": 7.647167682647705,
      "learning_rate": 4.791970802919708e-05,
      "loss": 5.0586,
      "step": 570
    },
    {
      "epoch": 4.233576642335766,
      "grad_norm": 7.413955211639404,
      "learning_rate": 4.788321167883212e-05,
      "loss": 4.9401,
      "step": 580
    },
    {
      "epoch": 4.306569343065694,
      "grad_norm": 7.511030197143555,
      "learning_rate": 4.784671532846716e-05,
      "loss": 4.8454,
      "step": 590
    },
    {
      "epoch": 4.37956204379562,
      "grad_norm": 7.407678604125977,
      "learning_rate": 4.7810218978102196e-05,
      "loss": 5.0024,
      "step": 600
    },
    {
      "epoch": 4.452554744525547,
      "grad_norm": 7.312450408935547,
      "learning_rate": 4.777372262773723e-05,
      "loss": 4.752,
      "step": 610
    },
    {
      "epoch": 4.525547445255475,
      "grad_norm": 7.3917646408081055,
      "learning_rate": 4.773722627737227e-05,
      "loss": 5.0371,
      "step": 620
    },
    {
      "epoch": 4.598540145985401,
      "grad_norm": 7.682239532470703,
      "learning_rate": 4.7700729927007306e-05,
      "loss": 4.6921,
      "step": 630
    },
    {
      "epoch": 4.671532846715328,
      "grad_norm": 7.329121112823486,
      "learning_rate": 4.766423357664234e-05,
      "loss": 4.8923,
      "step": 640
    },
    {
      "epoch": 4.744525547445256,
      "grad_norm": 7.251962184906006,
      "learning_rate": 4.762773722627738e-05,
      "loss": 4.6009,
      "step": 650
    },
    {
      "epoch": 4.817518248175182,
      "grad_norm": 7.5275797843933105,
      "learning_rate": 4.7591240875912416e-05,
      "loss": 4.7654,
      "step": 660
    },
    {
      "epoch": 4.89051094890511,
      "grad_norm": 7.1644511222839355,
      "learning_rate": 4.755474452554745e-05,
      "loss": 4.8652,
      "step": 670
    },
    {
      "epoch": 4.963503649635037,
      "grad_norm": 7.837331771850586,
      "learning_rate": 4.7518248175182487e-05,
      "loss": 4.621,
      "step": 680
    },
    {
      "epoch": 5.036496350364963,
      "grad_norm": 7.6906938552856445,
      "learning_rate": 4.748175182481752e-05,
      "loss": 4.8079,
      "step": 690
    },
    {
      "epoch": 5.109489051094891,
      "grad_norm": 8.20925521850586,
      "learning_rate": 4.744525547445256e-05,
      "loss": 4.4828,
      "step": 700
    },
    {
      "epoch": 5.182481751824818,
      "grad_norm": 7.794738292694092,
      "learning_rate": 4.740875912408759e-05,
      "loss": 5.0193,
      "step": 710
    },
    {
      "epoch": 5.255474452554744,
      "grad_norm": 7.657832622528076,
      "learning_rate": 4.737226277372263e-05,
      "loss": 4.8719,
      "step": 720
    },
    {
      "epoch": 5.328467153284672,
      "grad_norm": 7.907358169555664,
      "learning_rate": 4.733576642335767e-05,
      "loss": 4.5429,
      "step": 730
    },
    {
      "epoch": 5.401459854014599,
      "grad_norm": 7.3625969886779785,
      "learning_rate": 4.72992700729927e-05,
      "loss": 4.9262,
      "step": 740
    },
    {
      "epoch": 5.474452554744525,
      "grad_norm": 7.670666217803955,
      "learning_rate": 4.726277372262774e-05,
      "loss": 4.7103,
      "step": 750
    },
    {
      "epoch": 5.547445255474453,
      "grad_norm": 7.387008190155029,
      "learning_rate": 4.722627737226278e-05,
      "loss": 4.569,
      "step": 760
    },
    {
      "epoch": 5.62043795620438,
      "grad_norm": 7.25895881652832,
      "learning_rate": 4.718978102189781e-05,
      "loss": 4.7354,
      "step": 770
    },
    {
      "epoch": 5.693430656934306,
      "grad_norm": 7.694452285766602,
      "learning_rate": 4.715328467153285e-05,
      "loss": 4.9651,
      "step": 780
    },
    {
      "epoch": 5.766423357664234,
      "grad_norm": 7.767874240875244,
      "learning_rate": 4.7116788321167887e-05,
      "loss": 4.9404,
      "step": 790
    },
    {
      "epoch": 5.839416058394161,
      "grad_norm": 7.386264324188232,
      "learning_rate": 4.708029197080292e-05,
      "loss": 4.8475,
      "step": 800
    },
    {
      "epoch": 5.912408759124087,
      "grad_norm": 6.985630512237549,
      "learning_rate": 4.704379562043796e-05,
      "loss": 4.8454,
      "step": 810
    },
    {
      "epoch": 5.985401459854015,
      "grad_norm": 6.822415351867676,
      "learning_rate": 4.7007299270072996e-05,
      "loss": 4.6224,
      "step": 820
    },
    {
      "epoch": 6.0583941605839415,
      "grad_norm": 7.414081573486328,
      "learning_rate": 4.697080291970803e-05,
      "loss": 4.9122,
      "step": 830
    },
    {
      "epoch": 6.131386861313868,
      "grad_norm": 7.713131427764893,
      "learning_rate": 4.693430656934307e-05,
      "loss": 4.7293,
      "step": 840
    },
    {
      "epoch": 6.204379562043796,
      "grad_norm": 7.461742401123047,
      "learning_rate": 4.6897810218978106e-05,
      "loss": 4.7629,
      "step": 850
    },
    {
      "epoch": 6.2773722627737225,
      "grad_norm": 7.09768009185791,
      "learning_rate": 4.6861313868613145e-05,
      "loss": 4.6868,
      "step": 860
    },
    {
      "epoch": 6.350364963503649,
      "grad_norm": 7.733173370361328,
      "learning_rate": 4.682481751824818e-05,
      "loss": 4.6896,
      "step": 870
    },
    {
      "epoch": 6.423357664233577,
      "grad_norm": 7.132112979888916,
      "learning_rate": 4.6788321167883216e-05,
      "loss": 4.6641,
      "step": 880
    },
    {
      "epoch": 6.4963503649635035,
      "grad_norm": 7.378779411315918,
      "learning_rate": 4.6751824817518254e-05,
      "loss": 4.7281,
      "step": 890
    },
    {
      "epoch": 6.569343065693431,
      "grad_norm": 7.5042195320129395,
      "learning_rate": 4.6715328467153287e-05,
      "loss": 4.5484,
      "step": 900
    },
    {
      "epoch": 6.642335766423358,
      "grad_norm": 7.305045127868652,
      "learning_rate": 4.6678832116788325e-05,
      "loss": 5.1618,
      "step": 910
    },
    {
      "epoch": 6.7153284671532845,
      "grad_norm": 7.035285472869873,
      "learning_rate": 4.6642335766423364e-05,
      "loss": 4.4729,
      "step": 920
    },
    {
      "epoch": 6.788321167883212,
      "grad_norm": 7.510340690612793,
      "learning_rate": 4.6605839416058396e-05,
      "loss": 4.4726,
      "step": 930
    },
    {
      "epoch": 6.861313868613139,
      "grad_norm": 7.2215423583984375,
      "learning_rate": 4.6569343065693435e-05,
      "loss": 4.9496,
      "step": 940
    },
    {
      "epoch": 6.934306569343065,
      "grad_norm": 7.351651191711426,
      "learning_rate": 4.6532846715328474e-05,
      "loss": 4.6854,
      "step": 950
    },
    {
      "epoch": 7.007299270072993,
      "grad_norm": 7.5752763748168945,
      "learning_rate": 4.6496350364963506e-05,
      "loss": 4.7517,
      "step": 960
    },
    {
      "epoch": 7.08029197080292,
      "grad_norm": 7.249711513519287,
      "learning_rate": 4.6459854014598545e-05,
      "loss": 4.6576,
      "step": 970
    },
    {
      "epoch": 7.153284671532846,
      "grad_norm": 8.01574420928955,
      "learning_rate": 4.642335766423358e-05,
      "loss": 4.5186,
      "step": 980
    },
    {
      "epoch": 7.226277372262774,
      "grad_norm": 7.036163330078125,
      "learning_rate": 4.6386861313868616e-05,
      "loss": 4.8287,
      "step": 990
    },
    {
      "epoch": 7.299270072992701,
      "grad_norm": 7.008028030395508,
      "learning_rate": 4.635036496350365e-05,
      "loss": 4.5976,
      "step": 1000
    },
    {
      "epoch": 7.372262773722627,
      "grad_norm": 7.245133399963379,
      "learning_rate": 4.6313868613138686e-05,
      "loss": 4.8308,
      "step": 1010
    },
    {
      "epoch": 7.445255474452555,
      "grad_norm": 7.393959999084473,
      "learning_rate": 4.6277372262773725e-05,
      "loss": 4.645,
      "step": 1020
    },
    {
      "epoch": 7.518248175182482,
      "grad_norm": 7.321710109710693,
      "learning_rate": 4.624087591240876e-05,
      "loss": 4.793,
      "step": 1030
    },
    {
      "epoch": 7.591240875912408,
      "grad_norm": 7.5008864402771,
      "learning_rate": 4.6204379562043796e-05,
      "loss": 4.736,
      "step": 1040
    },
    {
      "epoch": 7.664233576642336,
      "grad_norm": 7.2764058113098145,
      "learning_rate": 4.6167883211678835e-05,
      "loss": 4.7991,
      "step": 1050
    },
    {
      "epoch": 7.737226277372263,
      "grad_norm": 7.362646579742432,
      "learning_rate": 4.613138686131387e-05,
      "loss": 4.759,
      "step": 1060
    },
    {
      "epoch": 7.81021897810219,
      "grad_norm": 7.136068820953369,
      "learning_rate": 4.6094890510948906e-05,
      "loss": 4.5205,
      "step": 1070
    },
    {
      "epoch": 7.883211678832117,
      "grad_norm": 7.619078636169434,
      "learning_rate": 4.6058394160583945e-05,
      "loss": 4.8423,
      "step": 1080
    },
    {
      "epoch": 7.956204379562044,
      "grad_norm": 7.419332504272461,
      "learning_rate": 4.602189781021898e-05,
      "loss": 4.5762,
      "step": 1090
    },
    {
      "epoch": 8.02919708029197,
      "grad_norm": 7.077867031097412,
      "learning_rate": 4.5985401459854016e-05,
      "loss": 4.5104,
      "step": 1100
    },
    {
      "epoch": 8.102189781021897,
      "grad_norm": 7.132941246032715,
      "learning_rate": 4.5948905109489054e-05,
      "loss": 4.5835,
      "step": 1110
    },
    {
      "epoch": 8.175182481751825,
      "grad_norm": 6.912570953369141,
      "learning_rate": 4.591240875912409e-05,
      "loss": 4.5027,
      "step": 1120
    },
    {
      "epoch": 8.248175182481752,
      "grad_norm": 7.783249855041504,
      "learning_rate": 4.5875912408759125e-05,
      "loss": 4.7563,
      "step": 1130
    },
    {
      "epoch": 8.321167883211679,
      "grad_norm": 7.36983585357666,
      "learning_rate": 4.5839416058394164e-05,
      "loss": 4.7984,
      "step": 1140
    },
    {
      "epoch": 8.394160583941606,
      "grad_norm": 7.041681289672852,
      "learning_rate": 4.58029197080292e-05,
      "loss": 4.909,
      "step": 1150
    },
    {
      "epoch": 8.467153284671532,
      "grad_norm": 7.193963050842285,
      "learning_rate": 4.5766423357664235e-05,
      "loss": 4.497,
      "step": 1160
    },
    {
      "epoch": 8.540145985401459,
      "grad_norm": 7.833805561065674,
      "learning_rate": 4.5729927007299274e-05,
      "loss": 4.6419,
      "step": 1170
    },
    {
      "epoch": 8.613138686131387,
      "grad_norm": 7.384265422821045,
      "learning_rate": 4.569343065693431e-05,
      "loss": 4.7217,
      "step": 1180
    },
    {
      "epoch": 8.686131386861314,
      "grad_norm": 7.479548454284668,
      "learning_rate": 4.5656934306569345e-05,
      "loss": 4.4873,
      "step": 1190
    },
    {
      "epoch": 8.75912408759124,
      "grad_norm": 7.766378879547119,
      "learning_rate": 4.5620437956204383e-05,
      "loss": 4.6461,
      "step": 1200
    },
    {
      "epoch": 8.832116788321168,
      "grad_norm": 7.826021671295166,
      "learning_rate": 4.558394160583942e-05,
      "loss": 4.9233,
      "step": 1210
    },
    {
      "epoch": 8.905109489051094,
      "grad_norm": 7.079723834991455,
      "learning_rate": 4.5547445255474454e-05,
      "loss": 4.65,
      "step": 1220
    },
    {
      "epoch": 8.978102189781023,
      "grad_norm": 7.420773983001709,
      "learning_rate": 4.551094890510949e-05,
      "loss": 4.4786,
      "step": 1230
    },
    {
      "epoch": 9.05109489051095,
      "grad_norm": 6.937910556793213,
      "learning_rate": 4.547445255474453e-05,
      "loss": 4.6248,
      "step": 1240
    },
    {
      "epoch": 9.124087591240876,
      "grad_norm": 7.315236568450928,
      "learning_rate": 4.5437956204379564e-05,
      "loss": 4.5721,
      "step": 1250
    },
    {
      "epoch": 9.197080291970803,
      "grad_norm": 7.260655403137207,
      "learning_rate": 4.54014598540146e-05,
      "loss": 4.5185,
      "step": 1260
    },
    {
      "epoch": 9.27007299270073,
      "grad_norm": 7.340148448944092,
      "learning_rate": 4.5364963503649635e-05,
      "loss": 4.7504,
      "step": 1270
    },
    {
      "epoch": 9.343065693430656,
      "grad_norm": 6.999653339385986,
      "learning_rate": 4.5328467153284674e-05,
      "loss": 4.4489,
      "step": 1280
    },
    {
      "epoch": 9.416058394160585,
      "grad_norm": 7.306241035461426,
      "learning_rate": 4.5291970802919706e-05,
      "loss": 4.6431,
      "step": 1290
    },
    {
      "epoch": 9.489051094890511,
      "grad_norm": 8.010822296142578,
      "learning_rate": 4.5255474452554745e-05,
      "loss": 4.9111,
      "step": 1300
    },
    {
      "epoch": 9.562043795620438,
      "grad_norm": 7.085443496704102,
      "learning_rate": 4.5218978102189783e-05,
      "loss": 4.6747,
      "step": 1310
    },
    {
      "epoch": 9.635036496350365,
      "grad_norm": 7.824183464050293,
      "learning_rate": 4.5182481751824815e-05,
      "loss": 4.8112,
      "step": 1320
    },
    {
      "epoch": 9.708029197080291,
      "grad_norm": 7.894230365753174,
      "learning_rate": 4.5145985401459854e-05,
      "loss": 4.4895,
      "step": 1330
    },
    {
      "epoch": 9.78102189781022,
      "grad_norm": 7.250264644622803,
      "learning_rate": 4.510948905109489e-05,
      "loss": 4.7185,
      "step": 1340
    },
    {
      "epoch": 9.854014598540147,
      "grad_norm": 7.2348527908325195,
      "learning_rate": 4.5072992700729925e-05,
      "loss": 4.4609,
      "step": 1350
    },
    {
      "epoch": 9.927007299270073,
      "grad_norm": 6.989708423614502,
      "learning_rate": 4.5036496350364964e-05,
      "loss": 4.399,
      "step": 1360
    },
    {
      "epoch": 10.0,
      "grad_norm": 10.464853286743164,
      "learning_rate": 4.5e-05,
      "loss": 4.3321,
      "step": 1370
    },
    {
      "epoch": 10.072992700729927,
      "grad_norm": 7.310829162597656,
      "learning_rate": 4.4963503649635035e-05,
      "loss": 4.5987,
      "step": 1380
    },
    {
      "epoch": 10.145985401459853,
      "grad_norm": 7.2755279541015625,
      "learning_rate": 4.4927007299270074e-05,
      "loss": 4.5378,
      "step": 1390
    },
    {
      "epoch": 10.218978102189782,
      "grad_norm": 7.422994613647461,
      "learning_rate": 4.489051094890511e-05,
      "loss": 4.5434,
      "step": 1400
    },
    {
      "epoch": 10.291970802919709,
      "grad_norm": 7.856685638427734,
      "learning_rate": 4.485401459854015e-05,
      "loss": 4.6665,
      "step": 1410
    },
    {
      "epoch": 10.364963503649635,
      "grad_norm": 7.4416823387146,
      "learning_rate": 4.4817518248175183e-05,
      "loss": 4.4449,
      "step": 1420
    },
    {
      "epoch": 10.437956204379562,
      "grad_norm": 7.364376068115234,
      "learning_rate": 4.478102189781022e-05,
      "loss": 4.3793,
      "step": 1430
    },
    {
      "epoch": 10.510948905109489,
      "grad_norm": 7.556809902191162,
      "learning_rate": 4.474452554744526e-05,
      "loss": 4.647,
      "step": 1440
    },
    {
      "epoch": 10.583941605839415,
      "grad_norm": 7.2317118644714355,
      "learning_rate": 4.470802919708029e-05,
      "loss": 4.838,
      "step": 1450
    },
    {
      "epoch": 10.656934306569344,
      "grad_norm": 7.103042125701904,
      "learning_rate": 4.467153284671533e-05,
      "loss": 4.6064,
      "step": 1460
    },
    {
      "epoch": 10.72992700729927,
      "grad_norm": 7.257227420806885,
      "learning_rate": 4.463503649635037e-05,
      "loss": 4.5356,
      "step": 1470
    },
    {
      "epoch": 10.802919708029197,
      "grad_norm": 7.26214075088501,
      "learning_rate": 4.45985401459854e-05,
      "loss": 4.5477,
      "step": 1480
    },
    {
      "epoch": 10.875912408759124,
      "grad_norm": 7.2449140548706055,
      "learning_rate": 4.456204379562044e-05,
      "loss": 4.5563,
      "step": 1490
    },
    {
      "epoch": 10.94890510948905,
      "grad_norm": 7.191660404205322,
      "learning_rate": 4.452554744525548e-05,
      "loss": 4.5399,
      "step": 1500
    },
    {
      "epoch": 11.021897810218977,
      "grad_norm": 7.529964447021484,
      "learning_rate": 4.448905109489051e-05,
      "loss": 4.524,
      "step": 1510
    },
    {
      "epoch": 11.094890510948906,
      "grad_norm": 7.6756978034973145,
      "learning_rate": 4.445255474452555e-05,
      "loss": 4.3489,
      "step": 1520
    },
    {
      "epoch": 11.167883211678832,
      "grad_norm": 7.3582763671875,
      "learning_rate": 4.441605839416059e-05,
      "loss": 4.4108,
      "step": 1530
    },
    {
      "epoch": 11.24087591240876,
      "grad_norm": 7.349076271057129,
      "learning_rate": 4.437956204379562e-05,
      "loss": 4.6237,
      "step": 1540
    },
    {
      "epoch": 11.313868613138686,
      "grad_norm": 6.991945266723633,
      "learning_rate": 4.434306569343066e-05,
      "loss": 4.5225,
      "step": 1550
    },
    {
      "epoch": 11.386861313868613,
      "grad_norm": 7.250125885009766,
      "learning_rate": 4.430656934306569e-05,
      "loss": 4.3941,
      "step": 1560
    },
    {
      "epoch": 11.459854014598541,
      "grad_norm": 7.7934956550598145,
      "learning_rate": 4.427007299270073e-05,
      "loss": 4.8913,
      "step": 1570
    },
    {
      "epoch": 11.532846715328468,
      "grad_norm": 7.203270435333252,
      "learning_rate": 4.4233576642335764e-05,
      "loss": 4.5654,
      "step": 1580
    },
    {
      "epoch": 11.605839416058394,
      "grad_norm": 7.530772686004639,
      "learning_rate": 4.41970802919708e-05,
      "loss": 4.4447,
      "step": 1590
    },
    {
      "epoch": 11.678832116788321,
      "grad_norm": 7.772860050201416,
      "learning_rate": 4.416058394160584e-05,
      "loss": 4.292,
      "step": 1600
    },
    {
      "epoch": 11.751824817518248,
      "grad_norm": 7.260403633117676,
      "learning_rate": 4.4124087591240874e-05,
      "loss": 4.9651,
      "step": 1610
    },
    {
      "epoch": 11.824817518248175,
      "grad_norm": 7.164624214172363,
      "learning_rate": 4.408759124087591e-05,
      "loss": 4.4181,
      "step": 1620
    },
    {
      "epoch": 11.897810218978103,
      "grad_norm": 6.70605993270874,
      "learning_rate": 4.405109489051095e-05,
      "loss": 4.6125,
      "step": 1630
    },
    {
      "epoch": 11.97080291970803,
      "grad_norm": 7.259494781494141,
      "learning_rate": 4.401459854014598e-05,
      "loss": 4.758,
      "step": 1640
    },
    {
      "epoch": 12.043795620437956,
      "grad_norm": 7.26070499420166,
      "learning_rate": 4.397810218978102e-05,
      "loss": 4.4818,
      "step": 1650
    },
    {
      "epoch": 12.116788321167883,
      "grad_norm": 7.01319694519043,
      "learning_rate": 4.394160583941606e-05,
      "loss": 4.6566,
      "step": 1660
    },
    {
      "epoch": 12.18978102189781,
      "grad_norm": 7.680394172668457,
      "learning_rate": 4.39051094890511e-05,
      "loss": 4.3798,
      "step": 1670
    },
    {
      "epoch": 12.262773722627736,
      "grad_norm": 7.195776462554932,
      "learning_rate": 4.386861313868613e-05,
      "loss": 4.3918,
      "step": 1680
    },
    {
      "epoch": 12.335766423357665,
      "grad_norm": 7.085999965667725,
      "learning_rate": 4.383211678832117e-05,
      "loss": 4.6246,
      "step": 1690
    },
    {
      "epoch": 12.408759124087592,
      "grad_norm": 6.96433687210083,
      "learning_rate": 4.379562043795621e-05,
      "loss": 4.7168,
      "step": 1700
    },
    {
      "epoch": 12.481751824817518,
      "grad_norm": 7.439911365509033,
      "learning_rate": 4.375912408759124e-05,
      "loss": 4.7487,
      "step": 1710
    },
    {
      "epoch": 12.554744525547445,
      "grad_norm": 7.977264404296875,
      "learning_rate": 4.372262773722628e-05,
      "loss": 4.3916,
      "step": 1720
    },
    {
      "epoch": 12.627737226277372,
      "grad_norm": 7.261090278625488,
      "learning_rate": 4.368613138686132e-05,
      "loss": 4.5472,
      "step": 1730
    },
    {
      "epoch": 12.700729927007298,
      "grad_norm": 7.57049036026001,
      "learning_rate": 4.364963503649635e-05,
      "loss": 4.5743,
      "step": 1740
    },
    {
      "epoch": 12.773722627737227,
      "grad_norm": 7.823626518249512,
      "learning_rate": 4.361313868613139e-05,
      "loss": 4.1956,
      "step": 1750
    },
    {
      "epoch": 12.846715328467154,
      "grad_norm": 7.61663293838501,
      "learning_rate": 4.357664233576643e-05,
      "loss": 4.3935,
      "step": 1760
    },
    {
      "epoch": 12.91970802919708,
      "grad_norm": 7.415049076080322,
      "learning_rate": 4.354014598540146e-05,
      "loss": 4.5048,
      "step": 1770
    },
    {
      "epoch": 12.992700729927007,
      "grad_norm": 8.108909606933594,
      "learning_rate": 4.35036496350365e-05,
      "loss": 4.2543,
      "step": 1780
    },
    {
      "epoch": 13.065693430656934,
      "grad_norm": 7.288646221160889,
      "learning_rate": 4.346715328467154e-05,
      "loss": 4.6757,
      "step": 1790
    },
    {
      "epoch": 13.138686131386862,
      "grad_norm": 7.261351108551025,
      "learning_rate": 4.343065693430657e-05,
      "loss": 4.3639,
      "step": 1800
    },
    {
      "epoch": 13.211678832116789,
      "grad_norm": 7.442083835601807,
      "learning_rate": 4.339416058394161e-05,
      "loss": 3.9946,
      "step": 1810
    },
    {
      "epoch": 13.284671532846716,
      "grad_norm": 7.846839904785156,
      "learning_rate": 4.335766423357664e-05,
      "loss": 4.4324,
      "step": 1820
    },
    {
      "epoch": 13.357664233576642,
      "grad_norm": 6.9641947746276855,
      "learning_rate": 4.332116788321168e-05,
      "loss": 4.3894,
      "step": 1830
    },
    {
      "epoch": 13.430656934306569,
      "grad_norm": 7.513419151306152,
      "learning_rate": 4.328467153284671e-05,
      "loss": 4.397,
      "step": 1840
    },
    {
      "epoch": 13.503649635036496,
      "grad_norm": 7.79481315612793,
      "learning_rate": 4.324817518248175e-05,
      "loss": 4.7594,
      "step": 1850
    },
    {
      "epoch": 13.576642335766424,
      "grad_norm": 7.361312389373779,
      "learning_rate": 4.321167883211679e-05,
      "loss": 4.3929,
      "step": 1860
    },
    {
      "epoch": 13.64963503649635,
      "grad_norm": 6.801125526428223,
      "learning_rate": 4.317518248175182e-05,
      "loss": 4.5498,
      "step": 1870
    },
    {
      "epoch": 13.722627737226277,
      "grad_norm": 7.313479423522949,
      "learning_rate": 4.313868613138686e-05,
      "loss": 4.7299,
      "step": 1880
    },
    {
      "epoch": 13.795620437956204,
      "grad_norm": 7.551488399505615,
      "learning_rate": 4.31021897810219e-05,
      "loss": 4.2965,
      "step": 1890
    },
    {
      "epoch": 13.86861313868613,
      "grad_norm": 9.725211143493652,
      "learning_rate": 4.306569343065693e-05,
      "loss": 4.4659,
      "step": 1900
    },
    {
      "epoch": 13.941605839416058,
      "grad_norm": 7.652050971984863,
      "learning_rate": 4.302919708029197e-05,
      "loss": 4.3329,
      "step": 1910
    },
    {
      "epoch": 14.014598540145986,
      "grad_norm": 7.764754772186279,
      "learning_rate": 4.299270072992701e-05,
      "loss": 4.6037,
      "step": 1920
    },
    {
      "epoch": 14.087591240875913,
      "grad_norm": 7.526381492614746,
      "learning_rate": 4.295620437956205e-05,
      "loss": 4.3407,
      "step": 1930
    },
    {
      "epoch": 14.16058394160584,
      "grad_norm": 7.268218994140625,
      "learning_rate": 4.291970802919708e-05,
      "loss": 4.0624,
      "step": 1940
    },
    {
      "epoch": 14.233576642335766,
      "grad_norm": 8.054327964782715,
      "learning_rate": 4.288321167883212e-05,
      "loss": 4.2033,
      "step": 1950
    },
    {
      "epoch": 14.306569343065693,
      "grad_norm": 7.432152271270752,
      "learning_rate": 4.284671532846716e-05,
      "loss": 4.4888,
      "step": 1960
    },
    {
      "epoch": 14.37956204379562,
      "grad_norm": 7.376285552978516,
      "learning_rate": 4.281021897810219e-05,
      "loss": 4.5066,
      "step": 1970
    },
    {
      "epoch": 14.452554744525548,
      "grad_norm": 7.233417987823486,
      "learning_rate": 4.277372262773723e-05,
      "loss": 4.6286,
      "step": 1980
    },
    {
      "epoch": 14.525547445255475,
      "grad_norm": 7.076282024383545,
      "learning_rate": 4.273722627737227e-05,
      "loss": 4.4367,
      "step": 1990
    },
    {
      "epoch": 14.598540145985401,
      "grad_norm": 8.092348098754883,
      "learning_rate": 4.27007299270073e-05,
      "loss": 4.4456,
      "step": 2000
    },
    {
      "epoch": 14.671532846715328,
      "grad_norm": 7.4702019691467285,
      "learning_rate": 4.266423357664234e-05,
      "loss": 4.4471,
      "step": 2010
    },
    {
      "epoch": 14.744525547445255,
      "grad_norm": 7.812666416168213,
      "learning_rate": 4.262773722627738e-05,
      "loss": 4.3088,
      "step": 2020
    },
    {
      "epoch": 14.817518248175183,
      "grad_norm": 7.216179847717285,
      "learning_rate": 4.259124087591241e-05,
      "loss": 4.5304,
      "step": 2030
    },
    {
      "epoch": 14.89051094890511,
      "grad_norm": 7.633621692657471,
      "learning_rate": 4.255474452554745e-05,
      "loss": 4.699,
      "step": 2040
    },
    {
      "epoch": 14.963503649635037,
      "grad_norm": 7.586386680603027,
      "learning_rate": 4.251824817518249e-05,
      "loss": 4.3399,
      "step": 2050
    },
    {
      "epoch": 15.036496350364963,
      "grad_norm": 7.022762298583984,
      "learning_rate": 4.2481751824817526e-05,
      "loss": 4.2179,
      "step": 2060
    },
    {
      "epoch": 15.10948905109489,
      "grad_norm": 7.225973606109619,
      "learning_rate": 4.244525547445256e-05,
      "loss": 4.3295,
      "step": 2070
    },
    {
      "epoch": 15.182481751824817,
      "grad_norm": 7.093343734741211,
      "learning_rate": 4.24087591240876e-05,
      "loss": 4.8037,
      "step": 2080
    },
    {
      "epoch": 15.255474452554745,
      "grad_norm": 7.159992218017578,
      "learning_rate": 4.237226277372263e-05,
      "loss": 4.56,
      "step": 2090
    },
    {
      "epoch": 15.328467153284672,
      "grad_norm": 7.711690425872803,
      "learning_rate": 4.233576642335767e-05,
      "loss": 4.4384,
      "step": 2100
    },
    {
      "epoch": 15.401459854014599,
      "grad_norm": 7.1989288330078125,
      "learning_rate": 4.22992700729927e-05,
      "loss": 4.1122,
      "step": 2110
    },
    {
      "epoch": 15.474452554744525,
      "grad_norm": 7.417302131652832,
      "learning_rate": 4.226277372262774e-05,
      "loss": 4.4585,
      "step": 2120
    },
    {
      "epoch": 15.547445255474452,
      "grad_norm": 6.938268184661865,
      "learning_rate": 4.222627737226277e-05,
      "loss": 4.0873,
      "step": 2130
    },
    {
      "epoch": 15.62043795620438,
      "grad_norm": 7.805516719818115,
      "learning_rate": 4.218978102189781e-05,
      "loss": 4.2232,
      "step": 2140
    },
    {
      "epoch": 15.693430656934307,
      "grad_norm": 7.493007183074951,
      "learning_rate": 4.215328467153285e-05,
      "loss": 4.6101,
      "step": 2150
    },
    {
      "epoch": 15.766423357664234,
      "grad_norm": 7.360262870788574,
      "learning_rate": 4.211678832116788e-05,
      "loss": 4.2376,
      "step": 2160
    },
    {
      "epoch": 15.83941605839416,
      "grad_norm": 7.086909294128418,
      "learning_rate": 4.208029197080292e-05,
      "loss": 4.5571,
      "step": 2170
    },
    {
      "epoch": 15.912408759124087,
      "grad_norm": 7.604654312133789,
      "learning_rate": 4.204379562043796e-05,
      "loss": 4.5409,
      "step": 2180
    },
    {
      "epoch": 15.985401459854014,
      "grad_norm": 7.462924003601074,
      "learning_rate": 4.2007299270073e-05,
      "loss": 4.2451,
      "step": 2190
    },
    {
      "epoch": 16.05839416058394,
      "grad_norm": 7.8769378662109375,
      "learning_rate": 4.197080291970803e-05,
      "loss": 4.4586,
      "step": 2200
    },
    {
      "epoch": 16.131386861313867,
      "grad_norm": 7.1559295654296875,
      "learning_rate": 4.193430656934307e-05,
      "loss": 4.3818,
      "step": 2210
    },
    {
      "epoch": 16.204379562043794,
      "grad_norm": 7.286435604095459,
      "learning_rate": 4.1897810218978106e-05,
      "loss": 4.4031,
      "step": 2220
    },
    {
      "epoch": 16.277372262773724,
      "grad_norm": 7.4597907066345215,
      "learning_rate": 4.186131386861314e-05,
      "loss": 4.4985,
      "step": 2230
    },
    {
      "epoch": 16.35036496350365,
      "grad_norm": 7.537620544433594,
      "learning_rate": 4.182481751824818e-05,
      "loss": 4.3731,
      "step": 2240
    },
    {
      "epoch": 16.423357664233578,
      "grad_norm": 7.313854217529297,
      "learning_rate": 4.1788321167883216e-05,
      "loss": 4.394,
      "step": 2250
    },
    {
      "epoch": 16.496350364963504,
      "grad_norm": 7.5041279792785645,
      "learning_rate": 4.175182481751825e-05,
      "loss": 4.2188,
      "step": 2260
    },
    {
      "epoch": 16.56934306569343,
      "grad_norm": 7.270535945892334,
      "learning_rate": 4.171532846715329e-05,
      "loss": 4.2057,
      "step": 2270
    },
    {
      "epoch": 16.642335766423358,
      "grad_norm": 7.23726224899292,
      "learning_rate": 4.1678832116788326e-05,
      "loss": 4.5025,
      "step": 2280
    },
    {
      "epoch": 16.715328467153284,
      "grad_norm": 7.175410747528076,
      "learning_rate": 4.164233576642336e-05,
      "loss": 4.3206,
      "step": 2290
    },
    {
      "epoch": 16.78832116788321,
      "grad_norm": 7.950537204742432,
      "learning_rate": 4.16058394160584e-05,
      "loss": 4.3473,
      "step": 2300
    },
    {
      "epoch": 16.861313868613138,
      "grad_norm": 7.241901874542236,
      "learning_rate": 4.1569343065693435e-05,
      "loss": 4.2958,
      "step": 2310
    },
    {
      "epoch": 16.934306569343065,
      "grad_norm": 7.9615654945373535,
      "learning_rate": 4.1532846715328474e-05,
      "loss": 4.3622,
      "step": 2320
    },
    {
      "epoch": 17.00729927007299,
      "grad_norm": 7.738574504852295,
      "learning_rate": 4.1496350364963506e-05,
      "loss": 4.3445,
      "step": 2330
    },
    {
      "epoch": 17.080291970802918,
      "grad_norm": 7.5570878982543945,
      "learning_rate": 4.1459854014598545e-05,
      "loss": 4.6587,
      "step": 2340
    },
    {
      "epoch": 17.153284671532848,
      "grad_norm": 7.326100826263428,
      "learning_rate": 4.1423357664233584e-05,
      "loss": 4.4083,
      "step": 2350
    },
    {
      "epoch": 17.226277372262775,
      "grad_norm": 8.460428237915039,
      "learning_rate": 4.1386861313868616e-05,
      "loss": 4.5955,
      "step": 2360
    },
    {
      "epoch": 17.2992700729927,
      "grad_norm": 7.17843770980835,
      "learning_rate": 4.1350364963503655e-05,
      "loss": 4.3555,
      "step": 2370
    },
    {
      "epoch": 17.37226277372263,
      "grad_norm": 7.384814262390137,
      "learning_rate": 4.131386861313869e-05,
      "loss": 4.2388,
      "step": 2380
    },
    {
      "epoch": 17.445255474452555,
      "grad_norm": 7.726002216339111,
      "learning_rate": 4.1277372262773726e-05,
      "loss": 4.4298,
      "step": 2390
    },
    {
      "epoch": 17.51824817518248,
      "grad_norm": 7.780425548553467,
      "learning_rate": 4.124087591240876e-05,
      "loss": 4.2066,
      "step": 2400
    },
    {
      "epoch": 17.59124087591241,
      "grad_norm": 8.101644515991211,
      "learning_rate": 4.1204379562043797e-05,
      "loss": 3.9255,
      "step": 2410
    },
    {
      "epoch": 17.664233576642335,
      "grad_norm": 6.969587326049805,
      "learning_rate": 4.116788321167883e-05,
      "loss": 4.2148,
      "step": 2420
    },
    {
      "epoch": 17.73722627737226,
      "grad_norm": 7.60669469833374,
      "learning_rate": 4.113138686131387e-05,
      "loss": 4.4926,
      "step": 2430
    },
    {
      "epoch": 17.81021897810219,
      "grad_norm": 7.157481670379639,
      "learning_rate": 4.1094890510948906e-05,
      "loss": 4.1486,
      "step": 2440
    },
    {
      "epoch": 17.883211678832115,
      "grad_norm": 7.393614768981934,
      "learning_rate": 4.1058394160583945e-05,
      "loss": 4.1865,
      "step": 2450
    },
    {
      "epoch": 17.956204379562045,
      "grad_norm": 7.5583295822143555,
      "learning_rate": 4.102189781021898e-05,
      "loss": 4.2467,
      "step": 2460
    },
    {
      "epoch": 18.029197080291972,
      "grad_norm": 7.452938556671143,
      "learning_rate": 4.0985401459854016e-05,
      "loss": 4.227,
      "step": 2470
    },
    {
      "epoch": 18.1021897810219,
      "grad_norm": 7.719915390014648,
      "learning_rate": 4.0948905109489055e-05,
      "loss": 4.149,
      "step": 2480
    },
    {
      "epoch": 18.175182481751825,
      "grad_norm": 7.674315452575684,
      "learning_rate": 4.091240875912409e-05,
      "loss": 4.316,
      "step": 2490
    },
    {
      "epoch": 18.248175182481752,
      "grad_norm": 7.998297214508057,
      "learning_rate": 4.0875912408759126e-05,
      "loss": 4.3372,
      "step": 2500
    },
    {
      "epoch": 18.32116788321168,
      "grad_norm": 7.563445568084717,
      "learning_rate": 4.0839416058394165e-05,
      "loss": 4.3411,
      "step": 2510
    },
    {
      "epoch": 18.394160583941606,
      "grad_norm": 7.547264575958252,
      "learning_rate": 4.0802919708029197e-05,
      "loss": 4.2749,
      "step": 2520
    },
    {
      "epoch": 18.467153284671532,
      "grad_norm": 7.9131855964660645,
      "learning_rate": 4.0766423357664235e-05,
      "loss": 4.407,
      "step": 2530
    },
    {
      "epoch": 18.54014598540146,
      "grad_norm": 7.174320697784424,
      "learning_rate": 4.0729927007299274e-05,
      "loss": 4.2761,
      "step": 2540
    },
    {
      "epoch": 18.613138686131386,
      "grad_norm": 8.034601211547852,
      "learning_rate": 4.0693430656934306e-05,
      "loss": 3.8582,
      "step": 2550
    },
    {
      "epoch": 18.686131386861312,
      "grad_norm": 7.596627235412598,
      "learning_rate": 4.0656934306569345e-05,
      "loss": 4.4672,
      "step": 2560
    },
    {
      "epoch": 18.75912408759124,
      "grad_norm": 7.250663757324219,
      "learning_rate": 4.0620437956204384e-05,
      "loss": 4.2758,
      "step": 2570
    },
    {
      "epoch": 18.83211678832117,
      "grad_norm": 7.529238224029541,
      "learning_rate": 4.058394160583942e-05,
      "loss": 4.3824,
      "step": 2580
    },
    {
      "epoch": 18.905109489051096,
      "grad_norm": 8.034608840942383,
      "learning_rate": 4.0547445255474455e-05,
      "loss": 4.6809,
      "step": 2590
    },
    {
      "epoch": 18.978102189781023,
      "grad_norm": 8.214339256286621,
      "learning_rate": 4.0510948905109494e-05,
      "loss": 4.5324,
      "step": 2600
    },
    {
      "epoch": 19.05109489051095,
      "grad_norm": 7.5203142166137695,
      "learning_rate": 4.047445255474453e-05,
      "loss": 4.2856,
      "step": 2610
    },
    {
      "epoch": 19.124087591240876,
      "grad_norm": 7.45388650894165,
      "learning_rate": 4.0437956204379564e-05,
      "loss": 4.0553,
      "step": 2620
    },
    {
      "epoch": 19.197080291970803,
      "grad_norm": 7.310797691345215,
      "learning_rate": 4.04014598540146e-05,
      "loss": 4.1203,
      "step": 2630
    },
    {
      "epoch": 19.27007299270073,
      "grad_norm": 6.86056661605835,
      "learning_rate": 4.0364963503649635e-05,
      "loss": 4.3283,
      "step": 2640
    },
    {
      "epoch": 19.343065693430656,
      "grad_norm": 7.234269618988037,
      "learning_rate": 4.0328467153284674e-05,
      "loss": 4.1307,
      "step": 2650
    },
    {
      "epoch": 19.416058394160583,
      "grad_norm": 7.558627128601074,
      "learning_rate": 4.0291970802919706e-05,
      "loss": 4.2946,
      "step": 2660
    },
    {
      "epoch": 19.48905109489051,
      "grad_norm": 8.117321014404297,
      "learning_rate": 4.0255474452554745e-05,
      "loss": 4.26,
      "step": 2670
    },
    {
      "epoch": 19.562043795620436,
      "grad_norm": 7.537919998168945,
      "learning_rate": 4.021897810218978e-05,
      "loss": 4.4547,
      "step": 2680
    },
    {
      "epoch": 19.635036496350367,
      "grad_norm": 7.53786039352417,
      "learning_rate": 4.0182481751824816e-05,
      "loss": 4.3944,
      "step": 2690
    },
    {
      "epoch": 19.708029197080293,
      "grad_norm": 8.20319652557373,
      "learning_rate": 4.0145985401459855e-05,
      "loss": 4.1339,
      "step": 2700
    },
    {
      "epoch": 19.78102189781022,
      "grad_norm": 7.572613716125488,
      "learning_rate": 4.0109489051094894e-05,
      "loss": 4.2864,
      "step": 2710
    },
    {
      "epoch": 19.854014598540147,
      "grad_norm": 7.745187282562256,
      "learning_rate": 4.0072992700729926e-05,
      "loss": 4.5703,
      "step": 2720
    },
    {
      "epoch": 19.927007299270073,
      "grad_norm": 7.486904144287109,
      "learning_rate": 4.0036496350364964e-05,
      "loss": 4.3011,
      "step": 2730
    },
    {
      "epoch": 20.0,
      "grad_norm": 11.11218547821045,
      "learning_rate": 4e-05,
      "loss": 4.0209,
      "step": 2740
    },
    {
      "epoch": 20.072992700729927,
      "grad_norm": 7.551990032196045,
      "learning_rate": 3.9963503649635035e-05,
      "loss": 4.1448,
      "step": 2750
    },
    {
      "epoch": 20.145985401459853,
      "grad_norm": 7.346619606018066,
      "learning_rate": 3.9927007299270074e-05,
      "loss": 4.3087,
      "step": 2760
    },
    {
      "epoch": 20.21897810218978,
      "grad_norm": 6.9197845458984375,
      "learning_rate": 3.989051094890511e-05,
      "loss": 4.005,
      "step": 2770
    },
    {
      "epoch": 20.291970802919707,
      "grad_norm": 7.388871669769287,
      "learning_rate": 3.9854014598540145e-05,
      "loss": 4.4384,
      "step": 2780
    },
    {
      "epoch": 20.364963503649633,
      "grad_norm": 9.679108619689941,
      "learning_rate": 3.9817518248175184e-05,
      "loss": 4.1351,
      "step": 2790
    },
    {
      "epoch": 20.437956204379564,
      "grad_norm": 7.39645528793335,
      "learning_rate": 3.978102189781022e-05,
      "loss": 4.2912,
      "step": 2800
    },
    {
      "epoch": 20.51094890510949,
      "grad_norm": 7.593797206878662,
      "learning_rate": 3.9744525547445255e-05,
      "loss": 4.3728,
      "step": 2810
    },
    {
      "epoch": 20.583941605839417,
      "grad_norm": 7.9932475090026855,
      "learning_rate": 3.9708029197080294e-05,
      "loss": 4.1897,
      "step": 2820
    },
    {
      "epoch": 20.656934306569344,
      "grad_norm": 7.487100601196289,
      "learning_rate": 3.967153284671533e-05,
      "loss": 4.182,
      "step": 2830
    },
    {
      "epoch": 20.72992700729927,
      "grad_norm": 8.333235740661621,
      "learning_rate": 3.963503649635037e-05,
      "loss": 4.2272,
      "step": 2840
    },
    {
      "epoch": 20.802919708029197,
      "grad_norm": 7.5825581550598145,
      "learning_rate": 3.95985401459854e-05,
      "loss": 4.3415,
      "step": 2850
    },
    {
      "epoch": 20.875912408759124,
      "grad_norm": 7.403556823730469,
      "learning_rate": 3.956204379562044e-05,
      "loss": 4.0693,
      "step": 2860
    },
    {
      "epoch": 20.94890510948905,
      "grad_norm": 7.731931209564209,
      "learning_rate": 3.952554744525548e-05,
      "loss": 4.1549,
      "step": 2870
    },
    {
      "epoch": 21.021897810218977,
      "grad_norm": 8.681455612182617,
      "learning_rate": 3.948905109489051e-05,
      "loss": 4.0778,
      "step": 2880
    },
    {
      "epoch": 21.094890510948904,
      "grad_norm": 8.042197227478027,
      "learning_rate": 3.945255474452555e-05,
      "loss": 4.3324,
      "step": 2890
    },
    {
      "epoch": 21.16788321167883,
      "grad_norm": 7.7034101486206055,
      "learning_rate": 3.941605839416059e-05,
      "loss": 4.4273,
      "step": 2900
    },
    {
      "epoch": 21.240875912408757,
      "grad_norm": 7.754088401794434,
      "learning_rate": 3.937956204379562e-05,
      "loss": 4.1304,
      "step": 2910
    },
    {
      "epoch": 21.313868613138688,
      "grad_norm": 7.919557571411133,
      "learning_rate": 3.934306569343066e-05,
      "loss": 4.2709,
      "step": 2920
    },
    {
      "epoch": 21.386861313868614,
      "grad_norm": 7.455059051513672,
      "learning_rate": 3.9306569343065693e-05,
      "loss": 4.0686,
      "step": 2930
    },
    {
      "epoch": 21.45985401459854,
      "grad_norm": 7.255256652832031,
      "learning_rate": 3.927007299270073e-05,
      "loss": 4.1995,
      "step": 2940
    },
    {
      "epoch": 21.532846715328468,
      "grad_norm": 7.602331161499023,
      "learning_rate": 3.9233576642335764e-05,
      "loss": 3.9462,
      "step": 2950
    },
    {
      "epoch": 21.605839416058394,
      "grad_norm": 7.935039520263672,
      "learning_rate": 3.91970802919708e-05,
      "loss": 4.0654,
      "step": 2960
    },
    {
      "epoch": 21.67883211678832,
      "grad_norm": 7.593833923339844,
      "learning_rate": 3.916058394160584e-05,
      "loss": 4.0951,
      "step": 2970
    },
    {
      "epoch": 21.751824817518248,
      "grad_norm": 7.514558792114258,
      "learning_rate": 3.9124087591240874e-05,
      "loss": 4.2825,
      "step": 2980
    },
    {
      "epoch": 21.824817518248175,
      "grad_norm": 7.4878010749816895,
      "learning_rate": 3.908759124087591e-05,
      "loss": 4.2419,
      "step": 2990
    },
    {
      "epoch": 21.8978102189781,
      "grad_norm": 7.380342483520508,
      "learning_rate": 3.905109489051095e-05,
      "loss": 4.1119,
      "step": 3000
    },
    {
      "epoch": 21.970802919708028,
      "grad_norm": 7.786533832550049,
      "learning_rate": 3.9014598540145984e-05,
      "loss": 4.5198,
      "step": 3010
    },
    {
      "epoch": 22.043795620437955,
      "grad_norm": 8.042341232299805,
      "learning_rate": 3.897810218978102e-05,
      "loss": 4.1129,
      "step": 3020
    },
    {
      "epoch": 22.116788321167885,
      "grad_norm": 7.369198799133301,
      "learning_rate": 3.894160583941606e-05,
      "loss": 4.3393,
      "step": 3030
    },
    {
      "epoch": 22.18978102189781,
      "grad_norm": 8.070775032043457,
      "learning_rate": 3.8905109489051093e-05,
      "loss": 4.259,
      "step": 3040
    },
    {
      "epoch": 22.26277372262774,
      "grad_norm": 7.742856025695801,
      "learning_rate": 3.886861313868613e-05,
      "loss": 4.3003,
      "step": 3050
    },
    {
      "epoch": 22.335766423357665,
      "grad_norm": 7.188717365264893,
      "learning_rate": 3.883211678832117e-05,
      "loss": 4.0759,
      "step": 3060
    },
    {
      "epoch": 22.40875912408759,
      "grad_norm": 8.032732963562012,
      "learning_rate": 3.87956204379562e-05,
      "loss": 4.1076,
      "step": 3070
    },
    {
      "epoch": 22.48175182481752,
      "grad_norm": 7.025274276733398,
      "learning_rate": 3.875912408759124e-05,
      "loss": 3.998,
      "step": 3080
    },
    {
      "epoch": 22.554744525547445,
      "grad_norm": 7.593867301940918,
      "learning_rate": 3.872262773722628e-05,
      "loss": 4.151,
      "step": 3090
    },
    {
      "epoch": 22.62773722627737,
      "grad_norm": 7.419483184814453,
      "learning_rate": 3.868613138686132e-05,
      "loss": 4.1274,
      "step": 3100
    },
    {
      "epoch": 22.7007299270073,
      "grad_norm": 7.660379886627197,
      "learning_rate": 3.864963503649635e-05,
      "loss": 4.039,
      "step": 3110
    },
    {
      "epoch": 22.773722627737225,
      "grad_norm": 6.884975433349609,
      "learning_rate": 3.861313868613139e-05,
      "loss": 3.991,
      "step": 3120
    },
    {
      "epoch": 22.846715328467152,
      "grad_norm": 7.8953447341918945,
      "learning_rate": 3.857664233576643e-05,
      "loss": 4.1074,
      "step": 3130
    },
    {
      "epoch": 22.919708029197082,
      "grad_norm": 7.7838664054870605,
      "learning_rate": 3.854014598540146e-05,
      "loss": 4.518,
      "step": 3140
    },
    {
      "epoch": 22.99270072992701,
      "grad_norm": 9.059623718261719,
      "learning_rate": 3.85036496350365e-05,
      "loss": 4.195,
      "step": 3150
    },
    {
      "epoch": 23.065693430656935,
      "grad_norm": 7.084708213806152,
      "learning_rate": 3.846715328467154e-05,
      "loss": 4.0657,
      "step": 3160
    },
    {
      "epoch": 23.138686131386862,
      "grad_norm": 7.55547571182251,
      "learning_rate": 3.843065693430657e-05,
      "loss": 4.268,
      "step": 3170
    },
    {
      "epoch": 23.21167883211679,
      "grad_norm": 7.19460916519165,
      "learning_rate": 3.839416058394161e-05,
      "loss": 3.973,
      "step": 3180
    },
    {
      "epoch": 23.284671532846716,
      "grad_norm": 7.930928707122803,
      "learning_rate": 3.835766423357665e-05,
      "loss": 4.04,
      "step": 3190
    },
    {
      "epoch": 23.357664233576642,
      "grad_norm": 7.645136833190918,
      "learning_rate": 3.832116788321168e-05,
      "loss": 3.9652,
      "step": 3200
    },
    {
      "epoch": 23.43065693430657,
      "grad_norm": 7.544434547424316,
      "learning_rate": 3.828467153284672e-05,
      "loss": 4.1298,
      "step": 3210
    },
    {
      "epoch": 23.503649635036496,
      "grad_norm": 7.828090190887451,
      "learning_rate": 3.824817518248175e-05,
      "loss": 4.053,
      "step": 3220
    },
    {
      "epoch": 23.576642335766422,
      "grad_norm": 7.604361534118652,
      "learning_rate": 3.821167883211679e-05,
      "loss": 4.189,
      "step": 3230
    },
    {
      "epoch": 23.64963503649635,
      "grad_norm": 7.655752658843994,
      "learning_rate": 3.817518248175182e-05,
      "loss": 4.0165,
      "step": 3240
    },
    {
      "epoch": 23.722627737226276,
      "grad_norm": 7.9536237716674805,
      "learning_rate": 3.813868613138686e-05,
      "loss": 3.9773,
      "step": 3250
    },
    {
      "epoch": 23.795620437956206,
      "grad_norm": 8.318456649780273,
      "learning_rate": 3.81021897810219e-05,
      "loss": 4.276,
      "step": 3260
    },
    {
      "epoch": 23.868613138686133,
      "grad_norm": 7.841681003570557,
      "learning_rate": 3.806569343065693e-05,
      "loss": 4.4101,
      "step": 3270
    },
    {
      "epoch": 23.94160583941606,
      "grad_norm": 7.692175388336182,
      "learning_rate": 3.802919708029197e-05,
      "loss": 4.4026,
      "step": 3280
    },
    {
      "epoch": 24.014598540145986,
      "grad_norm": 7.60285758972168,
      "learning_rate": 3.799270072992701e-05,
      "loss": 4.1494,
      "step": 3290
    },
    {
      "epoch": 24.087591240875913,
      "grad_norm": 7.786563873291016,
      "learning_rate": 3.795620437956204e-05,
      "loss": 4.1798,
      "step": 3300
    },
    {
      "epoch": 24.16058394160584,
      "grad_norm": 7.23672342300415,
      "learning_rate": 3.791970802919708e-05,
      "loss": 4.2929,
      "step": 3310
    },
    {
      "epoch": 24.233576642335766,
      "grad_norm": 7.5764851570129395,
      "learning_rate": 3.788321167883212e-05,
      "loss": 4.1133,
      "step": 3320
    },
    {
      "epoch": 24.306569343065693,
      "grad_norm": 8.210042953491211,
      "learning_rate": 3.784671532846715e-05,
      "loss": 4.0674,
      "step": 3330
    },
    {
      "epoch": 24.37956204379562,
      "grad_norm": 7.940158367156982,
      "learning_rate": 3.781021897810219e-05,
      "loss": 4.3103,
      "step": 3340
    },
    {
      "epoch": 24.452554744525546,
      "grad_norm": 10.049332618713379,
      "learning_rate": 3.777372262773723e-05,
      "loss": 3.8834,
      "step": 3350
    },
    {
      "epoch": 24.525547445255473,
      "grad_norm": 7.512208461761475,
      "learning_rate": 3.773722627737227e-05,
      "loss": 4.3153,
      "step": 3360
    },
    {
      "epoch": 24.598540145985403,
      "grad_norm": 7.568037509918213,
      "learning_rate": 3.77007299270073e-05,
      "loss": 4.1496,
      "step": 3370
    },
    {
      "epoch": 24.67153284671533,
      "grad_norm": 7.547598838806152,
      "learning_rate": 3.766423357664234e-05,
      "loss": 4.0197,
      "step": 3380
    },
    {
      "epoch": 24.744525547445257,
      "grad_norm": 7.391713619232178,
      "learning_rate": 3.762773722627738e-05,
      "loss": 4.186,
      "step": 3390
    },
    {
      "epoch": 24.817518248175183,
      "grad_norm": 7.8366193771362305,
      "learning_rate": 3.759124087591241e-05,
      "loss": 4.0563,
      "step": 3400
    },
    {
      "epoch": 24.89051094890511,
      "grad_norm": 7.7546610832214355,
      "learning_rate": 3.755474452554745e-05,
      "loss": 3.9678,
      "step": 3410
    },
    {
      "epoch": 24.963503649635037,
      "grad_norm": 7.6500244140625,
      "learning_rate": 3.751824817518249e-05,
      "loss": 3.9739,
      "step": 3420
    },
    {
      "epoch": 25.036496350364963,
      "grad_norm": 8.017026901245117,
      "learning_rate": 3.748175182481752e-05,
      "loss": 3.6577,
      "step": 3430
    },
    {
      "epoch": 25.10948905109489,
      "grad_norm": 7.502564430236816,
      "learning_rate": 3.744525547445256e-05,
      "loss": 3.9195,
      "step": 3440
    },
    {
      "epoch": 25.182481751824817,
      "grad_norm": 7.8531880378723145,
      "learning_rate": 3.74087591240876e-05,
      "loss": 4.2649,
      "step": 3450
    },
    {
      "epoch": 25.255474452554743,
      "grad_norm": 7.034709453582764,
      "learning_rate": 3.737226277372263e-05,
      "loss": 4.1807,
      "step": 3460
    },
    {
      "epoch": 25.32846715328467,
      "grad_norm": 7.740236282348633,
      "learning_rate": 3.733576642335767e-05,
      "loss": 3.8603,
      "step": 3470
    },
    {
      "epoch": 25.401459854014597,
      "grad_norm": 7.93159818649292,
      "learning_rate": 3.729927007299271e-05,
      "loss": 4.0588,
      "step": 3480
    },
    {
      "epoch": 25.474452554744527,
      "grad_norm": 7.854516506195068,
      "learning_rate": 3.726277372262774e-05,
      "loss": 4.0055,
      "step": 3490
    },
    {
      "epoch": 25.547445255474454,
      "grad_norm": 7.785900115966797,
      "learning_rate": 3.722627737226278e-05,
      "loss": 4.158,
      "step": 3500
    },
    {
      "epoch": 25.62043795620438,
      "grad_norm": 7.307540416717529,
      "learning_rate": 3.718978102189781e-05,
      "loss": 4.0055,
      "step": 3510
    },
    {
      "epoch": 25.693430656934307,
      "grad_norm": 7.969390869140625,
      "learning_rate": 3.715328467153285e-05,
      "loss": 4.2507,
      "step": 3520
    },
    {
      "epoch": 25.766423357664234,
      "grad_norm": 7.5568060874938965,
      "learning_rate": 3.711678832116788e-05,
      "loss": 4.1142,
      "step": 3530
    },
    {
      "epoch": 25.83941605839416,
      "grad_norm": 7.7367377281188965,
      "learning_rate": 3.708029197080292e-05,
      "loss": 3.8729,
      "step": 3540
    },
    {
      "epoch": 25.912408759124087,
      "grad_norm": 7.834653854370117,
      "learning_rate": 3.704379562043796e-05,
      "loss": 4.085,
      "step": 3550
    },
    {
      "epoch": 25.985401459854014,
      "grad_norm": 7.7311930656433105,
      "learning_rate": 3.700729927007299e-05,
      "loss": 4.2244,
      "step": 3560
    },
    {
      "epoch": 26.05839416058394,
      "grad_norm": 7.556469917297363,
      "learning_rate": 3.697080291970803e-05,
      "loss": 4.2897,
      "step": 3570
    },
    {
      "epoch": 26.131386861313867,
      "grad_norm": 7.892083168029785,
      "learning_rate": 3.693430656934307e-05,
      "loss": 3.9427,
      "step": 3580
    },
    {
      "epoch": 26.204379562043794,
      "grad_norm": 7.148812770843506,
      "learning_rate": 3.68978102189781e-05,
      "loss": 3.808,
      "step": 3590
    },
    {
      "epoch": 26.277372262773724,
      "grad_norm": 7.233242511749268,
      "learning_rate": 3.686131386861314e-05,
      "loss": 3.9605,
      "step": 3600
    },
    {
      "epoch": 26.35036496350365,
      "grad_norm": 7.753896236419678,
      "learning_rate": 3.682481751824818e-05,
      "loss": 3.7145,
      "step": 3610
    },
    {
      "epoch": 26.423357664233578,
      "grad_norm": 8.3526611328125,
      "learning_rate": 3.678832116788321e-05,
      "loss": 4.1862,
      "step": 3620
    },
    {
      "epoch": 26.496350364963504,
      "grad_norm": 6.838152885437012,
      "learning_rate": 3.675182481751825e-05,
      "loss": 4.086,
      "step": 3630
    },
    {
      "epoch": 26.56934306569343,
      "grad_norm": 7.723752498626709,
      "learning_rate": 3.671532846715329e-05,
      "loss": 4.1584,
      "step": 3640
    },
    {
      "epoch": 26.642335766423358,
      "grad_norm": 7.508020401000977,
      "learning_rate": 3.6678832116788326e-05,
      "loss": 4.3703,
      "step": 3650
    },
    {
      "epoch": 26.715328467153284,
      "grad_norm": 7.525270462036133,
      "learning_rate": 3.664233576642336e-05,
      "loss": 3.9723,
      "step": 3660
    },
    {
      "epoch": 26.78832116788321,
      "grad_norm": 7.956385135650635,
      "learning_rate": 3.66058394160584e-05,
      "loss": 3.9469,
      "step": 3670
    },
    {
      "epoch": 26.861313868613138,
      "grad_norm": 7.642937183380127,
      "learning_rate": 3.6569343065693436e-05,
      "loss": 3.9887,
      "step": 3680
    },
    {
      "epoch": 26.934306569343065,
      "grad_norm": 7.866372108459473,
      "learning_rate": 3.653284671532847e-05,
      "loss": 4.1365,
      "step": 3690
    },
    {
      "epoch": 27.00729927007299,
      "grad_norm": 7.601169586181641,
      "learning_rate": 3.649635036496351e-05,
      "loss": 3.9982,
      "step": 3700
    },
    {
      "epoch": 27.080291970802918,
      "grad_norm": 7.944977760314941,
      "learning_rate": 3.6459854014598546e-05,
      "loss": 3.8376,
      "step": 3710
    },
    {
      "epoch": 27.153284671532848,
      "grad_norm": 7.343291759490967,
      "learning_rate": 3.642335766423358e-05,
      "loss": 3.8955,
      "step": 3720
    },
    {
      "epoch": 27.226277372262775,
      "grad_norm": 7.791492462158203,
      "learning_rate": 3.6386861313868616e-05,
      "loss": 3.8144,
      "step": 3730
    },
    {
      "epoch": 27.2992700729927,
      "grad_norm": 7.751132965087891,
      "learning_rate": 3.6350364963503655e-05,
      "loss": 4.3278,
      "step": 3740
    },
    {
      "epoch": 27.37226277372263,
      "grad_norm": 7.914296627044678,
      "learning_rate": 3.631386861313869e-05,
      "loss": 3.9196,
      "step": 3750
    },
    {
      "epoch": 27.445255474452555,
      "grad_norm": 7.488288402557373,
      "learning_rate": 3.6277372262773726e-05,
      "loss": 4.1104,
      "step": 3760
    },
    {
      "epoch": 27.51824817518248,
      "grad_norm": 7.332892417907715,
      "learning_rate": 3.624087591240876e-05,
      "loss": 4.1139,
      "step": 3770
    },
    {
      "epoch": 27.59124087591241,
      "grad_norm": 8.291781425476074,
      "learning_rate": 3.62043795620438e-05,
      "loss": 4.1474,
      "step": 3780
    },
    {
      "epoch": 27.664233576642335,
      "grad_norm": 7.604449272155762,
      "learning_rate": 3.616788321167883e-05,
      "loss": 4.0795,
      "step": 3790
    },
    {
      "epoch": 27.73722627737226,
      "grad_norm": 7.183543682098389,
      "learning_rate": 3.613138686131387e-05,
      "loss": 3.9886,
      "step": 3800
    },
    {
      "epoch": 27.81021897810219,
      "grad_norm": 7.694664001464844,
      "learning_rate": 3.609489051094891e-05,
      "loss": 4.1555,
      "step": 3810
    },
    {
      "epoch": 27.883211678832115,
      "grad_norm": 8.070929527282715,
      "learning_rate": 3.605839416058394e-05,
      "loss": 4.1047,
      "step": 3820
    },
    {
      "epoch": 27.956204379562045,
      "grad_norm": 7.557915687561035,
      "learning_rate": 3.602189781021898e-05,
      "loss": 3.7481,
      "step": 3830
    },
    {
      "epoch": 28.029197080291972,
      "grad_norm": 7.830638885498047,
      "learning_rate": 3.5985401459854016e-05,
      "loss": 3.7851,
      "step": 3840
    },
    {
      "epoch": 28.1021897810219,
      "grad_norm": 7.729414463043213,
      "learning_rate": 3.594890510948905e-05,
      "loss": 4.2171,
      "step": 3850
    },
    {
      "epoch": 28.175182481751825,
      "grad_norm": 9.448827743530273,
      "learning_rate": 3.591240875912409e-05,
      "loss": 4.1317,
      "step": 3860
    },
    {
      "epoch": 28.248175182481752,
      "grad_norm": 7.081716060638428,
      "learning_rate": 3.5875912408759126e-05,
      "loss": 4.0681,
      "step": 3870
    },
    {
      "epoch": 28.32116788321168,
      "grad_norm": 7.439062595367432,
      "learning_rate": 3.583941605839416e-05,
      "loss": 3.9664,
      "step": 3880
    },
    {
      "epoch": 28.394160583941606,
      "grad_norm": 8.18199634552002,
      "learning_rate": 3.58029197080292e-05,
      "loss": 3.8361,
      "step": 3890
    },
    {
      "epoch": 28.467153284671532,
      "grad_norm": 7.501232624053955,
      "learning_rate": 3.5766423357664236e-05,
      "loss": 4.0773,
      "step": 3900
    },
    {
      "epoch": 28.54014598540146,
      "grad_norm": 8.01221752166748,
      "learning_rate": 3.5729927007299275e-05,
      "loss": 3.9284,
      "step": 3910
    },
    {
      "epoch": 28.613138686131386,
      "grad_norm": 7.7121052742004395,
      "learning_rate": 3.569343065693431e-05,
      "loss": 3.963,
      "step": 3920
    },
    {
      "epoch": 28.686131386861312,
      "grad_norm": 7.534969329833984,
      "learning_rate": 3.5656934306569346e-05,
      "loss": 4.0898,
      "step": 3930
    },
    {
      "epoch": 28.75912408759124,
      "grad_norm": 8.02595043182373,
      "learning_rate": 3.5620437956204384e-05,
      "loss": 4.0536,
      "step": 3940
    },
    {
      "epoch": 28.83211678832117,
      "grad_norm": 8.032930374145508,
      "learning_rate": 3.5583941605839416e-05,
      "loss": 3.569,
      "step": 3950
    },
    {
      "epoch": 28.905109489051096,
      "grad_norm": 7.507706642150879,
      "learning_rate": 3.5547445255474455e-05,
      "loss": 4.2101,
      "step": 3960
    },
    {
      "epoch": 28.978102189781023,
      "grad_norm": 7.8721747398376465,
      "learning_rate": 3.5510948905109494e-05,
      "loss": 3.8173,
      "step": 3970
    },
    {
      "epoch": 29.05109489051095,
      "grad_norm": 8.225110054016113,
      "learning_rate": 3.5474452554744526e-05,
      "loss": 4.3461,
      "step": 3980
    },
    {
      "epoch": 29.124087591240876,
      "grad_norm": 7.691401481628418,
      "learning_rate": 3.5437956204379565e-05,
      "loss": 4.1332,
      "step": 3990
    },
    {
      "epoch": 29.197080291970803,
      "grad_norm": 7.876151084899902,
      "learning_rate": 3.5401459854014604e-05,
      "loss": 3.893,
      "step": 4000
    },
    {
      "epoch": 29.27007299270073,
      "grad_norm": 7.977269649505615,
      "learning_rate": 3.5364963503649636e-05,
      "loss": 3.7208,
      "step": 4010
    },
    {
      "epoch": 29.343065693430656,
      "grad_norm": 8.179547309875488,
      "learning_rate": 3.5328467153284675e-05,
      "loss": 4.2269,
      "step": 4020
    },
    {
      "epoch": 29.416058394160583,
      "grad_norm": 7.151674270629883,
      "learning_rate": 3.5291970802919713e-05,
      "loss": 3.8129,
      "step": 4030
    },
    {
      "epoch": 29.48905109489051,
      "grad_norm": 7.817486763000488,
      "learning_rate": 3.5255474452554745e-05,
      "loss": 3.8435,
      "step": 4040
    },
    {
      "epoch": 29.562043795620436,
      "grad_norm": 7.5643463134765625,
      "learning_rate": 3.5218978102189784e-05,
      "loss": 4.0816,
      "step": 4050
    },
    {
      "epoch": 29.635036496350367,
      "grad_norm": 7.836452007293701,
      "learning_rate": 3.5182481751824816e-05,
      "loss": 4.0745,
      "step": 4060
    },
    {
      "epoch": 29.708029197080293,
      "grad_norm": 7.495109558105469,
      "learning_rate": 3.5145985401459855e-05,
      "loss": 3.8783,
      "step": 4070
    },
    {
      "epoch": 29.78102189781022,
      "grad_norm": 7.559866905212402,
      "learning_rate": 3.510948905109489e-05,
      "loss": 4.0362,
      "step": 4080
    },
    {
      "epoch": 29.854014598540147,
      "grad_norm": 8.430127143859863,
      "learning_rate": 3.5072992700729926e-05,
      "loss": 3.9047,
      "step": 4090
    },
    {
      "epoch": 29.927007299270073,
      "grad_norm": 8.356234550476074,
      "learning_rate": 3.5036496350364965e-05,
      "loss": 3.9109,
      "step": 4100
    },
    {
      "epoch": 30.0,
      "grad_norm": 12.857635498046875,
      "learning_rate": 3.5e-05,
      "loss": 3.859,
      "step": 4110
    },
    {
      "epoch": 30.072992700729927,
      "grad_norm": 7.77901554107666,
      "learning_rate": 3.4963503649635036e-05,
      "loss": 3.965,
      "step": 4120
    },
    {
      "epoch": 30.145985401459853,
      "grad_norm": 7.430036544799805,
      "learning_rate": 3.4927007299270075e-05,
      "loss": 3.5962,
      "step": 4130
    },
    {
      "epoch": 30.21897810218978,
      "grad_norm": 8.140474319458008,
      "learning_rate": 3.489051094890511e-05,
      "loss": 3.9456,
      "step": 4140
    },
    {
      "epoch": 30.291970802919707,
      "grad_norm": 6.992396831512451,
      "learning_rate": 3.4854014598540145e-05,
      "loss": 4.011,
      "step": 4150
    },
    {
      "epoch": 30.364963503649633,
      "grad_norm": 8.06061840057373,
      "learning_rate": 3.4817518248175184e-05,
      "loss": 3.8285,
      "step": 4160
    },
    {
      "epoch": 30.437956204379564,
      "grad_norm": 7.69810676574707,
      "learning_rate": 3.478102189781022e-05,
      "loss": 3.8486,
      "step": 4170
    },
    {
      "epoch": 30.51094890510949,
      "grad_norm": 8.153173446655273,
      "learning_rate": 3.4744525547445255e-05,
      "loss": 3.9172,
      "step": 4180
    },
    {
      "epoch": 30.583941605839417,
      "grad_norm": 7.494333267211914,
      "learning_rate": 3.4708029197080294e-05,
      "loss": 4.2688,
      "step": 4190
    },
    {
      "epoch": 30.656934306569344,
      "grad_norm": 8.121801376342773,
      "learning_rate": 3.467153284671533e-05,
      "loss": 3.8045,
      "step": 4200
    },
    {
      "epoch": 30.72992700729927,
      "grad_norm": 6.8805251121521,
      "learning_rate": 3.4635036496350365e-05,
      "loss": 3.7838,
      "step": 4210
    },
    {
      "epoch": 30.802919708029197,
      "grad_norm": 8.64405345916748,
      "learning_rate": 3.4598540145985404e-05,
      "loss": 4.1193,
      "step": 4220
    },
    {
      "epoch": 30.875912408759124,
      "grad_norm": 7.925131320953369,
      "learning_rate": 3.456204379562044e-05,
      "loss": 4.0446,
      "step": 4230
    },
    {
      "epoch": 30.94890510948905,
      "grad_norm": 8.333813667297363,
      "learning_rate": 3.4525547445255475e-05,
      "loss": 3.7797,
      "step": 4240
    },
    {
      "epoch": 31.021897810218977,
      "grad_norm": 8.119563102722168,
      "learning_rate": 3.448905109489051e-05,
      "loss": 3.9425,
      "step": 4250
    },
    {
      "epoch": 31.094890510948904,
      "grad_norm": 8.167462348937988,
      "learning_rate": 3.445255474452555e-05,
      "loss": 3.903,
      "step": 4260
    },
    {
      "epoch": 31.16788321167883,
      "grad_norm": 8.146284103393555,
      "learning_rate": 3.4416058394160584e-05,
      "loss": 3.9484,
      "step": 4270
    },
    {
      "epoch": 31.240875912408757,
      "grad_norm": 8.261472702026367,
      "learning_rate": 3.437956204379562e-05,
      "loss": 3.8328,
      "step": 4280
    },
    {
      "epoch": 31.313868613138688,
      "grad_norm": 8.09416389465332,
      "learning_rate": 3.434306569343066e-05,
      "loss": 4.0221,
      "step": 4290
    },
    {
      "epoch": 31.386861313868614,
      "grad_norm": 6.871715545654297,
      "learning_rate": 3.43065693430657e-05,
      "loss": 3.9451,
      "step": 4300
    },
    {
      "epoch": 31.45985401459854,
      "grad_norm": 7.97794771194458,
      "learning_rate": 3.427007299270073e-05,
      "loss": 4.1197,
      "step": 4310
    },
    {
      "epoch": 31.532846715328468,
      "grad_norm": 7.389154434204102,
      "learning_rate": 3.423357664233577e-05,
      "loss": 3.7254,
      "step": 4320
    },
    {
      "epoch": 31.605839416058394,
      "grad_norm": 7.536012649536133,
      "learning_rate": 3.4197080291970804e-05,
      "loss": 3.6772,
      "step": 4330
    },
    {
      "epoch": 31.67883211678832,
      "grad_norm": 7.937350749969482,
      "learning_rate": 3.416058394160584e-05,
      "loss": 3.8245,
      "step": 4340
    },
    {
      "epoch": 31.751824817518248,
      "grad_norm": 8.363725662231445,
      "learning_rate": 3.4124087591240875e-05,
      "loss": 3.7956,
      "step": 4350
    },
    {
      "epoch": 31.824817518248175,
      "grad_norm": 7.824801921844482,
      "learning_rate": 3.408759124087591e-05,
      "loss": 3.6678,
      "step": 4360
    },
    {
      "epoch": 31.8978102189781,
      "grad_norm": 7.871519088745117,
      "learning_rate": 3.4051094890510945e-05,
      "loss": 4.2608,
      "step": 4370
    },
    {
      "epoch": 31.970802919708028,
      "grad_norm": 8.379526138305664,
      "learning_rate": 3.4014598540145984e-05,
      "loss": 3.9261,
      "step": 4380
    },
    {
      "epoch": 32.043795620437955,
      "grad_norm": 7.600019931793213,
      "learning_rate": 3.397810218978102e-05,
      "loss": 3.9711,
      "step": 4390
    },
    {
      "epoch": 32.11678832116788,
      "grad_norm": 7.765198230743408,
      "learning_rate": 3.3941605839416055e-05,
      "loss": 3.5803,
      "step": 4400
    },
    {
      "epoch": 32.18978102189781,
      "grad_norm": 7.220388412475586,
      "learning_rate": 3.3905109489051094e-05,
      "loss": 3.8729,
      "step": 4410
    },
    {
      "epoch": 32.262773722627735,
      "grad_norm": 8.168052673339844,
      "learning_rate": 3.386861313868613e-05,
      "loss": 4.0139,
      "step": 4420
    },
    {
      "epoch": 32.33576642335766,
      "grad_norm": 8.323444366455078,
      "learning_rate": 3.383211678832117e-05,
      "loss": 3.7997,
      "step": 4430
    },
    {
      "epoch": 32.40875912408759,
      "grad_norm": 8.7293119430542,
      "learning_rate": 3.3795620437956204e-05,
      "loss": 3.924,
      "step": 4440
    },
    {
      "epoch": 32.481751824817515,
      "grad_norm": 8.991660118103027,
      "learning_rate": 3.375912408759124e-05,
      "loss": 3.9201,
      "step": 4450
    },
    {
      "epoch": 32.55474452554745,
      "grad_norm": 6.931682586669922,
      "learning_rate": 3.372262773722628e-05,
      "loss": 3.5391,
      "step": 4460
    },
    {
      "epoch": 32.627737226277375,
      "grad_norm": 7.586480140686035,
      "learning_rate": 3.368613138686131e-05,
      "loss": 4.1689,
      "step": 4470
    },
    {
      "epoch": 32.7007299270073,
      "grad_norm": 7.421200275421143,
      "learning_rate": 3.364963503649635e-05,
      "loss": 3.9429,
      "step": 4480
    },
    {
      "epoch": 32.77372262773723,
      "grad_norm": 8.151211738586426,
      "learning_rate": 3.361313868613139e-05,
      "loss": 4.024,
      "step": 4490
    },
    {
      "epoch": 32.846715328467155,
      "grad_norm": 8.396034240722656,
      "learning_rate": 3.357664233576642e-05,
      "loss": 3.854,
      "step": 4500
    },
    {
      "epoch": 32.91970802919708,
      "grad_norm": 7.573148250579834,
      "learning_rate": 3.354014598540146e-05,
      "loss": 3.9151,
      "step": 4510
    },
    {
      "epoch": 32.99270072992701,
      "grad_norm": 7.952142238616943,
      "learning_rate": 3.35036496350365e-05,
      "loss": 3.9789,
      "step": 4520
    },
    {
      "epoch": 33.065693430656935,
      "grad_norm": 7.960296154022217,
      "learning_rate": 3.346715328467153e-05,
      "loss": 3.5307,
      "step": 4530
    },
    {
      "epoch": 33.13868613138686,
      "grad_norm": 8.208379745483398,
      "learning_rate": 3.343065693430657e-05,
      "loss": 3.6628,
      "step": 4540
    },
    {
      "epoch": 33.21167883211679,
      "grad_norm": 7.789066314697266,
      "learning_rate": 3.339416058394161e-05,
      "loss": 3.9969,
      "step": 4550
    },
    {
      "epoch": 33.284671532846716,
      "grad_norm": 8.078826904296875,
      "learning_rate": 3.335766423357665e-05,
      "loss": 3.8502,
      "step": 4560
    },
    {
      "epoch": 33.35766423357664,
      "grad_norm": 7.143886566162109,
      "learning_rate": 3.332116788321168e-05,
      "loss": 3.9364,
      "step": 4570
    },
    {
      "epoch": 33.43065693430657,
      "grad_norm": 7.774343013763428,
      "learning_rate": 3.328467153284672e-05,
      "loss": 3.6792,
      "step": 4580
    },
    {
      "epoch": 33.503649635036496,
      "grad_norm": 8.25738525390625,
      "learning_rate": 3.324817518248176e-05,
      "loss": 3.8608,
      "step": 4590
    },
    {
      "epoch": 33.57664233576642,
      "grad_norm": 8.103355407714844,
      "learning_rate": 3.321167883211679e-05,
      "loss": 3.6597,
      "step": 4600
    },
    {
      "epoch": 33.64963503649635,
      "grad_norm": 8.013894081115723,
      "learning_rate": 3.317518248175183e-05,
      "loss": 4.114,
      "step": 4610
    },
    {
      "epoch": 33.722627737226276,
      "grad_norm": 8.362420082092285,
      "learning_rate": 3.313868613138686e-05,
      "loss": 3.9071,
      "step": 4620
    },
    {
      "epoch": 33.7956204379562,
      "grad_norm": 7.6587934494018555,
      "learning_rate": 3.31021897810219e-05,
      "loss": 3.6109,
      "step": 4630
    },
    {
      "epoch": 33.86861313868613,
      "grad_norm": 7.089950084686279,
      "learning_rate": 3.306569343065693e-05,
      "loss": 3.7563,
      "step": 4640
    },
    {
      "epoch": 33.941605839416056,
      "grad_norm": 7.648879528045654,
      "learning_rate": 3.302919708029197e-05,
      "loss": 3.8347,
      "step": 4650
    },
    {
      "epoch": 34.01459854014598,
      "grad_norm": 7.814327239990234,
      "learning_rate": 3.2992700729927004e-05,
      "loss": 4.0994,
      "step": 4660
    },
    {
      "epoch": 34.08759124087591,
      "grad_norm": 7.69065523147583,
      "learning_rate": 3.295620437956204e-05,
      "loss": 3.6182,
      "step": 4670
    },
    {
      "epoch": 34.160583941605836,
      "grad_norm": 7.821003437042236,
      "learning_rate": 3.291970802919708e-05,
      "loss": 3.7306,
      "step": 4680
    },
    {
      "epoch": 34.23357664233577,
      "grad_norm": 8.379040718078613,
      "learning_rate": 3.288321167883212e-05,
      "loss": 3.8609,
      "step": 4690
    },
    {
      "epoch": 34.306569343065696,
      "grad_norm": 7.785658359527588,
      "learning_rate": 3.284671532846715e-05,
      "loss": 3.7735,
      "step": 4700
    },
    {
      "epoch": 34.37956204379562,
      "grad_norm": 7.890344619750977,
      "learning_rate": 3.281021897810219e-05,
      "loss": 3.7421,
      "step": 4710
    },
    {
      "epoch": 34.45255474452555,
      "grad_norm": 8.116576194763184,
      "learning_rate": 3.277372262773723e-05,
      "loss": 4.0798,
      "step": 4720
    },
    {
      "epoch": 34.52554744525548,
      "grad_norm": 8.150056838989258,
      "learning_rate": 3.273722627737226e-05,
      "loss": 3.8915,
      "step": 4730
    },
    {
      "epoch": 34.5985401459854,
      "grad_norm": 8.899024963378906,
      "learning_rate": 3.27007299270073e-05,
      "loss": 3.7947,
      "step": 4740
    },
    {
      "epoch": 34.67153284671533,
      "grad_norm": 8.07667350769043,
      "learning_rate": 3.266423357664234e-05,
      "loss": 3.8729,
      "step": 4750
    },
    {
      "epoch": 34.74452554744526,
      "grad_norm": 8.288115501403809,
      "learning_rate": 3.262773722627737e-05,
      "loss": 3.8102,
      "step": 4760
    },
    {
      "epoch": 34.81751824817518,
      "grad_norm": 7.21767520904541,
      "learning_rate": 3.259124087591241e-05,
      "loss": 3.9408,
      "step": 4770
    },
    {
      "epoch": 34.89051094890511,
      "grad_norm": 8.134021759033203,
      "learning_rate": 3.255474452554745e-05,
      "loss": 3.7043,
      "step": 4780
    },
    {
      "epoch": 34.96350364963504,
      "grad_norm": 7.820280075073242,
      "learning_rate": 3.251824817518248e-05,
      "loss": 3.7425,
      "step": 4790
    },
    {
      "epoch": 35.03649635036496,
      "grad_norm": 7.624057769775391,
      "learning_rate": 3.248175182481752e-05,
      "loss": 3.8453,
      "step": 4800
    },
    {
      "epoch": 35.10948905109489,
      "grad_norm": 7.297326564788818,
      "learning_rate": 3.244525547445256e-05,
      "loss": 3.8238,
      "step": 4810
    },
    {
      "epoch": 35.18248175182482,
      "grad_norm": 8.780601501464844,
      "learning_rate": 3.24087591240876e-05,
      "loss": 3.8994,
      "step": 4820
    },
    {
      "epoch": 35.25547445255474,
      "grad_norm": 8.064642906188965,
      "learning_rate": 3.237226277372263e-05,
      "loss": 3.88,
      "step": 4830
    },
    {
      "epoch": 35.32846715328467,
      "grad_norm": 8.022940635681152,
      "learning_rate": 3.233576642335767e-05,
      "loss": 3.5511,
      "step": 4840
    },
    {
      "epoch": 35.4014598540146,
      "grad_norm": 7.7897725105285645,
      "learning_rate": 3.229927007299271e-05,
      "loss": 3.4752,
      "step": 4850
    },
    {
      "epoch": 35.47445255474452,
      "grad_norm": 7.745570182800293,
      "learning_rate": 3.226277372262774e-05,
      "loss": 3.8329,
      "step": 4860
    },
    {
      "epoch": 35.54744525547445,
      "grad_norm": 8.01976203918457,
      "learning_rate": 3.222627737226278e-05,
      "loss": 3.7464,
      "step": 4870
    },
    {
      "epoch": 35.62043795620438,
      "grad_norm": 9.010587692260742,
      "learning_rate": 3.218978102189781e-05,
      "loss": 3.8937,
      "step": 4880
    },
    {
      "epoch": 35.693430656934304,
      "grad_norm": 7.94787073135376,
      "learning_rate": 3.215328467153285e-05,
      "loss": 3.8311,
      "step": 4890
    },
    {
      "epoch": 35.76642335766423,
      "grad_norm": 8.346183776855469,
      "learning_rate": 3.211678832116788e-05,
      "loss": 3.8351,
      "step": 4900
    },
    {
      "epoch": 35.839416058394164,
      "grad_norm": 7.965537071228027,
      "learning_rate": 3.208029197080292e-05,
      "loss": 4.0462,
      "step": 4910
    },
    {
      "epoch": 35.91240875912409,
      "grad_norm": 7.436064720153809,
      "learning_rate": 3.204379562043795e-05,
      "loss": 3.6675,
      "step": 4920
    },
    {
      "epoch": 35.98540145985402,
      "grad_norm": 7.987760543823242,
      "learning_rate": 3.200729927007299e-05,
      "loss": 4.1853,
      "step": 4930
    },
    {
      "epoch": 36.058394160583944,
      "grad_norm": 7.91705322265625,
      "learning_rate": 3.197080291970803e-05,
      "loss": 3.6714,
      "step": 4940
    },
    {
      "epoch": 36.13138686131387,
      "grad_norm": 8.214399337768555,
      "learning_rate": 3.193430656934307e-05,
      "loss": 3.7851,
      "step": 4950
    },
    {
      "epoch": 36.2043795620438,
      "grad_norm": 8.17931079864502,
      "learning_rate": 3.18978102189781e-05,
      "loss": 3.7933,
      "step": 4960
    },
    {
      "epoch": 36.277372262773724,
      "grad_norm": 7.888434886932373,
      "learning_rate": 3.186131386861314e-05,
      "loss": 4.0208,
      "step": 4970
    },
    {
      "epoch": 36.35036496350365,
      "grad_norm": 8.345888137817383,
      "learning_rate": 3.182481751824818e-05,
      "loss": 3.9796,
      "step": 4980
    },
    {
      "epoch": 36.42335766423358,
      "grad_norm": 7.899182319641113,
      "learning_rate": 3.178832116788321e-05,
      "loss": 3.777,
      "step": 4990
    },
    {
      "epoch": 36.496350364963504,
      "grad_norm": 6.535954475402832,
      "learning_rate": 3.175182481751825e-05,
      "loss": 3.6248,
      "step": 5000
    },
    {
      "epoch": 36.56934306569343,
      "grad_norm": 7.895934581756592,
      "learning_rate": 3.171532846715329e-05,
      "loss": 3.7856,
      "step": 5010
    },
    {
      "epoch": 36.64233576642336,
      "grad_norm": 7.822312831878662,
      "learning_rate": 3.167883211678832e-05,
      "loss": 3.6235,
      "step": 5020
    },
    {
      "epoch": 36.715328467153284,
      "grad_norm": 7.741706371307373,
      "learning_rate": 3.164233576642336e-05,
      "loss": 3.757,
      "step": 5030
    },
    {
      "epoch": 36.78832116788321,
      "grad_norm": 8.012714385986328,
      "learning_rate": 3.16058394160584e-05,
      "loss": 3.6374,
      "step": 5040
    },
    {
      "epoch": 36.86131386861314,
      "grad_norm": 7.157893657684326,
      "learning_rate": 3.156934306569343e-05,
      "loss": 3.8432,
      "step": 5050
    },
    {
      "epoch": 36.934306569343065,
      "grad_norm": 7.4412946701049805,
      "learning_rate": 3.153284671532847e-05,
      "loss": 3.5887,
      "step": 5060
    },
    {
      "epoch": 37.00729927007299,
      "grad_norm": 8.664003372192383,
      "learning_rate": 3.149635036496351e-05,
      "loss": 3.7231,
      "step": 5070
    },
    {
      "epoch": 37.08029197080292,
      "grad_norm": 8.110139846801758,
      "learning_rate": 3.1459854014598546e-05,
      "loss": 3.5203,
      "step": 5080
    },
    {
      "epoch": 37.153284671532845,
      "grad_norm": 7.954463958740234,
      "learning_rate": 3.142335766423358e-05,
      "loss": 3.5155,
      "step": 5090
    },
    {
      "epoch": 37.22627737226277,
      "grad_norm": 8.30629825592041,
      "learning_rate": 3.138686131386862e-05,
      "loss": 3.7215,
      "step": 5100
    },
    {
      "epoch": 37.2992700729927,
      "grad_norm": 7.933290481567383,
      "learning_rate": 3.1350364963503656e-05,
      "loss": 3.6346,
      "step": 5110
    },
    {
      "epoch": 37.372262773722625,
      "grad_norm": 8.358909606933594,
      "learning_rate": 3.131386861313869e-05,
      "loss": 3.7521,
      "step": 5120
    },
    {
      "epoch": 37.44525547445255,
      "grad_norm": 8.147857666015625,
      "learning_rate": 3.127737226277373e-05,
      "loss": 3.535,
      "step": 5130
    },
    {
      "epoch": 37.518248175182485,
      "grad_norm": 8.303050994873047,
      "learning_rate": 3.1240875912408765e-05,
      "loss": 3.8469,
      "step": 5140
    },
    {
      "epoch": 37.59124087591241,
      "grad_norm": 7.538724899291992,
      "learning_rate": 3.12043795620438e-05,
      "loss": 3.8005,
      "step": 5150
    },
    {
      "epoch": 37.66423357664234,
      "grad_norm": 7.840960502624512,
      "learning_rate": 3.1167883211678836e-05,
      "loss": 3.7295,
      "step": 5160
    },
    {
      "epoch": 37.737226277372265,
      "grad_norm": 7.905936241149902,
      "learning_rate": 3.113138686131387e-05,
      "loss": 3.5537,
      "step": 5170
    },
    {
      "epoch": 37.81021897810219,
      "grad_norm": 8.204377174377441,
      "learning_rate": 3.109489051094891e-05,
      "loss": 3.9371,
      "step": 5180
    },
    {
      "epoch": 37.88321167883212,
      "grad_norm": 8.38400936126709,
      "learning_rate": 3.105839416058394e-05,
      "loss": 3.8656,
      "step": 5190
    },
    {
      "epoch": 37.956204379562045,
      "grad_norm": 7.553292751312256,
      "learning_rate": 3.102189781021898e-05,
      "loss": 3.9124,
      "step": 5200
    },
    {
      "epoch": 38.02919708029197,
      "grad_norm": 8.20479965209961,
      "learning_rate": 3.098540145985402e-05,
      "loss": 3.8266,
      "step": 5210
    },
    {
      "epoch": 38.1021897810219,
      "grad_norm": 8.064396858215332,
      "learning_rate": 3.094890510948905e-05,
      "loss": 3.8426,
      "step": 5220
    },
    {
      "epoch": 38.175182481751825,
      "grad_norm": 7.999447345733643,
      "learning_rate": 3.091240875912409e-05,
      "loss": 3.9801,
      "step": 5230
    },
    {
      "epoch": 38.24817518248175,
      "grad_norm": 7.639752388000488,
      "learning_rate": 3.0875912408759127e-05,
      "loss": 3.7551,
      "step": 5240
    },
    {
      "epoch": 38.32116788321168,
      "grad_norm": 8.267860412597656,
      "learning_rate": 3.083941605839416e-05,
      "loss": 3.6395,
      "step": 5250
    },
    {
      "epoch": 38.394160583941606,
      "grad_norm": 8.515091896057129,
      "learning_rate": 3.08029197080292e-05,
      "loss": 3.8604,
      "step": 5260
    },
    {
      "epoch": 38.46715328467153,
      "grad_norm": 7.866826057434082,
      "learning_rate": 3.0766423357664236e-05,
      "loss": 3.395,
      "step": 5270
    },
    {
      "epoch": 38.54014598540146,
      "grad_norm": 8.34557819366455,
      "learning_rate": 3.072992700729927e-05,
      "loss": 3.7186,
      "step": 5280
    },
    {
      "epoch": 38.613138686131386,
      "grad_norm": 8.0323486328125,
      "learning_rate": 3.069343065693431e-05,
      "loss": 3.4934,
      "step": 5290
    },
    {
      "epoch": 38.68613138686131,
      "grad_norm": 8.415875434875488,
      "learning_rate": 3.0656934306569346e-05,
      "loss": 3.5788,
      "step": 5300
    },
    {
      "epoch": 38.75912408759124,
      "grad_norm": 7.696229457855225,
      "learning_rate": 3.062043795620438e-05,
      "loss": 3.8687,
      "step": 5310
    },
    {
      "epoch": 38.832116788321166,
      "grad_norm": 8.261825561523438,
      "learning_rate": 3.058394160583942e-05,
      "loss": 3.6971,
      "step": 5320
    },
    {
      "epoch": 38.90510948905109,
      "grad_norm": 7.968800067901611,
      "learning_rate": 3.0547445255474456e-05,
      "loss": 3.5343,
      "step": 5330
    },
    {
      "epoch": 38.97810218978102,
      "grad_norm": 8.226643562316895,
      "learning_rate": 3.0510948905109494e-05,
      "loss": 3.8476,
      "step": 5340
    },
    {
      "epoch": 39.051094890510946,
      "grad_norm": 7.557297706604004,
      "learning_rate": 3.0474452554744527e-05,
      "loss": 3.4166,
      "step": 5350
    },
    {
      "epoch": 39.12408759124087,
      "grad_norm": 8.048580169677734,
      "learning_rate": 3.0437956204379565e-05,
      "loss": 3.7541,
      "step": 5360
    },
    {
      "epoch": 39.197080291970806,
      "grad_norm": 8.311365127563477,
      "learning_rate": 3.04014598540146e-05,
      "loss": 3.8818,
      "step": 5370
    },
    {
      "epoch": 39.27007299270073,
      "grad_norm": 8.365543365478516,
      "learning_rate": 3.0364963503649636e-05,
      "loss": 3.841,
      "step": 5380
    },
    {
      "epoch": 39.34306569343066,
      "grad_norm": 7.756567001342773,
      "learning_rate": 3.032846715328467e-05,
      "loss": 3.639,
      "step": 5390
    },
    {
      "epoch": 39.416058394160586,
      "grad_norm": 7.742913246154785,
      "learning_rate": 3.029197080291971e-05,
      "loss": 3.7329,
      "step": 5400
    },
    {
      "epoch": 39.48905109489051,
      "grad_norm": 8.125569343566895,
      "learning_rate": 3.0255474452554743e-05,
      "loss": 3.8626,
      "step": 5410
    },
    {
      "epoch": 39.56204379562044,
      "grad_norm": 8.73172378540039,
      "learning_rate": 3.021897810218978e-05,
      "loss": 3.8641,
      "step": 5420
    },
    {
      "epoch": 39.63503649635037,
      "grad_norm": 7.777368545532227,
      "learning_rate": 3.018248175182482e-05,
      "loss": 3.8488,
      "step": 5430
    },
    {
      "epoch": 39.70802919708029,
      "grad_norm": 8.366100311279297,
      "learning_rate": 3.0145985401459852e-05,
      "loss": 3.7181,
      "step": 5440
    },
    {
      "epoch": 39.78102189781022,
      "grad_norm": 7.770913124084473,
      "learning_rate": 3.010948905109489e-05,
      "loss": 3.6605,
      "step": 5450
    },
    {
      "epoch": 39.85401459854015,
      "grad_norm": 7.94188117980957,
      "learning_rate": 3.007299270072993e-05,
      "loss": 3.5958,
      "step": 5460
    },
    {
      "epoch": 39.92700729927007,
      "grad_norm": 7.813695430755615,
      "learning_rate": 3.003649635036497e-05,
      "loss": 3.4412,
      "step": 5470
    },
    {
      "epoch": 40.0,
      "grad_norm": 11.899393081665039,
      "learning_rate": 3e-05,
      "loss": 3.6521,
      "step": 5480
    },
    {
      "epoch": 40.07299270072993,
      "grad_norm": 8.4926118850708,
      "learning_rate": 2.996350364963504e-05,
      "loss": 3.8264,
      "step": 5490
    },
    {
      "epoch": 40.14598540145985,
      "grad_norm": 7.558690071105957,
      "learning_rate": 2.992700729927008e-05,
      "loss": 3.529,
      "step": 5500
    },
    {
      "epoch": 40.21897810218978,
      "grad_norm": 7.576190948486328,
      "learning_rate": 2.989051094890511e-05,
      "loss": 3.7028,
      "step": 5510
    },
    {
      "epoch": 40.29197080291971,
      "grad_norm": 8.10704231262207,
      "learning_rate": 2.985401459854015e-05,
      "loss": 3.6584,
      "step": 5520
    },
    {
      "epoch": 40.36496350364963,
      "grad_norm": 8.455584526062012,
      "learning_rate": 2.9817518248175185e-05,
      "loss": 3.5995,
      "step": 5530
    },
    {
      "epoch": 40.43795620437956,
      "grad_norm": 7.688269138336182,
      "learning_rate": 2.9781021897810217e-05,
      "loss": 3.594,
      "step": 5540
    },
    {
      "epoch": 40.51094890510949,
      "grad_norm": 7.426294326782227,
      "learning_rate": 2.9744525547445256e-05,
      "loss": 3.4084,
      "step": 5550
    },
    {
      "epoch": 40.583941605839414,
      "grad_norm": 7.541268348693848,
      "learning_rate": 2.9708029197080294e-05,
      "loss": 3.9593,
      "step": 5560
    },
    {
      "epoch": 40.65693430656934,
      "grad_norm": 7.568296909332275,
      "learning_rate": 2.9671532846715326e-05,
      "loss": 3.6101,
      "step": 5570
    },
    {
      "epoch": 40.72992700729927,
      "grad_norm": 8.888641357421875,
      "learning_rate": 2.9635036496350365e-05,
      "loss": 3.5812,
      "step": 5580
    },
    {
      "epoch": 40.802919708029194,
      "grad_norm": 7.369569301605225,
      "learning_rate": 2.9598540145985404e-05,
      "loss": 3.5306,
      "step": 5590
    },
    {
      "epoch": 40.87591240875913,
      "grad_norm": 9.205187797546387,
      "learning_rate": 2.9562043795620443e-05,
      "loss": 3.6085,
      "step": 5600
    },
    {
      "epoch": 40.948905109489054,
      "grad_norm": 7.948982238769531,
      "learning_rate": 2.9525547445255475e-05,
      "loss": 3.9563,
      "step": 5610
    },
    {
      "epoch": 41.02189781021898,
      "grad_norm": 8.018291473388672,
      "learning_rate": 2.9489051094890514e-05,
      "loss": 3.7281,
      "step": 5620
    },
    {
      "epoch": 41.09489051094891,
      "grad_norm": 7.84907341003418,
      "learning_rate": 2.9452554744525553e-05,
      "loss": 3.7366,
      "step": 5630
    },
    {
      "epoch": 41.167883211678834,
      "grad_norm": 8.140570640563965,
      "learning_rate": 2.9416058394160585e-05,
      "loss": 3.8813,
      "step": 5640
    },
    {
      "epoch": 41.24087591240876,
      "grad_norm": 8.62986946105957,
      "learning_rate": 2.9379562043795624e-05,
      "loss": 3.4375,
      "step": 5650
    },
    {
      "epoch": 41.31386861313869,
      "grad_norm": 7.762149333953857,
      "learning_rate": 2.934306569343066e-05,
      "loss": 3.446,
      "step": 5660
    },
    {
      "epoch": 41.386861313868614,
      "grad_norm": 7.948569297790527,
      "learning_rate": 2.9306569343065694e-05,
      "loss": 3.9077,
      "step": 5670
    },
    {
      "epoch": 41.45985401459854,
      "grad_norm": 8.107309341430664,
      "learning_rate": 2.927007299270073e-05,
      "loss": 3.7029,
      "step": 5680
    },
    {
      "epoch": 41.53284671532847,
      "grad_norm": 8.21334457397461,
      "learning_rate": 2.923357664233577e-05,
      "loss": 3.8523,
      "step": 5690
    },
    {
      "epoch": 41.605839416058394,
      "grad_norm": 8.154523849487305,
      "learning_rate": 2.91970802919708e-05,
      "loss": 3.4117,
      "step": 5700
    },
    {
      "epoch": 41.67883211678832,
      "grad_norm": 8.559976577758789,
      "learning_rate": 2.916058394160584e-05,
      "loss": 3.6126,
      "step": 5710
    },
    {
      "epoch": 41.75182481751825,
      "grad_norm": 9.094162940979004,
      "learning_rate": 2.912408759124088e-05,
      "loss": 3.8196,
      "step": 5720
    },
    {
      "epoch": 41.824817518248175,
      "grad_norm": 8.257936477661133,
      "learning_rate": 2.908759124087591e-05,
      "loss": 3.4604,
      "step": 5730
    },
    {
      "epoch": 41.8978102189781,
      "grad_norm": 8.039892196655273,
      "learning_rate": 2.905109489051095e-05,
      "loss": 3.4673,
      "step": 5740
    },
    {
      "epoch": 41.97080291970803,
      "grad_norm": 6.88253927230835,
      "learning_rate": 2.9014598540145988e-05,
      "loss": 3.5207,
      "step": 5750
    },
    {
      "epoch": 42.043795620437955,
      "grad_norm": 7.997035026550293,
      "learning_rate": 2.8978102189781027e-05,
      "loss": 3.4706,
      "step": 5760
    },
    {
      "epoch": 42.11678832116788,
      "grad_norm": 8.09158992767334,
      "learning_rate": 2.894160583941606e-05,
      "loss": 3.8419,
      "step": 5770
    },
    {
      "epoch": 42.18978102189781,
      "grad_norm": 7.615670204162598,
      "learning_rate": 2.8905109489051098e-05,
      "loss": 3.717,
      "step": 5780
    },
    {
      "epoch": 42.262773722627735,
      "grad_norm": 9.337701797485352,
      "learning_rate": 2.8868613138686133e-05,
      "loss": 3.3156,
      "step": 5790
    },
    {
      "epoch": 42.33576642335766,
      "grad_norm": 6.552276611328125,
      "learning_rate": 2.883211678832117e-05,
      "loss": 3.5317,
      "step": 5800
    },
    {
      "epoch": 42.40875912408759,
      "grad_norm": 8.517627716064453,
      "learning_rate": 2.8795620437956204e-05,
      "loss": 3.6093,
      "step": 5810
    },
    {
      "epoch": 42.481751824817515,
      "grad_norm": 9.380637168884277,
      "learning_rate": 2.8759124087591243e-05,
      "loss": 3.7048,
      "step": 5820
    },
    {
      "epoch": 42.55474452554745,
      "grad_norm": 7.8172101974487305,
      "learning_rate": 2.8722627737226275e-05,
      "loss": 3.3079,
      "step": 5830
    },
    {
      "epoch": 42.627737226277375,
      "grad_norm": 7.99652624130249,
      "learning_rate": 2.8686131386861314e-05,
      "loss": 3.6962,
      "step": 5840
    },
    {
      "epoch": 42.7007299270073,
      "grad_norm": 7.700027942657471,
      "learning_rate": 2.8649635036496353e-05,
      "loss": 3.6278,
      "step": 5850
    },
    {
      "epoch": 42.77372262773723,
      "grad_norm": 7.78715705871582,
      "learning_rate": 2.8613138686131385e-05,
      "loss": 3.5385,
      "step": 5860
    },
    {
      "epoch": 42.846715328467155,
      "grad_norm": 8.913165092468262,
      "learning_rate": 2.8576642335766423e-05,
      "loss": 3.5858,
      "step": 5870
    },
    {
      "epoch": 42.91970802919708,
      "grad_norm": 7.555653095245361,
      "learning_rate": 2.8540145985401462e-05,
      "loss": 3.7505,
      "step": 5880
    },
    {
      "epoch": 42.99270072992701,
      "grad_norm": 8.003693580627441,
      "learning_rate": 2.85036496350365e-05,
      "loss": 3.8201,
      "step": 5890
    },
    {
      "epoch": 43.065693430656935,
      "grad_norm": 8.099377632141113,
      "learning_rate": 2.8467153284671533e-05,
      "loss": 3.8647,
      "step": 5900
    },
    {
      "epoch": 43.13868613138686,
      "grad_norm": 8.168622016906738,
      "learning_rate": 2.8430656934306572e-05,
      "loss": 3.8991,
      "step": 5910
    },
    {
      "epoch": 43.21167883211679,
      "grad_norm": 7.09322452545166,
      "learning_rate": 2.839416058394161e-05,
      "loss": 3.5392,
      "step": 5920
    },
    {
      "epoch": 43.284671532846716,
      "grad_norm": 7.525152683258057,
      "learning_rate": 2.8357664233576643e-05,
      "loss": 3.8197,
      "step": 5930
    },
    {
      "epoch": 43.35766423357664,
      "grad_norm": 7.566796779632568,
      "learning_rate": 2.832116788321168e-05,
      "loss": 3.7751,
      "step": 5940
    },
    {
      "epoch": 43.43065693430657,
      "grad_norm": 10.01728630065918,
      "learning_rate": 2.8284671532846717e-05,
      "loss": 3.5618,
      "step": 5950
    },
    {
      "epoch": 43.503649635036496,
      "grad_norm": 8.295966148376465,
      "learning_rate": 2.8248175182481753e-05,
      "loss": 3.5233,
      "step": 5960
    },
    {
      "epoch": 43.57664233576642,
      "grad_norm": 8.629579544067383,
      "learning_rate": 2.8211678832116788e-05,
      "loss": 3.3875,
      "step": 5970
    },
    {
      "epoch": 43.64963503649635,
      "grad_norm": 6.857757568359375,
      "learning_rate": 2.8175182481751827e-05,
      "loss": 3.6178,
      "step": 5980
    },
    {
      "epoch": 43.722627737226276,
      "grad_norm": 8.394000053405762,
      "learning_rate": 2.813868613138686e-05,
      "loss": 3.5749,
      "step": 5990
    },
    {
      "epoch": 43.7956204379562,
      "grad_norm": 7.619281768798828,
      "learning_rate": 2.8102189781021898e-05,
      "loss": 3.2015,
      "step": 6000
    },
    {
      "epoch": 43.86861313868613,
      "grad_norm": 7.622190475463867,
      "learning_rate": 2.8065693430656936e-05,
      "loss": 3.4936,
      "step": 6010
    },
    {
      "epoch": 43.941605839416056,
      "grad_norm": 7.964332103729248,
      "learning_rate": 2.8029197080291975e-05,
      "loss": 3.5036,
      "step": 6020
    },
    {
      "epoch": 44.01459854014598,
      "grad_norm": 8.6869478225708,
      "learning_rate": 2.7992700729927007e-05,
      "loss": 3.7636,
      "step": 6030
    },
    {
      "epoch": 44.08759124087591,
      "grad_norm": 7.813320159912109,
      "learning_rate": 2.7956204379562046e-05,
      "loss": 3.4786,
      "step": 6040
    },
    {
      "epoch": 44.160583941605836,
      "grad_norm": 9.913349151611328,
      "learning_rate": 2.7919708029197085e-05,
      "loss": 3.7494,
      "step": 6050
    },
    {
      "epoch": 44.23357664233577,
      "grad_norm": 7.066738128662109,
      "learning_rate": 2.7883211678832117e-05,
      "loss": 2.9863,
      "step": 6060
    },
    {
      "epoch": 44.306569343065696,
      "grad_norm": 8.100532531738281,
      "learning_rate": 2.7846715328467156e-05,
      "loss": 3.6899,
      "step": 6070
    },
    {
      "epoch": 44.37956204379562,
      "grad_norm": 7.801630020141602,
      "learning_rate": 2.781021897810219e-05,
      "loss": 3.5343,
      "step": 6080
    },
    {
      "epoch": 44.45255474452555,
      "grad_norm": 8.892243385314941,
      "learning_rate": 2.7773722627737227e-05,
      "loss": 3.2253,
      "step": 6090
    },
    {
      "epoch": 44.52554744525548,
      "grad_norm": 8.221635818481445,
      "learning_rate": 2.7737226277372262e-05,
      "loss": 3.496,
      "step": 6100
    },
    {
      "epoch": 44.5985401459854,
      "grad_norm": 8.155669212341309,
      "learning_rate": 2.77007299270073e-05,
      "loss": 3.856,
      "step": 6110
    },
    {
      "epoch": 44.67153284671533,
      "grad_norm": 8.549468994140625,
      "learning_rate": 2.7664233576642333e-05,
      "loss": 3.9724,
      "step": 6120
    },
    {
      "epoch": 44.74452554744526,
      "grad_norm": 8.746366500854492,
      "learning_rate": 2.7627737226277372e-05,
      "loss": 3.6012,
      "step": 6130
    },
    {
      "epoch": 44.81751824817518,
      "grad_norm": 7.159778594970703,
      "learning_rate": 2.759124087591241e-05,
      "loss": 3.5767,
      "step": 6140
    },
    {
      "epoch": 44.89051094890511,
      "grad_norm": 7.944240570068359,
      "learning_rate": 2.755474452554745e-05,
      "loss": 3.6094,
      "step": 6150
    },
    {
      "epoch": 44.96350364963504,
      "grad_norm": 8.879155158996582,
      "learning_rate": 2.751824817518248e-05,
      "loss": 3.9228,
      "step": 6160
    },
    {
      "epoch": 45.03649635036496,
      "grad_norm": 7.326132774353027,
      "learning_rate": 2.748175182481752e-05,
      "loss": 3.281,
      "step": 6170
    },
    {
      "epoch": 45.10948905109489,
      "grad_norm": 7.868738174438477,
      "learning_rate": 2.744525547445256e-05,
      "loss": 3.5726,
      "step": 6180
    },
    {
      "epoch": 45.18248175182482,
      "grad_norm": 7.931753635406494,
      "learning_rate": 2.740875912408759e-05,
      "loss": 3.662,
      "step": 6190
    },
    {
      "epoch": 45.25547445255474,
      "grad_norm": 7.645521640777588,
      "learning_rate": 2.737226277372263e-05,
      "loss": 3.5217,
      "step": 6200
    },
    {
      "epoch": 45.32846715328467,
      "grad_norm": 8.369278907775879,
      "learning_rate": 2.7335766423357666e-05,
      "loss": 3.4422,
      "step": 6210
    },
    {
      "epoch": 45.4014598540146,
      "grad_norm": 8.160076141357422,
      "learning_rate": 2.72992700729927e-05,
      "loss": 3.4454,
      "step": 6220
    },
    {
      "epoch": 45.47445255474452,
      "grad_norm": 7.58540678024292,
      "learning_rate": 2.7262773722627736e-05,
      "loss": 3.7532,
      "step": 6230
    },
    {
      "epoch": 45.54744525547445,
      "grad_norm": 7.151008129119873,
      "learning_rate": 2.7226277372262775e-05,
      "loss": 3.7669,
      "step": 6240
    },
    {
      "epoch": 45.62043795620438,
      "grad_norm": 8.397934913635254,
      "learning_rate": 2.7189781021897807e-05,
      "loss": 3.583,
      "step": 6250
    },
    {
      "epoch": 45.693430656934304,
      "grad_norm": 7.811270713806152,
      "learning_rate": 2.7153284671532846e-05,
      "loss": 3.6262,
      "step": 6260
    },
    {
      "epoch": 45.76642335766423,
      "grad_norm": 7.142053127288818,
      "learning_rate": 2.7116788321167885e-05,
      "loss": 3.3943,
      "step": 6270
    },
    {
      "epoch": 45.839416058394164,
      "grad_norm": 7.626404285430908,
      "learning_rate": 2.7080291970802924e-05,
      "loss": 3.4162,
      "step": 6280
    },
    {
      "epoch": 45.91240875912409,
      "grad_norm": 7.409607887268066,
      "learning_rate": 2.7043795620437956e-05,
      "loss": 3.6475,
      "step": 6290
    },
    {
      "epoch": 45.98540145985402,
      "grad_norm": 7.784621238708496,
      "learning_rate": 2.7007299270072995e-05,
      "loss": 3.301,
      "step": 6300
    },
    {
      "epoch": 46.058394160583944,
      "grad_norm": 8.21030330657959,
      "learning_rate": 2.6970802919708033e-05,
      "loss": 3.4936,
      "step": 6310
    },
    {
      "epoch": 46.13138686131387,
      "grad_norm": 8.13833236694336,
      "learning_rate": 2.6934306569343065e-05,
      "loss": 3.5741,
      "step": 6320
    },
    {
      "epoch": 46.2043795620438,
      "grad_norm": 7.749118804931641,
      "learning_rate": 2.6897810218978104e-05,
      "loss": 3.7235,
      "step": 6330
    },
    {
      "epoch": 46.277372262773724,
      "grad_norm": 7.066703796386719,
      "learning_rate": 2.6861313868613143e-05,
      "loss": 3.3054,
      "step": 6340
    },
    {
      "epoch": 46.35036496350365,
      "grad_norm": 9.45998764038086,
      "learning_rate": 2.6824817518248175e-05,
      "loss": 3.4467,
      "step": 6350
    },
    {
      "epoch": 46.42335766423358,
      "grad_norm": 8.808113098144531,
      "learning_rate": 2.6788321167883214e-05,
      "loss": 3.8558,
      "step": 6360
    },
    {
      "epoch": 46.496350364963504,
      "grad_norm": 8.374433517456055,
      "learning_rate": 2.675182481751825e-05,
      "loss": 3.7734,
      "step": 6370
    },
    {
      "epoch": 46.56934306569343,
      "grad_norm": 7.818503379821777,
      "learning_rate": 2.6715328467153285e-05,
      "loss": 3.2571,
      "step": 6380
    },
    {
      "epoch": 46.64233576642336,
      "grad_norm": 8.212701797485352,
      "learning_rate": 2.667883211678832e-05,
      "loss": 3.535,
      "step": 6390
    },
    {
      "epoch": 46.715328467153284,
      "grad_norm": 8.175796508789062,
      "learning_rate": 2.664233576642336e-05,
      "loss": 3.7726,
      "step": 6400
    },
    {
      "epoch": 46.78832116788321,
      "grad_norm": 7.376982688903809,
      "learning_rate": 2.6605839416058398e-05,
      "loss": 3.4722,
      "step": 6410
    },
    {
      "epoch": 46.86131386861314,
      "grad_norm": 7.760914325714111,
      "learning_rate": 2.656934306569343e-05,
      "loss": 3.1576,
      "step": 6420
    },
    {
      "epoch": 46.934306569343065,
      "grad_norm": 10.440899848937988,
      "learning_rate": 2.653284671532847e-05,
      "loss": 3.7305,
      "step": 6430
    },
    {
      "epoch": 47.00729927007299,
      "grad_norm": 8.069110870361328,
      "learning_rate": 2.6496350364963508e-05,
      "loss": 3.1902,
      "step": 6440
    },
    {
      "epoch": 47.08029197080292,
      "grad_norm": 8.406234741210938,
      "learning_rate": 2.645985401459854e-05,
      "loss": 3.4866,
      "step": 6450
    },
    {
      "epoch": 47.153284671532845,
      "grad_norm": 7.3692731857299805,
      "learning_rate": 2.642335766423358e-05,
      "loss": 3.4383,
      "step": 6460
    },
    {
      "epoch": 47.22627737226277,
      "grad_norm": 8.284090995788574,
      "learning_rate": 2.6386861313868617e-05,
      "loss": 3.6355,
      "step": 6470
    },
    {
      "epoch": 47.2992700729927,
      "grad_norm": 7.929568767547607,
      "learning_rate": 2.635036496350365e-05,
      "loss": 3.4015,
      "step": 6480
    },
    {
      "epoch": 47.372262773722625,
      "grad_norm": 7.3018083572387695,
      "learning_rate": 2.6313868613138688e-05,
      "loss": 3.7345,
      "step": 6490
    },
    {
      "epoch": 47.44525547445255,
      "grad_norm": 7.885068893432617,
      "learning_rate": 2.6277372262773724e-05,
      "loss": 3.5381,
      "step": 6500
    },
    {
      "epoch": 47.518248175182485,
      "grad_norm": 8.385900497436523,
      "learning_rate": 2.624087591240876e-05,
      "loss": 3.5062,
      "step": 6510
    },
    {
      "epoch": 47.59124087591241,
      "grad_norm": 7.336706161499023,
      "learning_rate": 2.6204379562043795e-05,
      "loss": 3.6188,
      "step": 6520
    },
    {
      "epoch": 47.66423357664234,
      "grad_norm": 8.625252723693848,
      "learning_rate": 2.6167883211678833e-05,
      "loss": 3.4901,
      "step": 6530
    },
    {
      "epoch": 47.737226277372265,
      "grad_norm": 7.657783508300781,
      "learning_rate": 2.6131386861313872e-05,
      "loss": 3.439,
      "step": 6540
    },
    {
      "epoch": 47.81021897810219,
      "grad_norm": 8.49029541015625,
      "learning_rate": 2.6094890510948904e-05,
      "loss": 3.6295,
      "step": 6550
    },
    {
      "epoch": 47.88321167883212,
      "grad_norm": 7.666962623596191,
      "learning_rate": 2.6058394160583943e-05,
      "loss": 3.3742,
      "step": 6560
    },
    {
      "epoch": 47.956204379562045,
      "grad_norm": 7.735264778137207,
      "learning_rate": 2.6021897810218982e-05,
      "loss": 3.4828,
      "step": 6570
    },
    {
      "epoch": 48.02919708029197,
      "grad_norm": 7.43145227432251,
      "learning_rate": 2.5985401459854014e-05,
      "loss": 3.6748,
      "step": 6580
    },
    {
      "epoch": 48.1021897810219,
      "grad_norm": 7.094806671142578,
      "learning_rate": 2.5948905109489053e-05,
      "loss": 3.7282,
      "step": 6590
    },
    {
      "epoch": 48.175182481751825,
      "grad_norm": 8.348381042480469,
      "learning_rate": 2.591240875912409e-05,
      "loss": 3.5588,
      "step": 6600
    },
    {
      "epoch": 48.24817518248175,
      "grad_norm": 9.016258239746094,
      "learning_rate": 2.5875912408759124e-05,
      "loss": 3.3841,
      "step": 6610
    },
    {
      "epoch": 48.32116788321168,
      "grad_norm": 8.117606163024902,
      "learning_rate": 2.5839416058394162e-05,
      "loss": 3.4749,
      "step": 6620
    },
    {
      "epoch": 48.394160583941606,
      "grad_norm": 7.541048049926758,
      "learning_rate": 2.5802919708029198e-05,
      "loss": 3.7207,
      "step": 6630
    },
    {
      "epoch": 48.46715328467153,
      "grad_norm": 7.82916784286499,
      "learning_rate": 2.5766423357664233e-05,
      "loss": 3.3325,
      "step": 6640
    },
    {
      "epoch": 48.54014598540146,
      "grad_norm": 7.713593482971191,
      "learning_rate": 2.572992700729927e-05,
      "loss": 3.3677,
      "step": 6650
    },
    {
      "epoch": 48.613138686131386,
      "grad_norm": 7.595371723175049,
      "learning_rate": 2.5693430656934308e-05,
      "loss": 3.7257,
      "step": 6660
    },
    {
      "epoch": 48.68613138686131,
      "grad_norm": 8.635075569152832,
      "learning_rate": 2.5656934306569346e-05,
      "loss": 3.5272,
      "step": 6670
    },
    {
      "epoch": 48.75912408759124,
      "grad_norm": 8.123774528503418,
      "learning_rate": 2.562043795620438e-05,
      "loss": 3.5537,
      "step": 6680
    },
    {
      "epoch": 48.832116788321166,
      "grad_norm": 8.861408233642578,
      "learning_rate": 2.5583941605839417e-05,
      "loss": 3.4112,
      "step": 6690
    },
    {
      "epoch": 48.90510948905109,
      "grad_norm": 8.085060119628906,
      "learning_rate": 2.5547445255474456e-05,
      "loss": 3.2433,
      "step": 6700
    },
    {
      "epoch": 48.97810218978102,
      "grad_norm": 7.263186931610107,
      "learning_rate": 2.5510948905109488e-05,
      "loss": 3.3096,
      "step": 6710
    },
    {
      "epoch": 49.051094890510946,
      "grad_norm": 9.076574325561523,
      "learning_rate": 2.5474452554744527e-05,
      "loss": 3.6622,
      "step": 6720
    },
    {
      "epoch": 49.12408759124087,
      "grad_norm": 7.724746227264404,
      "learning_rate": 2.5437956204379566e-05,
      "loss": 3.3361,
      "step": 6730
    },
    {
      "epoch": 49.197080291970806,
      "grad_norm": 8.425528526306152,
      "learning_rate": 2.5401459854014598e-05,
      "loss": 3.253,
      "step": 6740
    },
    {
      "epoch": 49.27007299270073,
      "grad_norm": 8.646194458007812,
      "learning_rate": 2.5364963503649637e-05,
      "loss": 3.4743,
      "step": 6750
    },
    {
      "epoch": 49.34306569343066,
      "grad_norm": 10.040129661560059,
      "learning_rate": 2.5328467153284675e-05,
      "loss": 3.235,
      "step": 6760
    },
    {
      "epoch": 49.416058394160586,
      "grad_norm": 8.316064834594727,
      "learning_rate": 2.5291970802919708e-05,
      "loss": 3.6191,
      "step": 6770
    },
    {
      "epoch": 49.48905109489051,
      "grad_norm": 7.684844970703125,
      "learning_rate": 2.5255474452554746e-05,
      "loss": 3.5897,
      "step": 6780
    },
    {
      "epoch": 49.56204379562044,
      "grad_norm": 7.896094799041748,
      "learning_rate": 2.5218978102189782e-05,
      "loss": 3.4534,
      "step": 6790
    },
    {
      "epoch": 49.63503649635037,
      "grad_norm": 7.079845905303955,
      "learning_rate": 2.518248175182482e-05,
      "loss": 3.3869,
      "step": 6800
    },
    {
      "epoch": 49.70802919708029,
      "grad_norm": 7.7462239265441895,
      "learning_rate": 2.5145985401459853e-05,
      "loss": 3.6768,
      "step": 6810
    },
    {
      "epoch": 49.78102189781022,
      "grad_norm": 8.265769958496094,
      "learning_rate": 2.510948905109489e-05,
      "loss": 3.5288,
      "step": 6820
    },
    {
      "epoch": 49.85401459854015,
      "grad_norm": 8.251630783081055,
      "learning_rate": 2.507299270072993e-05,
      "loss": 3.4174,
      "step": 6830
    },
    {
      "epoch": 49.92700729927007,
      "grad_norm": 8.203079223632812,
      "learning_rate": 2.5036496350364962e-05,
      "loss": 3.8112,
      "step": 6840
    },
    {
      "epoch": 50.0,
      "grad_norm": 11.963591575622559,
      "learning_rate": 2.5e-05,
      "loss": 3.7127,
      "step": 6850
    },
    {
      "epoch": 50.07299270072993,
      "grad_norm": 7.561886310577393,
      "learning_rate": 2.4963503649635037e-05,
      "loss": 3.5397,
      "step": 6860
    },
    {
      "epoch": 50.14598540145985,
      "grad_norm": 8.376357078552246,
      "learning_rate": 2.4927007299270075e-05,
      "loss": 3.5349,
      "step": 6870
    },
    {
      "epoch": 50.21897810218978,
      "grad_norm": 9.191319465637207,
      "learning_rate": 2.489051094890511e-05,
      "loss": 3.8028,
      "step": 6880
    },
    {
      "epoch": 50.29197080291971,
      "grad_norm": 8.151298522949219,
      "learning_rate": 2.485401459854015e-05,
      "loss": 3.4175,
      "step": 6890
    },
    {
      "epoch": 50.36496350364963,
      "grad_norm": 8.972550392150879,
      "learning_rate": 2.4817518248175185e-05,
      "loss": 3.5836,
      "step": 6900
    },
    {
      "epoch": 50.43795620437956,
      "grad_norm": 8.290366172790527,
      "learning_rate": 2.478102189781022e-05,
      "loss": 3.4601,
      "step": 6910
    },
    {
      "epoch": 50.51094890510949,
      "grad_norm": 8.55213737487793,
      "learning_rate": 2.4744525547445256e-05,
      "loss": 3.2435,
      "step": 6920
    },
    {
      "epoch": 50.583941605839414,
      "grad_norm": 8.138646125793457,
      "learning_rate": 2.470802919708029e-05,
      "loss": 3.523,
      "step": 6930
    },
    {
      "epoch": 50.65693430656934,
      "grad_norm": 9.198735237121582,
      "learning_rate": 2.4671532846715327e-05,
      "loss": 3.0876,
      "step": 6940
    },
    {
      "epoch": 50.72992700729927,
      "grad_norm": 8.66354751586914,
      "learning_rate": 2.4635036496350366e-05,
      "loss": 3.3392,
      "step": 6950
    },
    {
      "epoch": 50.802919708029194,
      "grad_norm": 7.569438457489014,
      "learning_rate": 2.45985401459854e-05,
      "loss": 3.3696,
      "step": 6960
    },
    {
      "epoch": 50.87591240875913,
      "grad_norm": 8.090798377990723,
      "learning_rate": 2.456204379562044e-05,
      "loss": 3.5923,
      "step": 6970
    },
    {
      "epoch": 50.948905109489054,
      "grad_norm": 8.674786567687988,
      "learning_rate": 2.4525547445255475e-05,
      "loss": 3.2851,
      "step": 6980
    },
    {
      "epoch": 51.02189781021898,
      "grad_norm": 8.05288028717041,
      "learning_rate": 2.448905109489051e-05,
      "loss": 3.2555,
      "step": 6990
    },
    {
      "epoch": 51.09489051094891,
      "grad_norm": 8.466028213500977,
      "learning_rate": 2.445255474452555e-05,
      "loss": 3.4734,
      "step": 7000
    },
    {
      "epoch": 51.167883211678834,
      "grad_norm": 7.655229091644287,
      "learning_rate": 2.4416058394160585e-05,
      "loss": 3.6304,
      "step": 7010
    },
    {
      "epoch": 51.24087591240876,
      "grad_norm": 9.513826370239258,
      "learning_rate": 2.4379562043795624e-05,
      "loss": 3.552,
      "step": 7020
    },
    {
      "epoch": 51.31386861313869,
      "grad_norm": 7.923069953918457,
      "learning_rate": 2.434306569343066e-05,
      "loss": 3.6648,
      "step": 7030
    },
    {
      "epoch": 51.386861313868614,
      "grad_norm": 8.58703327178955,
      "learning_rate": 2.4306569343065695e-05,
      "loss": 3.5543,
      "step": 7040
    },
    {
      "epoch": 51.45985401459854,
      "grad_norm": 7.772241115570068,
      "learning_rate": 2.4270072992700734e-05,
      "loss": 3.4483,
      "step": 7050
    },
    {
      "epoch": 51.53284671532847,
      "grad_norm": 8.58901596069336,
      "learning_rate": 2.423357664233577e-05,
      "loss": 3.374,
      "step": 7060
    },
    {
      "epoch": 51.605839416058394,
      "grad_norm": 8.387413024902344,
      "learning_rate": 2.4197080291970805e-05,
      "loss": 3.5463,
      "step": 7070
    },
    {
      "epoch": 51.67883211678832,
      "grad_norm": 7.045012474060059,
      "learning_rate": 2.416058394160584e-05,
      "loss": 3.1796,
      "step": 7080
    },
    {
      "epoch": 51.75182481751825,
      "grad_norm": 7.851344585418701,
      "learning_rate": 2.4124087591240875e-05,
      "loss": 3.3149,
      "step": 7090
    },
    {
      "epoch": 51.824817518248175,
      "grad_norm": 8.498517036437988,
      "learning_rate": 2.4087591240875914e-05,
      "loss": 3.5165,
      "step": 7100
    },
    {
      "epoch": 51.8978102189781,
      "grad_norm": 8.581799507141113,
      "learning_rate": 2.405109489051095e-05,
      "loss": 3.3351,
      "step": 7110
    },
    {
      "epoch": 51.97080291970803,
      "grad_norm": 8.643013954162598,
      "learning_rate": 2.4014598540145985e-05,
      "loss": 3.4448,
      "step": 7120
    },
    {
      "epoch": 52.043795620437955,
      "grad_norm": 8.814323425292969,
      "learning_rate": 2.3978102189781024e-05,
      "loss": 3.6926,
      "step": 7130
    },
    {
      "epoch": 52.11678832116788,
      "grad_norm": 5.927119255065918,
      "learning_rate": 2.394160583941606e-05,
      "loss": 3.4201,
      "step": 7140
    },
    {
      "epoch": 52.18978102189781,
      "grad_norm": 8.322831153869629,
      "learning_rate": 2.3905109489051098e-05,
      "loss": 3.468,
      "step": 7150
    },
    {
      "epoch": 52.262773722627735,
      "grad_norm": 7.65065336227417,
      "learning_rate": 2.3868613138686134e-05,
      "loss": 3.4438,
      "step": 7160
    },
    {
      "epoch": 52.33576642335766,
      "grad_norm": 7.800563335418701,
      "learning_rate": 2.383211678832117e-05,
      "loss": 3.6226,
      "step": 7170
    },
    {
      "epoch": 52.40875912408759,
      "grad_norm": 8.746719360351562,
      "learning_rate": 2.3795620437956208e-05,
      "loss": 3.9012,
      "step": 7180
    },
    {
      "epoch": 52.481751824817515,
      "grad_norm": 8.620786666870117,
      "learning_rate": 2.3759124087591243e-05,
      "loss": 3.3754,
      "step": 7190
    },
    {
      "epoch": 52.55474452554745,
      "grad_norm": 8.17624282836914,
      "learning_rate": 2.372262773722628e-05,
      "loss": 3.2101,
      "step": 7200
    },
    {
      "epoch": 52.627737226277375,
      "grad_norm": 8.178439140319824,
      "learning_rate": 2.3686131386861314e-05,
      "loss": 3.435,
      "step": 7210
    },
    {
      "epoch": 52.7007299270073,
      "grad_norm": 7.453904151916504,
      "learning_rate": 2.364963503649635e-05,
      "loss": 3.2702,
      "step": 7220
    },
    {
      "epoch": 52.77372262773723,
      "grad_norm": 7.8226399421691895,
      "learning_rate": 2.361313868613139e-05,
      "loss": 3.1143,
      "step": 7230
    },
    {
      "epoch": 52.846715328467155,
      "grad_norm": 8.446122169494629,
      "learning_rate": 2.3576642335766424e-05,
      "loss": 3.2712,
      "step": 7240
    },
    {
      "epoch": 52.91970802919708,
      "grad_norm": 7.973777770996094,
      "learning_rate": 2.354014598540146e-05,
      "loss": 3.5131,
      "step": 7250
    },
    {
      "epoch": 52.99270072992701,
      "grad_norm": 8.189268112182617,
      "learning_rate": 2.3503649635036498e-05,
      "loss": 3.3508,
      "step": 7260
    },
    {
      "epoch": 53.065693430656935,
      "grad_norm": 8.569228172302246,
      "learning_rate": 2.3467153284671534e-05,
      "loss": 3.1169,
      "step": 7270
    },
    {
      "epoch": 53.13868613138686,
      "grad_norm": 8.175981521606445,
      "learning_rate": 2.3430656934306572e-05,
      "loss": 3.5522,
      "step": 7280
    },
    {
      "epoch": 53.21167883211679,
      "grad_norm": 7.417541980743408,
      "learning_rate": 2.3394160583941608e-05,
      "loss": 3.2885,
      "step": 7290
    },
    {
      "epoch": 53.284671532846716,
      "grad_norm": 7.906580448150635,
      "learning_rate": 2.3357664233576643e-05,
      "loss": 3.4657,
      "step": 7300
    },
    {
      "epoch": 53.35766423357664,
      "grad_norm": 7.140699863433838,
      "learning_rate": 2.3321167883211682e-05,
      "loss": 3.2365,
      "step": 7310
    },
    {
      "epoch": 53.43065693430657,
      "grad_norm": 7.111433982849121,
      "learning_rate": 2.3284671532846718e-05,
      "loss": 3.3175,
      "step": 7320
    },
    {
      "epoch": 53.503649635036496,
      "grad_norm": 7.053133487701416,
      "learning_rate": 2.3248175182481753e-05,
      "loss": 3.5168,
      "step": 7330
    },
    {
      "epoch": 53.57664233576642,
      "grad_norm": 7.646513938903809,
      "learning_rate": 2.321167883211679e-05,
      "loss": 3.467,
      "step": 7340
    },
    {
      "epoch": 53.64963503649635,
      "grad_norm": 9.377659797668457,
      "learning_rate": 2.3175182481751824e-05,
      "loss": 3.5174,
      "step": 7350
    },
    {
      "epoch": 53.722627737226276,
      "grad_norm": 9.639979362487793,
      "learning_rate": 2.3138686131386863e-05,
      "loss": 3.4495,
      "step": 7360
    },
    {
      "epoch": 53.7956204379562,
      "grad_norm": 8.764842987060547,
      "learning_rate": 2.3102189781021898e-05,
      "loss": 3.3209,
      "step": 7370
    },
    {
      "epoch": 53.86861313868613,
      "grad_norm": 8.643269538879395,
      "learning_rate": 2.3065693430656934e-05,
      "loss": 3.4042,
      "step": 7380
    },
    {
      "epoch": 53.941605839416056,
      "grad_norm": 8.287020683288574,
      "learning_rate": 2.3029197080291972e-05,
      "loss": 3.8091,
      "step": 7390
    },
    {
      "epoch": 54.01459854014598,
      "grad_norm": 8.300947189331055,
      "learning_rate": 2.2992700729927008e-05,
      "loss": 3.3136,
      "step": 7400
    },
    {
      "epoch": 54.08759124087591,
      "grad_norm": 8.103714942932129,
      "learning_rate": 2.2956204379562047e-05,
      "loss": 3.7673,
      "step": 7410
    },
    {
      "epoch": 54.160583941605836,
      "grad_norm": 8.420425415039062,
      "learning_rate": 2.2919708029197082e-05,
      "loss": 3.4203,
      "step": 7420
    },
    {
      "epoch": 54.23357664233577,
      "grad_norm": 8.289770126342773,
      "learning_rate": 2.2883211678832117e-05,
      "loss": 3.425,
      "step": 7430
    },
    {
      "epoch": 54.306569343065696,
      "grad_norm": 9.198966026306152,
      "learning_rate": 2.2846715328467156e-05,
      "loss": 3.5582,
      "step": 7440
    },
    {
      "epoch": 54.37956204379562,
      "grad_norm": 7.840702056884766,
      "learning_rate": 2.2810218978102192e-05,
      "loss": 3.3905,
      "step": 7450
    },
    {
      "epoch": 54.45255474452555,
      "grad_norm": 7.796397686004639,
      "learning_rate": 2.2773722627737227e-05,
      "loss": 3.1065,
      "step": 7460
    },
    {
      "epoch": 54.52554744525548,
      "grad_norm": 8.143489837646484,
      "learning_rate": 2.2737226277372266e-05,
      "loss": 3.3081,
      "step": 7470
    },
    {
      "epoch": 54.5985401459854,
      "grad_norm": 8.395155906677246,
      "learning_rate": 2.27007299270073e-05,
      "loss": 3.2701,
      "step": 7480
    },
    {
      "epoch": 54.67153284671533,
      "grad_norm": 8.660283088684082,
      "learning_rate": 2.2664233576642337e-05,
      "loss": 3.2944,
      "step": 7490
    },
    {
      "epoch": 54.74452554744526,
      "grad_norm": 8.175629615783691,
      "learning_rate": 2.2627737226277372e-05,
      "loss": 2.9809,
      "step": 7500
    },
    {
      "epoch": 54.81751824817518,
      "grad_norm": 8.477581977844238,
      "learning_rate": 2.2591240875912408e-05,
      "loss": 3.3897,
      "step": 7510
    },
    {
      "epoch": 54.89051094890511,
      "grad_norm": 8.18190860748291,
      "learning_rate": 2.2554744525547447e-05,
      "loss": 3.4602,
      "step": 7520
    },
    {
      "epoch": 54.96350364963504,
      "grad_norm": 8.168266296386719,
      "learning_rate": 2.2518248175182482e-05,
      "loss": 3.2639,
      "step": 7530
    },
    {
      "epoch": 55.03649635036496,
      "grad_norm": 8.487994194030762,
      "learning_rate": 2.2481751824817517e-05,
      "loss": 3.6944,
      "step": 7540
    },
    {
      "epoch": 55.10948905109489,
      "grad_norm": 8.10265827178955,
      "learning_rate": 2.2445255474452556e-05,
      "loss": 3.3818,
      "step": 7550
    },
    {
      "epoch": 55.18248175182482,
      "grad_norm": 7.489218235015869,
      "learning_rate": 2.2408759124087592e-05,
      "loss": 3.643,
      "step": 7560
    },
    {
      "epoch": 55.25547445255474,
      "grad_norm": 8.146860122680664,
      "learning_rate": 2.237226277372263e-05,
      "loss": 3.5075,
      "step": 7570
    },
    {
      "epoch": 55.32846715328467,
      "grad_norm": 8.648726463317871,
      "learning_rate": 2.2335766423357666e-05,
      "loss": 3.3966,
      "step": 7580
    },
    {
      "epoch": 55.4014598540146,
      "grad_norm": 9.85315227508545,
      "learning_rate": 2.22992700729927e-05,
      "loss": 3.33,
      "step": 7590
    },
    {
      "epoch": 55.47445255474452,
      "grad_norm": 8.480706214904785,
      "learning_rate": 2.226277372262774e-05,
      "loss": 3.268,
      "step": 7600
    },
    {
      "epoch": 55.54744525547445,
      "grad_norm": 7.061861038208008,
      "learning_rate": 2.2226277372262776e-05,
      "loss": 3.3037,
      "step": 7610
    },
    {
      "epoch": 55.62043795620438,
      "grad_norm": 9.314593315124512,
      "learning_rate": 2.218978102189781e-05,
      "loss": 3.4284,
      "step": 7620
    },
    {
      "epoch": 55.693430656934304,
      "grad_norm": 8.177908897399902,
      "learning_rate": 2.2153284671532847e-05,
      "loss": 3.1941,
      "step": 7630
    },
    {
      "epoch": 55.76642335766423,
      "grad_norm": 7.6103925704956055,
      "learning_rate": 2.2116788321167882e-05,
      "loss": 3.1453,
      "step": 7640
    },
    {
      "epoch": 55.839416058394164,
      "grad_norm": 8.31942367553711,
      "learning_rate": 2.208029197080292e-05,
      "loss": 3.4558,
      "step": 7650
    },
    {
      "epoch": 55.91240875912409,
      "grad_norm": 7.890887260437012,
      "learning_rate": 2.2043795620437956e-05,
      "loss": 3.3097,
      "step": 7660
    },
    {
      "epoch": 55.98540145985402,
      "grad_norm": 8.255609512329102,
      "learning_rate": 2.200729927007299e-05,
      "loss": 3.5001,
      "step": 7670
    },
    {
      "epoch": 56.058394160583944,
      "grad_norm": 8.326403617858887,
      "learning_rate": 2.197080291970803e-05,
      "loss": 3.2053,
      "step": 7680
    },
    {
      "epoch": 56.13138686131387,
      "grad_norm": 7.754173755645752,
      "learning_rate": 2.1934306569343066e-05,
      "loss": 3.6547,
      "step": 7690
    },
    {
      "epoch": 56.2043795620438,
      "grad_norm": 8.278169631958008,
      "learning_rate": 2.1897810218978105e-05,
      "loss": 3.4245,
      "step": 7700
    },
    {
      "epoch": 56.277372262773724,
      "grad_norm": 7.966693878173828,
      "learning_rate": 2.186131386861314e-05,
      "loss": 3.461,
      "step": 7710
    },
    {
      "epoch": 56.35036496350365,
      "grad_norm": 8.01395034790039,
      "learning_rate": 2.1824817518248176e-05,
      "loss": 3.4976,
      "step": 7720
    },
    {
      "epoch": 56.42335766423358,
      "grad_norm": 8.313104629516602,
      "learning_rate": 2.1788321167883214e-05,
      "loss": 3.4318,
      "step": 7730
    },
    {
      "epoch": 56.496350364963504,
      "grad_norm": 8.776217460632324,
      "learning_rate": 2.175182481751825e-05,
      "loss": 3.3774,
      "step": 7740
    },
    {
      "epoch": 56.56934306569343,
      "grad_norm": 7.808816909790039,
      "learning_rate": 2.1715328467153285e-05,
      "loss": 3.4497,
      "step": 7750
    },
    {
      "epoch": 56.64233576642336,
      "grad_norm": 8.103089332580566,
      "learning_rate": 2.167883211678832e-05,
      "loss": 3.0013,
      "step": 7760
    },
    {
      "epoch": 56.715328467153284,
      "grad_norm": 8.65432357788086,
      "learning_rate": 2.1642335766423356e-05,
      "loss": 3.5014,
      "step": 7770
    },
    {
      "epoch": 56.78832116788321,
      "grad_norm": 7.529245376586914,
      "learning_rate": 2.1605839416058395e-05,
      "loss": 3.334,
      "step": 7780
    },
    {
      "epoch": 56.86131386861314,
      "grad_norm": 8.242671966552734,
      "learning_rate": 2.156934306569343e-05,
      "loss": 3.2642,
      "step": 7790
    },
    {
      "epoch": 56.934306569343065,
      "grad_norm": 9.819234848022461,
      "learning_rate": 2.1532846715328466e-05,
      "loss": 3.2974,
      "step": 7800
    },
    {
      "epoch": 57.00729927007299,
      "grad_norm": 7.2935028076171875,
      "learning_rate": 2.1496350364963505e-05,
      "loss": 3.2775,
      "step": 7810
    },
    {
      "epoch": 57.08029197080292,
      "grad_norm": 8.83671760559082,
      "learning_rate": 2.145985401459854e-05,
      "loss": 3.1732,
      "step": 7820
    },
    {
      "epoch": 57.153284671532845,
      "grad_norm": 8.17612075805664,
      "learning_rate": 2.142335766423358e-05,
      "loss": 3.0745,
      "step": 7830
    },
    {
      "epoch": 57.22627737226277,
      "grad_norm": 7.38718843460083,
      "learning_rate": 2.1386861313868614e-05,
      "loss": 3.5379,
      "step": 7840
    },
    {
      "epoch": 57.2992700729927,
      "grad_norm": 8.304351806640625,
      "learning_rate": 2.135036496350365e-05,
      "loss": 3.4801,
      "step": 7850
    },
    {
      "epoch": 57.372262773722625,
      "grad_norm": 7.701568126678467,
      "learning_rate": 2.131386861313869e-05,
      "loss": 3.2836,
      "step": 7860
    },
    {
      "epoch": 57.44525547445255,
      "grad_norm": 8.842419624328613,
      "learning_rate": 2.1277372262773724e-05,
      "loss": 3.3759,
      "step": 7870
    },
    {
      "epoch": 57.518248175182485,
      "grad_norm": 8.675475120544434,
      "learning_rate": 2.1240875912408763e-05,
      "loss": 3.2813,
      "step": 7880
    },
    {
      "epoch": 57.59124087591241,
      "grad_norm": 7.816008567810059,
      "learning_rate": 2.12043795620438e-05,
      "loss": 3.3374,
      "step": 7890
    },
    {
      "epoch": 57.66423357664234,
      "grad_norm": 7.806909561157227,
      "learning_rate": 2.1167883211678834e-05,
      "loss": 3.1578,
      "step": 7900
    },
    {
      "epoch": 57.737226277372265,
      "grad_norm": 6.573065757751465,
      "learning_rate": 2.113138686131387e-05,
      "loss": 3.6803,
      "step": 7910
    },
    {
      "epoch": 57.81021897810219,
      "grad_norm": 7.825007915496826,
      "learning_rate": 2.1094890510948905e-05,
      "loss": 3.37,
      "step": 7920
    },
    {
      "epoch": 57.88321167883212,
      "grad_norm": 8.919045448303223,
      "learning_rate": 2.105839416058394e-05,
      "loss": 3.39,
      "step": 7930
    },
    {
      "epoch": 57.956204379562045,
      "grad_norm": 7.690638542175293,
      "learning_rate": 2.102189781021898e-05,
      "loss": 3.2619,
      "step": 7940
    },
    {
      "epoch": 58.02919708029197,
      "grad_norm": 8.558355331420898,
      "learning_rate": 2.0985401459854014e-05,
      "loss": 3.5203,
      "step": 7950
    },
    {
      "epoch": 58.1021897810219,
      "grad_norm": 7.389588832855225,
      "learning_rate": 2.0948905109489053e-05,
      "loss": 3.4059,
      "step": 7960
    },
    {
      "epoch": 58.175182481751825,
      "grad_norm": 8.598287582397461,
      "learning_rate": 2.091240875912409e-05,
      "loss": 3.3434,
      "step": 7970
    },
    {
      "epoch": 58.24817518248175,
      "grad_norm": 8.23180866241455,
      "learning_rate": 2.0875912408759124e-05,
      "loss": 3.017,
      "step": 7980
    },
    {
      "epoch": 58.32116788321168,
      "grad_norm": 8.975979804992676,
      "learning_rate": 2.0839416058394163e-05,
      "loss": 3.0795,
      "step": 7990
    },
    {
      "epoch": 58.394160583941606,
      "grad_norm": 6.60477876663208,
      "learning_rate": 2.08029197080292e-05,
      "loss": 3.3931,
      "step": 8000
    },
    {
      "epoch": 58.46715328467153,
      "grad_norm": 8.55471420288086,
      "learning_rate": 2.0766423357664237e-05,
      "loss": 3.6233,
      "step": 8010
    },
    {
      "epoch": 58.54014598540146,
      "grad_norm": 8.073392868041992,
      "learning_rate": 2.0729927007299273e-05,
      "loss": 3.4383,
      "step": 8020
    },
    {
      "epoch": 58.613138686131386,
      "grad_norm": 7.22628927230835,
      "learning_rate": 2.0693430656934308e-05,
      "loss": 3.0943,
      "step": 8030
    },
    {
      "epoch": 58.68613138686131,
      "grad_norm": 8.675400733947754,
      "learning_rate": 2.0656934306569343e-05,
      "loss": 3.5475,
      "step": 8040
    },
    {
      "epoch": 58.75912408759124,
      "grad_norm": 8.055691719055176,
      "learning_rate": 2.062043795620438e-05,
      "loss": 3.288,
      "step": 8050
    },
    {
      "epoch": 58.832116788321166,
      "grad_norm": 8.533610343933105,
      "learning_rate": 2.0583941605839414e-05,
      "loss": 3.3264,
      "step": 8060
    },
    {
      "epoch": 58.90510948905109,
      "grad_norm": 8.037834167480469,
      "learning_rate": 2.0547445255474453e-05,
      "loss": 3.3615,
      "step": 8070
    },
    {
      "epoch": 58.97810218978102,
      "grad_norm": 8.350539207458496,
      "learning_rate": 2.051094890510949e-05,
      "loss": 3.2663,
      "step": 8080
    },
    {
      "epoch": 59.051094890510946,
      "grad_norm": 9.301389694213867,
      "learning_rate": 2.0474452554744527e-05,
      "loss": 3.3236,
      "step": 8090
    },
    {
      "epoch": 59.12408759124087,
      "grad_norm": 7.408793926239014,
      "learning_rate": 2.0437956204379563e-05,
      "loss": 3.1659,
      "step": 8100
    },
    {
      "epoch": 59.197080291970806,
      "grad_norm": 7.250572204589844,
      "learning_rate": 2.0401459854014598e-05,
      "loss": 3.1276,
      "step": 8110
    },
    {
      "epoch": 59.27007299270073,
      "grad_norm": 8.330832481384277,
      "learning_rate": 2.0364963503649637e-05,
      "loss": 3.3224,
      "step": 8120
    },
    {
      "epoch": 59.34306569343066,
      "grad_norm": 7.858590602874756,
      "learning_rate": 2.0328467153284673e-05,
      "loss": 3.2962,
      "step": 8130
    },
    {
      "epoch": 59.416058394160586,
      "grad_norm": 8.496456146240234,
      "learning_rate": 2.029197080291971e-05,
      "loss": 3.2837,
      "step": 8140
    },
    {
      "epoch": 59.48905109489051,
      "grad_norm": 7.99558162689209,
      "learning_rate": 2.0255474452554747e-05,
      "loss": 3.343,
      "step": 8150
    },
    {
      "epoch": 59.56204379562044,
      "grad_norm": 8.433855056762695,
      "learning_rate": 2.0218978102189782e-05,
      "loss": 3.3138,
      "step": 8160
    },
    {
      "epoch": 59.63503649635037,
      "grad_norm": 7.805866718292236,
      "learning_rate": 2.0182481751824818e-05,
      "loss": 3.2429,
      "step": 8170
    },
    {
      "epoch": 59.70802919708029,
      "grad_norm": 6.6203837394714355,
      "learning_rate": 2.0145985401459853e-05,
      "loss": 3.1798,
      "step": 8180
    },
    {
      "epoch": 59.78102189781022,
      "grad_norm": 8.157658576965332,
      "learning_rate": 2.010948905109489e-05,
      "loss": 3.6085,
      "step": 8190
    },
    {
      "epoch": 59.85401459854015,
      "grad_norm": 8.214692115783691,
      "learning_rate": 2.0072992700729927e-05,
      "loss": 3.3408,
      "step": 8200
    },
    {
      "epoch": 59.92700729927007,
      "grad_norm": 6.989428997039795,
      "learning_rate": 2.0036496350364963e-05,
      "loss": 3.3093,
      "step": 8210
    },
    {
      "epoch": 60.0,
      "grad_norm": 11.938752174377441,
      "learning_rate": 2e-05,
      "loss": 3.449,
      "step": 8220
    },
    {
      "epoch": 60.07299270072993,
      "grad_norm": 8.197775840759277,
      "learning_rate": 1.9963503649635037e-05,
      "loss": 3.2174,
      "step": 8230
    },
    {
      "epoch": 60.14598540145985,
      "grad_norm": 8.126640319824219,
      "learning_rate": 1.9927007299270073e-05,
      "loss": 3.4105,
      "step": 8240
    },
    {
      "epoch": 60.21897810218978,
      "grad_norm": 9.153376579284668,
      "learning_rate": 1.989051094890511e-05,
      "loss": 3.226,
      "step": 8250
    },
    {
      "epoch": 60.29197080291971,
      "grad_norm": 8.166413307189941,
      "learning_rate": 1.9854014598540147e-05,
      "loss": 3.1445,
      "step": 8260
    },
    {
      "epoch": 60.36496350364963,
      "grad_norm": 8.079069137573242,
      "learning_rate": 1.9817518248175186e-05,
      "loss": 2.9344,
      "step": 8270
    },
    {
      "epoch": 60.43795620437956,
      "grad_norm": 8.361371040344238,
      "learning_rate": 1.978102189781022e-05,
      "loss": 3.3402,
      "step": 8280
    },
    {
      "epoch": 60.51094890510949,
      "grad_norm": 8.583609580993652,
      "learning_rate": 1.9744525547445256e-05,
      "loss": 3.6384,
      "step": 8290
    },
    {
      "epoch": 60.583941605839414,
      "grad_norm": 7.407052040100098,
      "learning_rate": 1.9708029197080295e-05,
      "loss": 3.3708,
      "step": 8300
    },
    {
      "epoch": 60.65693430656934,
      "grad_norm": 8.284639358520508,
      "learning_rate": 1.967153284671533e-05,
      "loss": 3.1831,
      "step": 8310
    },
    {
      "epoch": 60.72992700729927,
      "grad_norm": 8.780817031860352,
      "learning_rate": 1.9635036496350366e-05,
      "loss": 3.5191,
      "step": 8320
    },
    {
      "epoch": 60.802919708029194,
      "grad_norm": 9.049874305725098,
      "learning_rate": 1.95985401459854e-05,
      "loss": 3.4184,
      "step": 8330
    },
    {
      "epoch": 60.87591240875913,
      "grad_norm": 6.54339075088501,
      "learning_rate": 1.9562043795620437e-05,
      "loss": 3.1308,
      "step": 8340
    },
    {
      "epoch": 60.948905109489054,
      "grad_norm": 8.58963394165039,
      "learning_rate": 1.9525547445255476e-05,
      "loss": 3.3454,
      "step": 8350
    },
    {
      "epoch": 61.02189781021898,
      "grad_norm": 7.6106977462768555,
      "learning_rate": 1.948905109489051e-05,
      "loss": 3.3297,
      "step": 8360
    },
    {
      "epoch": 61.09489051094891,
      "grad_norm": 7.108867645263672,
      "learning_rate": 1.9452554744525547e-05,
      "loss": 3.2947,
      "step": 8370
    },
    {
      "epoch": 61.167883211678834,
      "grad_norm": 7.742537498474121,
      "learning_rate": 1.9416058394160586e-05,
      "loss": 3.4489,
      "step": 8380
    },
    {
      "epoch": 61.24087591240876,
      "grad_norm": 7.889101028442383,
      "learning_rate": 1.937956204379562e-05,
      "loss": 3.2282,
      "step": 8390
    },
    {
      "epoch": 61.31386861313869,
      "grad_norm": 9.537635803222656,
      "learning_rate": 1.934306569343066e-05,
      "loss": 3.2664,
      "step": 8400
    },
    {
      "epoch": 61.386861313868614,
      "grad_norm": 6.459443092346191,
      "learning_rate": 1.9306569343065695e-05,
      "loss": 3.1932,
      "step": 8410
    },
    {
      "epoch": 61.45985401459854,
      "grad_norm": 8.37774658203125,
      "learning_rate": 1.927007299270073e-05,
      "loss": 3.6602,
      "step": 8420
    },
    {
      "epoch": 61.53284671532847,
      "grad_norm": 8.357449531555176,
      "learning_rate": 1.923357664233577e-05,
      "loss": 3.0483,
      "step": 8430
    },
    {
      "epoch": 61.605839416058394,
      "grad_norm": 8.822393417358398,
      "learning_rate": 1.9197080291970805e-05,
      "loss": 3.1907,
      "step": 8440
    },
    {
      "epoch": 61.67883211678832,
      "grad_norm": 8.889200210571289,
      "learning_rate": 1.916058394160584e-05,
      "loss": 3.535,
      "step": 8450
    },
    {
      "epoch": 61.75182481751825,
      "grad_norm": 6.841025352478027,
      "learning_rate": 1.9124087591240876e-05,
      "loss": 3.1792,
      "step": 8460
    },
    {
      "epoch": 61.824817518248175,
      "grad_norm": 6.705099105834961,
      "learning_rate": 1.908759124087591e-05,
      "loss": 3.0783,
      "step": 8470
    },
    {
      "epoch": 61.8978102189781,
      "grad_norm": 8.255151748657227,
      "learning_rate": 1.905109489051095e-05,
      "loss": 3.2459,
      "step": 8480
    },
    {
      "epoch": 61.97080291970803,
      "grad_norm": 8.086745262145996,
      "learning_rate": 1.9014598540145986e-05,
      "loss": 3.2115,
      "step": 8490
    },
    {
      "epoch": 62.043795620437955,
      "grad_norm": 8.515350341796875,
      "learning_rate": 1.897810218978102e-05,
      "loss": 3.0627,
      "step": 8500
    },
    {
      "epoch": 62.11678832116788,
      "grad_norm": 8.761509895324707,
      "learning_rate": 1.894160583941606e-05,
      "loss": 3.2384,
      "step": 8510
    },
    {
      "epoch": 62.18978102189781,
      "grad_norm": 8.305510520935059,
      "learning_rate": 1.8905109489051095e-05,
      "loss": 3.09,
      "step": 8520
    },
    {
      "epoch": 62.262773722627735,
      "grad_norm": 8.347512245178223,
      "learning_rate": 1.8868613138686134e-05,
      "loss": 3.5245,
      "step": 8530
    },
    {
      "epoch": 62.33576642335766,
      "grad_norm": 9.048562049865723,
      "learning_rate": 1.883211678832117e-05,
      "loss": 3.3081,
      "step": 8540
    },
    {
      "epoch": 62.40875912408759,
      "grad_norm": 8.557610511779785,
      "learning_rate": 1.8795620437956205e-05,
      "loss": 3.1704,
      "step": 8550
    },
    {
      "epoch": 62.481751824817515,
      "grad_norm": 8.400361061096191,
      "learning_rate": 1.8759124087591244e-05,
      "loss": 3.1121,
      "step": 8560
    },
    {
      "epoch": 62.55474452554745,
      "grad_norm": 8.635299682617188,
      "learning_rate": 1.872262773722628e-05,
      "loss": 3.4595,
      "step": 8570
    },
    {
      "epoch": 62.627737226277375,
      "grad_norm": 7.719759941101074,
      "learning_rate": 1.8686131386861315e-05,
      "loss": 3.2524,
      "step": 8580
    },
    {
      "epoch": 62.7007299270073,
      "grad_norm": 8.417027473449707,
      "learning_rate": 1.8649635036496353e-05,
      "loss": 3.2081,
      "step": 8590
    },
    {
      "epoch": 62.77372262773723,
      "grad_norm": 7.687353134155273,
      "learning_rate": 1.861313868613139e-05,
      "loss": 3.4319,
      "step": 8600
    },
    {
      "epoch": 62.846715328467155,
      "grad_norm": 9.209714889526367,
      "learning_rate": 1.8576642335766424e-05,
      "loss": 3.6927,
      "step": 8610
    },
    {
      "epoch": 62.91970802919708,
      "grad_norm": 8.342525482177734,
      "learning_rate": 1.854014598540146e-05,
      "loss": 3.1475,
      "step": 8620
    },
    {
      "epoch": 62.99270072992701,
      "grad_norm": 6.877465724945068,
      "learning_rate": 1.8503649635036495e-05,
      "loss": 3.1674,
      "step": 8630
    },
    {
      "epoch": 63.065693430656935,
      "grad_norm": 8.976770401000977,
      "learning_rate": 1.8467153284671534e-05,
      "loss": 3.2317,
      "step": 8640
    },
    {
      "epoch": 63.13868613138686,
      "grad_norm": 8.101214408874512,
      "learning_rate": 1.843065693430657e-05,
      "loss": 3.5727,
      "step": 8650
    },
    {
      "epoch": 63.21167883211679,
      "grad_norm": 8.465210914611816,
      "learning_rate": 1.8394160583941605e-05,
      "loss": 2.9753,
      "step": 8660
    },
    {
      "epoch": 63.284671532846716,
      "grad_norm": 8.372601509094238,
      "learning_rate": 1.8357664233576644e-05,
      "loss": 3.2259,
      "step": 8670
    },
    {
      "epoch": 63.35766423357664,
      "grad_norm": 7.496294021606445,
      "learning_rate": 1.832116788321168e-05,
      "loss": 3.2806,
      "step": 8680
    },
    {
      "epoch": 63.43065693430657,
      "grad_norm": 7.828521251678467,
      "learning_rate": 1.8284671532846718e-05,
      "loss": 2.9937,
      "step": 8690
    },
    {
      "epoch": 63.503649635036496,
      "grad_norm": 9.468475341796875,
      "learning_rate": 1.8248175182481753e-05,
      "loss": 3.2106,
      "step": 8700
    },
    {
      "epoch": 63.57664233576642,
      "grad_norm": 8.916985511779785,
      "learning_rate": 1.821167883211679e-05,
      "loss": 3.4651,
      "step": 8710
    },
    {
      "epoch": 63.64963503649635,
      "grad_norm": 9.545893669128418,
      "learning_rate": 1.8175182481751828e-05,
      "loss": 3.3563,
      "step": 8720
    },
    {
      "epoch": 63.722627737226276,
      "grad_norm": 7.718785762786865,
      "learning_rate": 1.8138686131386863e-05,
      "loss": 3.4298,
      "step": 8730
    },
    {
      "epoch": 63.7956204379562,
      "grad_norm": 7.557237148284912,
      "learning_rate": 1.81021897810219e-05,
      "loss": 2.9232,
      "step": 8740
    },
    {
      "epoch": 63.86861313868613,
      "grad_norm": 9.737364768981934,
      "learning_rate": 1.8065693430656934e-05,
      "loss": 3.506,
      "step": 8750
    },
    {
      "epoch": 63.941605839416056,
      "grad_norm": 7.937217712402344,
      "learning_rate": 1.802919708029197e-05,
      "loss": 3.2227,
      "step": 8760
    },
    {
      "epoch": 64.01459854014598,
      "grad_norm": 6.817962169647217,
      "learning_rate": 1.7992700729927008e-05,
      "loss": 3.4178,
      "step": 8770
    },
    {
      "epoch": 64.08759124087591,
      "grad_norm": 8.076855659484863,
      "learning_rate": 1.7956204379562044e-05,
      "loss": 3.4483,
      "step": 8780
    },
    {
      "epoch": 64.16058394160584,
      "grad_norm": 7.803386211395264,
      "learning_rate": 1.791970802919708e-05,
      "loss": 3.3693,
      "step": 8790
    },
    {
      "epoch": 64.23357664233576,
      "grad_norm": 8.455836296081543,
      "learning_rate": 1.7883211678832118e-05,
      "loss": 3.2082,
      "step": 8800
    },
    {
      "epoch": 64.30656934306569,
      "grad_norm": 7.955556869506836,
      "learning_rate": 1.7846715328467153e-05,
      "loss": 3.1532,
      "step": 8810
    },
    {
      "epoch": 64.37956204379562,
      "grad_norm": 9.555113792419434,
      "learning_rate": 1.7810218978102192e-05,
      "loss": 3.4338,
      "step": 8820
    },
    {
      "epoch": 64.45255474452554,
      "grad_norm": 8.508180618286133,
      "learning_rate": 1.7773722627737228e-05,
      "loss": 3.056,
      "step": 8830
    },
    {
      "epoch": 64.52554744525547,
      "grad_norm": 7.78057336807251,
      "learning_rate": 1.7737226277372263e-05,
      "loss": 3.3343,
      "step": 8840
    },
    {
      "epoch": 64.5985401459854,
      "grad_norm": 7.736941814422607,
      "learning_rate": 1.7700729927007302e-05,
      "loss": 3.2617,
      "step": 8850
    },
    {
      "epoch": 64.67153284671532,
      "grad_norm": 7.937233924865723,
      "learning_rate": 1.7664233576642337e-05,
      "loss": 3.426,
      "step": 8860
    },
    {
      "epoch": 64.74452554744525,
      "grad_norm": 8.263364791870117,
      "learning_rate": 1.7627737226277373e-05,
      "loss": 3.0916,
      "step": 8870
    },
    {
      "epoch": 64.81751824817518,
      "grad_norm": 8.950098037719727,
      "learning_rate": 1.7591240875912408e-05,
      "loss": 3.6977,
      "step": 8880
    },
    {
      "epoch": 64.8905109489051,
      "grad_norm": 8.358551025390625,
      "learning_rate": 1.7554744525547444e-05,
      "loss": 2.8932,
      "step": 8890
    },
    {
      "epoch": 64.96350364963503,
      "grad_norm": 7.84201192855835,
      "learning_rate": 1.7518248175182482e-05,
      "loss": 2.9625,
      "step": 8900
    },
    {
      "epoch": 65.03649635036497,
      "grad_norm": 7.996252536773682,
      "learning_rate": 1.7481751824817518e-05,
      "loss": 3.4258,
      "step": 8910
    },
    {
      "epoch": 65.1094890510949,
      "grad_norm": 6.498648643493652,
      "learning_rate": 1.7445255474452553e-05,
      "loss": 3.3799,
      "step": 8920
    },
    {
      "epoch": 65.18248175182482,
      "grad_norm": 8.259618759155273,
      "learning_rate": 1.7408759124087592e-05,
      "loss": 3.4082,
      "step": 8930
    },
    {
      "epoch": 65.25547445255475,
      "grad_norm": 8.770793914794922,
      "learning_rate": 1.7372262773722628e-05,
      "loss": 3.1051,
      "step": 8940
    },
    {
      "epoch": 65.32846715328468,
      "grad_norm": 8.147664070129395,
      "learning_rate": 1.7335766423357666e-05,
      "loss": 3.245,
      "step": 8950
    },
    {
      "epoch": 65.4014598540146,
      "grad_norm": 7.902184009552002,
      "learning_rate": 1.7299270072992702e-05,
      "loss": 3.1192,
      "step": 8960
    },
    {
      "epoch": 65.47445255474453,
      "grad_norm": 8.074464797973633,
      "learning_rate": 1.7262773722627737e-05,
      "loss": 3.3999,
      "step": 8970
    },
    {
      "epoch": 65.54744525547446,
      "grad_norm": 7.7280192375183105,
      "learning_rate": 1.7226277372262776e-05,
      "loss": 3.0279,
      "step": 8980
    },
    {
      "epoch": 65.62043795620438,
      "grad_norm": 7.731208324432373,
      "learning_rate": 1.718978102189781e-05,
      "loss": 3.4481,
      "step": 8990
    },
    {
      "epoch": 65.69343065693431,
      "grad_norm": 8.286343574523926,
      "learning_rate": 1.715328467153285e-05,
      "loss": 3.3235,
      "step": 9000
    },
    {
      "epoch": 65.76642335766424,
      "grad_norm": 8.446731567382812,
      "learning_rate": 1.7116788321167886e-05,
      "loss": 3.0899,
      "step": 9010
    },
    {
      "epoch": 65.83941605839416,
      "grad_norm": 8.233607292175293,
      "learning_rate": 1.708029197080292e-05,
      "loss": 3.4801,
      "step": 9020
    },
    {
      "epoch": 65.91240875912409,
      "grad_norm": 8.067926406860352,
      "learning_rate": 1.7043795620437957e-05,
      "loss": 3.0931,
      "step": 9030
    },
    {
      "epoch": 65.98540145985402,
      "grad_norm": 7.765594959259033,
      "learning_rate": 1.7007299270072992e-05,
      "loss": 2.9866,
      "step": 9040
    },
    {
      "epoch": 66.05839416058394,
      "grad_norm": 7.812912464141846,
      "learning_rate": 1.6970802919708028e-05,
      "loss": 3.1858,
      "step": 9050
    },
    {
      "epoch": 66.13138686131387,
      "grad_norm": 8.648848533630371,
      "learning_rate": 1.6934306569343066e-05,
      "loss": 3.1005,
      "step": 9060
    },
    {
      "epoch": 66.2043795620438,
      "grad_norm": 7.967554569244385,
      "learning_rate": 1.6897810218978102e-05,
      "loss": 3.0883,
      "step": 9070
    },
    {
      "epoch": 66.27737226277372,
      "grad_norm": 8.007014274597168,
      "learning_rate": 1.686131386861314e-05,
      "loss": 3.1499,
      "step": 9080
    },
    {
      "epoch": 66.35036496350365,
      "grad_norm": 9.230464935302734,
      "learning_rate": 1.6824817518248176e-05,
      "loss": 3.2254,
      "step": 9090
    },
    {
      "epoch": 66.42335766423358,
      "grad_norm": 9.584505081176758,
      "learning_rate": 1.678832116788321e-05,
      "loss": 3.2094,
      "step": 9100
    },
    {
      "epoch": 66.4963503649635,
      "grad_norm": 8.089061737060547,
      "learning_rate": 1.675182481751825e-05,
      "loss": 3.2151,
      "step": 9110
    },
    {
      "epoch": 66.56934306569343,
      "grad_norm": 8.804558753967285,
      "learning_rate": 1.6715328467153286e-05,
      "loss": 3.2637,
      "step": 9120
    },
    {
      "epoch": 66.64233576642336,
      "grad_norm": 8.837138175964355,
      "learning_rate": 1.6678832116788325e-05,
      "loss": 3.3513,
      "step": 9130
    },
    {
      "epoch": 66.71532846715328,
      "grad_norm": 8.2321138381958,
      "learning_rate": 1.664233576642336e-05,
      "loss": 3.1963,
      "step": 9140
    },
    {
      "epoch": 66.78832116788321,
      "grad_norm": 7.477272987365723,
      "learning_rate": 1.6605839416058395e-05,
      "loss": 3.2508,
      "step": 9150
    },
    {
      "epoch": 66.86131386861314,
      "grad_norm": 8.390877723693848,
      "learning_rate": 1.656934306569343e-05,
      "loss": 3.2647,
      "step": 9160
    },
    {
      "epoch": 66.93430656934306,
      "grad_norm": 9.392843246459961,
      "learning_rate": 1.6532846715328466e-05,
      "loss": 3.1117,
      "step": 9170
    },
    {
      "epoch": 67.00729927007299,
      "grad_norm": 8.19734001159668,
      "learning_rate": 1.6496350364963502e-05,
      "loss": 3.4305,
      "step": 9180
    },
    {
      "epoch": 67.08029197080292,
      "grad_norm": 8.0506591796875,
      "learning_rate": 1.645985401459854e-05,
      "loss": 3.3456,
      "step": 9190
    },
    {
      "epoch": 67.15328467153284,
      "grad_norm": 8.411321640014648,
      "learning_rate": 1.6423357664233576e-05,
      "loss": 3.0911,
      "step": 9200
    },
    {
      "epoch": 67.22627737226277,
      "grad_norm": 8.864967346191406,
      "learning_rate": 1.6386861313868615e-05,
      "loss": 2.91,
      "step": 9210
    },
    {
      "epoch": 67.2992700729927,
      "grad_norm": 9.089869499206543,
      "learning_rate": 1.635036496350365e-05,
      "loss": 3.4068,
      "step": 9220
    },
    {
      "epoch": 67.37226277372262,
      "grad_norm": 7.929286479949951,
      "learning_rate": 1.6313868613138686e-05,
      "loss": 3.3379,
      "step": 9230
    },
    {
      "epoch": 67.44525547445255,
      "grad_norm": 9.130585670471191,
      "learning_rate": 1.6277372262773725e-05,
      "loss": 3.3016,
      "step": 9240
    },
    {
      "epoch": 67.51824817518248,
      "grad_norm": 9.59737491607666,
      "learning_rate": 1.624087591240876e-05,
      "loss": 3.1617,
      "step": 9250
    },
    {
      "epoch": 67.5912408759124,
      "grad_norm": 8.891002655029297,
      "learning_rate": 1.62043795620438e-05,
      "loss": 3.1366,
      "step": 9260
    },
    {
      "epoch": 67.66423357664233,
      "grad_norm": 7.986046314239502,
      "learning_rate": 1.6167883211678834e-05,
      "loss": 3.2386,
      "step": 9270
    },
    {
      "epoch": 67.73722627737226,
      "grad_norm": 9.15904426574707,
      "learning_rate": 1.613138686131387e-05,
      "loss": 3.255,
      "step": 9280
    },
    {
      "epoch": 67.81021897810218,
      "grad_norm": 8.333683967590332,
      "learning_rate": 1.6094890510948905e-05,
      "loss": 3.1561,
      "step": 9290
    },
    {
      "epoch": 67.88321167883211,
      "grad_norm": 7.823874473571777,
      "learning_rate": 1.605839416058394e-05,
      "loss": 2.8632,
      "step": 9300
    },
    {
      "epoch": 67.95620437956204,
      "grad_norm": 8.12868881225586,
      "learning_rate": 1.6021897810218976e-05,
      "loss": 3.5373,
      "step": 9310
    },
    {
      "epoch": 68.02919708029196,
      "grad_norm": 6.942448139190674,
      "learning_rate": 1.5985401459854015e-05,
      "loss": 3.3636,
      "step": 9320
    },
    {
      "epoch": 68.10218978102189,
      "grad_norm": 8.125059127807617,
      "learning_rate": 1.594890510948905e-05,
      "loss": 3.1188,
      "step": 9330
    },
    {
      "epoch": 68.17518248175182,
      "grad_norm": 7.465132713317871,
      "learning_rate": 1.591240875912409e-05,
      "loss": 3.0942,
      "step": 9340
    },
    {
      "epoch": 68.24817518248175,
      "grad_norm": 7.100604057312012,
      "learning_rate": 1.5875912408759125e-05,
      "loss": 2.9872,
      "step": 9350
    },
    {
      "epoch": 68.32116788321167,
      "grad_norm": 11.50788402557373,
      "learning_rate": 1.583941605839416e-05,
      "loss": 3.2026,
      "step": 9360
    },
    {
      "epoch": 68.39416058394161,
      "grad_norm": 9.448953628540039,
      "learning_rate": 1.58029197080292e-05,
      "loss": 3.5453,
      "step": 9370
    },
    {
      "epoch": 68.46715328467154,
      "grad_norm": 8.057597160339355,
      "learning_rate": 1.5766423357664234e-05,
      "loss": 3.5227,
      "step": 9380
    },
    {
      "epoch": 68.54014598540147,
      "grad_norm": 8.650731086730957,
      "learning_rate": 1.5729927007299273e-05,
      "loss": 3.5135,
      "step": 9390
    },
    {
      "epoch": 68.61313868613139,
      "grad_norm": 7.381697177886963,
      "learning_rate": 1.569343065693431e-05,
      "loss": 3.1924,
      "step": 9400
    },
    {
      "epoch": 68.68613138686132,
      "grad_norm": 7.8919267654418945,
      "learning_rate": 1.5656934306569344e-05,
      "loss": 3.0938,
      "step": 9410
    },
    {
      "epoch": 68.75912408759125,
      "grad_norm": 7.2083234786987305,
      "learning_rate": 1.5620437956204383e-05,
      "loss": 3.0255,
      "step": 9420
    },
    {
      "epoch": 68.83211678832117,
      "grad_norm": 9.306270599365234,
      "learning_rate": 1.5583941605839418e-05,
      "loss": 3.2065,
      "step": 9430
    },
    {
      "epoch": 68.9051094890511,
      "grad_norm": 8.527395248413086,
      "learning_rate": 1.5547445255474454e-05,
      "loss": 3.3087,
      "step": 9440
    },
    {
      "epoch": 68.97810218978103,
      "grad_norm": 9.168340682983398,
      "learning_rate": 1.551094890510949e-05,
      "loss": 2.9571,
      "step": 9450
    },
    {
      "epoch": 69.05109489051095,
      "grad_norm": 8.441242218017578,
      "learning_rate": 1.5474452554744524e-05,
      "loss": 3.1662,
      "step": 9460
    },
    {
      "epoch": 69.12408759124088,
      "grad_norm": 8.072664260864258,
      "learning_rate": 1.5437956204379563e-05,
      "loss": 3.1941,
      "step": 9470
    },
    {
      "epoch": 69.1970802919708,
      "grad_norm": 8.738810539245605,
      "learning_rate": 1.54014598540146e-05,
      "loss": 3.3212,
      "step": 9480
    },
    {
      "epoch": 69.27007299270073,
      "grad_norm": 7.213425636291504,
      "learning_rate": 1.5364963503649634e-05,
      "loss": 3.2209,
      "step": 9490
    },
    {
      "epoch": 69.34306569343066,
      "grad_norm": 7.712834358215332,
      "learning_rate": 1.5328467153284673e-05,
      "loss": 3.0589,
      "step": 9500
    },
    {
      "epoch": 69.41605839416059,
      "grad_norm": 8.081045150756836,
      "learning_rate": 1.529197080291971e-05,
      "loss": 3.0566,
      "step": 9510
    },
    {
      "epoch": 69.48905109489051,
      "grad_norm": 8.74109172821045,
      "learning_rate": 1.5255474452554747e-05,
      "loss": 3.188,
      "step": 9520
    },
    {
      "epoch": 69.56204379562044,
      "grad_norm": 8.444268226623535,
      "learning_rate": 1.5218978102189783e-05,
      "loss": 3.5085,
      "step": 9530
    },
    {
      "epoch": 69.63503649635037,
      "grad_norm": 8.53330135345459,
      "learning_rate": 1.5182481751824818e-05,
      "loss": 3.3153,
      "step": 9540
    },
    {
      "epoch": 69.7080291970803,
      "grad_norm": 8.347549438476562,
      "learning_rate": 1.5145985401459855e-05,
      "loss": 3.1454,
      "step": 9550
    },
    {
      "epoch": 69.78102189781022,
      "grad_norm": 7.302128314971924,
      "learning_rate": 1.510948905109489e-05,
      "loss": 3.25,
      "step": 9560
    },
    {
      "epoch": 69.85401459854015,
      "grad_norm": 8.043061256408691,
      "learning_rate": 1.5072992700729926e-05,
      "loss": 3.2874,
      "step": 9570
    },
    {
      "epoch": 69.92700729927007,
      "grad_norm": 6.739607810974121,
      "learning_rate": 1.5036496350364965e-05,
      "loss": 2.9884,
      "step": 9580
    },
    {
      "epoch": 70.0,
      "grad_norm": 15.88250732421875,
      "learning_rate": 1.5e-05,
      "loss": 3.4762,
      "step": 9590
    },
    {
      "epoch": 70.07299270072993,
      "grad_norm": 8.472956657409668,
      "learning_rate": 1.496350364963504e-05,
      "loss": 3.2077,
      "step": 9600
    },
    {
      "epoch": 70.14598540145985,
      "grad_norm": 7.351100444793701,
      "learning_rate": 1.4927007299270075e-05,
      "loss": 3.1412,
      "step": 9610
    },
    {
      "epoch": 70.21897810218978,
      "grad_norm": 8.435386657714844,
      "learning_rate": 1.4890510948905108e-05,
      "loss": 2.8128,
      "step": 9620
    },
    {
      "epoch": 70.2919708029197,
      "grad_norm": 8.366180419921875,
      "learning_rate": 1.4854014598540147e-05,
      "loss": 3.1986,
      "step": 9630
    },
    {
      "epoch": 70.36496350364963,
      "grad_norm": 8.27025032043457,
      "learning_rate": 1.4817518248175183e-05,
      "loss": 3.0207,
      "step": 9640
    },
    {
      "epoch": 70.43795620437956,
      "grad_norm": 9.579460144042969,
      "learning_rate": 1.4781021897810221e-05,
      "loss": 3.2559,
      "step": 9650
    },
    {
      "epoch": 70.51094890510949,
      "grad_norm": 9.998053550720215,
      "learning_rate": 1.4744525547445257e-05,
      "loss": 3.4825,
      "step": 9660
    },
    {
      "epoch": 70.58394160583941,
      "grad_norm": 7.679966926574707,
      "learning_rate": 1.4708029197080292e-05,
      "loss": 2.9734,
      "step": 9670
    },
    {
      "epoch": 70.65693430656934,
      "grad_norm": 8.203052520751953,
      "learning_rate": 1.467153284671533e-05,
      "loss": 3.2691,
      "step": 9680
    },
    {
      "epoch": 70.72992700729927,
      "grad_norm": 8.040369987487793,
      "learning_rate": 1.4635036496350365e-05,
      "loss": 3.0733,
      "step": 9690
    },
    {
      "epoch": 70.8029197080292,
      "grad_norm": 8.351529121398926,
      "learning_rate": 1.45985401459854e-05,
      "loss": 3.0724,
      "step": 9700
    },
    {
      "epoch": 70.87591240875912,
      "grad_norm": 8.335641860961914,
      "learning_rate": 1.456204379562044e-05,
      "loss": 3.3514,
      "step": 9710
    },
    {
      "epoch": 70.94890510948905,
      "grad_norm": 9.030468940734863,
      "learning_rate": 1.4525547445255475e-05,
      "loss": 3.5415,
      "step": 9720
    },
    {
      "epoch": 71.02189781021897,
      "grad_norm": 7.76982307434082,
      "learning_rate": 1.4489051094890513e-05,
      "loss": 2.8963,
      "step": 9730
    },
    {
      "epoch": 71.0948905109489,
      "grad_norm": 9.956693649291992,
      "learning_rate": 1.4452554744525549e-05,
      "loss": 3.4724,
      "step": 9740
    },
    {
      "epoch": 71.16788321167883,
      "grad_norm": 8.213247299194336,
      "learning_rate": 1.4416058394160584e-05,
      "loss": 3.2506,
      "step": 9750
    },
    {
      "epoch": 71.24087591240875,
      "grad_norm": 7.811453819274902,
      "learning_rate": 1.4379562043795621e-05,
      "loss": 3.0395,
      "step": 9760
    },
    {
      "epoch": 71.31386861313868,
      "grad_norm": 9.13078784942627,
      "learning_rate": 1.4343065693430657e-05,
      "loss": 3.3114,
      "step": 9770
    },
    {
      "epoch": 71.38686131386861,
      "grad_norm": 10.483742713928223,
      "learning_rate": 1.4306569343065692e-05,
      "loss": 3.2502,
      "step": 9780
    },
    {
      "epoch": 71.45985401459853,
      "grad_norm": 7.053420066833496,
      "learning_rate": 1.4270072992700731e-05,
      "loss": 3.3074,
      "step": 9790
    },
    {
      "epoch": 71.53284671532846,
      "grad_norm": 7.676218032836914,
      "learning_rate": 1.4233576642335767e-05,
      "loss": 3.1432,
      "step": 9800
    },
    {
      "epoch": 71.60583941605839,
      "grad_norm": 9.790167808532715,
      "learning_rate": 1.4197080291970805e-05,
      "loss": 3.2697,
      "step": 9810
    },
    {
      "epoch": 71.67883211678833,
      "grad_norm": 8.383313179016113,
      "learning_rate": 1.416058394160584e-05,
      "loss": 3.1333,
      "step": 9820
    },
    {
      "epoch": 71.75182481751825,
      "grad_norm": 8.283195495605469,
      "learning_rate": 1.4124087591240876e-05,
      "loss": 3.2185,
      "step": 9830
    },
    {
      "epoch": 71.82481751824818,
      "grad_norm": 11.392265319824219,
      "learning_rate": 1.4087591240875913e-05,
      "loss": 3.2216,
      "step": 9840
    },
    {
      "epoch": 71.89781021897811,
      "grad_norm": 10.034920692443848,
      "learning_rate": 1.4051094890510949e-05,
      "loss": 3.0424,
      "step": 9850
    },
    {
      "epoch": 71.97080291970804,
      "grad_norm": 7.89932918548584,
      "learning_rate": 1.4014598540145988e-05,
      "loss": 2.8323,
      "step": 9860
    },
    {
      "epoch": 72.04379562043796,
      "grad_norm": 8.394793510437012,
      "learning_rate": 1.3978102189781023e-05,
      "loss": 3.0063,
      "step": 9870
    },
    {
      "epoch": 72.11678832116789,
      "grad_norm": 8.696660041809082,
      "learning_rate": 1.3941605839416059e-05,
      "loss": 3.0992,
      "step": 9880
    },
    {
      "epoch": 72.18978102189782,
      "grad_norm": 8.455552101135254,
      "learning_rate": 1.3905109489051096e-05,
      "loss": 3.327,
      "step": 9890
    },
    {
      "epoch": 72.26277372262774,
      "grad_norm": 9.243514060974121,
      "learning_rate": 1.3868613138686131e-05,
      "loss": 3.3147,
      "step": 9900
    },
    {
      "epoch": 72.33576642335767,
      "grad_norm": 10.487627983093262,
      "learning_rate": 1.3832116788321167e-05,
      "loss": 3.2117,
      "step": 9910
    },
    {
      "epoch": 72.4087591240876,
      "grad_norm": 7.734048843383789,
      "learning_rate": 1.3795620437956205e-05,
      "loss": 3.2448,
      "step": 9920
    },
    {
      "epoch": 72.48175182481752,
      "grad_norm": 7.873833179473877,
      "learning_rate": 1.375912408759124e-05,
      "loss": 3.2062,
      "step": 9930
    },
    {
      "epoch": 72.55474452554745,
      "grad_norm": 8.053908348083496,
      "learning_rate": 1.372262773722628e-05,
      "loss": 3.2253,
      "step": 9940
    },
    {
      "epoch": 72.62773722627738,
      "grad_norm": 7.392707347869873,
      "learning_rate": 1.3686131386861315e-05,
      "loss": 3.2211,
      "step": 9950
    },
    {
      "epoch": 72.7007299270073,
      "grad_norm": 8.784921646118164,
      "learning_rate": 1.364963503649635e-05,
      "loss": 2.7299,
      "step": 9960
    },
    {
      "epoch": 72.77372262773723,
      "grad_norm": 8.351558685302734,
      "learning_rate": 1.3613138686131388e-05,
      "loss": 3.205,
      "step": 9970
    },
    {
      "epoch": 72.84671532846716,
      "grad_norm": 8.323537826538086,
      "learning_rate": 1.3576642335766423e-05,
      "loss": 2.7328,
      "step": 9980
    },
    {
      "epoch": 72.91970802919708,
      "grad_norm": 7.3974738121032715,
      "learning_rate": 1.3540145985401462e-05,
      "loss": 3.4736,
      "step": 9990
    },
    {
      "epoch": 72.99270072992701,
      "grad_norm": 9.154519081115723,
      "learning_rate": 1.3503649635036497e-05,
      "loss": 3.1571,
      "step": 10000
    },
    {
      "epoch": 73.06569343065694,
      "grad_norm": 8.024650573730469,
      "learning_rate": 1.3467153284671533e-05,
      "loss": 3.0005,
      "step": 10010
    },
    {
      "epoch": 73.13868613138686,
      "grad_norm": 6.822159767150879,
      "learning_rate": 1.3430656934306572e-05,
      "loss": 3.0571,
      "step": 10020
    },
    {
      "epoch": 73.21167883211679,
      "grad_norm": 7.684527397155762,
      "learning_rate": 1.3394160583941607e-05,
      "loss": 3.0096,
      "step": 10030
    },
    {
      "epoch": 73.28467153284672,
      "grad_norm": 8.294203758239746,
      "learning_rate": 1.3357664233576642e-05,
      "loss": 3.1068,
      "step": 10040
    },
    {
      "epoch": 73.35766423357664,
      "grad_norm": 7.963696002960205,
      "learning_rate": 1.332116788321168e-05,
      "loss": 3.142,
      "step": 10050
    },
    {
      "epoch": 73.43065693430657,
      "grad_norm": 9.374089241027832,
      "learning_rate": 1.3284671532846715e-05,
      "loss": 3.242,
      "step": 10060
    },
    {
      "epoch": 73.5036496350365,
      "grad_norm": 8.375274658203125,
      "learning_rate": 1.3248175182481754e-05,
      "loss": 3.6494,
      "step": 10070
    },
    {
      "epoch": 73.57664233576642,
      "grad_norm": 7.4777140617370605,
      "learning_rate": 1.321167883211679e-05,
      "loss": 3.1055,
      "step": 10080
    },
    {
      "epoch": 73.64963503649635,
      "grad_norm": 7.925044536590576,
      "learning_rate": 1.3175182481751825e-05,
      "loss": 3.1823,
      "step": 10090
    },
    {
      "epoch": 73.72262773722628,
      "grad_norm": 8.22048282623291,
      "learning_rate": 1.3138686131386862e-05,
      "loss": 3.2673,
      "step": 10100
    },
    {
      "epoch": 73.7956204379562,
      "grad_norm": 7.274602890014648,
      "learning_rate": 1.3102189781021897e-05,
      "loss": 2.8737,
      "step": 10110
    },
    {
      "epoch": 73.86861313868613,
      "grad_norm": 8.390380859375,
      "learning_rate": 1.3065693430656936e-05,
      "loss": 2.7812,
      "step": 10120
    },
    {
      "epoch": 73.94160583941606,
      "grad_norm": 8.5595121383667,
      "learning_rate": 1.3029197080291972e-05,
      "loss": 3.4676,
      "step": 10130
    },
    {
      "epoch": 74.01459854014598,
      "grad_norm": 8.37248420715332,
      "learning_rate": 1.2992700729927007e-05,
      "loss": 3.1644,
      "step": 10140
    },
    {
      "epoch": 74.08759124087591,
      "grad_norm": 8.12571907043457,
      "learning_rate": 1.2956204379562046e-05,
      "loss": 3.1515,
      "step": 10150
    },
    {
      "epoch": 74.16058394160584,
      "grad_norm": 9.854904174804688,
      "learning_rate": 1.2919708029197081e-05,
      "loss": 2.9602,
      "step": 10160
    },
    {
      "epoch": 74.23357664233576,
      "grad_norm": 7.8378586769104,
      "learning_rate": 1.2883211678832117e-05,
      "loss": 3.3222,
      "step": 10170
    },
    {
      "epoch": 74.30656934306569,
      "grad_norm": 6.73621129989624,
      "learning_rate": 1.2846715328467154e-05,
      "loss": 3.2011,
      "step": 10180
    },
    {
      "epoch": 74.37956204379562,
      "grad_norm": 6.737027168273926,
      "learning_rate": 1.281021897810219e-05,
      "loss": 2.8865,
      "step": 10190
    },
    {
      "epoch": 74.45255474452554,
      "grad_norm": 8.59170913696289,
      "learning_rate": 1.2773722627737228e-05,
      "loss": 3.4224,
      "step": 10200
    },
    {
      "epoch": 74.52554744525547,
      "grad_norm": 8.960975646972656,
      "learning_rate": 1.2737226277372263e-05,
      "loss": 2.9826,
      "step": 10210
    },
    {
      "epoch": 74.5985401459854,
      "grad_norm": 8.521669387817383,
      "learning_rate": 1.2700729927007299e-05,
      "loss": 3.4438,
      "step": 10220
    },
    {
      "epoch": 74.67153284671532,
      "grad_norm": 7.493401527404785,
      "learning_rate": 1.2664233576642338e-05,
      "loss": 3.3081,
      "step": 10230
    },
    {
      "epoch": 74.74452554744525,
      "grad_norm": 7.676817417144775,
      "learning_rate": 1.2627737226277373e-05,
      "loss": 3.1979,
      "step": 10240
    },
    {
      "epoch": 74.81751824817518,
      "grad_norm": 8.04015064239502,
      "learning_rate": 1.259124087591241e-05,
      "loss": 2.7532,
      "step": 10250
    },
    {
      "epoch": 74.8905109489051,
      "grad_norm": 8.437562942504883,
      "learning_rate": 1.2554744525547446e-05,
      "loss": 3.1034,
      "step": 10260
    },
    {
      "epoch": 74.96350364963503,
      "grad_norm": 7.424150466918945,
      "learning_rate": 1.2518248175182481e-05,
      "loss": 2.813,
      "step": 10270
    },
    {
      "epoch": 75.03649635036497,
      "grad_norm": 8.940069198608398,
      "learning_rate": 1.2481751824817518e-05,
      "loss": 3.3794,
      "step": 10280
    },
    {
      "epoch": 75.1094890510949,
      "grad_norm": 8.64054012298584,
      "learning_rate": 1.2445255474452555e-05,
      "loss": 3.0322,
      "step": 10290
    },
    {
      "epoch": 75.18248175182482,
      "grad_norm": 8.732172966003418,
      "learning_rate": 1.2408759124087593e-05,
      "loss": 2.7714,
      "step": 10300
    },
    {
      "epoch": 75.25547445255475,
      "grad_norm": 7.759947299957275,
      "learning_rate": 1.2372262773722628e-05,
      "loss": 3.2159,
      "step": 10310
    },
    {
      "epoch": 75.32846715328468,
      "grad_norm": 7.882131576538086,
      "learning_rate": 1.2335766423357663e-05,
      "loss": 2.6695,
      "step": 10320
    },
    {
      "epoch": 75.4014598540146,
      "grad_norm": 8.076010704040527,
      "learning_rate": 1.22992700729927e-05,
      "loss": 3.4308,
      "step": 10330
    },
    {
      "epoch": 75.47445255474453,
      "grad_norm": 8.622905731201172,
      "learning_rate": 1.2262773722627738e-05,
      "loss": 3.012,
      "step": 10340
    },
    {
      "epoch": 75.54744525547446,
      "grad_norm": 8.261821746826172,
      "learning_rate": 1.2226277372262775e-05,
      "loss": 3.0177,
      "step": 10350
    },
    {
      "epoch": 75.62043795620438,
      "grad_norm": 7.253929615020752,
      "learning_rate": 1.2189781021897812e-05,
      "loss": 2.8428,
      "step": 10360
    },
    {
      "epoch": 75.69343065693431,
      "grad_norm": 8.47049617767334,
      "learning_rate": 1.2153284671532847e-05,
      "loss": 3.2026,
      "step": 10370
    },
    {
      "epoch": 75.76642335766424,
      "grad_norm": 9.674086570739746,
      "learning_rate": 1.2116788321167885e-05,
      "loss": 3.4179,
      "step": 10380
    },
    {
      "epoch": 75.83941605839416,
      "grad_norm": 9.594524383544922,
      "learning_rate": 1.208029197080292e-05,
      "loss": 3.5141,
      "step": 10390
    },
    {
      "epoch": 75.91240875912409,
      "grad_norm": 8.58212947845459,
      "learning_rate": 1.2043795620437957e-05,
      "loss": 3.0697,
      "step": 10400
    },
    {
      "epoch": 75.98540145985402,
      "grad_norm": 8.343399047851562,
      "learning_rate": 1.2007299270072993e-05,
      "loss": 3.1681,
      "step": 10410
    },
    {
      "epoch": 76.05839416058394,
      "grad_norm": 9.335860252380371,
      "learning_rate": 1.197080291970803e-05,
      "loss": 3.4177,
      "step": 10420
    },
    {
      "epoch": 76.13138686131387,
      "grad_norm": 7.670100212097168,
      "learning_rate": 1.1934306569343067e-05,
      "loss": 3.3907,
      "step": 10430
    },
    {
      "epoch": 76.2043795620438,
      "grad_norm": 8.011870384216309,
      "learning_rate": 1.1897810218978104e-05,
      "loss": 3.1357,
      "step": 10440
    },
    {
      "epoch": 76.27737226277372,
      "grad_norm": 7.60646390914917,
      "learning_rate": 1.186131386861314e-05,
      "loss": 3.0269,
      "step": 10450
    },
    {
      "epoch": 76.35036496350365,
      "grad_norm": 9.034248352050781,
      "learning_rate": 1.1824817518248175e-05,
      "loss": 3.0893,
      "step": 10460
    },
    {
      "epoch": 76.42335766423358,
      "grad_norm": 7.720851898193359,
      "learning_rate": 1.1788321167883212e-05,
      "loss": 2.8453,
      "step": 10470
    },
    {
      "epoch": 76.4963503649635,
      "grad_norm": 7.948637008666992,
      "learning_rate": 1.1751824817518249e-05,
      "loss": 3.3694,
      "step": 10480
    },
    {
      "epoch": 76.56934306569343,
      "grad_norm": 8.111089706420898,
      "learning_rate": 1.1715328467153286e-05,
      "loss": 3.1081,
      "step": 10490
    },
    {
      "epoch": 76.64233576642336,
      "grad_norm": 8.345881462097168,
      "learning_rate": 1.1678832116788322e-05,
      "loss": 3.319,
      "step": 10500
    },
    {
      "epoch": 76.71532846715328,
      "grad_norm": 8.027329444885254,
      "learning_rate": 1.1642335766423359e-05,
      "loss": 3.0153,
      "step": 10510
    },
    {
      "epoch": 76.78832116788321,
      "grad_norm": 8.190778732299805,
      "learning_rate": 1.1605839416058394e-05,
      "loss": 3.2372,
      "step": 10520
    },
    {
      "epoch": 76.86131386861314,
      "grad_norm": 8.774988174438477,
      "learning_rate": 1.1569343065693431e-05,
      "loss": 3.3381,
      "step": 10530
    },
    {
      "epoch": 76.93430656934306,
      "grad_norm": 8.066832542419434,
      "learning_rate": 1.1532846715328467e-05,
      "loss": 3.0903,
      "step": 10540
    },
    {
      "epoch": 77.00729927007299,
      "grad_norm": 7.60321569442749,
      "learning_rate": 1.1496350364963504e-05,
      "loss": 2.877,
      "step": 10550
    },
    {
      "epoch": 77.08029197080292,
      "grad_norm": 8.56500244140625,
      "learning_rate": 1.1459854014598541e-05,
      "loss": 3.2511,
      "step": 10560
    },
    {
      "epoch": 77.15328467153284,
      "grad_norm": 8.553597450256348,
      "learning_rate": 1.1423357664233578e-05,
      "loss": 3.0591,
      "step": 10570
    },
    {
      "epoch": 77.22627737226277,
      "grad_norm": 8.585834503173828,
      "learning_rate": 1.1386861313868614e-05,
      "loss": 3.0647,
      "step": 10580
    },
    {
      "epoch": 77.2992700729927,
      "grad_norm": 7.895683288574219,
      "learning_rate": 1.135036496350365e-05,
      "loss": 2.9332,
      "step": 10590
    },
    {
      "epoch": 77.37226277372262,
      "grad_norm": 8.105191230773926,
      "learning_rate": 1.1313868613138686e-05,
      "loss": 3.3193,
      "step": 10600
    },
    {
      "epoch": 77.44525547445255,
      "grad_norm": 8.368762016296387,
      "learning_rate": 1.1277372262773723e-05,
      "loss": 3.4514,
      "step": 10610
    },
    {
      "epoch": 77.51824817518248,
      "grad_norm": 7.7477922439575195,
      "learning_rate": 1.1240875912408759e-05,
      "loss": 3.3141,
      "step": 10620
    },
    {
      "epoch": 77.5912408759124,
      "grad_norm": 7.420414447784424,
      "learning_rate": 1.1204379562043796e-05,
      "loss": 2.8667,
      "step": 10630
    },
    {
      "epoch": 77.66423357664233,
      "grad_norm": 8.050944328308105,
      "learning_rate": 1.1167883211678833e-05,
      "loss": 2.944,
      "step": 10640
    },
    {
      "epoch": 77.73722627737226,
      "grad_norm": 8.853759765625,
      "learning_rate": 1.113138686131387e-05,
      "loss": 3.1761,
      "step": 10650
    },
    {
      "epoch": 77.81021897810218,
      "grad_norm": 8.707343101501465,
      "learning_rate": 1.1094890510948906e-05,
      "loss": 3.2714,
      "step": 10660
    },
    {
      "epoch": 77.88321167883211,
      "grad_norm": 8.460920333862305,
      "learning_rate": 1.1058394160583941e-05,
      "loss": 2.9221,
      "step": 10670
    },
    {
      "epoch": 77.95620437956204,
      "grad_norm": 8.367565155029297,
      "learning_rate": 1.1021897810218978e-05,
      "loss": 3.1669,
      "step": 10680
    },
    {
      "epoch": 78.02919708029196,
      "grad_norm": 8.60799789428711,
      "learning_rate": 1.0985401459854015e-05,
      "loss": 3.0258,
      "step": 10690
    },
    {
      "epoch": 78.10218978102189,
      "grad_norm": 7.212940216064453,
      "learning_rate": 1.0948905109489052e-05,
      "loss": 3.0581,
      "step": 10700
    },
    {
      "epoch": 78.17518248175182,
      "grad_norm": 7.969385623931885,
      "learning_rate": 1.0912408759124088e-05,
      "loss": 3.1792,
      "step": 10710
    },
    {
      "epoch": 78.24817518248175,
      "grad_norm": 7.999425411224365,
      "learning_rate": 1.0875912408759125e-05,
      "loss": 2.7656,
      "step": 10720
    },
    {
      "epoch": 78.32116788321167,
      "grad_norm": 8.437032699584961,
      "learning_rate": 1.083941605839416e-05,
      "loss": 3.4157,
      "step": 10730
    },
    {
      "epoch": 78.39416058394161,
      "grad_norm": 8.506275177001953,
      "learning_rate": 1.0802919708029198e-05,
      "loss": 3.385,
      "step": 10740
    },
    {
      "epoch": 78.46715328467154,
      "grad_norm": 7.866514682769775,
      "learning_rate": 1.0766423357664233e-05,
      "loss": 3.0634,
      "step": 10750
    },
    {
      "epoch": 78.54014598540147,
      "grad_norm": 8.277971267700195,
      "learning_rate": 1.072992700729927e-05,
      "loss": 3.2412,
      "step": 10760
    },
    {
      "epoch": 78.61313868613139,
      "grad_norm": 7.358039855957031,
      "learning_rate": 1.0693430656934307e-05,
      "loss": 3.1266,
      "step": 10770
    },
    {
      "epoch": 78.68613138686132,
      "grad_norm": 7.769307613372803,
      "learning_rate": 1.0656934306569344e-05,
      "loss": 3.1904,
      "step": 10780
    },
    {
      "epoch": 78.75912408759125,
      "grad_norm": 8.262094497680664,
      "learning_rate": 1.0620437956204381e-05,
      "loss": 2.576,
      "step": 10790
    },
    {
      "epoch": 78.83211678832117,
      "grad_norm": 9.081521987915039,
      "learning_rate": 1.0583941605839417e-05,
      "loss": 2.7675,
      "step": 10800
    },
    {
      "epoch": 78.9051094890511,
      "grad_norm": 9.53588581085205,
      "learning_rate": 1.0547445255474452e-05,
      "loss": 3.3202,
      "step": 10810
    },
    {
      "epoch": 78.97810218978103,
      "grad_norm": 9.846070289611816,
      "learning_rate": 1.051094890510949e-05,
      "loss": 3.0129,
      "step": 10820
    },
    {
      "epoch": 79.05109489051095,
      "grad_norm": 7.367036819458008,
      "learning_rate": 1.0474452554744527e-05,
      "loss": 3.1543,
      "step": 10830
    },
    {
      "epoch": 79.12408759124088,
      "grad_norm": 7.118824005126953,
      "learning_rate": 1.0437956204379562e-05,
      "loss": 3.2398,
      "step": 10840
    },
    {
      "epoch": 79.1970802919708,
      "grad_norm": 10.350268363952637,
      "learning_rate": 1.04014598540146e-05,
      "loss": 3.0266,
      "step": 10850
    },
    {
      "epoch": 79.27007299270073,
      "grad_norm": 8.679213523864746,
      "learning_rate": 1.0364963503649636e-05,
      "loss": 2.8005,
      "step": 10860
    },
    {
      "epoch": 79.34306569343066,
      "grad_norm": 10.08918285369873,
      "learning_rate": 1.0328467153284672e-05,
      "loss": 3.2014,
      "step": 10870
    },
    {
      "epoch": 79.41605839416059,
      "grad_norm": 10.867410659790039,
      "learning_rate": 1.0291970802919707e-05,
      "loss": 2.9274,
      "step": 10880
    },
    {
      "epoch": 79.48905109489051,
      "grad_norm": 8.895776748657227,
      "learning_rate": 1.0255474452554744e-05,
      "loss": 3.0747,
      "step": 10890
    },
    {
      "epoch": 79.56204379562044,
      "grad_norm": 8.248690605163574,
      "learning_rate": 1.0218978102189781e-05,
      "loss": 3.3211,
      "step": 10900
    },
    {
      "epoch": 79.63503649635037,
      "grad_norm": 7.927706241607666,
      "learning_rate": 1.0182481751824819e-05,
      "loss": 3.1252,
      "step": 10910
    },
    {
      "epoch": 79.7080291970803,
      "grad_norm": 8.76894760131836,
      "learning_rate": 1.0145985401459856e-05,
      "loss": 3.4627,
      "step": 10920
    },
    {
      "epoch": 79.78102189781022,
      "grad_norm": 7.173868179321289,
      "learning_rate": 1.0109489051094891e-05,
      "loss": 3.0821,
      "step": 10930
    },
    {
      "epoch": 79.85401459854015,
      "grad_norm": 7.947012424468994,
      "learning_rate": 1.0072992700729927e-05,
      "loss": 2.8675,
      "step": 10940
    },
    {
      "epoch": 79.92700729927007,
      "grad_norm": 9.06082534790039,
      "learning_rate": 1.0036496350364964e-05,
      "loss": 3.1979,
      "step": 10950
    },
    {
      "epoch": 80.0,
      "grad_norm": 14.565901756286621,
      "learning_rate": 1e-05,
      "loss": 3.0366,
      "step": 10960
    },
    {
      "epoch": 80.07299270072993,
      "grad_norm": 8.088135719299316,
      "learning_rate": 9.963503649635036e-06,
      "loss": 2.8483,
      "step": 10970
    },
    {
      "epoch": 80.14598540145985,
      "grad_norm": 8.413543701171875,
      "learning_rate": 9.927007299270073e-06,
      "loss": 2.8753,
      "step": 10980
    },
    {
      "epoch": 80.21897810218978,
      "grad_norm": 6.989898681640625,
      "learning_rate": 9.89051094890511e-06,
      "loss": 2.9595,
      "step": 10990
    },
    {
      "epoch": 80.2919708029197,
      "grad_norm": 7.897457122802734,
      "learning_rate": 9.854014598540148e-06,
      "loss": 3.2668,
      "step": 11000
    },
    {
      "epoch": 80.36496350364963,
      "grad_norm": 9.023724555969238,
      "learning_rate": 9.817518248175183e-06,
      "loss": 3.4356,
      "step": 11010
    },
    {
      "epoch": 80.43795620437956,
      "grad_norm": 9.01236343383789,
      "learning_rate": 9.781021897810219e-06,
      "loss": 3.4435,
      "step": 11020
    },
    {
      "epoch": 80.51094890510949,
      "grad_norm": 8.901565551757812,
      "learning_rate": 9.744525547445256e-06,
      "loss": 3.1385,
      "step": 11030
    },
    {
      "epoch": 80.58394160583941,
      "grad_norm": 8.862886428833008,
      "learning_rate": 9.708029197080293e-06,
      "loss": 3.0349,
      "step": 11040
    },
    {
      "epoch": 80.65693430656934,
      "grad_norm": 8.57606315612793,
      "learning_rate": 9.67153284671533e-06,
      "loss": 3.1267,
      "step": 11050
    },
    {
      "epoch": 80.72992700729927,
      "grad_norm": 7.156962871551514,
      "learning_rate": 9.635036496350365e-06,
      "loss": 3.0614,
      "step": 11060
    },
    {
      "epoch": 80.8029197080292,
      "grad_norm": 8.871358871459961,
      "learning_rate": 9.598540145985402e-06,
      "loss": 3.0361,
      "step": 11070
    },
    {
      "epoch": 80.87591240875912,
      "grad_norm": 7.719926834106445,
      "learning_rate": 9.562043795620438e-06,
      "loss": 2.7726,
      "step": 11080
    },
    {
      "epoch": 80.94890510948905,
      "grad_norm": 8.374144554138184,
      "learning_rate": 9.525547445255475e-06,
      "loss": 2.9511,
      "step": 11090
    },
    {
      "epoch": 81.02189781021897,
      "grad_norm": 8.246490478515625,
      "learning_rate": 9.48905109489051e-06,
      "loss": 3.7046,
      "step": 11100
    },
    {
      "epoch": 81.0948905109489,
      "grad_norm": 9.06971263885498,
      "learning_rate": 9.452554744525548e-06,
      "loss": 3.0623,
      "step": 11110
    },
    {
      "epoch": 81.16788321167883,
      "grad_norm": 8.12252140045166,
      "learning_rate": 9.416058394160585e-06,
      "loss": 3.2233,
      "step": 11120
    },
    {
      "epoch": 81.24087591240875,
      "grad_norm": 8.41614055633545,
      "learning_rate": 9.379562043795622e-06,
      "loss": 2.9638,
      "step": 11130
    },
    {
      "epoch": 81.31386861313868,
      "grad_norm": 7.948331832885742,
      "learning_rate": 9.343065693430657e-06,
      "loss": 2.8609,
      "step": 11140
    },
    {
      "epoch": 81.38686131386861,
      "grad_norm": 7.737222194671631,
      "learning_rate": 9.306569343065694e-06,
      "loss": 3.0816,
      "step": 11150
    },
    {
      "epoch": 81.45985401459853,
      "grad_norm": 8.690890312194824,
      "learning_rate": 9.27007299270073e-06,
      "loss": 3.3011,
      "step": 11160
    },
    {
      "epoch": 81.53284671532846,
      "grad_norm": 9.0421142578125,
      "learning_rate": 9.233576642335767e-06,
      "loss": 3.391,
      "step": 11170
    },
    {
      "epoch": 81.60583941605839,
      "grad_norm": 8.25110149383545,
      "learning_rate": 9.197080291970802e-06,
      "loss": 2.931,
      "step": 11180
    },
    {
      "epoch": 81.67883211678833,
      "grad_norm": 8.610844612121582,
      "learning_rate": 9.16058394160584e-06,
      "loss": 3.2466,
      "step": 11190
    },
    {
      "epoch": 81.75182481751825,
      "grad_norm": 8.418537139892578,
      "learning_rate": 9.124087591240877e-06,
      "loss": 2.8694,
      "step": 11200
    },
    {
      "epoch": 81.82481751824818,
      "grad_norm": 8.434505462646484,
      "learning_rate": 9.087591240875914e-06,
      "loss": 3.0353,
      "step": 11210
    },
    {
      "epoch": 81.89781021897811,
      "grad_norm": 8.640362739562988,
      "learning_rate": 9.05109489051095e-06,
      "loss": 3.091,
      "step": 11220
    },
    {
      "epoch": 81.97080291970804,
      "grad_norm": 8.473085403442383,
      "learning_rate": 9.014598540145985e-06,
      "loss": 3.0742,
      "step": 11230
    },
    {
      "epoch": 82.04379562043796,
      "grad_norm": 7.804281711578369,
      "learning_rate": 8.978102189781022e-06,
      "loss": 3.1624,
      "step": 11240
    },
    {
      "epoch": 82.11678832116789,
      "grad_norm": 8.880688667297363,
      "learning_rate": 8.941605839416059e-06,
      "loss": 2.9616,
      "step": 11250
    },
    {
      "epoch": 82.18978102189782,
      "grad_norm": 8.416964530944824,
      "learning_rate": 8.905109489051096e-06,
      "loss": 2.9406,
      "step": 11260
    },
    {
      "epoch": 82.26277372262774,
      "grad_norm": 8.525668144226074,
      "learning_rate": 8.868613138686132e-06,
      "loss": 3.0825,
      "step": 11270
    },
    {
      "epoch": 82.33576642335767,
      "grad_norm": 8.54672908782959,
      "learning_rate": 8.832116788321169e-06,
      "loss": 3.1433,
      "step": 11280
    },
    {
      "epoch": 82.4087591240876,
      "grad_norm": 7.930583953857422,
      "learning_rate": 8.795620437956204e-06,
      "loss": 3.3409,
      "step": 11290
    },
    {
      "epoch": 82.48175182481752,
      "grad_norm": 8.28052806854248,
      "learning_rate": 8.759124087591241e-06,
      "loss": 2.8908,
      "step": 11300
    },
    {
      "epoch": 82.55474452554745,
      "grad_norm": 7.871609210968018,
      "learning_rate": 8.722627737226277e-06,
      "loss": 2.821,
      "step": 11310
    },
    {
      "epoch": 82.62773722627738,
      "grad_norm": 8.249415397644043,
      "learning_rate": 8.686131386861314e-06,
      "loss": 3.0419,
      "step": 11320
    },
    {
      "epoch": 82.7007299270073,
      "grad_norm": 7.787832736968994,
      "learning_rate": 8.649635036496351e-06,
      "loss": 3.1371,
      "step": 11330
    },
    {
      "epoch": 82.77372262773723,
      "grad_norm": 9.581523895263672,
      "learning_rate": 8.613138686131388e-06,
      "loss": 3.1992,
      "step": 11340
    },
    {
      "epoch": 82.84671532846716,
      "grad_norm": 8.385931015014648,
      "learning_rate": 8.576642335766425e-06,
      "loss": 3.0971,
      "step": 11350
    },
    {
      "epoch": 82.91970802919708,
      "grad_norm": 8.840215682983398,
      "learning_rate": 8.54014598540146e-06,
      "loss": 3.1431,
      "step": 11360
    },
    {
      "epoch": 82.99270072992701,
      "grad_norm": 9.249574661254883,
      "learning_rate": 8.503649635036496e-06,
      "loss": 3.3111,
      "step": 11370
    },
    {
      "epoch": 83.06569343065694,
      "grad_norm": 9.305383682250977,
      "learning_rate": 8.467153284671533e-06,
      "loss": 2.9484,
      "step": 11380
    },
    {
      "epoch": 83.13868613138686,
      "grad_norm": 7.9273762702941895,
      "learning_rate": 8.43065693430657e-06,
      "loss": 2.9499,
      "step": 11390
    },
    {
      "epoch": 83.21167883211679,
      "grad_norm": 7.179479598999023,
      "learning_rate": 8.394160583941606e-06,
      "loss": 3.3473,
      "step": 11400
    },
    {
      "epoch": 83.28467153284672,
      "grad_norm": 8.240626335144043,
      "learning_rate": 8.357664233576643e-06,
      "loss": 3.1119,
      "step": 11410
    },
    {
      "epoch": 83.35766423357664,
      "grad_norm": 9.081059455871582,
      "learning_rate": 8.32116788321168e-06,
      "loss": 3.149,
      "step": 11420
    },
    {
      "epoch": 83.43065693430657,
      "grad_norm": 10.341752052307129,
      "learning_rate": 8.284671532846715e-06,
      "loss": 2.8839,
      "step": 11430
    },
    {
      "epoch": 83.5036496350365,
      "grad_norm": 8.176554679870605,
      "learning_rate": 8.248175182481751e-06,
      "loss": 3.018,
      "step": 11440
    },
    {
      "epoch": 83.57664233576642,
      "grad_norm": 9.206415176391602,
      "learning_rate": 8.211678832116788e-06,
      "loss": 3.1395,
      "step": 11450
    },
    {
      "epoch": 83.64963503649635,
      "grad_norm": 7.36716365814209,
      "learning_rate": 8.175182481751825e-06,
      "loss": 2.9145,
      "step": 11460
    },
    {
      "epoch": 83.72262773722628,
      "grad_norm": 8.027748107910156,
      "learning_rate": 8.138686131386862e-06,
      "loss": 3.0225,
      "step": 11470
    },
    {
      "epoch": 83.7956204379562,
      "grad_norm": 8.512922286987305,
      "learning_rate": 8.1021897810219e-06,
      "loss": 3.1388,
      "step": 11480
    },
    {
      "epoch": 83.86861313868613,
      "grad_norm": 8.27391242980957,
      "learning_rate": 8.065693430656935e-06,
      "loss": 3.179,
      "step": 11490
    },
    {
      "epoch": 83.94160583941606,
      "grad_norm": 8.680924415588379,
      "learning_rate": 8.02919708029197e-06,
      "loss": 3.3906,
      "step": 11500
    },
    {
      "epoch": 84.01459854014598,
      "grad_norm": 6.550340175628662,
      "learning_rate": 7.992700729927007e-06,
      "loss": 2.6975,
      "step": 11510
    },
    {
      "epoch": 84.08759124087591,
      "grad_norm": 6.992182731628418,
      "learning_rate": 7.956204379562045e-06,
      "loss": 3.0215,
      "step": 11520
    },
    {
      "epoch": 84.16058394160584,
      "grad_norm": 7.818706035614014,
      "learning_rate": 7.91970802919708e-06,
      "loss": 3.1159,
      "step": 11530
    },
    {
      "epoch": 84.23357664233576,
      "grad_norm": 7.6733317375183105,
      "learning_rate": 7.883211678832117e-06,
      "loss": 3.3412,
      "step": 11540
    },
    {
      "epoch": 84.30656934306569,
      "grad_norm": 7.769700527191162,
      "learning_rate": 7.846715328467154e-06,
      "loss": 3.1503,
      "step": 11550
    },
    {
      "epoch": 84.37956204379562,
      "grad_norm": 8.33581256866455,
      "learning_rate": 7.810218978102191e-06,
      "loss": 3.1509,
      "step": 11560
    },
    {
      "epoch": 84.45255474452554,
      "grad_norm": 8.404448509216309,
      "learning_rate": 7.773722627737227e-06,
      "loss": 3.0713,
      "step": 11570
    },
    {
      "epoch": 84.52554744525547,
      "grad_norm": 7.9376349449157715,
      "learning_rate": 7.737226277372262e-06,
      "loss": 3.1839,
      "step": 11580
    },
    {
      "epoch": 84.5985401459854,
      "grad_norm": 9.123908042907715,
      "learning_rate": 7.7007299270073e-06,
      "loss": 2.9967,
      "step": 11590
    },
    {
      "epoch": 84.67153284671532,
      "grad_norm": 7.943930149078369,
      "learning_rate": 7.664233576642336e-06,
      "loss": 3.0174,
      "step": 11600
    },
    {
      "epoch": 84.74452554744525,
      "grad_norm": 8.186111450195312,
      "learning_rate": 7.627737226277374e-06,
      "loss": 3.0994,
      "step": 11610
    },
    {
      "epoch": 84.81751824817518,
      "grad_norm": 9.309778213500977,
      "learning_rate": 7.591240875912409e-06,
      "loss": 2.8414,
      "step": 11620
    },
    {
      "epoch": 84.8905109489051,
      "grad_norm": 8.352876663208008,
      "learning_rate": 7.554744525547445e-06,
      "loss": 3.4466,
      "step": 11630
    },
    {
      "epoch": 84.96350364963503,
      "grad_norm": 9.066286087036133,
      "learning_rate": 7.5182481751824825e-06,
      "loss": 3.0375,
      "step": 11640
    },
    {
      "epoch": 85.03649635036497,
      "grad_norm": 8.559529304504395,
      "learning_rate": 7.48175182481752e-06,
      "loss": 3.1019,
      "step": 11650
    },
    {
      "epoch": 85.1094890510949,
      "grad_norm": 8.970255851745605,
      "learning_rate": 7.445255474452554e-06,
      "loss": 3.3511,
      "step": 11660
    },
    {
      "epoch": 85.18248175182482,
      "grad_norm": 8.606267929077148,
      "learning_rate": 7.408759124087591e-06,
      "loss": 3.3345,
      "step": 11670
    },
    {
      "epoch": 85.25547445255475,
      "grad_norm": 8.319904327392578,
      "learning_rate": 7.3722627737226285e-06,
      "loss": 3.1297,
      "step": 11680
    },
    {
      "epoch": 85.32846715328468,
      "grad_norm": 10.732491493225098,
      "learning_rate": 7.335766423357665e-06,
      "loss": 2.9625,
      "step": 11690
    },
    {
      "epoch": 85.4014598540146,
      "grad_norm": 10.004240989685059,
      "learning_rate": 7.2992700729927e-06,
      "loss": 2.8638,
      "step": 11700
    },
    {
      "epoch": 85.47445255474453,
      "grad_norm": 7.7671589851379395,
      "learning_rate": 7.262773722627737e-06,
      "loss": 3.2417,
      "step": 11710
    },
    {
      "epoch": 85.54744525547446,
      "grad_norm": 9.383378028869629,
      "learning_rate": 7.2262773722627744e-06,
      "loss": 3.1629,
      "step": 11720
    },
    {
      "epoch": 85.62043795620438,
      "grad_norm": 8.172054290771484,
      "learning_rate": 7.189781021897811e-06,
      "loss": 2.8925,
      "step": 11730
    },
    {
      "epoch": 85.69343065693431,
      "grad_norm": 8.392204284667969,
      "learning_rate": 7.153284671532846e-06,
      "loss": 2.7963,
      "step": 11740
    },
    {
      "epoch": 85.76642335766424,
      "grad_norm": 8.694194793701172,
      "learning_rate": 7.116788321167883e-06,
      "loss": 2.7213,
      "step": 11750
    },
    {
      "epoch": 85.83941605839416,
      "grad_norm": 9.230310440063477,
      "learning_rate": 7.08029197080292e-06,
      "loss": 3.1413,
      "step": 11760
    },
    {
      "epoch": 85.91240875912409,
      "grad_norm": 8.210772514343262,
      "learning_rate": 7.043795620437957e-06,
      "loss": 3.2275,
      "step": 11770
    },
    {
      "epoch": 85.98540145985402,
      "grad_norm": 8.408273696899414,
      "learning_rate": 7.007299270072994e-06,
      "loss": 2.7438,
      "step": 11780
    },
    {
      "epoch": 86.05839416058394,
      "grad_norm": 8.498709678649902,
      "learning_rate": 6.970802919708029e-06,
      "loss": 2.7887,
      "step": 11790
    },
    {
      "epoch": 86.13138686131387,
      "grad_norm": 8.565074920654297,
      "learning_rate": 6.9343065693430655e-06,
      "loss": 3.069,
      "step": 11800
    },
    {
      "epoch": 86.2043795620438,
      "grad_norm": 8.329985618591309,
      "learning_rate": 6.897810218978103e-06,
      "loss": 2.9975,
      "step": 11810
    },
    {
      "epoch": 86.27737226277372,
      "grad_norm": 7.576961040496826,
      "learning_rate": 6.86131386861314e-06,
      "loss": 2.99,
      "step": 11820
    },
    {
      "epoch": 86.35036496350365,
      "grad_norm": 8.159995079040527,
      "learning_rate": 6.824817518248175e-06,
      "loss": 2.99,
      "step": 11830
    },
    {
      "epoch": 86.42335766423358,
      "grad_norm": 8.330647468566895,
      "learning_rate": 6.7883211678832115e-06,
      "loss": 2.8967,
      "step": 11840
    },
    {
      "epoch": 86.4963503649635,
      "grad_norm": 7.96615743637085,
      "learning_rate": 6.751824817518249e-06,
      "loss": 2.8721,
      "step": 11850
    },
    {
      "epoch": 86.56934306569343,
      "grad_norm": 9.772777557373047,
      "learning_rate": 6.715328467153286e-06,
      "loss": 3.0695,
      "step": 11860
    },
    {
      "epoch": 86.64233576642336,
      "grad_norm": 6.847565174102783,
      "learning_rate": 6.678832116788321e-06,
      "loss": 2.5964,
      "step": 11870
    },
    {
      "epoch": 86.71532846715328,
      "grad_norm": 8.255918502807617,
      "learning_rate": 6.6423357664233575e-06,
      "loss": 3.2261,
      "step": 11880
    },
    {
      "epoch": 86.78832116788321,
      "grad_norm": 8.363816261291504,
      "learning_rate": 6.605839416058395e-06,
      "loss": 3.4119,
      "step": 11890
    },
    {
      "epoch": 86.86131386861314,
      "grad_norm": 7.926512241363525,
      "learning_rate": 6.569343065693431e-06,
      "loss": 3.0023,
      "step": 11900
    },
    {
      "epoch": 86.93430656934306,
      "grad_norm": 8.76502513885498,
      "learning_rate": 6.532846715328468e-06,
      "loss": 3.3561,
      "step": 11910
    },
    {
      "epoch": 87.00729927007299,
      "grad_norm": 7.11747407913208,
      "learning_rate": 6.4963503649635035e-06,
      "loss": 3.2294,
      "step": 11920
    },
    {
      "epoch": 87.08029197080292,
      "grad_norm": 8.154460906982422,
      "learning_rate": 6.459854014598541e-06,
      "loss": 2.9967,
      "step": 11930
    },
    {
      "epoch": 87.15328467153284,
      "grad_norm": 8.190034866333008,
      "learning_rate": 6.423357664233577e-06,
      "loss": 2.9622,
      "step": 11940
    },
    {
      "epoch": 87.22627737226277,
      "grad_norm": 6.216557502746582,
      "learning_rate": 6.386861313868614e-06,
      "loss": 2.9506,
      "step": 11950
    },
    {
      "epoch": 87.2992700729927,
      "grad_norm": 7.0579400062561035,
      "learning_rate": 6.3503649635036495e-06,
      "loss": 2.8868,
      "step": 11960
    },
    {
      "epoch": 87.37226277372262,
      "grad_norm": 7.060156345367432,
      "learning_rate": 6.313868613138687e-06,
      "loss": 3.2354,
      "step": 11970
    },
    {
      "epoch": 87.44525547445255,
      "grad_norm": 10.462090492248535,
      "learning_rate": 6.277372262773723e-06,
      "loss": 3.1198,
      "step": 11980
    },
    {
      "epoch": 87.51824817518248,
      "grad_norm": 7.504970550537109,
      "learning_rate": 6.240875912408759e-06,
      "loss": 2.9792,
      "step": 11990
    },
    {
      "epoch": 87.5912408759124,
      "grad_norm": 8.62747573852539,
      "learning_rate": 6.204379562043796e-06,
      "loss": 3.0278,
      "step": 12000
    },
    {
      "epoch": 87.66423357664233,
      "grad_norm": 7.8161797523498535,
      "learning_rate": 6.167883211678832e-06,
      "loss": 3.453,
      "step": 12010
    },
    {
      "epoch": 87.73722627737226,
      "grad_norm": 8.985589981079102,
      "learning_rate": 6.131386861313869e-06,
      "loss": 3.0263,
      "step": 12020
    },
    {
      "epoch": 87.81021897810218,
      "grad_norm": 6.222493648529053,
      "learning_rate": 6.094890510948906e-06,
      "loss": 3.0434,
      "step": 12030
    },
    {
      "epoch": 87.88321167883211,
      "grad_norm": 8.193878173828125,
      "learning_rate": 6.058394160583942e-06,
      "loss": 2.8344,
      "step": 12040
    },
    {
      "epoch": 87.95620437956204,
      "grad_norm": 10.366094589233398,
      "learning_rate": 6.0218978102189786e-06,
      "loss": 3.1919,
      "step": 12050
    },
    {
      "epoch": 88.02919708029196,
      "grad_norm": 7.992607593536377,
      "learning_rate": 5.985401459854015e-06,
      "loss": 3.0193,
      "step": 12060
    },
    {
      "epoch": 88.10218978102189,
      "grad_norm": 8.549687385559082,
      "learning_rate": 5.948905109489052e-06,
      "loss": 3.0876,
      "step": 12070
    },
    {
      "epoch": 88.17518248175182,
      "grad_norm": 8.137785911560059,
      "learning_rate": 5.912408759124087e-06,
      "loss": 3.1306,
      "step": 12080
    },
    {
      "epoch": 88.24817518248175,
      "grad_norm": 8.745278358459473,
      "learning_rate": 5.8759124087591245e-06,
      "loss": 3.1464,
      "step": 12090
    },
    {
      "epoch": 88.32116788321167,
      "grad_norm": 8.059061050415039,
      "learning_rate": 5.839416058394161e-06,
      "loss": 3.0432,
      "step": 12100
    },
    {
      "epoch": 88.39416058394161,
      "grad_norm": 8.72642707824707,
      "learning_rate": 5.802919708029197e-06,
      "loss": 2.9166,
      "step": 12110
    },
    {
      "epoch": 88.46715328467154,
      "grad_norm": 7.881951808929443,
      "learning_rate": 5.766423357664233e-06,
      "loss": 3.2009,
      "step": 12120
    },
    {
      "epoch": 88.54014598540147,
      "grad_norm": 8.677145957946777,
      "learning_rate": 5.7299270072992705e-06,
      "loss": 2.7458,
      "step": 12130
    },
    {
      "epoch": 88.61313868613139,
      "grad_norm": 8.781041145324707,
      "learning_rate": 5.693430656934307e-06,
      "loss": 3.2492,
      "step": 12140
    },
    {
      "epoch": 88.68613138686132,
      "grad_norm": 8.666497230529785,
      "learning_rate": 5.656934306569343e-06,
      "loss": 2.9575,
      "step": 12150
    },
    {
      "epoch": 88.75912408759125,
      "grad_norm": 11.436715126037598,
      "learning_rate": 5.620437956204379e-06,
      "loss": 2.7644,
      "step": 12160
    },
    {
      "epoch": 88.83211678832117,
      "grad_norm": 7.729480743408203,
      "learning_rate": 5.5839416058394165e-06,
      "loss": 3.0772,
      "step": 12170
    },
    {
      "epoch": 88.9051094890511,
      "grad_norm": 9.178189277648926,
      "learning_rate": 5.547445255474453e-06,
      "loss": 3.2669,
      "step": 12180
    },
    {
      "epoch": 88.97810218978103,
      "grad_norm": 9.231500625610352,
      "learning_rate": 5.510948905109489e-06,
      "loss": 3.0765,
      "step": 12190
    },
    {
      "epoch": 89.05109489051095,
      "grad_norm": 8.791790962219238,
      "learning_rate": 5.474452554744526e-06,
      "loss": 2.7406,
      "step": 12200
    },
    {
      "epoch": 89.12408759124088,
      "grad_norm": 11.196455001831055,
      "learning_rate": 5.4379562043795625e-06,
      "loss": 3.0638,
      "step": 12210
    },
    {
      "epoch": 89.1970802919708,
      "grad_norm": 7.375912189483643,
      "learning_rate": 5.401459854014599e-06,
      "loss": 3.0679,
      "step": 12220
    },
    {
      "epoch": 89.27007299270073,
      "grad_norm": 8.549043655395508,
      "learning_rate": 5.364963503649635e-06,
      "loss": 3.0777,
      "step": 12230
    },
    {
      "epoch": 89.34306569343066,
      "grad_norm": 8.872052192687988,
      "learning_rate": 5.328467153284672e-06,
      "loss": 3.0784,
      "step": 12240
    },
    {
      "epoch": 89.41605839416059,
      "grad_norm": 8.607952117919922,
      "learning_rate": 5.2919708029197084e-06,
      "loss": 2.9312,
      "step": 12250
    },
    {
      "epoch": 89.48905109489051,
      "grad_norm": 8.124313354492188,
      "learning_rate": 5.255474452554745e-06,
      "loss": 2.7902,
      "step": 12260
    },
    {
      "epoch": 89.56204379562044,
      "grad_norm": 8.778668403625488,
      "learning_rate": 5.218978102189781e-06,
      "loss": 3.1668,
      "step": 12270
    },
    {
      "epoch": 89.63503649635037,
      "grad_norm": 7.26279354095459,
      "learning_rate": 5.182481751824818e-06,
      "loss": 2.9505,
      "step": 12280
    },
    {
      "epoch": 89.7080291970803,
      "grad_norm": 7.972878932952881,
      "learning_rate": 5.145985401459854e-06,
      "loss": 3.3706,
      "step": 12290
    },
    {
      "epoch": 89.78102189781022,
      "grad_norm": 8.208849906921387,
      "learning_rate": 5.109489051094891e-06,
      "loss": 3.268,
      "step": 12300
    },
    {
      "epoch": 89.85401459854015,
      "grad_norm": 8.589055061340332,
      "learning_rate": 5.072992700729928e-06,
      "loss": 3.0775,
      "step": 12310
    },
    {
      "epoch": 89.92700729927007,
      "grad_norm": 7.347495079040527,
      "learning_rate": 5.036496350364963e-06,
      "loss": 3.0023,
      "step": 12320
    },
    {
      "epoch": 90.0,
      "grad_norm": 10.317842483520508,
      "learning_rate": 5e-06,
      "loss": 2.8888,
      "step": 12330
    },
    {
      "epoch": 90.07299270072993,
      "grad_norm": 9.561662673950195,
      "learning_rate": 4.963503649635037e-06,
      "loss": 3.2711,
      "step": 12340
    },
    {
      "epoch": 90.14598540145985,
      "grad_norm": 8.01632308959961,
      "learning_rate": 4.927007299270074e-06,
      "loss": 2.9932,
      "step": 12350
    },
    {
      "epoch": 90.21897810218978,
      "grad_norm": 7.051939487457275,
      "learning_rate": 4.890510948905109e-06,
      "loss": 2.7675,
      "step": 12360
    },
    {
      "epoch": 90.2919708029197,
      "grad_norm": 7.059228897094727,
      "learning_rate": 4.854014598540146e-06,
      "loss": 3.1915,
      "step": 12370
    },
    {
      "epoch": 90.36496350364963,
      "grad_norm": 8.416441917419434,
      "learning_rate": 4.817518248175183e-06,
      "loss": 3.2628,
      "step": 12380
    },
    {
      "epoch": 90.43795620437956,
      "grad_norm": 11.143021583557129,
      "learning_rate": 4.781021897810219e-06,
      "loss": 3.0504,
      "step": 12390
    },
    {
      "epoch": 90.51094890510949,
      "grad_norm": 7.564677715301514,
      "learning_rate": 4.744525547445255e-06,
      "loss": 3.0806,
      "step": 12400
    },
    {
      "epoch": 90.58394160583941,
      "grad_norm": 7.915548324584961,
      "learning_rate": 4.708029197080292e-06,
      "loss": 3.0288,
      "step": 12410
    },
    {
      "epoch": 90.65693430656934,
      "grad_norm": 7.801031589508057,
      "learning_rate": 4.671532846715329e-06,
      "loss": 2.918,
      "step": 12420
    },
    {
      "epoch": 90.72992700729927,
      "grad_norm": 8.815118789672852,
      "learning_rate": 4.635036496350365e-06,
      "loss": 2.8443,
      "step": 12430
    },
    {
      "epoch": 90.8029197080292,
      "grad_norm": 7.6954026222229,
      "learning_rate": 4.598540145985401e-06,
      "loss": 3.1465,
      "step": 12440
    },
    {
      "epoch": 90.87591240875912,
      "grad_norm": 7.381702899932861,
      "learning_rate": 4.562043795620438e-06,
      "loss": 3.0077,
      "step": 12450
    },
    {
      "epoch": 90.94890510948905,
      "grad_norm": 10.057334899902344,
      "learning_rate": 4.525547445255475e-06,
      "loss": 3.2617,
      "step": 12460
    },
    {
      "epoch": 91.02189781021897,
      "grad_norm": 7.806931972503662,
      "learning_rate": 4.489051094890511e-06,
      "loss": 3.1509,
      "step": 12470
    },
    {
      "epoch": 91.0948905109489,
      "grad_norm": 8.385812759399414,
      "learning_rate": 4.452554744525548e-06,
      "loss": 3.2428,
      "step": 12480
    },
    {
      "epoch": 91.16788321167883,
      "grad_norm": 7.9624924659729,
      "learning_rate": 4.416058394160584e-06,
      "loss": 2.8457,
      "step": 12490
    },
    {
      "epoch": 91.24087591240875,
      "grad_norm": 8.640623092651367,
      "learning_rate": 4.379562043795621e-06,
      "loss": 3.0158,
      "step": 12500
    },
    {
      "epoch": 91.31386861313868,
      "grad_norm": 8.039769172668457,
      "learning_rate": 4.343065693430657e-06,
      "loss": 2.9961,
      "step": 12510
    },
    {
      "epoch": 91.38686131386861,
      "grad_norm": 7.237402439117432,
      "learning_rate": 4.306569343065694e-06,
      "loss": 2.9666,
      "step": 12520
    },
    {
      "epoch": 91.45985401459853,
      "grad_norm": 8.733746528625488,
      "learning_rate": 4.27007299270073e-06,
      "loss": 2.8689,
      "step": 12530
    },
    {
      "epoch": 91.53284671532846,
      "grad_norm": 8.471284866333008,
      "learning_rate": 4.233576642335767e-06,
      "loss": 2.88,
      "step": 12540
    },
    {
      "epoch": 91.60583941605839,
      "grad_norm": 7.369313716888428,
      "learning_rate": 4.197080291970803e-06,
      "loss": 3.1094,
      "step": 12550
    },
    {
      "epoch": 91.67883211678833,
      "grad_norm": 8.514034271240234,
      "learning_rate": 4.16058394160584e-06,
      "loss": 2.9923,
      "step": 12560
    },
    {
      "epoch": 91.75182481751825,
      "grad_norm": 6.961448669433594,
      "learning_rate": 4.1240875912408754e-06,
      "loss": 2.7965,
      "step": 12570
    },
    {
      "epoch": 91.82481751824818,
      "grad_norm": 8.046936988830566,
      "learning_rate": 4.0875912408759126e-06,
      "loss": 3.1302,
      "step": 12580
    },
    {
      "epoch": 91.89781021897811,
      "grad_norm": 9.739418029785156,
      "learning_rate": 4.05109489051095e-06,
      "loss": 3.6056,
      "step": 12590
    },
    {
      "epoch": 91.97080291970804,
      "grad_norm": 8.995491981506348,
      "learning_rate": 4.014598540145985e-06,
      "loss": 3.1795,
      "step": 12600
    },
    {
      "epoch": 92.04379562043796,
      "grad_norm": 8.309297561645508,
      "learning_rate": 3.978102189781022e-06,
      "loss": 2.9467,
      "step": 12610
    },
    {
      "epoch": 92.11678832116789,
      "grad_norm": 8.449600219726562,
      "learning_rate": 3.9416058394160585e-06,
      "loss": 2.834,
      "step": 12620
    },
    {
      "epoch": 92.18978102189782,
      "grad_norm": 7.912436008453369,
      "learning_rate": 3.905109489051096e-06,
      "loss": 2.9617,
      "step": 12630
    },
    {
      "epoch": 92.26277372262774,
      "grad_norm": 8.151073455810547,
      "learning_rate": 3.868613138686131e-06,
      "loss": 3.0614,
      "step": 12640
    },
    {
      "epoch": 92.33576642335767,
      "grad_norm": 7.936684608459473,
      "learning_rate": 3.832116788321168e-06,
      "loss": 3.219,
      "step": 12650
    },
    {
      "epoch": 92.4087591240876,
      "grad_norm": 7.675193786621094,
      "learning_rate": 3.7956204379562045e-06,
      "loss": 3.1854,
      "step": 12660
    },
    {
      "epoch": 92.48175182481752,
      "grad_norm": 8.166749954223633,
      "learning_rate": 3.7591240875912412e-06,
      "loss": 3.2523,
      "step": 12670
    },
    {
      "epoch": 92.55474452554745,
      "grad_norm": 7.747161865234375,
      "learning_rate": 3.722627737226277e-06,
      "loss": 2.705,
      "step": 12680
    },
    {
      "epoch": 92.62773722627738,
      "grad_norm": 6.822691917419434,
      "learning_rate": 3.6861313868613142e-06,
      "loss": 3.0536,
      "step": 12690
    },
    {
      "epoch": 92.7007299270073,
      "grad_norm": 8.785948753356934,
      "learning_rate": 3.64963503649635e-06,
      "loss": 3.2256,
      "step": 12700
    },
    {
      "epoch": 92.77372262773723,
      "grad_norm": 8.113571166992188,
      "learning_rate": 3.6131386861313872e-06,
      "loss": 2.8398,
      "step": 12710
    },
    {
      "epoch": 92.84671532846716,
      "grad_norm": 8.10431957244873,
      "learning_rate": 3.576642335766423e-06,
      "loss": 3.088,
      "step": 12720
    },
    {
      "epoch": 92.91970802919708,
      "grad_norm": 8.537425994873047,
      "learning_rate": 3.54014598540146e-06,
      "loss": 3.2747,
      "step": 12730
    },
    {
      "epoch": 92.99270072992701,
      "grad_norm": 8.11185359954834,
      "learning_rate": 3.503649635036497e-06,
      "loss": 3.2054,
      "step": 12740
    },
    {
      "epoch": 93.06569343065694,
      "grad_norm": 7.248510837554932,
      "learning_rate": 3.4671532846715328e-06,
      "loss": 2.7041,
      "step": 12750
    },
    {
      "epoch": 93.13868613138686,
      "grad_norm": 8.71280574798584,
      "learning_rate": 3.43065693430657e-06,
      "loss": 3.452,
      "step": 12760
    },
    {
      "epoch": 93.21167883211679,
      "grad_norm": 8.367746353149414,
      "learning_rate": 3.3941605839416058e-06,
      "loss": 2.7766,
      "step": 12770
    },
    {
      "epoch": 93.28467153284672,
      "grad_norm": 8.648796081542969,
      "learning_rate": 3.357664233576643e-06,
      "loss": 3.0668,
      "step": 12780
    },
    {
      "epoch": 93.35766423357664,
      "grad_norm": 7.546871185302734,
      "learning_rate": 3.3211678832116788e-06,
      "loss": 3.1325,
      "step": 12790
    },
    {
      "epoch": 93.43065693430657,
      "grad_norm": 9.390228271484375,
      "learning_rate": 3.2846715328467155e-06,
      "loss": 2.9201,
      "step": 12800
    },
    {
      "epoch": 93.5036496350365,
      "grad_norm": 8.13111686706543,
      "learning_rate": 3.2481751824817517e-06,
      "loss": 3.2044,
      "step": 12810
    },
    {
      "epoch": 93.57664233576642,
      "grad_norm": 8.520145416259766,
      "learning_rate": 3.2116788321167884e-06,
      "loss": 3.0685,
      "step": 12820
    },
    {
      "epoch": 93.64963503649635,
      "grad_norm": 7.864901542663574,
      "learning_rate": 3.1751824817518247e-06,
      "loss": 2.8228,
      "step": 12830
    },
    {
      "epoch": 93.72262773722628,
      "grad_norm": 7.431266784667969,
      "learning_rate": 3.1386861313868614e-06,
      "loss": 3.0861,
      "step": 12840
    },
    {
      "epoch": 93.7956204379562,
      "grad_norm": 8.047242164611816,
      "learning_rate": 3.102189781021898e-06,
      "loss": 2.8664,
      "step": 12850
    },
    {
      "epoch": 93.86861313868613,
      "grad_norm": 8.506698608398438,
      "learning_rate": 3.0656934306569344e-06,
      "loss": 2.9884,
      "step": 12860
    },
    {
      "epoch": 93.94160583941606,
      "grad_norm": 8.530653953552246,
      "learning_rate": 3.029197080291971e-06,
      "loss": 3.2506,
      "step": 12870
    },
    {
      "epoch": 94.01459854014598,
      "grad_norm": 9.697714805603027,
      "learning_rate": 2.9927007299270074e-06,
      "loss": 3.0593,
      "step": 12880
    },
    {
      "epoch": 94.08759124087591,
      "grad_norm": 7.545621871948242,
      "learning_rate": 2.9562043795620437e-06,
      "loss": 2.9544,
      "step": 12890
    },
    {
      "epoch": 94.16058394160584,
      "grad_norm": 8.2058687210083,
      "learning_rate": 2.9197080291970804e-06,
      "loss": 3.1929,
      "step": 12900
    },
    {
      "epoch": 94.23357664233576,
      "grad_norm": 7.932612419128418,
      "learning_rate": 2.8832116788321167e-06,
      "loss": 2.9891,
      "step": 12910
    },
    {
      "epoch": 94.30656934306569,
      "grad_norm": 8.579239845275879,
      "learning_rate": 2.8467153284671534e-06,
      "loss": 2.6979,
      "step": 12920
    },
    {
      "epoch": 94.37956204379562,
      "grad_norm": 7.829825401306152,
      "learning_rate": 2.8102189781021897e-06,
      "loss": 3.2435,
      "step": 12930
    },
    {
      "epoch": 94.45255474452554,
      "grad_norm": 8.1825590133667,
      "learning_rate": 2.7737226277372264e-06,
      "loss": 3.4697,
      "step": 12940
    },
    {
      "epoch": 94.52554744525547,
      "grad_norm": 7.993537902832031,
      "learning_rate": 2.737226277372263e-06,
      "loss": 3.0163,
      "step": 12950
    },
    {
      "epoch": 94.5985401459854,
      "grad_norm": 8.56247329711914,
      "learning_rate": 2.7007299270072994e-06,
      "loss": 3.1148,
      "step": 12960
    },
    {
      "epoch": 94.67153284671532,
      "grad_norm": 9.118521690368652,
      "learning_rate": 2.664233576642336e-06,
      "loss": 2.8304,
      "step": 12970
    },
    {
      "epoch": 94.74452554744525,
      "grad_norm": 8.091293334960938,
      "learning_rate": 2.6277372262773724e-06,
      "loss": 2.6325,
      "step": 12980
    },
    {
      "epoch": 94.81751824817518,
      "grad_norm": 7.658874988555908,
      "learning_rate": 2.591240875912409e-06,
      "loss": 3.1036,
      "step": 12990
    },
    {
      "epoch": 94.8905109489051,
      "grad_norm": 7.767941951751709,
      "learning_rate": 2.5547445255474454e-06,
      "loss": 3.1152,
      "step": 13000
    }
  ],
  "logging_steps": 10,
  "max_steps": 13700,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 59655559620.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
