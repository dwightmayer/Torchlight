{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 992.7007299270073,
  "eval_steps": 500,
  "global_step": 136000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.072992700729927,
      "grad_norm": 7.233448028564453,
      "learning_rate": 4.999635036496351e-05,
      "loss": 5.3415,
      "step": 10
    },
    {
      "epoch": 0.145985401459854,
      "grad_norm": 7.710277080535889,
      "learning_rate": 4.999270072992701e-05,
      "loss": 5.2191,
      "step": 20
    },
    {
      "epoch": 0.21897810218978103,
      "grad_norm": 7.878846168518066,
      "learning_rate": 4.998905109489052e-05,
      "loss": 5.0764,
      "step": 30
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 7.746251106262207,
      "learning_rate": 4.998540145985401e-05,
      "loss": 5.1698,
      "step": 40
    },
    {
      "epoch": 0.36496350364963503,
      "grad_norm": 7.185173034667969,
      "learning_rate": 4.998175182481752e-05,
      "loss": 4.9842,
      "step": 50
    },
    {
      "epoch": 0.43795620437956206,
      "grad_norm": 7.737087726593018,
      "learning_rate": 4.9978102189781026e-05,
      "loss": 5.0979,
      "step": 60
    },
    {
      "epoch": 0.5109489051094891,
      "grad_norm": 7.432634353637695,
      "learning_rate": 4.9974452554744526e-05,
      "loss": 5.4533,
      "step": 70
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 7.324274063110352,
      "learning_rate": 4.9970802919708033e-05,
      "loss": 5.4244,
      "step": 80
    },
    {
      "epoch": 0.656934306569343,
      "grad_norm": 7.495400905609131,
      "learning_rate": 4.9967153284671534e-05,
      "loss": 5.2082,
      "step": 90
    },
    {
      "epoch": 0.7299270072992701,
      "grad_norm": 7.387503623962402,
      "learning_rate": 4.996350364963504e-05,
      "loss": 5.3736,
      "step": 100
    },
    {
      "epoch": 0.8029197080291971,
      "grad_norm": 7.647914886474609,
      "learning_rate": 4.995985401459854e-05,
      "loss": 5.1931,
      "step": 110
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 8.189261436462402,
      "learning_rate": 4.995620437956204e-05,
      "loss": 5.1505,
      "step": 120
    },
    {
      "epoch": 0.948905109489051,
      "grad_norm": 7.656612873077393,
      "learning_rate": 4.995255474452555e-05,
      "loss": 5.2519,
      "step": 130
    },
    {
      "epoch": 1.0218978102189782,
      "grad_norm": 7.890734672546387,
      "learning_rate": 4.994890510948905e-05,
      "loss": 5.0528,
      "step": 140
    },
    {
      "epoch": 1.094890510948905,
      "grad_norm": 7.479572772979736,
      "learning_rate": 4.994525547445256e-05,
      "loss": 4.9679,
      "step": 150
    },
    {
      "epoch": 1.167883211678832,
      "grad_norm": 7.767549514770508,
      "learning_rate": 4.9941605839416065e-05,
      "loss": 4.9272,
      "step": 160
    },
    {
      "epoch": 1.2408759124087592,
      "grad_norm": 7.210399150848389,
      "learning_rate": 4.9937956204379565e-05,
      "loss": 5.1786,
      "step": 170
    },
    {
      "epoch": 1.313868613138686,
      "grad_norm": 7.05111837387085,
      "learning_rate": 4.993430656934307e-05,
      "loss": 5.102,
      "step": 180
    },
    {
      "epoch": 1.3868613138686132,
      "grad_norm": 7.743473529815674,
      "learning_rate": 4.993065693430657e-05,
      "loss": 5.2114,
      "step": 190
    },
    {
      "epoch": 1.4598540145985401,
      "grad_norm": 7.301614284515381,
      "learning_rate": 4.992700729927007e-05,
      "loss": 5.0319,
      "step": 200
    },
    {
      "epoch": 1.5328467153284673,
      "grad_norm": 7.592796325683594,
      "learning_rate": 4.992335766423358e-05,
      "loss": 5.3491,
      "step": 210
    },
    {
      "epoch": 1.6058394160583942,
      "grad_norm": 7.291212558746338,
      "learning_rate": 4.991970802919708e-05,
      "loss": 5.1048,
      "step": 220
    },
    {
      "epoch": 1.6788321167883211,
      "grad_norm": 7.620528697967529,
      "learning_rate": 4.991605839416059e-05,
      "loss": 5.1762,
      "step": 230
    },
    {
      "epoch": 1.7518248175182483,
      "grad_norm": 7.328744888305664,
      "learning_rate": 4.991240875912409e-05,
      "loss": 4.8436,
      "step": 240
    },
    {
      "epoch": 1.8248175182481752,
      "grad_norm": 7.44127893447876,
      "learning_rate": 4.9908759124087596e-05,
      "loss": 4.9213,
      "step": 250
    },
    {
      "epoch": 1.897810218978102,
      "grad_norm": 7.753246784210205,
      "learning_rate": 4.9905109489051097e-05,
      "loss": 5.1677,
      "step": 260
    },
    {
      "epoch": 1.9708029197080292,
      "grad_norm": 8.092667579650879,
      "learning_rate": 4.99014598540146e-05,
      "loss": 5.4145,
      "step": 270
    },
    {
      "epoch": 2.0437956204379564,
      "grad_norm": 7.129220485687256,
      "learning_rate": 4.9897810218978104e-05,
      "loss": 5.1284,
      "step": 280
    },
    {
      "epoch": 2.116788321167883,
      "grad_norm": 7.1641316413879395,
      "learning_rate": 4.9894160583941605e-05,
      "loss": 5.1893,
      "step": 290
    },
    {
      "epoch": 2.18978102189781,
      "grad_norm": 7.7331461906433105,
      "learning_rate": 4.989051094890511e-05,
      "loss": 4.9792,
      "step": 300
    },
    {
      "epoch": 2.2627737226277373,
      "grad_norm": 8.175004005432129,
      "learning_rate": 4.988686131386861e-05,
      "loss": 5.415,
      "step": 310
    },
    {
      "epoch": 2.335766423357664,
      "grad_norm": 7.03812313079834,
      "learning_rate": 4.988321167883212e-05,
      "loss": 5.0487,
      "step": 320
    },
    {
      "epoch": 2.408759124087591,
      "grad_norm": 7.389960765838623,
      "learning_rate": 4.987956204379563e-05,
      "loss": 4.8633,
      "step": 330
    },
    {
      "epoch": 2.4817518248175183,
      "grad_norm": 7.51225471496582,
      "learning_rate": 4.987591240875913e-05,
      "loss": 5.0612,
      "step": 340
    },
    {
      "epoch": 2.554744525547445,
      "grad_norm": 7.567586421966553,
      "learning_rate": 4.987226277372263e-05,
      "loss": 5.3462,
      "step": 350
    },
    {
      "epoch": 2.627737226277372,
      "grad_norm": 8.109453201293945,
      "learning_rate": 4.9868613138686135e-05,
      "loss": 4.7821,
      "step": 360
    },
    {
      "epoch": 2.7007299270072993,
      "grad_norm": 7.2880449295043945,
      "learning_rate": 4.9864963503649636e-05,
      "loss": 4.9887,
      "step": 370
    },
    {
      "epoch": 2.7737226277372264,
      "grad_norm": 7.6605706214904785,
      "learning_rate": 4.986131386861314e-05,
      "loss": 5.1603,
      "step": 380
    },
    {
      "epoch": 2.846715328467153,
      "grad_norm": 9.409475326538086,
      "learning_rate": 4.9857664233576644e-05,
      "loss": 4.7396,
      "step": 390
    },
    {
      "epoch": 2.9197080291970803,
      "grad_norm": 8.065736770629883,
      "learning_rate": 4.985401459854015e-05,
      "loss": 4.9479,
      "step": 400
    },
    {
      "epoch": 2.9927007299270074,
      "grad_norm": 7.4485039710998535,
      "learning_rate": 4.985036496350365e-05,
      "loss": 4.9872,
      "step": 410
    },
    {
      "epoch": 3.065693430656934,
      "grad_norm": 7.944391250610352,
      "learning_rate": 4.984671532846716e-05,
      "loss": 4.9151,
      "step": 420
    },
    {
      "epoch": 3.1386861313868613,
      "grad_norm": 7.429919242858887,
      "learning_rate": 4.984306569343066e-05,
      "loss": 5.0795,
      "step": 430
    },
    {
      "epoch": 3.2116788321167884,
      "grad_norm": 7.629611492156982,
      "learning_rate": 4.983941605839416e-05,
      "loss": 4.8276,
      "step": 440
    },
    {
      "epoch": 3.2846715328467155,
      "grad_norm": 7.868202209472656,
      "learning_rate": 4.983576642335767e-05,
      "loss": 4.9803,
      "step": 450
    },
    {
      "epoch": 3.3576642335766422,
      "grad_norm": 7.715244770050049,
      "learning_rate": 4.983211678832117e-05,
      "loss": 5.0671,
      "step": 460
    },
    {
      "epoch": 3.4306569343065694,
      "grad_norm": 7.625002861022949,
      "learning_rate": 4.9828467153284675e-05,
      "loss": 4.871,
      "step": 470
    },
    {
      "epoch": 3.5036496350364965,
      "grad_norm": 7.622501373291016,
      "learning_rate": 4.982481751824818e-05,
      "loss": 4.8039,
      "step": 480
    },
    {
      "epoch": 3.576642335766423,
      "grad_norm": 7.63705587387085,
      "learning_rate": 4.982116788321168e-05,
      "loss": 5.0699,
      "step": 490
    },
    {
      "epoch": 3.6496350364963503,
      "grad_norm": 7.565423011779785,
      "learning_rate": 4.981751824817518e-05,
      "loss": 5.015,
      "step": 500
    },
    {
      "epoch": 3.7226277372262775,
      "grad_norm": 7.748472213745117,
      "learning_rate": 4.9813868613138683e-05,
      "loss": 4.9349,
      "step": 510
    },
    {
      "epoch": 3.795620437956204,
      "grad_norm": 7.133779525756836,
      "learning_rate": 4.981021897810219e-05,
      "loss": 5.0795,
      "step": 520
    },
    {
      "epoch": 3.8686131386861313,
      "grad_norm": 7.4835124015808105,
      "learning_rate": 4.98065693430657e-05,
      "loss": 5.03,
      "step": 530
    },
    {
      "epoch": 3.9416058394160585,
      "grad_norm": 7.532141208648682,
      "learning_rate": 4.98029197080292e-05,
      "loss": 4.6884,
      "step": 540
    },
    {
      "epoch": 4.014598540145985,
      "grad_norm": 7.423975944519043,
      "learning_rate": 4.9799270072992706e-05,
      "loss": 5.0058,
      "step": 550
    },
    {
      "epoch": 4.087591240875913,
      "grad_norm": 7.641940593719482,
      "learning_rate": 4.9795620437956206e-05,
      "loss": 4.9878,
      "step": 560
    },
    {
      "epoch": 4.160583941605839,
      "grad_norm": 7.445023536682129,
      "learning_rate": 4.9791970802919714e-05,
      "loss": 4.971,
      "step": 570
    },
    {
      "epoch": 4.233576642335766,
      "grad_norm": 7.835362434387207,
      "learning_rate": 4.9788321167883214e-05,
      "loss": 4.7709,
      "step": 580
    },
    {
      "epoch": 4.306569343065694,
      "grad_norm": 7.127959728240967,
      "learning_rate": 4.9784671532846715e-05,
      "loss": 4.9054,
      "step": 590
    },
    {
      "epoch": 4.37956204379562,
      "grad_norm": 7.157235622406006,
      "learning_rate": 4.978102189781022e-05,
      "loss": 4.7159,
      "step": 600
    },
    {
      "epoch": 4.452554744525547,
      "grad_norm": 7.672778129577637,
      "learning_rate": 4.977737226277372e-05,
      "loss": 5.3305,
      "step": 610
    },
    {
      "epoch": 4.525547445255475,
      "grad_norm": 6.952612400054932,
      "learning_rate": 4.977372262773723e-05,
      "loss": 4.8352,
      "step": 620
    },
    {
      "epoch": 4.598540145985401,
      "grad_norm": 7.314266681671143,
      "learning_rate": 4.977007299270074e-05,
      "loss": 4.8215,
      "step": 630
    },
    {
      "epoch": 4.671532846715328,
      "grad_norm": 7.758754730224609,
      "learning_rate": 4.976642335766424e-05,
      "loss": 5.1082,
      "step": 640
    },
    {
      "epoch": 4.744525547445256,
      "grad_norm": 8.258482933044434,
      "learning_rate": 4.976277372262774e-05,
      "loss": 5.0294,
      "step": 650
    },
    {
      "epoch": 4.817518248175182,
      "grad_norm": 7.150320529937744,
      "learning_rate": 4.975912408759124e-05,
      "loss": 4.8035,
      "step": 660
    },
    {
      "epoch": 4.89051094890511,
      "grad_norm": 7.30391263961792,
      "learning_rate": 4.9755474452554746e-05,
      "loss": 4.8588,
      "step": 670
    },
    {
      "epoch": 4.963503649635037,
      "grad_norm": 7.492269992828369,
      "learning_rate": 4.975182481751825e-05,
      "loss": 4.6214,
      "step": 680
    },
    {
      "epoch": 5.036496350364963,
      "grad_norm": 7.340161323547363,
      "learning_rate": 4.974817518248175e-05,
      "loss": 4.6687,
      "step": 690
    },
    {
      "epoch": 5.109489051094891,
      "grad_norm": 7.667232513427734,
      "learning_rate": 4.974452554744526e-05,
      "loss": 4.6613,
      "step": 700
    },
    {
      "epoch": 5.182481751824818,
      "grad_norm": 7.9329609870910645,
      "learning_rate": 4.974087591240876e-05,
      "loss": 4.9421,
      "step": 710
    },
    {
      "epoch": 5.255474452554744,
      "grad_norm": 7.173572540283203,
      "learning_rate": 4.973722627737227e-05,
      "loss": 4.8285,
      "step": 720
    },
    {
      "epoch": 5.328467153284672,
      "grad_norm": 6.954929351806641,
      "learning_rate": 4.973357664233577e-05,
      "loss": 4.7941,
      "step": 730
    },
    {
      "epoch": 5.401459854014599,
      "grad_norm": 7.41072940826416,
      "learning_rate": 4.972992700729927e-05,
      "loss": 4.8934,
      "step": 740
    },
    {
      "epoch": 5.474452554744525,
      "grad_norm": 8.072057723999023,
      "learning_rate": 4.972627737226278e-05,
      "loss": 4.7738,
      "step": 750
    },
    {
      "epoch": 5.547445255474453,
      "grad_norm": 7.325993061065674,
      "learning_rate": 4.972262773722628e-05,
      "loss": 4.9454,
      "step": 760
    },
    {
      "epoch": 5.62043795620438,
      "grad_norm": 7.3861823081970215,
      "learning_rate": 4.9718978102189784e-05,
      "loss": 4.9027,
      "step": 770
    },
    {
      "epoch": 5.693430656934306,
      "grad_norm": 7.651785373687744,
      "learning_rate": 4.9715328467153285e-05,
      "loss": 4.7806,
      "step": 780
    },
    {
      "epoch": 5.766423357664234,
      "grad_norm": 7.205080509185791,
      "learning_rate": 4.971167883211679e-05,
      "loss": 4.9587,
      "step": 790
    },
    {
      "epoch": 5.839416058394161,
      "grad_norm": 7.671054363250732,
      "learning_rate": 4.97080291970803e-05,
      "loss": 4.8891,
      "step": 800
    },
    {
      "epoch": 5.912408759124087,
      "grad_norm": 7.282037734985352,
      "learning_rate": 4.970437956204379e-05,
      "loss": 4.7997,
      "step": 810
    },
    {
      "epoch": 5.985401459854015,
      "grad_norm": 7.299559116363525,
      "learning_rate": 4.97007299270073e-05,
      "loss": 4.623,
      "step": 820
    },
    {
      "epoch": 6.0583941605839415,
      "grad_norm": 7.366621494293213,
      "learning_rate": 4.969708029197081e-05,
      "loss": 5.1839,
      "step": 830
    },
    {
      "epoch": 6.131386861313868,
      "grad_norm": 7.488300323486328,
      "learning_rate": 4.969343065693431e-05,
      "loss": 4.8539,
      "step": 840
    },
    {
      "epoch": 6.204379562043796,
      "grad_norm": 7.30263090133667,
      "learning_rate": 4.9689781021897815e-05,
      "loss": 4.7861,
      "step": 850
    },
    {
      "epoch": 6.2773722627737225,
      "grad_norm": 7.001586437225342,
      "learning_rate": 4.9686131386861316e-05,
      "loss": 4.9283,
      "step": 860
    },
    {
      "epoch": 6.350364963503649,
      "grad_norm": 7.28912353515625,
      "learning_rate": 4.968248175182482e-05,
      "loss": 4.7641,
      "step": 870
    },
    {
      "epoch": 6.423357664233577,
      "grad_norm": 7.5755767822265625,
      "learning_rate": 4.9678832116788324e-05,
      "loss": 4.7465,
      "step": 880
    },
    {
      "epoch": 6.4963503649635035,
      "grad_norm": 7.406926155090332,
      "learning_rate": 4.9675182481751824e-05,
      "loss": 4.6989,
      "step": 890
    },
    {
      "epoch": 6.569343065693431,
      "grad_norm": 7.2845540046691895,
      "learning_rate": 4.967153284671533e-05,
      "loss": 4.6892,
      "step": 900
    },
    {
      "epoch": 6.642335766423358,
      "grad_norm": 7.8952107429504395,
      "learning_rate": 4.966788321167883e-05,
      "loss": 4.9373,
      "step": 910
    },
    {
      "epoch": 6.7153284671532845,
      "grad_norm": 7.082705974578857,
      "learning_rate": 4.966423357664234e-05,
      "loss": 4.5528,
      "step": 920
    },
    {
      "epoch": 6.788321167883212,
      "grad_norm": 7.4001383781433105,
      "learning_rate": 4.966058394160584e-05,
      "loss": 4.6896,
      "step": 930
    },
    {
      "epoch": 6.861313868613139,
      "grad_norm": 7.117550849914551,
      "learning_rate": 4.965693430656935e-05,
      "loss": 4.8358,
      "step": 940
    },
    {
      "epoch": 6.934306569343065,
      "grad_norm": 7.402454376220703,
      "learning_rate": 4.9653284671532854e-05,
      "loss": 4.7418,
      "step": 950
    },
    {
      "epoch": 7.007299270072993,
      "grad_norm": 8.569323539733887,
      "learning_rate": 4.964963503649635e-05,
      "loss": 4.6554,
      "step": 960
    },
    {
      "epoch": 7.08029197080292,
      "grad_norm": 7.148552894592285,
      "learning_rate": 4.9645985401459855e-05,
      "loss": 4.9784,
      "step": 970
    },
    {
      "epoch": 7.153284671532846,
      "grad_norm": 7.142611503601074,
      "learning_rate": 4.9642335766423356e-05,
      "loss": 4.568,
      "step": 980
    },
    {
      "epoch": 7.226277372262774,
      "grad_norm": 7.675945281982422,
      "learning_rate": 4.963868613138686e-05,
      "loss": 4.7567,
      "step": 990
    },
    {
      "epoch": 7.299270072992701,
      "grad_norm": 7.6412787437438965,
      "learning_rate": 4.963503649635037e-05,
      "loss": 4.806,
      "step": 1000
    },
    {
      "epoch": 7.372262773722627,
      "grad_norm": 7.223672389984131,
      "learning_rate": 4.963138686131387e-05,
      "loss": 4.6233,
      "step": 1010
    },
    {
      "epoch": 7.445255474452555,
      "grad_norm": 7.117713928222656,
      "learning_rate": 4.962773722627738e-05,
      "loss": 4.6633,
      "step": 1020
    },
    {
      "epoch": 7.518248175182482,
      "grad_norm": 7.211491107940674,
      "learning_rate": 4.962408759124088e-05,
      "loss": 4.9392,
      "step": 1030
    },
    {
      "epoch": 7.591240875912408,
      "grad_norm": 7.770771503448486,
      "learning_rate": 4.962043795620438e-05,
      "loss": 4.7415,
      "step": 1040
    },
    {
      "epoch": 7.664233576642336,
      "grad_norm": 7.403050899505615,
      "learning_rate": 4.9616788321167886e-05,
      "loss": 4.9195,
      "step": 1050
    },
    {
      "epoch": 7.737226277372263,
      "grad_norm": 7.230221271514893,
      "learning_rate": 4.961313868613139e-05,
      "loss": 4.7148,
      "step": 1060
    },
    {
      "epoch": 7.81021897810219,
      "grad_norm": 7.140359401702881,
      "learning_rate": 4.9609489051094894e-05,
      "loss": 4.5194,
      "step": 1070
    },
    {
      "epoch": 7.883211678832117,
      "grad_norm": 7.7395100593566895,
      "learning_rate": 4.9605839416058395e-05,
      "loss": 4.9096,
      "step": 1080
    },
    {
      "epoch": 7.956204379562044,
      "grad_norm": 7.365215301513672,
      "learning_rate": 4.96021897810219e-05,
      "loss": 4.6141,
      "step": 1090
    },
    {
      "epoch": 8.02919708029197,
      "grad_norm": 7.54665994644165,
      "learning_rate": 4.959854014598541e-05,
      "loss": 4.6009,
      "step": 1100
    },
    {
      "epoch": 8.102189781021897,
      "grad_norm": 7.056748867034912,
      "learning_rate": 4.95948905109489e-05,
      "loss": 4.6253,
      "step": 1110
    },
    {
      "epoch": 8.175182481751825,
      "grad_norm": 7.515693664550781,
      "learning_rate": 4.959124087591241e-05,
      "loss": 4.6951,
      "step": 1120
    },
    {
      "epoch": 8.248175182481752,
      "grad_norm": 7.525048732757568,
      "learning_rate": 4.958759124087591e-05,
      "loss": 4.7978,
      "step": 1130
    },
    {
      "epoch": 8.321167883211679,
      "grad_norm": 7.068617820739746,
      "learning_rate": 4.958394160583942e-05,
      "loss": 4.9195,
      "step": 1140
    },
    {
      "epoch": 8.394160583941606,
      "grad_norm": 7.6600446701049805,
      "learning_rate": 4.9580291970802925e-05,
      "loss": 4.8926,
      "step": 1150
    },
    {
      "epoch": 8.467153284671532,
      "grad_norm": 7.045804023742676,
      "learning_rate": 4.9576642335766426e-05,
      "loss": 4.5631,
      "step": 1160
    },
    {
      "epoch": 8.540145985401459,
      "grad_norm": 7.662268161773682,
      "learning_rate": 4.957299270072993e-05,
      "loss": 4.5387,
      "step": 1170
    },
    {
      "epoch": 8.613138686131387,
      "grad_norm": 7.248735427856445,
      "learning_rate": 4.9569343065693433e-05,
      "loss": 4.8471,
      "step": 1180
    },
    {
      "epoch": 8.686131386861314,
      "grad_norm": 7.219252109527588,
      "learning_rate": 4.9565693430656934e-05,
      "loss": 4.5846,
      "step": 1190
    },
    {
      "epoch": 8.75912408759124,
      "grad_norm": 7.438129425048828,
      "learning_rate": 4.956204379562044e-05,
      "loss": 4.463,
      "step": 1200
    },
    {
      "epoch": 8.832116788321168,
      "grad_norm": 7.072973728179932,
      "learning_rate": 4.955839416058394e-05,
      "loss": 4.9377,
      "step": 1210
    },
    {
      "epoch": 8.905109489051094,
      "grad_norm": 7.1557111740112305,
      "learning_rate": 4.955474452554745e-05,
      "loss": 4.7158,
      "step": 1220
    },
    {
      "epoch": 8.978102189781023,
      "grad_norm": 8.036341667175293,
      "learning_rate": 4.955109489051095e-05,
      "loss": 4.6721,
      "step": 1230
    },
    {
      "epoch": 9.05109489051095,
      "grad_norm": 7.6620635986328125,
      "learning_rate": 4.954744525547446e-05,
      "loss": 4.7275,
      "step": 1240
    },
    {
      "epoch": 9.124087591240876,
      "grad_norm": 7.748382568359375,
      "learning_rate": 4.954379562043796e-05,
      "loss": 4.5182,
      "step": 1250
    },
    {
      "epoch": 9.197080291970803,
      "grad_norm": 7.330703258514404,
      "learning_rate": 4.9540145985401464e-05,
      "loss": 4.4322,
      "step": 1260
    },
    {
      "epoch": 9.27007299270073,
      "grad_norm": 7.381096363067627,
      "learning_rate": 4.9536496350364965e-05,
      "loss": 4.8178,
      "step": 1270
    },
    {
      "epoch": 9.343065693430656,
      "grad_norm": 7.263308048248291,
      "learning_rate": 4.9532846715328465e-05,
      "loss": 4.6187,
      "step": 1280
    },
    {
      "epoch": 9.416058394160585,
      "grad_norm": 7.35212516784668,
      "learning_rate": 4.952919708029197e-05,
      "loss": 4.6743,
      "step": 1290
    },
    {
      "epoch": 9.489051094890511,
      "grad_norm": 7.362287998199463,
      "learning_rate": 4.952554744525548e-05,
      "loss": 4.5823,
      "step": 1300
    },
    {
      "epoch": 9.562043795620438,
      "grad_norm": 7.630438804626465,
      "learning_rate": 4.952189781021898e-05,
      "loss": 4.9973,
      "step": 1310
    },
    {
      "epoch": 9.635036496350365,
      "grad_norm": 7.403014183044434,
      "learning_rate": 4.951824817518249e-05,
      "loss": 4.7452,
      "step": 1320
    },
    {
      "epoch": 9.708029197080291,
      "grad_norm": 6.930708885192871,
      "learning_rate": 4.951459854014599e-05,
      "loss": 4.5414,
      "step": 1330
    },
    {
      "epoch": 9.78102189781022,
      "grad_norm": 7.377804279327393,
      "learning_rate": 4.951094890510949e-05,
      "loss": 4.7567,
      "step": 1340
    },
    {
      "epoch": 9.854014598540147,
      "grad_norm": 7.060654163360596,
      "learning_rate": 4.9507299270072996e-05,
      "loss": 4.4837,
      "step": 1350
    },
    {
      "epoch": 9.927007299270073,
      "grad_norm": 7.235537052154541,
      "learning_rate": 4.9503649635036497e-05,
      "loss": 4.7299,
      "step": 1360
    },
    {
      "epoch": 10.0,
      "grad_norm": 10.311055183410645,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 4.1499,
      "step": 1370
    },
    {
      "epoch": 10.072992700729927,
      "grad_norm": 7.465965747833252,
      "learning_rate": 4.9496350364963504e-05,
      "loss": 4.6872,
      "step": 1380
    },
    {
      "epoch": 10.145985401459853,
      "grad_norm": 7.486293315887451,
      "learning_rate": 4.949270072992701e-05,
      "loss": 4.5298,
      "step": 1390
    },
    {
      "epoch": 10.218978102189782,
      "grad_norm": 7.3736186027526855,
      "learning_rate": 4.948905109489051e-05,
      "loss": 4.5769,
      "step": 1400
    },
    {
      "epoch": 10.291970802919709,
      "grad_norm": 7.007763862609863,
      "learning_rate": 4.948540145985402e-05,
      "loss": 4.8051,
      "step": 1410
    },
    {
      "epoch": 10.364963503649635,
      "grad_norm": 7.211528778076172,
      "learning_rate": 4.948175182481752e-05,
      "loss": 4.4052,
      "step": 1420
    },
    {
      "epoch": 10.437956204379562,
      "grad_norm": 7.625153064727783,
      "learning_rate": 4.947810218978102e-05,
      "loss": 4.6061,
      "step": 1430
    },
    {
      "epoch": 10.510948905109489,
      "grad_norm": 7.507622241973877,
      "learning_rate": 4.947445255474453e-05,
      "loss": 4.8932,
      "step": 1440
    },
    {
      "epoch": 10.583941605839415,
      "grad_norm": 7.813236236572266,
      "learning_rate": 4.947080291970803e-05,
      "loss": 4.5846,
      "step": 1450
    },
    {
      "epoch": 10.656934306569344,
      "grad_norm": 7.028400897979736,
      "learning_rate": 4.9467153284671535e-05,
      "loss": 4.6039,
      "step": 1460
    },
    {
      "epoch": 10.72992700729927,
      "grad_norm": 7.343578338623047,
      "learning_rate": 4.946350364963504e-05,
      "loss": 4.7329,
      "step": 1470
    },
    {
      "epoch": 10.802919708029197,
      "grad_norm": 7.269529342651367,
      "learning_rate": 4.945985401459854e-05,
      "loss": 4.7615,
      "step": 1480
    },
    {
      "epoch": 10.875912408759124,
      "grad_norm": 7.052208423614502,
      "learning_rate": 4.945620437956205e-05,
      "loss": 4.4063,
      "step": 1490
    },
    {
      "epoch": 10.94890510948905,
      "grad_norm": 7.148497104644775,
      "learning_rate": 4.945255474452555e-05,
      "loss": 4.4575,
      "step": 1500
    },
    {
      "epoch": 11.021897810218977,
      "grad_norm": 6.92938232421875,
      "learning_rate": 4.944890510948905e-05,
      "loss": 4.5308,
      "step": 1510
    },
    {
      "epoch": 11.094890510948906,
      "grad_norm": 7.264023303985596,
      "learning_rate": 4.944525547445256e-05,
      "loss": 4.4207,
      "step": 1520
    },
    {
      "epoch": 11.167883211678832,
      "grad_norm": 7.417516231536865,
      "learning_rate": 4.944160583941606e-05,
      "loss": 4.4569,
      "step": 1530
    },
    {
      "epoch": 11.24087591240876,
      "grad_norm": 7.53663969039917,
      "learning_rate": 4.9437956204379566e-05,
      "loss": 4.5498,
      "step": 1540
    },
    {
      "epoch": 11.313868613138686,
      "grad_norm": 7.468682765960693,
      "learning_rate": 4.943430656934307e-05,
      "loss": 4.5428,
      "step": 1550
    },
    {
      "epoch": 11.386861313868613,
      "grad_norm": 7.470537185668945,
      "learning_rate": 4.9430656934306574e-05,
      "loss": 4.3486,
      "step": 1560
    },
    {
      "epoch": 11.459854014598541,
      "grad_norm": 7.344502925872803,
      "learning_rate": 4.9427007299270075e-05,
      "loss": 4.8279,
      "step": 1570
    },
    {
      "epoch": 11.532846715328468,
      "grad_norm": 7.121424674987793,
      "learning_rate": 4.9423357664233575e-05,
      "loss": 4.4228,
      "step": 1580
    },
    {
      "epoch": 11.605839416058394,
      "grad_norm": 8.808384895324707,
      "learning_rate": 4.941970802919708e-05,
      "loss": 4.5519,
      "step": 1590
    },
    {
      "epoch": 11.678832116788321,
      "grad_norm": 7.344605922698975,
      "learning_rate": 4.941605839416058e-05,
      "loss": 4.4597,
      "step": 1600
    },
    {
      "epoch": 11.751824817518248,
      "grad_norm": 6.857353687286377,
      "learning_rate": 4.941240875912409e-05,
      "loss": 4.7894,
      "step": 1610
    },
    {
      "epoch": 11.824817518248175,
      "grad_norm": 7.602167129516602,
      "learning_rate": 4.94087591240876e-05,
      "loss": 4.5959,
      "step": 1620
    },
    {
      "epoch": 11.897810218978103,
      "grad_norm": 7.041274070739746,
      "learning_rate": 4.94051094890511e-05,
      "loss": 4.7881,
      "step": 1630
    },
    {
      "epoch": 11.97080291970803,
      "grad_norm": 7.262753009796143,
      "learning_rate": 4.9401459854014605e-05,
      "loss": 4.8172,
      "step": 1640
    },
    {
      "epoch": 12.043795620437956,
      "grad_norm": 7.443511009216309,
      "learning_rate": 4.93978102189781e-05,
      "loss": 4.389,
      "step": 1650
    },
    {
      "epoch": 12.116788321167883,
      "grad_norm": 6.696727752685547,
      "learning_rate": 4.9394160583941606e-05,
      "loss": 4.4689,
      "step": 1660
    },
    {
      "epoch": 12.18978102189781,
      "grad_norm": 7.294945240020752,
      "learning_rate": 4.9390510948905113e-05,
      "loss": 4.6704,
      "step": 1670
    },
    {
      "epoch": 12.262773722627736,
      "grad_norm": 7.212015151977539,
      "learning_rate": 4.9386861313868614e-05,
      "loss": 4.4638,
      "step": 1680
    },
    {
      "epoch": 12.335766423357665,
      "grad_norm": 7.660646438598633,
      "learning_rate": 4.938321167883212e-05,
      "loss": 4.4679,
      "step": 1690
    },
    {
      "epoch": 12.408759124087592,
      "grad_norm": 7.372898101806641,
      "learning_rate": 4.937956204379562e-05,
      "loss": 4.3959,
      "step": 1700
    },
    {
      "epoch": 12.481751824817518,
      "grad_norm": 7.634535312652588,
      "learning_rate": 4.937591240875913e-05,
      "loss": 4.842,
      "step": 1710
    },
    {
      "epoch": 12.554744525547445,
      "grad_norm": 7.567924499511719,
      "learning_rate": 4.937226277372263e-05,
      "loss": 4.3684,
      "step": 1720
    },
    {
      "epoch": 12.627737226277372,
      "grad_norm": 7.148011684417725,
      "learning_rate": 4.936861313868613e-05,
      "loss": 4.5412,
      "step": 1730
    },
    {
      "epoch": 12.700729927007298,
      "grad_norm": 7.310725212097168,
      "learning_rate": 4.936496350364964e-05,
      "loss": 4.6308,
      "step": 1740
    },
    {
      "epoch": 12.773722627737227,
      "grad_norm": 7.443009376525879,
      "learning_rate": 4.936131386861314e-05,
      "loss": 4.5466,
      "step": 1750
    },
    {
      "epoch": 12.846715328467154,
      "grad_norm": 7.436630725860596,
      "learning_rate": 4.9357664233576645e-05,
      "loss": 4.3334,
      "step": 1760
    },
    {
      "epoch": 12.91970802919708,
      "grad_norm": 7.443007469177246,
      "learning_rate": 4.935401459854015e-05,
      "loss": 4.4938,
      "step": 1770
    },
    {
      "epoch": 12.992700729927007,
      "grad_norm": 7.988845348358154,
      "learning_rate": 4.935036496350365e-05,
      "loss": 4.7223,
      "step": 1780
    },
    {
      "epoch": 13.065693430656934,
      "grad_norm": 7.2566447257995605,
      "learning_rate": 4.934671532846716e-05,
      "loss": 4.641,
      "step": 1790
    },
    {
      "epoch": 13.138686131386862,
      "grad_norm": 7.113708972930908,
      "learning_rate": 4.9343065693430654e-05,
      "loss": 4.417,
      "step": 1800
    },
    {
      "epoch": 13.211678832116789,
      "grad_norm": 7.602547645568848,
      "learning_rate": 4.933941605839416e-05,
      "loss": 4.4153,
      "step": 1810
    },
    {
      "epoch": 13.284671532846716,
      "grad_norm": 7.633074760437012,
      "learning_rate": 4.933576642335767e-05,
      "loss": 4.4053,
      "step": 1820
    },
    {
      "epoch": 13.357664233576642,
      "grad_norm": 7.302372932434082,
      "learning_rate": 4.933211678832117e-05,
      "loss": 4.4096,
      "step": 1830
    },
    {
      "epoch": 13.430656934306569,
      "grad_norm": 6.834271430969238,
      "learning_rate": 4.9328467153284676e-05,
      "loss": 4.3511,
      "step": 1840
    },
    {
      "epoch": 13.503649635036496,
      "grad_norm": 7.77001428604126,
      "learning_rate": 4.9324817518248177e-05,
      "loss": 4.7505,
      "step": 1850
    },
    {
      "epoch": 13.576642335766424,
      "grad_norm": 7.890257835388184,
      "learning_rate": 4.9321167883211684e-05,
      "loss": 4.458,
      "step": 1860
    },
    {
      "epoch": 13.64963503649635,
      "grad_norm": 7.067137241363525,
      "learning_rate": 4.9317518248175184e-05,
      "loss": 4.4033,
      "step": 1870
    },
    {
      "epoch": 13.722627737226277,
      "grad_norm": 7.452314853668213,
      "learning_rate": 4.9313868613138685e-05,
      "loss": 4.3455,
      "step": 1880
    },
    {
      "epoch": 13.795620437956204,
      "grad_norm": 7.4249420166015625,
      "learning_rate": 4.931021897810219e-05,
      "loss": 4.356,
      "step": 1890
    },
    {
      "epoch": 13.86861313868613,
      "grad_norm": 10.507268905639648,
      "learning_rate": 4.930656934306569e-05,
      "loss": 4.6965,
      "step": 1900
    },
    {
      "epoch": 13.941605839416058,
      "grad_norm": 7.0041890144348145,
      "learning_rate": 4.93029197080292e-05,
      "loss": 4.5335,
      "step": 1910
    },
    {
      "epoch": 14.014598540145986,
      "grad_norm": 7.666934013366699,
      "learning_rate": 4.92992700729927e-05,
      "loss": 4.4317,
      "step": 1920
    },
    {
      "epoch": 14.087591240875913,
      "grad_norm": 7.5557475090026855,
      "learning_rate": 4.929562043795621e-05,
      "loss": 4.5534,
      "step": 1930
    },
    {
      "epoch": 14.16058394160584,
      "grad_norm": 7.140507698059082,
      "learning_rate": 4.9291970802919715e-05,
      "loss": 4.0654,
      "step": 1940
    },
    {
      "epoch": 14.233576642335766,
      "grad_norm": 7.0641961097717285,
      "learning_rate": 4.928832116788321e-05,
      "loss": 4.2609,
      "step": 1950
    },
    {
      "epoch": 14.306569343065693,
      "grad_norm": 7.764932632446289,
      "learning_rate": 4.9284671532846716e-05,
      "loss": 4.1631,
      "step": 1960
    },
    {
      "epoch": 14.37956204379562,
      "grad_norm": 7.251323699951172,
      "learning_rate": 4.928102189781022e-05,
      "loss": 4.5347,
      "step": 1970
    },
    {
      "epoch": 14.452554744525548,
      "grad_norm": 7.440316677093506,
      "learning_rate": 4.9277372262773724e-05,
      "loss": 4.6316,
      "step": 1980
    },
    {
      "epoch": 14.525547445255475,
      "grad_norm": 7.448154449462891,
      "learning_rate": 4.927372262773723e-05,
      "loss": 4.5561,
      "step": 1990
    },
    {
      "epoch": 14.598540145985401,
      "grad_norm": 7.45594596862793,
      "learning_rate": 4.927007299270073e-05,
      "loss": 4.3217,
      "step": 2000
    },
    {
      "epoch": 14.671532846715328,
      "grad_norm": 7.529463291168213,
      "learning_rate": 4.926642335766424e-05,
      "loss": 4.563,
      "step": 2010
    },
    {
      "epoch": 14.744525547445255,
      "grad_norm": 7.157505035400391,
      "learning_rate": 4.926277372262774e-05,
      "loss": 4.2141,
      "step": 2020
    },
    {
      "epoch": 14.817518248175183,
      "grad_norm": 7.149452209472656,
      "learning_rate": 4.925912408759124e-05,
      "loss": 4.8484,
      "step": 2030
    },
    {
      "epoch": 14.89051094890511,
      "grad_norm": 7.70466947555542,
      "learning_rate": 4.925547445255475e-05,
      "loss": 4.7783,
      "step": 2040
    },
    {
      "epoch": 14.963503649635037,
      "grad_norm": 7.512763500213623,
      "learning_rate": 4.925182481751825e-05,
      "loss": 4.2692,
      "step": 2050
    },
    {
      "epoch": 15.036496350364963,
      "grad_norm": 6.945285797119141,
      "learning_rate": 4.9248175182481755e-05,
      "loss": 4.3363,
      "step": 2060
    },
    {
      "epoch": 15.10948905109489,
      "grad_norm": 6.939780235290527,
      "learning_rate": 4.9244525547445255e-05,
      "loss": 4.4906,
      "step": 2070
    },
    {
      "epoch": 15.182481751824817,
      "grad_norm": 7.576019287109375,
      "learning_rate": 4.924087591240876e-05,
      "loss": 4.6228,
      "step": 2080
    },
    {
      "epoch": 15.255474452554745,
      "grad_norm": 7.680514335632324,
      "learning_rate": 4.923722627737227e-05,
      "loss": 4.3312,
      "step": 2090
    },
    {
      "epoch": 15.328467153284672,
      "grad_norm": 7.9071478843688965,
      "learning_rate": 4.923357664233577e-05,
      "loss": 4.3177,
      "step": 2100
    },
    {
      "epoch": 15.401459854014599,
      "grad_norm": 7.418558597564697,
      "learning_rate": 4.922992700729927e-05,
      "loss": 4.2783,
      "step": 2110
    },
    {
      "epoch": 15.474452554744525,
      "grad_norm": 8.03282356262207,
      "learning_rate": 4.922627737226277e-05,
      "loss": 4.5507,
      "step": 2120
    },
    {
      "epoch": 15.547445255474452,
      "grad_norm": 7.375392436981201,
      "learning_rate": 4.922262773722628e-05,
      "loss": 4.3811,
      "step": 2130
    },
    {
      "epoch": 15.62043795620438,
      "grad_norm": 7.306727409362793,
      "learning_rate": 4.9218978102189786e-05,
      "loss": 4.2555,
      "step": 2140
    },
    {
      "epoch": 15.693430656934307,
      "grad_norm": 7.333770275115967,
      "learning_rate": 4.9215328467153286e-05,
      "loss": 4.4135,
      "step": 2150
    },
    {
      "epoch": 15.766423357664234,
      "grad_norm": 7.5260162353515625,
      "learning_rate": 4.9211678832116794e-05,
      "loss": 4.3572,
      "step": 2160
    },
    {
      "epoch": 15.83941605839416,
      "grad_norm": 7.315721035003662,
      "learning_rate": 4.9208029197080294e-05,
      "loss": 4.594,
      "step": 2170
    },
    {
      "epoch": 15.912408759124087,
      "grad_norm": 7.96028995513916,
      "learning_rate": 4.9204379562043795e-05,
      "loss": 4.4832,
      "step": 2180
    },
    {
      "epoch": 15.985401459854014,
      "grad_norm": 7.372739791870117,
      "learning_rate": 4.92007299270073e-05,
      "loss": 4.1036,
      "step": 2190
    },
    {
      "epoch": 16.05839416058394,
      "grad_norm": 7.328399658203125,
      "learning_rate": 4.91970802919708e-05,
      "loss": 4.5025,
      "step": 2200
    },
    {
      "epoch": 16.131386861313867,
      "grad_norm": 7.654944896697998,
      "learning_rate": 4.919343065693431e-05,
      "loss": 4.3148,
      "step": 2210
    },
    {
      "epoch": 16.204379562043794,
      "grad_norm": 7.325509071350098,
      "learning_rate": 4.918978102189781e-05,
      "loss": 4.1825,
      "step": 2220
    },
    {
      "epoch": 16.277372262773724,
      "grad_norm": 7.454590797424316,
      "learning_rate": 4.918613138686132e-05,
      "loss": 4.1817,
      "step": 2230
    },
    {
      "epoch": 16.35036496350365,
      "grad_norm": 7.451061248779297,
      "learning_rate": 4.9182481751824825e-05,
      "loss": 4.4907,
      "step": 2240
    },
    {
      "epoch": 16.423357664233578,
      "grad_norm": 7.391155242919922,
      "learning_rate": 4.9178832116788325e-05,
      "loss": 4.3266,
      "step": 2250
    },
    {
      "epoch": 16.496350364963504,
      "grad_norm": 7.148794174194336,
      "learning_rate": 4.9175182481751826e-05,
      "loss": 4.1772,
      "step": 2260
    },
    {
      "epoch": 16.56934306569343,
      "grad_norm": 6.735761642456055,
      "learning_rate": 4.9171532846715326e-05,
      "loss": 4.1557,
      "step": 2270
    },
    {
      "epoch": 16.642335766423358,
      "grad_norm": 7.316784381866455,
      "learning_rate": 4.916788321167883e-05,
      "loss": 4.1919,
      "step": 2280
    },
    {
      "epoch": 16.715328467153284,
      "grad_norm": 7.598353385925293,
      "learning_rate": 4.916423357664234e-05,
      "loss": 4.402,
      "step": 2290
    },
    {
      "epoch": 16.78832116788321,
      "grad_norm": 7.382293224334717,
      "learning_rate": 4.916058394160584e-05,
      "loss": 4.3833,
      "step": 2300
    },
    {
      "epoch": 16.861313868613138,
      "grad_norm": 7.674577236175537,
      "learning_rate": 4.915693430656935e-05,
      "loss": 4.33,
      "step": 2310
    },
    {
      "epoch": 16.934306569343065,
      "grad_norm": 7.766595840454102,
      "learning_rate": 4.915328467153285e-05,
      "loss": 4.8288,
      "step": 2320
    },
    {
      "epoch": 17.00729927007299,
      "grad_norm": 7.338203430175781,
      "learning_rate": 4.9149635036496356e-05,
      "loss": 4.1752,
      "step": 2330
    },
    {
      "epoch": 17.080291970802918,
      "grad_norm": 7.517383098602295,
      "learning_rate": 4.914598540145986e-05,
      "loss": 4.7334,
      "step": 2340
    },
    {
      "epoch": 17.153284671532848,
      "grad_norm": 7.004671573638916,
      "learning_rate": 4.914233576642336e-05,
      "loss": 4.3208,
      "step": 2350
    },
    {
      "epoch": 17.226277372262775,
      "grad_norm": 7.263256549835205,
      "learning_rate": 4.9138686131386864e-05,
      "loss": 4.5701,
      "step": 2360
    },
    {
      "epoch": 17.2992700729927,
      "grad_norm": 7.62792444229126,
      "learning_rate": 4.9135036496350365e-05,
      "loss": 4.3782,
      "step": 2370
    },
    {
      "epoch": 17.37226277372263,
      "grad_norm": 7.553756237030029,
      "learning_rate": 4.913138686131387e-05,
      "loss": 4.1199,
      "step": 2380
    },
    {
      "epoch": 17.445255474452555,
      "grad_norm": 7.937853813171387,
      "learning_rate": 4.912773722627737e-05,
      "loss": 4.5742,
      "step": 2390
    },
    {
      "epoch": 17.51824817518248,
      "grad_norm": 7.633818626403809,
      "learning_rate": 4.912408759124088e-05,
      "loss": 4.4503,
      "step": 2400
    },
    {
      "epoch": 17.59124087591241,
      "grad_norm": 7.60698127746582,
      "learning_rate": 4.912043795620438e-05,
      "loss": 4.035,
      "step": 2410
    },
    {
      "epoch": 17.664233576642335,
      "grad_norm": 7.275970935821533,
      "learning_rate": 4.911678832116788e-05,
      "loss": 4.2299,
      "step": 2420
    },
    {
      "epoch": 17.73722627737226,
      "grad_norm": 7.388701438903809,
      "learning_rate": 4.911313868613139e-05,
      "loss": 4.2674,
      "step": 2430
    },
    {
      "epoch": 17.81021897810219,
      "grad_norm": 7.198484897613525,
      "learning_rate": 4.9109489051094895e-05,
      "loss": 4.0317,
      "step": 2440
    },
    {
      "epoch": 17.883211678832115,
      "grad_norm": 7.731367588043213,
      "learning_rate": 4.9105839416058396e-05,
      "loss": 4.1249,
      "step": 2450
    },
    {
      "epoch": 17.956204379562045,
      "grad_norm": 7.430767059326172,
      "learning_rate": 4.91021897810219e-05,
      "loss": 4.2644,
      "step": 2460
    },
    {
      "epoch": 18.029197080291972,
      "grad_norm": 7.6808881759643555,
      "learning_rate": 4.9098540145985404e-05,
      "loss": 4.226,
      "step": 2470
    },
    {
      "epoch": 18.1021897810219,
      "grad_norm": 7.742594242095947,
      "learning_rate": 4.909489051094891e-05,
      "loss": 4.3522,
      "step": 2480
    },
    {
      "epoch": 18.175182481751825,
      "grad_norm": 7.254876136779785,
      "learning_rate": 4.909124087591241e-05,
      "loss": 4.0688,
      "step": 2490
    },
    {
      "epoch": 18.248175182481752,
      "grad_norm": 8.164995193481445,
      "learning_rate": 4.908759124087591e-05,
      "loss": 4.5742,
      "step": 2500
    },
    {
      "epoch": 18.32116788321168,
      "grad_norm": 7.348723888397217,
      "learning_rate": 4.908394160583942e-05,
      "loss": 4.1762,
      "step": 2510
    },
    {
      "epoch": 18.394160583941606,
      "grad_norm": 7.698125839233398,
      "learning_rate": 4.908029197080292e-05,
      "loss": 4.0112,
      "step": 2520
    },
    {
      "epoch": 18.467153284671532,
      "grad_norm": 7.646859645843506,
      "learning_rate": 4.907664233576643e-05,
      "loss": 4.3608,
      "step": 2530
    },
    {
      "epoch": 18.54014598540146,
      "grad_norm": 7.361717700958252,
      "learning_rate": 4.907299270072993e-05,
      "loss": 4.5175,
      "step": 2540
    },
    {
      "epoch": 18.613138686131386,
      "grad_norm": 7.463237762451172,
      "learning_rate": 4.9069343065693435e-05,
      "loss": 4.149,
      "step": 2550
    },
    {
      "epoch": 18.686131386861312,
      "grad_norm": 7.572547435760498,
      "learning_rate": 4.906569343065694e-05,
      "loss": 4.2968,
      "step": 2560
    },
    {
      "epoch": 18.75912408759124,
      "grad_norm": 7.2728142738342285,
      "learning_rate": 4.9062043795620436e-05,
      "loss": 4.3233,
      "step": 2570
    },
    {
      "epoch": 18.83211678832117,
      "grad_norm": 7.661006450653076,
      "learning_rate": 4.905839416058394e-05,
      "loss": 4.3263,
      "step": 2580
    },
    {
      "epoch": 18.905109489051096,
      "grad_norm": 7.772633075714111,
      "learning_rate": 4.9054744525547444e-05,
      "loss": 4.4075,
      "step": 2590
    },
    {
      "epoch": 18.978102189781023,
      "grad_norm": 7.607119560241699,
      "learning_rate": 4.905109489051095e-05,
      "loss": 4.3822,
      "step": 2600
    },
    {
      "epoch": 19.05109489051095,
      "grad_norm": 7.5848469734191895,
      "learning_rate": 4.904744525547446e-05,
      "loss": 4.3301,
      "step": 2610
    },
    {
      "epoch": 19.124087591240876,
      "grad_norm": 7.415617942810059,
      "learning_rate": 4.904379562043796e-05,
      "loss": 3.998,
      "step": 2620
    },
    {
      "epoch": 19.197080291970803,
      "grad_norm": 7.363269329071045,
      "learning_rate": 4.9040145985401466e-05,
      "loss": 4.0964,
      "step": 2630
    },
    {
      "epoch": 19.27007299270073,
      "grad_norm": 8.142149925231934,
      "learning_rate": 4.9036496350364966e-05,
      "loss": 4.2076,
      "step": 2640
    },
    {
      "epoch": 19.343065693430656,
      "grad_norm": 7.346418857574463,
      "learning_rate": 4.903284671532847e-05,
      "loss": 4.0658,
      "step": 2650
    },
    {
      "epoch": 19.416058394160583,
      "grad_norm": 7.565029621124268,
      "learning_rate": 4.9029197080291974e-05,
      "loss": 4.0942,
      "step": 2660
    },
    {
      "epoch": 19.48905109489051,
      "grad_norm": 7.4708099365234375,
      "learning_rate": 4.9025547445255475e-05,
      "loss": 4.2161,
      "step": 2670
    },
    {
      "epoch": 19.562043795620436,
      "grad_norm": 7.684115409851074,
      "learning_rate": 4.902189781021898e-05,
      "loss": 4.2943,
      "step": 2680
    },
    {
      "epoch": 19.635036496350367,
      "grad_norm": 7.694754123687744,
      "learning_rate": 4.901824817518248e-05,
      "loss": 4.3783,
      "step": 2690
    },
    {
      "epoch": 19.708029197080293,
      "grad_norm": 7.6896209716796875,
      "learning_rate": 4.901459854014599e-05,
      "loss": 3.9604,
      "step": 2700
    },
    {
      "epoch": 19.78102189781022,
      "grad_norm": 7.663477420806885,
      "learning_rate": 4.90109489051095e-05,
      "loss": 4.5374,
      "step": 2710
    },
    {
      "epoch": 19.854014598540147,
      "grad_norm": 7.8337531089782715,
      "learning_rate": 4.900729927007299e-05,
      "loss": 4.4212,
      "step": 2720
    },
    {
      "epoch": 19.927007299270073,
      "grad_norm": 7.811975479125977,
      "learning_rate": 4.90036496350365e-05,
      "loss": 4.0554,
      "step": 2730
    },
    {
      "epoch": 20.0,
      "grad_norm": 11.405447006225586,
      "learning_rate": 4.9e-05,
      "loss": 4.5269,
      "step": 2740
    },
    {
      "epoch": 20.072992700729927,
      "grad_norm": 7.435583591461182,
      "learning_rate": 4.8996350364963506e-05,
      "loss": 4.1208,
      "step": 2750
    },
    {
      "epoch": 20.145985401459853,
      "grad_norm": 7.297494411468506,
      "learning_rate": 4.899270072992701e-05,
      "loss": 4.2801,
      "step": 2760
    },
    {
      "epoch": 20.21897810218978,
      "grad_norm": 7.967582702636719,
      "learning_rate": 4.8989051094890513e-05,
      "loss": 4.4574,
      "step": 2770
    },
    {
      "epoch": 20.291970802919707,
      "grad_norm": 7.566600799560547,
      "learning_rate": 4.898540145985402e-05,
      "loss": 4.2856,
      "step": 2780
    },
    {
      "epoch": 20.364963503649633,
      "grad_norm": 8.844108581542969,
      "learning_rate": 4.898175182481752e-05,
      "loss": 4.3431,
      "step": 2790
    },
    {
      "epoch": 20.437956204379564,
      "grad_norm": 7.668331623077393,
      "learning_rate": 4.897810218978102e-05,
      "loss": 4.3212,
      "step": 2800
    },
    {
      "epoch": 20.51094890510949,
      "grad_norm": 6.980903625488281,
      "learning_rate": 4.897445255474453e-05,
      "loss": 4.1243,
      "step": 2810
    },
    {
      "epoch": 20.583941605839417,
      "grad_norm": 7.689628601074219,
      "learning_rate": 4.897080291970803e-05,
      "loss": 3.9256,
      "step": 2820
    },
    {
      "epoch": 20.656934306569344,
      "grad_norm": 7.370724201202393,
      "learning_rate": 4.896715328467154e-05,
      "loss": 4.0438,
      "step": 2830
    },
    {
      "epoch": 20.72992700729927,
      "grad_norm": 7.564736843109131,
      "learning_rate": 4.896350364963504e-05,
      "loss": 4.2047,
      "step": 2840
    },
    {
      "epoch": 20.802919708029197,
      "grad_norm": 7.770359039306641,
      "learning_rate": 4.8959854014598545e-05,
      "loss": 4.1828,
      "step": 2850
    },
    {
      "epoch": 20.875912408759124,
      "grad_norm": 7.891435146331787,
      "learning_rate": 4.895620437956205e-05,
      "loss": 4.1809,
      "step": 2860
    },
    {
      "epoch": 20.94890510948905,
      "grad_norm": 7.793534755706787,
      "learning_rate": 4.8952554744525546e-05,
      "loss": 4.0424,
      "step": 2870
    },
    {
      "epoch": 21.021897810218977,
      "grad_norm": 7.804877758026123,
      "learning_rate": 4.894890510948905e-05,
      "loss": 4.1159,
      "step": 2880
    },
    {
      "epoch": 21.094890510948904,
      "grad_norm": 7.369792461395264,
      "learning_rate": 4.894525547445255e-05,
      "loss": 4.0937,
      "step": 2890
    },
    {
      "epoch": 21.16788321167883,
      "grad_norm": 7.613184928894043,
      "learning_rate": 4.894160583941606e-05,
      "loss": 4.4962,
      "step": 2900
    },
    {
      "epoch": 21.240875912408757,
      "grad_norm": 7.413180351257324,
      "learning_rate": 4.893795620437957e-05,
      "loss": 3.9724,
      "step": 2910
    },
    {
      "epoch": 21.313868613138688,
      "grad_norm": 7.843150615692139,
      "learning_rate": 4.893430656934307e-05,
      "loss": 4.0601,
      "step": 2920
    },
    {
      "epoch": 21.386861313868614,
      "grad_norm": 7.091639518737793,
      "learning_rate": 4.8930656934306576e-05,
      "loss": 4.2498,
      "step": 2930
    },
    {
      "epoch": 21.45985401459854,
      "grad_norm": 7.149531364440918,
      "learning_rate": 4.8927007299270076e-05,
      "loss": 4.2744,
      "step": 2940
    },
    {
      "epoch": 21.532846715328468,
      "grad_norm": 7.286204814910889,
      "learning_rate": 4.8923357664233577e-05,
      "loss": 3.9408,
      "step": 2950
    },
    {
      "epoch": 21.605839416058394,
      "grad_norm": 7.66386079788208,
      "learning_rate": 4.8919708029197084e-05,
      "loss": 4.0478,
      "step": 2960
    },
    {
      "epoch": 21.67883211678832,
      "grad_norm": 7.3503737449646,
      "learning_rate": 4.8916058394160584e-05,
      "loss": 4.2255,
      "step": 2970
    },
    {
      "epoch": 21.751824817518248,
      "grad_norm": 7.837303638458252,
      "learning_rate": 4.891240875912409e-05,
      "loss": 4.2548,
      "step": 2980
    },
    {
      "epoch": 21.824817518248175,
      "grad_norm": 8.189018249511719,
      "learning_rate": 4.890875912408759e-05,
      "loss": 4.0848,
      "step": 2990
    },
    {
      "epoch": 21.8978102189781,
      "grad_norm": 7.7778706550598145,
      "learning_rate": 4.89051094890511e-05,
      "loss": 4.1482,
      "step": 3000
    },
    {
      "epoch": 21.970802919708028,
      "grad_norm": 8.536205291748047,
      "learning_rate": 4.89014598540146e-05,
      "loss": 4.388,
      "step": 3010
    },
    {
      "epoch": 22.043795620437955,
      "grad_norm": 7.726274490356445,
      "learning_rate": 4.88978102189781e-05,
      "loss": 4.167,
      "step": 3020
    },
    {
      "epoch": 22.116788321167885,
      "grad_norm": 7.397102355957031,
      "learning_rate": 4.889416058394161e-05,
      "loss": 4.1902,
      "step": 3030
    },
    {
      "epoch": 22.18978102189781,
      "grad_norm": 7.733238220214844,
      "learning_rate": 4.889051094890511e-05,
      "loss": 4.3091,
      "step": 3040
    },
    {
      "epoch": 22.26277372262774,
      "grad_norm": 7.828170299530029,
      "learning_rate": 4.8886861313868615e-05,
      "loss": 4.0916,
      "step": 3050
    },
    {
      "epoch": 22.335766423357665,
      "grad_norm": 7.299426555633545,
      "learning_rate": 4.888321167883212e-05,
      "loss": 3.9986,
      "step": 3060
    },
    {
      "epoch": 22.40875912408759,
      "grad_norm": 7.505594730377197,
      "learning_rate": 4.887956204379562e-05,
      "loss": 3.8274,
      "step": 3070
    },
    {
      "epoch": 22.48175182481752,
      "grad_norm": 7.679154872894287,
      "learning_rate": 4.887591240875913e-05,
      "loss": 3.9282,
      "step": 3080
    },
    {
      "epoch": 22.554744525547445,
      "grad_norm": 8.016894340515137,
      "learning_rate": 4.887226277372263e-05,
      "loss": 4.0438,
      "step": 3090
    },
    {
      "epoch": 22.62773722627737,
      "grad_norm": 7.549180030822754,
      "learning_rate": 4.886861313868613e-05,
      "loss": 4.2119,
      "step": 3100
    },
    {
      "epoch": 22.7007299270073,
      "grad_norm": 7.930717468261719,
      "learning_rate": 4.886496350364964e-05,
      "loss": 3.8478,
      "step": 3110
    },
    {
      "epoch": 22.773722627737225,
      "grad_norm": 7.474782943725586,
      "learning_rate": 4.886131386861314e-05,
      "loss": 4.2402,
      "step": 3120
    },
    {
      "epoch": 22.846715328467152,
      "grad_norm": 7.653801918029785,
      "learning_rate": 4.8857664233576646e-05,
      "loss": 4.1531,
      "step": 3130
    },
    {
      "epoch": 22.919708029197082,
      "grad_norm": 7.569295406341553,
      "learning_rate": 4.885401459854015e-05,
      "loss": 4.3179,
      "step": 3140
    },
    {
      "epoch": 22.99270072992701,
      "grad_norm": 7.718873023986816,
      "learning_rate": 4.8850364963503654e-05,
      "loss": 3.9576,
      "step": 3150
    },
    {
      "epoch": 23.065693430656935,
      "grad_norm": 7.815807819366455,
      "learning_rate": 4.8846715328467155e-05,
      "loss": 4.0609,
      "step": 3160
    },
    {
      "epoch": 23.138686131386862,
      "grad_norm": 7.338711261749268,
      "learning_rate": 4.884306569343066e-05,
      "loss": 4.3956,
      "step": 3170
    },
    {
      "epoch": 23.21167883211679,
      "grad_norm": 8.387267112731934,
      "learning_rate": 4.883941605839416e-05,
      "loss": 4.0355,
      "step": 3180
    },
    {
      "epoch": 23.284671532846716,
      "grad_norm": 8.057732582092285,
      "learning_rate": 4.883576642335766e-05,
      "loss": 3.9011,
      "step": 3190
    },
    {
      "epoch": 23.357664233576642,
      "grad_norm": 7.875824928283691,
      "learning_rate": 4.883211678832117e-05,
      "loss": 4.0702,
      "step": 3200
    },
    {
      "epoch": 23.43065693430657,
      "grad_norm": 7.786950588226318,
      "learning_rate": 4.882846715328467e-05,
      "loss": 3.9661,
      "step": 3210
    },
    {
      "epoch": 23.503649635036496,
      "grad_norm": 8.382685661315918,
      "learning_rate": 4.882481751824818e-05,
      "loss": 3.8882,
      "step": 3220
    },
    {
      "epoch": 23.576642335766422,
      "grad_norm": 7.945895671844482,
      "learning_rate": 4.8821167883211685e-05,
      "loss": 4.2802,
      "step": 3230
    },
    {
      "epoch": 23.64963503649635,
      "grad_norm": 7.831360340118408,
      "learning_rate": 4.8817518248175186e-05,
      "loss": 4.0956,
      "step": 3240
    },
    {
      "epoch": 23.722627737226276,
      "grad_norm": 7.700159549713135,
      "learning_rate": 4.8813868613138686e-05,
      "loss": 3.8546,
      "step": 3250
    },
    {
      "epoch": 23.795620437956206,
      "grad_norm": 7.963235378265381,
      "learning_rate": 4.881021897810219e-05,
      "loss": 4.1562,
      "step": 3260
    },
    {
      "epoch": 23.868613138686133,
      "grad_norm": 7.445836544036865,
      "learning_rate": 4.8806569343065694e-05,
      "loss": 4.1505,
      "step": 3270
    },
    {
      "epoch": 23.94160583941606,
      "grad_norm": 7.997829437255859,
      "learning_rate": 4.88029197080292e-05,
      "loss": 4.1459,
      "step": 3280
    },
    {
      "epoch": 24.014598540145986,
      "grad_norm": 7.771926403045654,
      "learning_rate": 4.87992700729927e-05,
      "loss": 4.0211,
      "step": 3290
    },
    {
      "epoch": 24.087591240875913,
      "grad_norm": 7.347644805908203,
      "learning_rate": 4.879562043795621e-05,
      "loss": 4.0407,
      "step": 3300
    },
    {
      "epoch": 24.16058394160584,
      "grad_norm": 7.516901016235352,
      "learning_rate": 4.879197080291971e-05,
      "loss": 4.1605,
      "step": 3310
    },
    {
      "epoch": 24.233576642335766,
      "grad_norm": 7.86345911026001,
      "learning_rate": 4.878832116788322e-05,
      "loss": 4.0311,
      "step": 3320
    },
    {
      "epoch": 24.306569343065693,
      "grad_norm": 7.087184906005859,
      "learning_rate": 4.878467153284672e-05,
      "loss": 4.1863,
      "step": 3330
    },
    {
      "epoch": 24.37956204379562,
      "grad_norm": 7.824475288391113,
      "learning_rate": 4.878102189781022e-05,
      "loss": 4.2286,
      "step": 3340
    },
    {
      "epoch": 24.452554744525546,
      "grad_norm": 8.681591033935547,
      "learning_rate": 4.8777372262773725e-05,
      "loss": 3.6415,
      "step": 3350
    },
    {
      "epoch": 24.525547445255473,
      "grad_norm": 7.559118747711182,
      "learning_rate": 4.8773722627737226e-05,
      "loss": 3.9275,
      "step": 3360
    },
    {
      "epoch": 24.598540145985403,
      "grad_norm": 7.29359769821167,
      "learning_rate": 4.877007299270073e-05,
      "loss": 4.042,
      "step": 3370
    },
    {
      "epoch": 24.67153284671533,
      "grad_norm": 7.928584575653076,
      "learning_rate": 4.876642335766424e-05,
      "loss": 3.8796,
      "step": 3380
    },
    {
      "epoch": 24.744525547445257,
      "grad_norm": 7.4932966232299805,
      "learning_rate": 4.876277372262774e-05,
      "loss": 4.2012,
      "step": 3390
    },
    {
      "epoch": 24.817518248175183,
      "grad_norm": 8.041327476501465,
      "learning_rate": 4.875912408759125e-05,
      "loss": 3.9319,
      "step": 3400
    },
    {
      "epoch": 24.89051094890511,
      "grad_norm": 7.456400394439697,
      "learning_rate": 4.875547445255474e-05,
      "loss": 4.3808,
      "step": 3410
    },
    {
      "epoch": 24.963503649635037,
      "grad_norm": 7.6876654624938965,
      "learning_rate": 4.875182481751825e-05,
      "loss": 4.1024,
      "step": 3420
    },
    {
      "epoch": 25.036496350364963,
      "grad_norm": 7.91817569732666,
      "learning_rate": 4.8748175182481756e-05,
      "loss": 3.76,
      "step": 3430
    },
    {
      "epoch": 25.10948905109489,
      "grad_norm": 8.11446762084961,
      "learning_rate": 4.874452554744526e-05,
      "loss": 4.0057,
      "step": 3440
    },
    {
      "epoch": 25.182481751824817,
      "grad_norm": 8.330499649047852,
      "learning_rate": 4.8740875912408764e-05,
      "loss": 4.0138,
      "step": 3450
    },
    {
      "epoch": 25.255474452554743,
      "grad_norm": 7.905627727508545,
      "learning_rate": 4.8737226277372264e-05,
      "loss": 4.1656,
      "step": 3460
    },
    {
      "epoch": 25.32846715328467,
      "grad_norm": 7.771867275238037,
      "learning_rate": 4.873357664233577e-05,
      "loss": 3.7598,
      "step": 3470
    },
    {
      "epoch": 25.401459854014597,
      "grad_norm": 7.444144248962402,
      "learning_rate": 4.872992700729927e-05,
      "loss": 3.9735,
      "step": 3480
    },
    {
      "epoch": 25.474452554744527,
      "grad_norm": 8.112512588500977,
      "learning_rate": 4.872627737226277e-05,
      "loss": 4.1567,
      "step": 3490
    },
    {
      "epoch": 25.547445255474454,
      "grad_norm": 7.652038097381592,
      "learning_rate": 4.872262773722628e-05,
      "loss": 3.8766,
      "step": 3500
    },
    {
      "epoch": 25.62043795620438,
      "grad_norm": 7.659101963043213,
      "learning_rate": 4.871897810218978e-05,
      "loss": 3.7526,
      "step": 3510
    },
    {
      "epoch": 25.693430656934307,
      "grad_norm": 8.377890586853027,
      "learning_rate": 4.871532846715329e-05,
      "loss": 4.1895,
      "step": 3520
    },
    {
      "epoch": 25.766423357664234,
      "grad_norm": 7.82168436050415,
      "learning_rate": 4.8711678832116795e-05,
      "loss": 4.1263,
      "step": 3530
    },
    {
      "epoch": 25.83941605839416,
      "grad_norm": 7.626681804656982,
      "learning_rate": 4.8708029197080295e-05,
      "loss": 3.9241,
      "step": 3540
    },
    {
      "epoch": 25.912408759124087,
      "grad_norm": 7.772684097290039,
      "learning_rate": 4.87043795620438e-05,
      "loss": 3.9852,
      "step": 3550
    },
    {
      "epoch": 25.985401459854014,
      "grad_norm": 7.673300266265869,
      "learning_rate": 4.8700729927007296e-05,
      "loss": 4.0996,
      "step": 3560
    },
    {
      "epoch": 26.05839416058394,
      "grad_norm": 7.232963562011719,
      "learning_rate": 4.8697080291970804e-05,
      "loss": 4.1527,
      "step": 3570
    },
    {
      "epoch": 26.131386861313867,
      "grad_norm": 7.586287975311279,
      "learning_rate": 4.869343065693431e-05,
      "loss": 3.9065,
      "step": 3580
    },
    {
      "epoch": 26.204379562043794,
      "grad_norm": 7.554398059844971,
      "learning_rate": 4.868978102189781e-05,
      "loss": 3.8807,
      "step": 3590
    },
    {
      "epoch": 26.277372262773724,
      "grad_norm": 7.6303181648254395,
      "learning_rate": 4.868613138686132e-05,
      "loss": 3.6234,
      "step": 3600
    },
    {
      "epoch": 26.35036496350365,
      "grad_norm": 8.152688980102539,
      "learning_rate": 4.868248175182482e-05,
      "loss": 3.7261,
      "step": 3610
    },
    {
      "epoch": 26.423357664233578,
      "grad_norm": 7.68139123916626,
      "learning_rate": 4.8678832116788327e-05,
      "loss": 3.9682,
      "step": 3620
    },
    {
      "epoch": 26.496350364963504,
      "grad_norm": 8.209429740905762,
      "learning_rate": 4.867518248175183e-05,
      "loss": 4.0114,
      "step": 3630
    },
    {
      "epoch": 26.56934306569343,
      "grad_norm": 8.067296981811523,
      "learning_rate": 4.867153284671533e-05,
      "loss": 4.006,
      "step": 3640
    },
    {
      "epoch": 26.642335766423358,
      "grad_norm": 8.184966087341309,
      "learning_rate": 4.8667883211678835e-05,
      "loss": 4.1654,
      "step": 3650
    },
    {
      "epoch": 26.715328467153284,
      "grad_norm": 7.997569561004639,
      "learning_rate": 4.8664233576642335e-05,
      "loss": 4.2671,
      "step": 3660
    },
    {
      "epoch": 26.78832116788321,
      "grad_norm": 7.967447757720947,
      "learning_rate": 4.866058394160584e-05,
      "loss": 3.5396,
      "step": 3670
    },
    {
      "epoch": 26.861313868613138,
      "grad_norm": 7.7046942710876465,
      "learning_rate": 4.865693430656934e-05,
      "loss": 4.2245,
      "step": 3680
    },
    {
      "epoch": 26.934306569343065,
      "grad_norm": 8.23342514038086,
      "learning_rate": 4.865328467153285e-05,
      "loss": 3.9862,
      "step": 3690
    },
    {
      "epoch": 27.00729927007299,
      "grad_norm": 7.348336219787598,
      "learning_rate": 4.864963503649636e-05,
      "loss": 4.0784,
      "step": 3700
    },
    {
      "epoch": 27.080291970802918,
      "grad_norm": 7.782183647155762,
      "learning_rate": 4.864598540145985e-05,
      "loss": 3.7886,
      "step": 3710
    },
    {
      "epoch": 27.153284671532848,
      "grad_norm": 6.9337382316589355,
      "learning_rate": 4.864233576642336e-05,
      "loss": 3.8803,
      "step": 3720
    },
    {
      "epoch": 27.226277372262775,
      "grad_norm": 7.800741195678711,
      "learning_rate": 4.8638686131386866e-05,
      "loss": 3.7492,
      "step": 3730
    },
    {
      "epoch": 27.2992700729927,
      "grad_norm": 8.132234573364258,
      "learning_rate": 4.8635036496350366e-05,
      "loss": 4.1276,
      "step": 3740
    },
    {
      "epoch": 27.37226277372263,
      "grad_norm": 7.796888828277588,
      "learning_rate": 4.8631386861313874e-05,
      "loss": 3.6702,
      "step": 3750
    },
    {
      "epoch": 27.445255474452555,
      "grad_norm": 7.936736583709717,
      "learning_rate": 4.8627737226277374e-05,
      "loss": 4.1436,
      "step": 3760
    },
    {
      "epoch": 27.51824817518248,
      "grad_norm": 7.338785171508789,
      "learning_rate": 4.862408759124088e-05,
      "loss": 4.1265,
      "step": 3770
    },
    {
      "epoch": 27.59124087591241,
      "grad_norm": 8.019405364990234,
      "learning_rate": 4.862043795620438e-05,
      "loss": 3.9419,
      "step": 3780
    },
    {
      "epoch": 27.664233576642335,
      "grad_norm": 8.486802101135254,
      "learning_rate": 4.861678832116788e-05,
      "loss": 4.135,
      "step": 3790
    },
    {
      "epoch": 27.73722627737226,
      "grad_norm": 7.6563520431518555,
      "learning_rate": 4.861313868613139e-05,
      "loss": 3.8705,
      "step": 3800
    },
    {
      "epoch": 27.81021897810219,
      "grad_norm": 7.527950286865234,
      "learning_rate": 4.860948905109489e-05,
      "loss": 3.8906,
      "step": 3810
    },
    {
      "epoch": 27.883211678832115,
      "grad_norm": 8.38815975189209,
      "learning_rate": 4.86058394160584e-05,
      "loss": 3.772,
      "step": 3820
    },
    {
      "epoch": 27.956204379562045,
      "grad_norm": 7.672584533691406,
      "learning_rate": 4.86021897810219e-05,
      "loss": 3.7498,
      "step": 3830
    },
    {
      "epoch": 28.029197080291972,
      "grad_norm": 7.732068061828613,
      "learning_rate": 4.8598540145985405e-05,
      "loss": 4.0105,
      "step": 3840
    },
    {
      "epoch": 28.1021897810219,
      "grad_norm": 7.055285453796387,
      "learning_rate": 4.859489051094891e-05,
      "loss": 3.8695,
      "step": 3850
    },
    {
      "epoch": 28.175182481751825,
      "grad_norm": 9.278526306152344,
      "learning_rate": 4.859124087591241e-05,
      "loss": 3.9011,
      "step": 3860
    },
    {
      "epoch": 28.248175182481752,
      "grad_norm": 7.48703670501709,
      "learning_rate": 4.8587591240875913e-05,
      "loss": 4.0016,
      "step": 3870
    },
    {
      "epoch": 28.32116788321168,
      "grad_norm": 7.988772392272949,
      "learning_rate": 4.8583941605839414e-05,
      "loss": 3.7673,
      "step": 3880
    },
    {
      "epoch": 28.394160583941606,
      "grad_norm": 8.0973482131958,
      "learning_rate": 4.858029197080292e-05,
      "loss": 3.7534,
      "step": 3890
    },
    {
      "epoch": 28.467153284671532,
      "grad_norm": 7.822638511657715,
      "learning_rate": 4.857664233576643e-05,
      "loss": 3.8273,
      "step": 3900
    },
    {
      "epoch": 28.54014598540146,
      "grad_norm": 9.125839233398438,
      "learning_rate": 4.857299270072993e-05,
      "loss": 3.8331,
      "step": 3910
    },
    {
      "epoch": 28.613138686131386,
      "grad_norm": 8.042174339294434,
      "learning_rate": 4.8569343065693436e-05,
      "loss": 3.7016,
      "step": 3920
    },
    {
      "epoch": 28.686131386861312,
      "grad_norm": 7.640316486358643,
      "learning_rate": 4.856569343065694e-05,
      "loss": 4.044,
      "step": 3930
    },
    {
      "epoch": 28.75912408759124,
      "grad_norm": 7.7426838874816895,
      "learning_rate": 4.856204379562044e-05,
      "loss": 3.8725,
      "step": 3940
    },
    {
      "epoch": 28.83211678832117,
      "grad_norm": 8.6200590133667,
      "learning_rate": 4.8558394160583944e-05,
      "loss": 3.6137,
      "step": 3950
    },
    {
      "epoch": 28.905109489051096,
      "grad_norm": 7.622650146484375,
      "learning_rate": 4.8554744525547445e-05,
      "loss": 4.1531,
      "step": 3960
    },
    {
      "epoch": 28.978102189781023,
      "grad_norm": 8.026389122009277,
      "learning_rate": 4.855109489051095e-05,
      "loss": 3.8472,
      "step": 3970
    },
    {
      "epoch": 29.05109489051095,
      "grad_norm": 8.346522331237793,
      "learning_rate": 4.854744525547445e-05,
      "loss": 4.0061,
      "step": 3980
    },
    {
      "epoch": 29.124087591240876,
      "grad_norm": 8.207110404968262,
      "learning_rate": 4.854379562043796e-05,
      "loss": 3.6978,
      "step": 3990
    },
    {
      "epoch": 29.197080291970803,
      "grad_norm": 7.825653076171875,
      "learning_rate": 4.854014598540147e-05,
      "loss": 3.9762,
      "step": 4000
    },
    {
      "epoch": 29.27007299270073,
      "grad_norm": 8.433338165283203,
      "learning_rate": 4.853649635036497e-05,
      "loss": 3.6992,
      "step": 4010
    },
    {
      "epoch": 29.343065693430656,
      "grad_norm": 7.947121620178223,
      "learning_rate": 4.853284671532847e-05,
      "loss": 4.2528,
      "step": 4020
    },
    {
      "epoch": 29.416058394160583,
      "grad_norm": 7.467757701873779,
      "learning_rate": 4.852919708029197e-05,
      "loss": 3.7474,
      "step": 4030
    },
    {
      "epoch": 29.48905109489051,
      "grad_norm": 7.478351593017578,
      "learning_rate": 4.8525547445255476e-05,
      "loss": 4.0711,
      "step": 4040
    },
    {
      "epoch": 29.562043795620436,
      "grad_norm": 8.03101634979248,
      "learning_rate": 4.852189781021898e-05,
      "loss": 3.7682,
      "step": 4050
    },
    {
      "epoch": 29.635036496350367,
      "grad_norm": 7.692111968994141,
      "learning_rate": 4.8518248175182484e-05,
      "loss": 3.8237,
      "step": 4060
    },
    {
      "epoch": 29.708029197080293,
      "grad_norm": 7.556204795837402,
      "learning_rate": 4.851459854014599e-05,
      "loss": 3.4542,
      "step": 4070
    },
    {
      "epoch": 29.78102189781022,
      "grad_norm": 7.84270715713501,
      "learning_rate": 4.851094890510949e-05,
      "loss": 3.7406,
      "step": 4080
    },
    {
      "epoch": 29.854014598540147,
      "grad_norm": 7.705269813537598,
      "learning_rate": 4.8507299270073e-05,
      "loss": 4.1363,
      "step": 4090
    },
    {
      "epoch": 29.927007299270073,
      "grad_norm": 7.459754943847656,
      "learning_rate": 4.85036496350365e-05,
      "loss": 3.878,
      "step": 4100
    },
    {
      "epoch": 30.0,
      "grad_norm": 11.412891387939453,
      "learning_rate": 4.85e-05,
      "loss": 3.7753,
      "step": 4110
    },
    {
      "epoch": 30.072992700729927,
      "grad_norm": 7.74103307723999,
      "learning_rate": 4.849635036496351e-05,
      "loss": 3.8455,
      "step": 4120
    },
    {
      "epoch": 30.145985401459853,
      "grad_norm": 8.1289701461792,
      "learning_rate": 4.849270072992701e-05,
      "loss": 3.5282,
      "step": 4130
    },
    {
      "epoch": 30.21897810218978,
      "grad_norm": 7.056569576263428,
      "learning_rate": 4.8489051094890515e-05,
      "loss": 3.6779,
      "step": 4140
    },
    {
      "epoch": 30.291970802919707,
      "grad_norm": 7.410723686218262,
      "learning_rate": 4.8485401459854015e-05,
      "loss": 3.8777,
      "step": 4150
    },
    {
      "epoch": 30.364963503649633,
      "grad_norm": 8.291526794433594,
      "learning_rate": 4.848175182481752e-05,
      "loss": 3.5365,
      "step": 4160
    },
    {
      "epoch": 30.437956204379564,
      "grad_norm": 7.792575359344482,
      "learning_rate": 4.847810218978102e-05,
      "loss": 3.6503,
      "step": 4170
    },
    {
      "epoch": 30.51094890510949,
      "grad_norm": 8.288161277770996,
      "learning_rate": 4.8474452554744524e-05,
      "loss": 4.0678,
      "step": 4180
    },
    {
      "epoch": 30.583941605839417,
      "grad_norm": 7.286532402038574,
      "learning_rate": 4.847080291970803e-05,
      "loss": 4.0093,
      "step": 4190
    },
    {
      "epoch": 30.656934306569344,
      "grad_norm": 7.951979637145996,
      "learning_rate": 4.846715328467154e-05,
      "loss": 3.8485,
      "step": 4200
    },
    {
      "epoch": 30.72992700729927,
      "grad_norm": 7.831352233886719,
      "learning_rate": 4.846350364963504e-05,
      "loss": 3.9454,
      "step": 4210
    },
    {
      "epoch": 30.802919708029197,
      "grad_norm": 7.696122169494629,
      "learning_rate": 4.8459854014598546e-05,
      "loss": 3.9018,
      "step": 4220
    },
    {
      "epoch": 30.875912408759124,
      "grad_norm": 8.289660453796387,
      "learning_rate": 4.8456204379562046e-05,
      "loss": 3.9608,
      "step": 4230
    },
    {
      "epoch": 30.94890510948905,
      "grad_norm": 7.5497822761535645,
      "learning_rate": 4.8452554744525554e-05,
      "loss": 3.6147,
      "step": 4240
    },
    {
      "epoch": 31.021897810218977,
      "grad_norm": 8.500251770019531,
      "learning_rate": 4.8448905109489054e-05,
      "loss": 3.7511,
      "step": 4250
    },
    {
      "epoch": 31.094890510948904,
      "grad_norm": 7.835147380828857,
      "learning_rate": 4.8445255474452555e-05,
      "loss": 3.8364,
      "step": 4260
    },
    {
      "epoch": 31.16788321167883,
      "grad_norm": 7.738025665283203,
      "learning_rate": 4.844160583941606e-05,
      "loss": 3.6616,
      "step": 4270
    },
    {
      "epoch": 31.240875912408757,
      "grad_norm": 7.93483304977417,
      "learning_rate": 4.843795620437956e-05,
      "loss": 3.8558,
      "step": 4280
    },
    {
      "epoch": 31.313868613138688,
      "grad_norm": 8.075881004333496,
      "learning_rate": 4.843430656934307e-05,
      "loss": 4.0054,
      "step": 4290
    },
    {
      "epoch": 31.386861313868614,
      "grad_norm": 6.628086566925049,
      "learning_rate": 4.843065693430657e-05,
      "loss": 3.704,
      "step": 4300
    },
    {
      "epoch": 31.45985401459854,
      "grad_norm": 7.7825798988342285,
      "learning_rate": 4.842700729927008e-05,
      "loss": 3.9045,
      "step": 4310
    },
    {
      "epoch": 31.532846715328468,
      "grad_norm": 7.574813365936279,
      "learning_rate": 4.842335766423358e-05,
      "loss": 3.4932,
      "step": 4320
    },
    {
      "epoch": 31.605839416058394,
      "grad_norm": 7.582172393798828,
      "learning_rate": 4.841970802919708e-05,
      "loss": 3.3187,
      "step": 4330
    },
    {
      "epoch": 31.67883211678832,
      "grad_norm": 7.708037376403809,
      "learning_rate": 4.8416058394160586e-05,
      "loss": 4.0271,
      "step": 4340
    },
    {
      "epoch": 31.751824817518248,
      "grad_norm": 7.836090564727783,
      "learning_rate": 4.8412408759124086e-05,
      "loss": 3.463,
      "step": 4350
    },
    {
      "epoch": 31.824817518248175,
      "grad_norm": 7.776043891906738,
      "learning_rate": 4.8408759124087593e-05,
      "loss": 3.5141,
      "step": 4360
    },
    {
      "epoch": 31.8978102189781,
      "grad_norm": 8.578744888305664,
      "learning_rate": 4.84051094890511e-05,
      "loss": 4.0784,
      "step": 4370
    },
    {
      "epoch": 31.970802919708028,
      "grad_norm": 7.620358943939209,
      "learning_rate": 4.84014598540146e-05,
      "loss": 3.7958,
      "step": 4380
    },
    {
      "epoch": 32.043795620437955,
      "grad_norm": 7.927587509155273,
      "learning_rate": 4.839781021897811e-05,
      "loss": 3.8299,
      "step": 4390
    },
    {
      "epoch": 32.11678832116788,
      "grad_norm": 8.38922119140625,
      "learning_rate": 4.839416058394161e-05,
      "loss": 3.6116,
      "step": 4400
    },
    {
      "epoch": 32.18978102189781,
      "grad_norm": 7.857446670532227,
      "learning_rate": 4.839051094890511e-05,
      "loss": 3.7967,
      "step": 4410
    },
    {
      "epoch": 32.262773722627735,
      "grad_norm": 7.586593151092529,
      "learning_rate": 4.838686131386862e-05,
      "loss": 3.7731,
      "step": 4420
    },
    {
      "epoch": 32.33576642335766,
      "grad_norm": 7.409633159637451,
      "learning_rate": 4.838321167883212e-05,
      "loss": 3.5499,
      "step": 4430
    },
    {
      "epoch": 32.40875912408759,
      "grad_norm": 7.402190685272217,
      "learning_rate": 4.8379562043795625e-05,
      "loss": 3.6614,
      "step": 4440
    },
    {
      "epoch": 32.481751824817515,
      "grad_norm": 7.872944355010986,
      "learning_rate": 4.8375912408759125e-05,
      "loss": 3.3391,
      "step": 4450
    },
    {
      "epoch": 32.55474452554745,
      "grad_norm": 8.383614540100098,
      "learning_rate": 4.837226277372263e-05,
      "loss": 3.3801,
      "step": 4460
    },
    {
      "epoch": 32.627737226277375,
      "grad_norm": 7.796010971069336,
      "learning_rate": 4.836861313868614e-05,
      "loss": 3.9217,
      "step": 4470
    },
    {
      "epoch": 32.7007299270073,
      "grad_norm": 8.057827949523926,
      "learning_rate": 4.836496350364963e-05,
      "loss": 3.7849,
      "step": 4480
    },
    {
      "epoch": 32.77372262773723,
      "grad_norm": 8.26240348815918,
      "learning_rate": 4.836131386861314e-05,
      "loss": 3.7998,
      "step": 4490
    },
    {
      "epoch": 32.846715328467155,
      "grad_norm": 8.705080032348633,
      "learning_rate": 4.835766423357664e-05,
      "loss": 3.8511,
      "step": 4500
    },
    {
      "epoch": 32.91970802919708,
      "grad_norm": 8.273979187011719,
      "learning_rate": 4.835401459854015e-05,
      "loss": 3.9275,
      "step": 4510
    },
    {
      "epoch": 32.99270072992701,
      "grad_norm": 7.848669052124023,
      "learning_rate": 4.8350364963503656e-05,
      "loss": 3.9508,
      "step": 4520
    },
    {
      "epoch": 33.065693430656935,
      "grad_norm": 8.459717750549316,
      "learning_rate": 4.8346715328467156e-05,
      "loss": 3.5092,
      "step": 4530
    },
    {
      "epoch": 33.13868613138686,
      "grad_norm": 8.340760231018066,
      "learning_rate": 4.834306569343066e-05,
      "loss": 3.6633,
      "step": 4540
    },
    {
      "epoch": 33.21167883211679,
      "grad_norm": 8.971553802490234,
      "learning_rate": 4.833941605839416e-05,
      "loss": 3.5741,
      "step": 4550
    },
    {
      "epoch": 33.284671532846716,
      "grad_norm": 7.730582237243652,
      "learning_rate": 4.8335766423357664e-05,
      "loss": 3.7016,
      "step": 4560
    },
    {
      "epoch": 33.35766423357664,
      "grad_norm": 7.657076358795166,
      "learning_rate": 4.833211678832117e-05,
      "loss": 3.8723,
      "step": 4570
    },
    {
      "epoch": 33.43065693430657,
      "grad_norm": 8.056747436523438,
      "learning_rate": 4.832846715328467e-05,
      "loss": 3.4865,
      "step": 4580
    },
    {
      "epoch": 33.503649635036496,
      "grad_norm": 7.5588483810424805,
      "learning_rate": 4.832481751824818e-05,
      "loss": 3.468,
      "step": 4590
    },
    {
      "epoch": 33.57664233576642,
      "grad_norm": 8.42905044555664,
      "learning_rate": 4.832116788321168e-05,
      "loss": 3.7602,
      "step": 4600
    },
    {
      "epoch": 33.64963503649635,
      "grad_norm": 7.836867809295654,
      "learning_rate": 4.831751824817519e-05,
      "loss": 3.871,
      "step": 4610
    },
    {
      "epoch": 33.722627737226276,
      "grad_norm": 7.850124835968018,
      "learning_rate": 4.831386861313869e-05,
      "loss": 3.9584,
      "step": 4620
    },
    {
      "epoch": 33.7956204379562,
      "grad_norm": 8.737642288208008,
      "learning_rate": 4.831021897810219e-05,
      "loss": 3.5547,
      "step": 4630
    },
    {
      "epoch": 33.86861313868613,
      "grad_norm": 6.9552388191223145,
      "learning_rate": 4.8306569343065695e-05,
      "loss": 3.7878,
      "step": 4640
    },
    {
      "epoch": 33.941605839416056,
      "grad_norm": 7.731663703918457,
      "learning_rate": 4.8302919708029196e-05,
      "loss": 3.4752,
      "step": 4650
    },
    {
      "epoch": 34.01459854014598,
      "grad_norm": 8.296069145202637,
      "learning_rate": 4.82992700729927e-05,
      "loss": 3.8136,
      "step": 4660
    },
    {
      "epoch": 34.08759124087591,
      "grad_norm": 8.166831016540527,
      "learning_rate": 4.829562043795621e-05,
      "loss": 3.3989,
      "step": 4670
    },
    {
      "epoch": 34.160583941605836,
      "grad_norm": 8.080830574035645,
      "learning_rate": 4.829197080291971e-05,
      "loss": 3.4194,
      "step": 4680
    },
    {
      "epoch": 34.23357664233577,
      "grad_norm": 7.845851898193359,
      "learning_rate": 4.828832116788322e-05,
      "loss": 3.7153,
      "step": 4690
    },
    {
      "epoch": 34.306569343065696,
      "grad_norm": 7.52076530456543,
      "learning_rate": 4.828467153284672e-05,
      "loss": 3.6075,
      "step": 4700
    },
    {
      "epoch": 34.37956204379562,
      "grad_norm": 7.24553108215332,
      "learning_rate": 4.828102189781022e-05,
      "loss": 3.4988,
      "step": 4710
    },
    {
      "epoch": 34.45255474452555,
      "grad_norm": 7.908139228820801,
      "learning_rate": 4.8277372262773726e-05,
      "loss": 3.6877,
      "step": 4720
    },
    {
      "epoch": 34.52554744525548,
      "grad_norm": 7.607728958129883,
      "learning_rate": 4.827372262773723e-05,
      "loss": 3.4899,
      "step": 4730
    },
    {
      "epoch": 34.5985401459854,
      "grad_norm": 8.864029884338379,
      "learning_rate": 4.8270072992700734e-05,
      "loss": 4.0774,
      "step": 4740
    },
    {
      "epoch": 34.67153284671533,
      "grad_norm": 8.605986595153809,
      "learning_rate": 4.8266423357664235e-05,
      "loss": 3.811,
      "step": 4750
    },
    {
      "epoch": 34.74452554744526,
      "grad_norm": 9.491177558898926,
      "learning_rate": 4.826277372262774e-05,
      "loss": 3.7424,
      "step": 4760
    },
    {
      "epoch": 34.81751824817518,
      "grad_norm": 7.543201446533203,
      "learning_rate": 4.825912408759124e-05,
      "loss": 3.7526,
      "step": 4770
    },
    {
      "epoch": 34.89051094890511,
      "grad_norm": 8.37471866607666,
      "learning_rate": 4.825547445255474e-05,
      "loss": 3.3208,
      "step": 4780
    },
    {
      "epoch": 34.96350364963504,
      "grad_norm": 7.684347152709961,
      "learning_rate": 4.825182481751825e-05,
      "loss": 3.4031,
      "step": 4790
    },
    {
      "epoch": 35.03649635036496,
      "grad_norm": 7.9975504875183105,
      "learning_rate": 4.824817518248175e-05,
      "loss": 3.8439,
      "step": 4800
    },
    {
      "epoch": 35.10948905109489,
      "grad_norm": 7.446714878082275,
      "learning_rate": 4.824452554744526e-05,
      "loss": 3.3805,
      "step": 4810
    },
    {
      "epoch": 35.18248175182482,
      "grad_norm": 7.601352214813232,
      "learning_rate": 4.824087591240876e-05,
      "loss": 3.6544,
      "step": 4820
    },
    {
      "epoch": 35.25547445255474,
      "grad_norm": 7.449139595031738,
      "learning_rate": 4.8237226277372266e-05,
      "loss": 3.6301,
      "step": 4830
    },
    {
      "epoch": 35.32846715328467,
      "grad_norm": 8.523165702819824,
      "learning_rate": 4.823357664233577e-05,
      "loss": 3.6023,
      "step": 4840
    },
    {
      "epoch": 35.4014598540146,
      "grad_norm": 7.964961051940918,
      "learning_rate": 4.8229927007299274e-05,
      "loss": 3.3415,
      "step": 4850
    },
    {
      "epoch": 35.47445255474452,
      "grad_norm": 8.592876434326172,
      "learning_rate": 4.8226277372262774e-05,
      "loss": 3.8533,
      "step": 4860
    },
    {
      "epoch": 35.54744525547445,
      "grad_norm": 7.791708469390869,
      "learning_rate": 4.822262773722628e-05,
      "loss": 3.6426,
      "step": 4870
    },
    {
      "epoch": 35.62043795620438,
      "grad_norm": 7.642140865325928,
      "learning_rate": 4.821897810218978e-05,
      "loss": 3.63,
      "step": 4880
    },
    {
      "epoch": 35.693430656934304,
      "grad_norm": 8.1636381149292,
      "learning_rate": 4.821532846715329e-05,
      "loss": 3.6172,
      "step": 4890
    },
    {
      "epoch": 35.76642335766423,
      "grad_norm": 8.529643058776855,
      "learning_rate": 4.821167883211679e-05,
      "loss": 3.76,
      "step": 4900
    },
    {
      "epoch": 35.839416058394164,
      "grad_norm": 8.014500617980957,
      "learning_rate": 4.82080291970803e-05,
      "loss": 3.8429,
      "step": 4910
    },
    {
      "epoch": 35.91240875912409,
      "grad_norm": 7.0723347663879395,
      "learning_rate": 4.82043795620438e-05,
      "loss": 3.4221,
      "step": 4920
    },
    {
      "epoch": 35.98540145985402,
      "grad_norm": 8.182548522949219,
      "learning_rate": 4.8200729927007305e-05,
      "loss": 3.6455,
      "step": 4930
    },
    {
      "epoch": 36.058394160583944,
      "grad_norm": 8.556285858154297,
      "learning_rate": 4.8197080291970805e-05,
      "loss": 3.5699,
      "step": 4940
    },
    {
      "epoch": 36.13138686131387,
      "grad_norm": 8.224872589111328,
      "learning_rate": 4.8193430656934306e-05,
      "loss": 3.3928,
      "step": 4950
    },
    {
      "epoch": 36.2043795620438,
      "grad_norm": 7.59806489944458,
      "learning_rate": 4.818978102189781e-05,
      "loss": 3.3645,
      "step": 4960
    },
    {
      "epoch": 36.277372262773724,
      "grad_norm": 7.859370231628418,
      "learning_rate": 4.818613138686131e-05,
      "loss": 3.7871,
      "step": 4970
    },
    {
      "epoch": 36.35036496350365,
      "grad_norm": 8.498245239257812,
      "learning_rate": 4.818248175182482e-05,
      "loss": 3.6631,
      "step": 4980
    },
    {
      "epoch": 36.42335766423358,
      "grad_norm": 7.75554895401001,
      "learning_rate": 4.817883211678833e-05,
      "loss": 3.5868,
      "step": 4990
    },
    {
      "epoch": 36.496350364963504,
      "grad_norm": 6.834167957305908,
      "learning_rate": 4.817518248175183e-05,
      "loss": 3.6883,
      "step": 5000
    },
    {
      "epoch": 36.56934306569343,
      "grad_norm": 7.973153114318848,
      "learning_rate": 4.817153284671533e-05,
      "loss": 3.8606,
      "step": 5010
    },
    {
      "epoch": 36.64233576642336,
      "grad_norm": 8.367938041687012,
      "learning_rate": 4.816788321167883e-05,
      "loss": 3.4538,
      "step": 5020
    },
    {
      "epoch": 36.715328467153284,
      "grad_norm": 8.465568542480469,
      "learning_rate": 4.816423357664234e-05,
      "loss": 3.775,
      "step": 5030
    },
    {
      "epoch": 36.78832116788321,
      "grad_norm": 7.984766006469727,
      "learning_rate": 4.8160583941605844e-05,
      "loss": 3.458,
      "step": 5040
    },
    {
      "epoch": 36.86131386861314,
      "grad_norm": 7.885570526123047,
      "learning_rate": 4.8156934306569344e-05,
      "loss": 3.6382,
      "step": 5050
    },
    {
      "epoch": 36.934306569343065,
      "grad_norm": 7.751727104187012,
      "learning_rate": 4.815328467153285e-05,
      "loss": 3.4918,
      "step": 5060
    },
    {
      "epoch": 37.00729927007299,
      "grad_norm": 7.8749213218688965,
      "learning_rate": 4.814963503649635e-05,
      "loss": 3.188,
      "step": 5070
    },
    {
      "epoch": 37.08029197080292,
      "grad_norm": 8.98239517211914,
      "learning_rate": 4.814598540145986e-05,
      "loss": 3.5148,
      "step": 5080
    },
    {
      "epoch": 37.153284671532845,
      "grad_norm": 7.983080863952637,
      "learning_rate": 4.814233576642336e-05,
      "loss": 3.2805,
      "step": 5090
    },
    {
      "epoch": 37.22627737226277,
      "grad_norm": 7.651065349578857,
      "learning_rate": 4.813868613138686e-05,
      "loss": 3.3769,
      "step": 5100
    },
    {
      "epoch": 37.2992700729927,
      "grad_norm": 8.905315399169922,
      "learning_rate": 4.813503649635037e-05,
      "loss": 3.4247,
      "step": 5110
    },
    {
      "epoch": 37.372262773722625,
      "grad_norm": 7.717953681945801,
      "learning_rate": 4.813138686131387e-05,
      "loss": 3.5396,
      "step": 5120
    },
    {
      "epoch": 37.44525547445255,
      "grad_norm": 8.170761108398438,
      "learning_rate": 4.8127737226277375e-05,
      "loss": 3.4565,
      "step": 5130
    },
    {
      "epoch": 37.518248175182485,
      "grad_norm": 7.735912799835205,
      "learning_rate": 4.812408759124088e-05,
      "loss": 3.627,
      "step": 5140
    },
    {
      "epoch": 37.59124087591241,
      "grad_norm": 8.756954193115234,
      "learning_rate": 4.812043795620438e-05,
      "loss": 3.4775,
      "step": 5150
    },
    {
      "epoch": 37.66423357664234,
      "grad_norm": 7.630942344665527,
      "learning_rate": 4.811678832116789e-05,
      "loss": 3.7178,
      "step": 5160
    },
    {
      "epoch": 37.737226277372265,
      "grad_norm": 7.970333576202393,
      "learning_rate": 4.8113138686131384e-05,
      "loss": 3.5006,
      "step": 5170
    },
    {
      "epoch": 37.81021897810219,
      "grad_norm": 7.805846691131592,
      "learning_rate": 4.810948905109489e-05,
      "loss": 3.9233,
      "step": 5180
    },
    {
      "epoch": 37.88321167883212,
      "grad_norm": 8.106494903564453,
      "learning_rate": 4.81058394160584e-05,
      "loss": 3.6292,
      "step": 5190
    },
    {
      "epoch": 37.956204379562045,
      "grad_norm": 7.630003452301025,
      "learning_rate": 4.81021897810219e-05,
      "loss": 3.5748,
      "step": 5200
    },
    {
      "epoch": 38.02919708029197,
      "grad_norm": 7.444540023803711,
      "learning_rate": 4.8098540145985407e-05,
      "loss": 3.405,
      "step": 5210
    },
    {
      "epoch": 38.1021897810219,
      "grad_norm": 7.1685099601745605,
      "learning_rate": 4.809489051094891e-05,
      "loss": 3.4755,
      "step": 5220
    },
    {
      "epoch": 38.175182481751825,
      "grad_norm": 7.801115036010742,
      "learning_rate": 4.8091240875912414e-05,
      "loss": 3.6806,
      "step": 5230
    },
    {
      "epoch": 38.24817518248175,
      "grad_norm": 6.977494716644287,
      "learning_rate": 4.8087591240875915e-05,
      "loss": 3.5527,
      "step": 5240
    },
    {
      "epoch": 38.32116788321168,
      "grad_norm": 7.6545796394348145,
      "learning_rate": 4.8083941605839415e-05,
      "loss": 3.3136,
      "step": 5250
    },
    {
      "epoch": 38.394160583941606,
      "grad_norm": 7.864501953125,
      "learning_rate": 4.808029197080292e-05,
      "loss": 3.4614,
      "step": 5260
    },
    {
      "epoch": 38.46715328467153,
      "grad_norm": 7.947540760040283,
      "learning_rate": 4.807664233576642e-05,
      "loss": 3.6514,
      "step": 5270
    },
    {
      "epoch": 38.54014598540146,
      "grad_norm": 7.910742282867432,
      "learning_rate": 4.807299270072993e-05,
      "loss": 3.4754,
      "step": 5280
    },
    {
      "epoch": 38.613138686131386,
      "grad_norm": 8.706865310668945,
      "learning_rate": 4.806934306569343e-05,
      "loss": 3.1416,
      "step": 5290
    },
    {
      "epoch": 38.68613138686131,
      "grad_norm": 8.213972091674805,
      "learning_rate": 4.806569343065694e-05,
      "loss": 3.46,
      "step": 5300
    },
    {
      "epoch": 38.75912408759124,
      "grad_norm": 7.754948139190674,
      "learning_rate": 4.8062043795620445e-05,
      "loss": 3.6432,
      "step": 5310
    },
    {
      "epoch": 38.832116788321166,
      "grad_norm": 8.063849449157715,
      "learning_rate": 4.805839416058394e-05,
      "loss": 3.5726,
      "step": 5320
    },
    {
      "epoch": 38.90510948905109,
      "grad_norm": 8.260028839111328,
      "learning_rate": 4.8054744525547446e-05,
      "loss": 3.6472,
      "step": 5330
    },
    {
      "epoch": 38.97810218978102,
      "grad_norm": 8.385522842407227,
      "learning_rate": 4.8051094890510954e-05,
      "loss": 3.5822,
      "step": 5340
    },
    {
      "epoch": 39.051094890510946,
      "grad_norm": 9.119111061096191,
      "learning_rate": 4.8047445255474454e-05,
      "loss": 3.8066,
      "step": 5350
    },
    {
      "epoch": 39.12408759124087,
      "grad_norm": 7.678833484649658,
      "learning_rate": 4.804379562043796e-05,
      "loss": 3.3065,
      "step": 5360
    },
    {
      "epoch": 39.197080291970806,
      "grad_norm": 7.519107818603516,
      "learning_rate": 4.804014598540146e-05,
      "loss": 3.5248,
      "step": 5370
    },
    {
      "epoch": 39.27007299270073,
      "grad_norm": 8.764467239379883,
      "learning_rate": 4.803649635036497e-05,
      "loss": 3.3004,
      "step": 5380
    },
    {
      "epoch": 39.34306569343066,
      "grad_norm": 8.218254089355469,
      "learning_rate": 4.803284671532847e-05,
      "loss": 3.3107,
      "step": 5390
    },
    {
      "epoch": 39.416058394160586,
      "grad_norm": 7.388239860534668,
      "learning_rate": 4.802919708029197e-05,
      "loss": 3.6029,
      "step": 5400
    },
    {
      "epoch": 39.48905109489051,
      "grad_norm": 8.29238510131836,
      "learning_rate": 4.802554744525548e-05,
      "loss": 3.6584,
      "step": 5410
    },
    {
      "epoch": 39.56204379562044,
      "grad_norm": 8.503937721252441,
      "learning_rate": 4.802189781021898e-05,
      "loss": 3.7569,
      "step": 5420
    },
    {
      "epoch": 39.63503649635037,
      "grad_norm": 7.549898624420166,
      "learning_rate": 4.8018248175182485e-05,
      "loss": 3.7687,
      "step": 5430
    },
    {
      "epoch": 39.70802919708029,
      "grad_norm": 8.094426155090332,
      "learning_rate": 4.8014598540145986e-05,
      "loss": 3.2663,
      "step": 5440
    },
    {
      "epoch": 39.78102189781022,
      "grad_norm": 8.808469772338867,
      "learning_rate": 4.801094890510949e-05,
      "loss": 3.5077,
      "step": 5450
    },
    {
      "epoch": 39.85401459854015,
      "grad_norm": 8.591320991516113,
      "learning_rate": 4.8007299270073e-05,
      "loss": 3.2354,
      "step": 5460
    },
    {
      "epoch": 39.92700729927007,
      "grad_norm": 8.45156192779541,
      "learning_rate": 4.8003649635036494e-05,
      "loss": 3.4133,
      "step": 5470
    },
    {
      "epoch": 40.0,
      "grad_norm": 11.088104248046875,
      "learning_rate": 4.8e-05,
      "loss": 3.45,
      "step": 5480
    },
    {
      "epoch": 40.07299270072993,
      "grad_norm": 7.851196765899658,
      "learning_rate": 4.79963503649635e-05,
      "loss": 3.4784,
      "step": 5490
    },
    {
      "epoch": 40.14598540145985,
      "grad_norm": 7.833030700683594,
      "learning_rate": 4.799270072992701e-05,
      "loss": 3.2536,
      "step": 5500
    },
    {
      "epoch": 40.21897810218978,
      "grad_norm": 7.880434989929199,
      "learning_rate": 4.7989051094890516e-05,
      "loss": 3.4483,
      "step": 5510
    },
    {
      "epoch": 40.29197080291971,
      "grad_norm": 8.096253395080566,
      "learning_rate": 4.798540145985402e-05,
      "loss": 3.3397,
      "step": 5520
    },
    {
      "epoch": 40.36496350364963,
      "grad_norm": 8.032604217529297,
      "learning_rate": 4.7981751824817524e-05,
      "loss": 3.3018,
      "step": 5530
    },
    {
      "epoch": 40.43795620437956,
      "grad_norm": 8.032159805297852,
      "learning_rate": 4.7978102189781025e-05,
      "loss": 3.7694,
      "step": 5540
    },
    {
      "epoch": 40.51094890510949,
      "grad_norm": 8.186773300170898,
      "learning_rate": 4.7974452554744525e-05,
      "loss": 3.3668,
      "step": 5550
    },
    {
      "epoch": 40.583941605839414,
      "grad_norm": 8.991801261901855,
      "learning_rate": 4.797080291970803e-05,
      "loss": 3.856,
      "step": 5560
    },
    {
      "epoch": 40.65693430656934,
      "grad_norm": 7.750340938568115,
      "learning_rate": 4.796715328467153e-05,
      "loss": 3.4378,
      "step": 5570
    },
    {
      "epoch": 40.72992700729927,
      "grad_norm": 7.973230838775635,
      "learning_rate": 4.796350364963504e-05,
      "loss": 3.3758,
      "step": 5580
    },
    {
      "epoch": 40.802919708029194,
      "grad_norm": 8.188642501831055,
      "learning_rate": 4.795985401459854e-05,
      "loss": 3.3215,
      "step": 5590
    },
    {
      "epoch": 40.87591240875913,
      "grad_norm": 8.890199661254883,
      "learning_rate": 4.795620437956205e-05,
      "loss": 3.3235,
      "step": 5600
    },
    {
      "epoch": 40.948905109489054,
      "grad_norm": 8.327025413513184,
      "learning_rate": 4.7952554744525555e-05,
      "loss": 3.8562,
      "step": 5610
    },
    {
      "epoch": 41.02189781021898,
      "grad_norm": 8.276559829711914,
      "learning_rate": 4.794890510948905e-05,
      "loss": 3.4704,
      "step": 5620
    },
    {
      "epoch": 41.09489051094891,
      "grad_norm": 7.847341060638428,
      "learning_rate": 4.7945255474452556e-05,
      "loss": 3.5177,
      "step": 5630
    },
    {
      "epoch": 41.167883211678834,
      "grad_norm": 8.243897438049316,
      "learning_rate": 4.7941605839416057e-05,
      "loss": 3.6827,
      "step": 5640
    },
    {
      "epoch": 41.24087591240876,
      "grad_norm": 8.470718383789062,
      "learning_rate": 4.7937956204379564e-05,
      "loss": 3.3993,
      "step": 5650
    },
    {
      "epoch": 41.31386861313869,
      "grad_norm": 8.816664695739746,
      "learning_rate": 4.793430656934307e-05,
      "loss": 3.109,
      "step": 5660
    },
    {
      "epoch": 41.386861313868614,
      "grad_norm": 7.848477840423584,
      "learning_rate": 4.793065693430657e-05,
      "loss": 3.6092,
      "step": 5670
    },
    {
      "epoch": 41.45985401459854,
      "grad_norm": 8.592061042785645,
      "learning_rate": 4.792700729927008e-05,
      "loss": 3.2849,
      "step": 5680
    },
    {
      "epoch": 41.53284671532847,
      "grad_norm": 8.932568550109863,
      "learning_rate": 4.792335766423358e-05,
      "loss": 3.4836,
      "step": 5690
    },
    {
      "epoch": 41.605839416058394,
      "grad_norm": 8.16585636138916,
      "learning_rate": 4.791970802919708e-05,
      "loss": 3.3923,
      "step": 5700
    },
    {
      "epoch": 41.67883211678832,
      "grad_norm": 7.576832294464111,
      "learning_rate": 4.791605839416059e-05,
      "loss": 3.3418,
      "step": 5710
    },
    {
      "epoch": 41.75182481751825,
      "grad_norm": 8.280080795288086,
      "learning_rate": 4.791240875912409e-05,
      "loss": 3.6241,
      "step": 5720
    },
    {
      "epoch": 41.824817518248175,
      "grad_norm": 7.970485210418701,
      "learning_rate": 4.7908759124087595e-05,
      "loss": 3.1501,
      "step": 5730
    },
    {
      "epoch": 41.8978102189781,
      "grad_norm": 8.215784072875977,
      "learning_rate": 4.7905109489051095e-05,
      "loss": 3.6537,
      "step": 5740
    },
    {
      "epoch": 41.97080291970803,
      "grad_norm": 6.648679733276367,
      "learning_rate": 4.79014598540146e-05,
      "loss": 3.3843,
      "step": 5750
    },
    {
      "epoch": 42.043795620437955,
      "grad_norm": 8.224967956542969,
      "learning_rate": 4.78978102189781e-05,
      "loss": 3.4462,
      "step": 5760
    },
    {
      "epoch": 42.11678832116788,
      "grad_norm": 7.826653957366943,
      "learning_rate": 4.789416058394161e-05,
      "loss": 3.3728,
      "step": 5770
    },
    {
      "epoch": 42.18978102189781,
      "grad_norm": 9.278883934020996,
      "learning_rate": 4.789051094890511e-05,
      "loss": 3.4177,
      "step": 5780
    },
    {
      "epoch": 42.262773722627735,
      "grad_norm": 7.91762638092041,
      "learning_rate": 4.788686131386861e-05,
      "loss": 3.1888,
      "step": 5790
    },
    {
      "epoch": 42.33576642335766,
      "grad_norm": 7.0802741050720215,
      "learning_rate": 4.788321167883212e-05,
      "loss": 3.4716,
      "step": 5800
    },
    {
      "epoch": 42.40875912408759,
      "grad_norm": 8.203780174255371,
      "learning_rate": 4.7879562043795626e-05,
      "loss": 3.5643,
      "step": 5810
    },
    {
      "epoch": 42.481751824817515,
      "grad_norm": 8.095193862915039,
      "learning_rate": 4.7875912408759126e-05,
      "loss": 3.3392,
      "step": 5820
    },
    {
      "epoch": 42.55474452554745,
      "grad_norm": 7.561723709106445,
      "learning_rate": 4.7872262773722634e-05,
      "loss": 3.0836,
      "step": 5830
    },
    {
      "epoch": 42.627737226277375,
      "grad_norm": 8.400357246398926,
      "learning_rate": 4.7868613138686134e-05,
      "loss": 3.3204,
      "step": 5840
    },
    {
      "epoch": 42.7007299270073,
      "grad_norm": 8.270870208740234,
      "learning_rate": 4.7864963503649635e-05,
      "loss": 3.3011,
      "step": 5850
    },
    {
      "epoch": 42.77372262773723,
      "grad_norm": 8.199334144592285,
      "learning_rate": 4.786131386861314e-05,
      "loss": 3.3879,
      "step": 5860
    },
    {
      "epoch": 42.846715328467155,
      "grad_norm": 8.647783279418945,
      "learning_rate": 4.785766423357664e-05,
      "loss": 3.6197,
      "step": 5870
    },
    {
      "epoch": 42.91970802919708,
      "grad_norm": 7.040407657623291,
      "learning_rate": 4.785401459854015e-05,
      "loss": 3.373,
      "step": 5880
    },
    {
      "epoch": 42.99270072992701,
      "grad_norm": 7.9279608726501465,
      "learning_rate": 4.785036496350365e-05,
      "loss": 3.4574,
      "step": 5890
    },
    {
      "epoch": 43.065693430656935,
      "grad_norm": 7.815570831298828,
      "learning_rate": 4.784671532846716e-05,
      "loss": 3.8015,
      "step": 5900
    },
    {
      "epoch": 43.13868613138686,
      "grad_norm": 8.99423599243164,
      "learning_rate": 4.784306569343066e-05,
      "loss": 3.3535,
      "step": 5910
    },
    {
      "epoch": 43.21167883211679,
      "grad_norm": 7.802853107452393,
      "learning_rate": 4.7839416058394165e-05,
      "loss": 3.351,
      "step": 5920
    },
    {
      "epoch": 43.284671532846716,
      "grad_norm": 6.702826023101807,
      "learning_rate": 4.7835766423357666e-05,
      "loss": 3.4012,
      "step": 5930
    },
    {
      "epoch": 43.35766423357664,
      "grad_norm": 8.646587371826172,
      "learning_rate": 4.7832116788321166e-05,
      "loss": 3.3531,
      "step": 5940
    },
    {
      "epoch": 43.43065693430657,
      "grad_norm": 8.114013671875,
      "learning_rate": 4.7828467153284674e-05,
      "loss": 3.1752,
      "step": 5950
    },
    {
      "epoch": 43.503649635036496,
      "grad_norm": 7.471768856048584,
      "learning_rate": 4.7824817518248174e-05,
      "loss": 3.4698,
      "step": 5960
    },
    {
      "epoch": 43.57664233576642,
      "grad_norm": 7.253126621246338,
      "learning_rate": 4.782116788321168e-05,
      "loss": 3.0626,
      "step": 5970
    },
    {
      "epoch": 43.64963503649635,
      "grad_norm": 7.210217475891113,
      "learning_rate": 4.781751824817519e-05,
      "loss": 3.4915,
      "step": 5980
    },
    {
      "epoch": 43.722627737226276,
      "grad_norm": 7.758913040161133,
      "learning_rate": 4.781386861313869e-05,
      "loss": 3.1302,
      "step": 5990
    },
    {
      "epoch": 43.7956204379562,
      "grad_norm": 7.674396991729736,
      "learning_rate": 4.7810218978102196e-05,
      "loss": 3.0891,
      "step": 6000
    },
    {
      "epoch": 43.86861313868613,
      "grad_norm": 7.92799186706543,
      "learning_rate": 4.78065693430657e-05,
      "loss": 3.398,
      "step": 6010
    },
    {
      "epoch": 43.941605839416056,
      "grad_norm": 8.01329517364502,
      "learning_rate": 4.78029197080292e-05,
      "loss": 3.5968,
      "step": 6020
    },
    {
      "epoch": 44.01459854014598,
      "grad_norm": 7.623823642730713,
      "learning_rate": 4.7799270072992705e-05,
      "loss": 3.5915,
      "step": 6030
    },
    {
      "epoch": 44.08759124087591,
      "grad_norm": 8.074661254882812,
      "learning_rate": 4.7795620437956205e-05,
      "loss": 3.0483,
      "step": 6040
    },
    {
      "epoch": 44.160583941605836,
      "grad_norm": 8.408352851867676,
      "learning_rate": 4.779197080291971e-05,
      "loss": 3.0986,
      "step": 6050
    },
    {
      "epoch": 44.23357664233577,
      "grad_norm": 6.293351650238037,
      "learning_rate": 4.778832116788321e-05,
      "loss": 3.1777,
      "step": 6060
    },
    {
      "epoch": 44.306569343065696,
      "grad_norm": 7.9824957847595215,
      "learning_rate": 4.778467153284672e-05,
      "loss": 3.4047,
      "step": 6070
    },
    {
      "epoch": 44.37956204379562,
      "grad_norm": 9.020962715148926,
      "learning_rate": 4.778102189781022e-05,
      "loss": 3.3284,
      "step": 6080
    },
    {
      "epoch": 44.45255474452555,
      "grad_norm": 9.30832576751709,
      "learning_rate": 4.777737226277372e-05,
      "loss": 3.2013,
      "step": 6090
    },
    {
      "epoch": 44.52554744525548,
      "grad_norm": 8.706302642822266,
      "learning_rate": 4.777372262773723e-05,
      "loss": 3.2472,
      "step": 6100
    },
    {
      "epoch": 44.5985401459854,
      "grad_norm": 8.357048988342285,
      "learning_rate": 4.777007299270073e-05,
      "loss": 3.5373,
      "step": 6110
    },
    {
      "epoch": 44.67153284671533,
      "grad_norm": 9.157851219177246,
      "learning_rate": 4.7766423357664236e-05,
      "loss": 3.5699,
      "step": 6120
    },
    {
      "epoch": 44.74452554744526,
      "grad_norm": 8.813905715942383,
      "learning_rate": 4.7762773722627743e-05,
      "loss": 3.1916,
      "step": 6130
    },
    {
      "epoch": 44.81751824817518,
      "grad_norm": 7.631267070770264,
      "learning_rate": 4.7759124087591244e-05,
      "loss": 3.3818,
      "step": 6140
    },
    {
      "epoch": 44.89051094890511,
      "grad_norm": 9.422798156738281,
      "learning_rate": 4.775547445255475e-05,
      "loss": 3.1862,
      "step": 6150
    },
    {
      "epoch": 44.96350364963504,
      "grad_norm": 8.981171607971191,
      "learning_rate": 4.7751824817518245e-05,
      "loss": 3.5818,
      "step": 6160
    },
    {
      "epoch": 45.03649635036496,
      "grad_norm": 7.611003875732422,
      "learning_rate": 4.774817518248175e-05,
      "loss": 2.9212,
      "step": 6170
    },
    {
      "epoch": 45.10948905109489,
      "grad_norm": 8.631560325622559,
      "learning_rate": 4.774452554744526e-05,
      "loss": 3.5617,
      "step": 6180
    },
    {
      "epoch": 45.18248175182482,
      "grad_norm": 7.319159030914307,
      "learning_rate": 4.774087591240876e-05,
      "loss": 3.0562,
      "step": 6190
    },
    {
      "epoch": 45.25547445255474,
      "grad_norm": 8.584617614746094,
      "learning_rate": 4.773722627737227e-05,
      "loss": 3.2909,
      "step": 6200
    },
    {
      "epoch": 45.32846715328467,
      "grad_norm": 7.984152317047119,
      "learning_rate": 4.773357664233577e-05,
      "loss": 3.0322,
      "step": 6210
    },
    {
      "epoch": 45.4014598540146,
      "grad_norm": 8.619772911071777,
      "learning_rate": 4.7729927007299275e-05,
      "loss": 3.2377,
      "step": 6220
    },
    {
      "epoch": 45.47445255474452,
      "grad_norm": 8.109354019165039,
      "learning_rate": 4.7726277372262775e-05,
      "loss": 3.5697,
      "step": 6230
    },
    {
      "epoch": 45.54744525547445,
      "grad_norm": 6.780518531799316,
      "learning_rate": 4.7722627737226276e-05,
      "loss": 3.253,
      "step": 6240
    },
    {
      "epoch": 45.62043795620438,
      "grad_norm": 8.609785079956055,
      "learning_rate": 4.771897810218978e-05,
      "loss": 3.6896,
      "step": 6250
    },
    {
      "epoch": 45.693430656934304,
      "grad_norm": 8.477563858032227,
      "learning_rate": 4.7715328467153284e-05,
      "loss": 3.2695,
      "step": 6260
    },
    {
      "epoch": 45.76642335766423,
      "grad_norm": 7.173305034637451,
      "learning_rate": 4.771167883211679e-05,
      "loss": 3.4002,
      "step": 6270
    },
    {
      "epoch": 45.839416058394164,
      "grad_norm": 8.419568061828613,
      "learning_rate": 4.77080291970803e-05,
      "loss": 2.9949,
      "step": 6280
    },
    {
      "epoch": 45.91240875912409,
      "grad_norm": 7.53433084487915,
      "learning_rate": 4.77043795620438e-05,
      "loss": 3.2673,
      "step": 6290
    },
    {
      "epoch": 45.98540145985402,
      "grad_norm": 6.914299011230469,
      "learning_rate": 4.7700729927007306e-05,
      "loss": 3.044,
      "step": 6300
    },
    {
      "epoch": 46.058394160583944,
      "grad_norm": 8.319601058959961,
      "learning_rate": 4.76970802919708e-05,
      "loss": 3.1025,
      "step": 6310
    },
    {
      "epoch": 46.13138686131387,
      "grad_norm": 8.289743423461914,
      "learning_rate": 4.769343065693431e-05,
      "loss": 3.4498,
      "step": 6320
    },
    {
      "epoch": 46.2043795620438,
      "grad_norm": 7.827085971832275,
      "learning_rate": 4.7689781021897814e-05,
      "loss": 3.4867,
      "step": 6330
    },
    {
      "epoch": 46.277372262773724,
      "grad_norm": 7.695301055908203,
      "learning_rate": 4.7686131386861315e-05,
      "loss": 2.9184,
      "step": 6340
    },
    {
      "epoch": 46.35036496350365,
      "grad_norm": 8.588726043701172,
      "learning_rate": 4.768248175182482e-05,
      "loss": 3.2036,
      "step": 6350
    },
    {
      "epoch": 46.42335766423358,
      "grad_norm": 8.482800483703613,
      "learning_rate": 4.767883211678832e-05,
      "loss": 3.2342,
      "step": 6360
    },
    {
      "epoch": 46.496350364963504,
      "grad_norm": 7.755641937255859,
      "learning_rate": 4.767518248175183e-05,
      "loss": 3.1751,
      "step": 6370
    },
    {
      "epoch": 46.56934306569343,
      "grad_norm": 7.331522464752197,
      "learning_rate": 4.767153284671533e-05,
      "loss": 3.1142,
      "step": 6380
    },
    {
      "epoch": 46.64233576642336,
      "grad_norm": 8.841760635375977,
      "learning_rate": 4.766788321167883e-05,
      "loss": 3.2817,
      "step": 6390
    },
    {
      "epoch": 46.715328467153284,
      "grad_norm": 8.367350578308105,
      "learning_rate": 4.766423357664234e-05,
      "loss": 3.4592,
      "step": 6400
    },
    {
      "epoch": 46.78832116788321,
      "grad_norm": 7.9269819259643555,
      "learning_rate": 4.766058394160584e-05,
      "loss": 3.3734,
      "step": 6410
    },
    {
      "epoch": 46.86131386861314,
      "grad_norm": 7.648374557495117,
      "learning_rate": 4.7656934306569346e-05,
      "loss": 3.2354,
      "step": 6420
    },
    {
      "epoch": 46.934306569343065,
      "grad_norm": 8.340424537658691,
      "learning_rate": 4.7653284671532846e-05,
      "loss": 3.2012,
      "step": 6430
    },
    {
      "epoch": 47.00729927007299,
      "grad_norm": 7.593295574188232,
      "learning_rate": 4.7649635036496354e-05,
      "loss": 3.0115,
      "step": 6440
    },
    {
      "epoch": 47.08029197080292,
      "grad_norm": 8.265079498291016,
      "learning_rate": 4.764598540145986e-05,
      "loss": 3.2821,
      "step": 6450
    },
    {
      "epoch": 47.153284671532845,
      "grad_norm": 8.486672401428223,
      "learning_rate": 4.764233576642336e-05,
      "loss": 3.1055,
      "step": 6460
    },
    {
      "epoch": 47.22627737226277,
      "grad_norm": 8.751380920410156,
      "learning_rate": 4.763868613138686e-05,
      "loss": 3.2652,
      "step": 6470
    },
    {
      "epoch": 47.2992700729927,
      "grad_norm": 7.996803283691406,
      "learning_rate": 4.763503649635037e-05,
      "loss": 2.8484,
      "step": 6480
    },
    {
      "epoch": 47.372262773722625,
      "grad_norm": 6.5116095542907715,
      "learning_rate": 4.763138686131387e-05,
      "loss": 3.2133,
      "step": 6490
    },
    {
      "epoch": 47.44525547445255,
      "grad_norm": 7.70583438873291,
      "learning_rate": 4.762773722627738e-05,
      "loss": 3.4393,
      "step": 6500
    },
    {
      "epoch": 47.518248175182485,
      "grad_norm": 7.913601398468018,
      "learning_rate": 4.762408759124088e-05,
      "loss": 3.3748,
      "step": 6510
    },
    {
      "epoch": 47.59124087591241,
      "grad_norm": 8.797821044921875,
      "learning_rate": 4.7620437956204385e-05,
      "loss": 3.3289,
      "step": 6520
    },
    {
      "epoch": 47.66423357664234,
      "grad_norm": 9.293970108032227,
      "learning_rate": 4.7616788321167885e-05,
      "loss": 3.2616,
      "step": 6530
    },
    {
      "epoch": 47.737226277372265,
      "grad_norm": 8.983185768127441,
      "learning_rate": 4.7613138686131386e-05,
      "loss": 3.1633,
      "step": 6540
    },
    {
      "epoch": 47.81021897810219,
      "grad_norm": 8.024626731872559,
      "learning_rate": 4.760948905109489e-05,
      "loss": 3.0005,
      "step": 6550
    },
    {
      "epoch": 47.88321167883212,
      "grad_norm": 8.093104362487793,
      "learning_rate": 4.7605839416058393e-05,
      "loss": 3.0488,
      "step": 6560
    },
    {
      "epoch": 47.956204379562045,
      "grad_norm": 8.108990669250488,
      "learning_rate": 4.76021897810219e-05,
      "loss": 3.4313,
      "step": 6570
    },
    {
      "epoch": 48.02919708029197,
      "grad_norm": 6.5400390625,
      "learning_rate": 4.75985401459854e-05,
      "loss": 3.2367,
      "step": 6580
    },
    {
      "epoch": 48.1021897810219,
      "grad_norm": 6.52652645111084,
      "learning_rate": 4.759489051094891e-05,
      "loss": 3.1706,
      "step": 6590
    },
    {
      "epoch": 48.175182481751825,
      "grad_norm": 8.280606269836426,
      "learning_rate": 4.7591240875912416e-05,
      "loss": 3.4095,
      "step": 6600
    },
    {
      "epoch": 48.24817518248175,
      "grad_norm": 7.996752738952637,
      "learning_rate": 4.7587591240875916e-05,
      "loss": 2.9748,
      "step": 6610
    },
    {
      "epoch": 48.32116788321168,
      "grad_norm": 7.266270160675049,
      "learning_rate": 4.758394160583942e-05,
      "loss": 3.2486,
      "step": 6620
    },
    {
      "epoch": 48.394160583941606,
      "grad_norm": 8.208627700805664,
      "learning_rate": 4.758029197080292e-05,
      "loss": 3.1923,
      "step": 6630
    },
    {
      "epoch": 48.46715328467153,
      "grad_norm": 7.961183071136475,
      "learning_rate": 4.7576642335766424e-05,
      "loss": 3.1559,
      "step": 6640
    },
    {
      "epoch": 48.54014598540146,
      "grad_norm": 7.640244483947754,
      "learning_rate": 4.757299270072993e-05,
      "loss": 3.0511,
      "step": 6650
    },
    {
      "epoch": 48.613138686131386,
      "grad_norm": 9.09791374206543,
      "learning_rate": 4.756934306569343e-05,
      "loss": 3.2681,
      "step": 6660
    },
    {
      "epoch": 48.68613138686131,
      "grad_norm": 7.655438423156738,
      "learning_rate": 4.756569343065694e-05,
      "loss": 3.2983,
      "step": 6670
    },
    {
      "epoch": 48.75912408759124,
      "grad_norm": 8.613940238952637,
      "learning_rate": 4.756204379562044e-05,
      "loss": 3.1801,
      "step": 6680
    },
    {
      "epoch": 48.832116788321166,
      "grad_norm": 7.776800155639648,
      "learning_rate": 4.755839416058394e-05,
      "loss": 3.131,
      "step": 6690
    },
    {
      "epoch": 48.90510948905109,
      "grad_norm": 8.483904838562012,
      "learning_rate": 4.755474452554745e-05,
      "loss": 3.0963,
      "step": 6700
    },
    {
      "epoch": 48.97810218978102,
      "grad_norm": 9.357133865356445,
      "learning_rate": 4.755109489051095e-05,
      "loss": 3.2514,
      "step": 6710
    },
    {
      "epoch": 49.051094890510946,
      "grad_norm": 8.910223007202148,
      "learning_rate": 4.7547445255474456e-05,
      "loss": 3.1063,
      "step": 6720
    },
    {
      "epoch": 49.12408759124087,
      "grad_norm": 6.98513650894165,
      "learning_rate": 4.7543795620437956e-05,
      "loss": 3.0783,
      "step": 6730
    },
    {
      "epoch": 49.197080291970806,
      "grad_norm": 8.270976066589355,
      "learning_rate": 4.754014598540146e-05,
      "loss": 2.8322,
      "step": 6740
    },
    {
      "epoch": 49.27007299270073,
      "grad_norm": 7.9560980796813965,
      "learning_rate": 4.753649635036497e-05,
      "loss": 3.1194,
      "step": 6750
    },
    {
      "epoch": 49.34306569343066,
      "grad_norm": 9.021684646606445,
      "learning_rate": 4.753284671532847e-05,
      "loss": 3.2461,
      "step": 6760
    },
    {
      "epoch": 49.416058394160586,
      "grad_norm": 8.336380004882812,
      "learning_rate": 4.752919708029197e-05,
      "loss": 3.2588,
      "step": 6770
    },
    {
      "epoch": 49.48905109489051,
      "grad_norm": 7.956487655639648,
      "learning_rate": 4.752554744525547e-05,
      "loss": 3.2936,
      "step": 6780
    },
    {
      "epoch": 49.56204379562044,
      "grad_norm": 8.422191619873047,
      "learning_rate": 4.752189781021898e-05,
      "loss": 3.2509,
      "step": 6790
    },
    {
      "epoch": 49.63503649635037,
      "grad_norm": 8.445765495300293,
      "learning_rate": 4.7518248175182487e-05,
      "loss": 2.9764,
      "step": 6800
    },
    {
      "epoch": 49.70802919708029,
      "grad_norm": 8.798067092895508,
      "learning_rate": 4.751459854014599e-05,
      "loss": 3.1271,
      "step": 6810
    },
    {
      "epoch": 49.78102189781022,
      "grad_norm": 6.587914943695068,
      "learning_rate": 4.7510948905109494e-05,
      "loss": 3.1269,
      "step": 6820
    },
    {
      "epoch": 49.85401459854015,
      "grad_norm": 8.311006546020508,
      "learning_rate": 4.7507299270072995e-05,
      "loss": 2.8829,
      "step": 6830
    },
    {
      "epoch": 49.92700729927007,
      "grad_norm": 8.812559127807617,
      "learning_rate": 4.75036496350365e-05,
      "loss": 3.4239,
      "step": 6840
    },
    {
      "epoch": 50.0,
      "grad_norm": 13.155478477478027,
      "learning_rate": 4.75e-05,
      "loss": 3.0946,
      "step": 6850
    },
    {
      "epoch": 50.07299270072993,
      "grad_norm": 8.148968696594238,
      "learning_rate": 4.74963503649635e-05,
      "loss": 2.9701,
      "step": 6860
    },
    {
      "epoch": 50.14598540145985,
      "grad_norm": 8.93412971496582,
      "learning_rate": 4.749270072992701e-05,
      "loss": 3.4182,
      "step": 6870
    },
    {
      "epoch": 50.21897810218978,
      "grad_norm": 8.662405967712402,
      "learning_rate": 4.748905109489051e-05,
      "loss": 3.2595,
      "step": 6880
    },
    {
      "epoch": 50.29197080291971,
      "grad_norm": 9.27475643157959,
      "learning_rate": 4.748540145985402e-05,
      "loss": 3.1069,
      "step": 6890
    },
    {
      "epoch": 50.36496350364963,
      "grad_norm": 10.466789245605469,
      "learning_rate": 4.748175182481752e-05,
      "loss": 2.9941,
      "step": 6900
    },
    {
      "epoch": 50.43795620437956,
      "grad_norm": 7.1885271072387695,
      "learning_rate": 4.7478102189781026e-05,
      "loss": 3.2591,
      "step": 6910
    },
    {
      "epoch": 50.51094890510949,
      "grad_norm": 7.737522602081299,
      "learning_rate": 4.7474452554744526e-05,
      "loss": 3.0623,
      "step": 6920
    },
    {
      "epoch": 50.583941605839414,
      "grad_norm": 9.480949401855469,
      "learning_rate": 4.747080291970803e-05,
      "loss": 3.1109,
      "step": 6930
    },
    {
      "epoch": 50.65693430656934,
      "grad_norm": 9.150589942932129,
      "learning_rate": 4.7467153284671534e-05,
      "loss": 2.793,
      "step": 6940
    },
    {
      "epoch": 50.72992700729927,
      "grad_norm": 8.910369873046875,
      "learning_rate": 4.746350364963504e-05,
      "loss": 2.9977,
      "step": 6950
    },
    {
      "epoch": 50.802919708029194,
      "grad_norm": 8.2529878616333,
      "learning_rate": 4.745985401459854e-05,
      "loss": 3.2827,
      "step": 6960
    },
    {
      "epoch": 50.87591240875913,
      "grad_norm": 8.507355690002441,
      "learning_rate": 4.745620437956205e-05,
      "loss": 3.5537,
      "step": 6970
    },
    {
      "epoch": 50.948905109489054,
      "grad_norm": 8.433154106140137,
      "learning_rate": 4.745255474452555e-05,
      "loss": 2.8413,
      "step": 6980
    },
    {
      "epoch": 51.02189781021898,
      "grad_norm": 7.633495330810547,
      "learning_rate": 4.744890510948906e-05,
      "loss": 2.8885,
      "step": 6990
    },
    {
      "epoch": 51.09489051094891,
      "grad_norm": 7.928781509399414,
      "learning_rate": 4.744525547445256e-05,
      "loss": 3.0919,
      "step": 7000
    },
    {
      "epoch": 51.167883211678834,
      "grad_norm": 8.462050437927246,
      "learning_rate": 4.744160583941606e-05,
      "loss": 3.2603,
      "step": 7010
    },
    {
      "epoch": 51.24087591240876,
      "grad_norm": 9.324630737304688,
      "learning_rate": 4.7437956204379565e-05,
      "loss": 2.9565,
      "step": 7020
    },
    {
      "epoch": 51.31386861313869,
      "grad_norm": 9.284012794494629,
      "learning_rate": 4.7434306569343066e-05,
      "loss": 3.1831,
      "step": 7030
    },
    {
      "epoch": 51.386861313868614,
      "grad_norm": 7.401770114898682,
      "learning_rate": 4.743065693430657e-05,
      "loss": 3.2805,
      "step": 7040
    },
    {
      "epoch": 51.45985401459854,
      "grad_norm": 7.772216320037842,
      "learning_rate": 4.7427007299270073e-05,
      "loss": 2.8794,
      "step": 7050
    },
    {
      "epoch": 51.53284671532847,
      "grad_norm": 8.472151756286621,
      "learning_rate": 4.742335766423358e-05,
      "loss": 3.0764,
      "step": 7060
    },
    {
      "epoch": 51.605839416058394,
      "grad_norm": 8.688995361328125,
      "learning_rate": 4.741970802919709e-05,
      "loss": 3.246,
      "step": 7070
    },
    {
      "epoch": 51.67883211678832,
      "grad_norm": 6.8997626304626465,
      "learning_rate": 4.741605839416058e-05,
      "loss": 2.8745,
      "step": 7080
    },
    {
      "epoch": 51.75182481751825,
      "grad_norm": 8.30561637878418,
      "learning_rate": 4.741240875912409e-05,
      "loss": 2.8537,
      "step": 7090
    },
    {
      "epoch": 51.824817518248175,
      "grad_norm": 7.878106117248535,
      "learning_rate": 4.740875912408759e-05,
      "loss": 3.2463,
      "step": 7100
    },
    {
      "epoch": 51.8978102189781,
      "grad_norm": 8.159969329833984,
      "learning_rate": 4.74051094890511e-05,
      "loss": 2.8647,
      "step": 7110
    },
    {
      "epoch": 51.97080291970803,
      "grad_norm": 8.35611343383789,
      "learning_rate": 4.7401459854014604e-05,
      "loss": 3.2364,
      "step": 7120
    },
    {
      "epoch": 52.043795620437955,
      "grad_norm": 8.205087661743164,
      "learning_rate": 4.7397810218978105e-05,
      "loss": 3.3511,
      "step": 7130
    },
    {
      "epoch": 52.11678832116788,
      "grad_norm": 7.545895576477051,
      "learning_rate": 4.739416058394161e-05,
      "loss": 3.1315,
      "step": 7140
    },
    {
      "epoch": 52.18978102189781,
      "grad_norm": 9.007702827453613,
      "learning_rate": 4.739051094890511e-05,
      "loss": 2.8386,
      "step": 7150
    },
    {
      "epoch": 52.262773722627735,
      "grad_norm": 8.291172981262207,
      "learning_rate": 4.738686131386861e-05,
      "loss": 3.0585,
      "step": 7160
    },
    {
      "epoch": 52.33576642335766,
      "grad_norm": 7.77424430847168,
      "learning_rate": 4.738321167883212e-05,
      "loss": 3.2152,
      "step": 7170
    },
    {
      "epoch": 52.40875912408759,
      "grad_norm": 7.736053943634033,
      "learning_rate": 4.737956204379562e-05,
      "loss": 3.2079,
      "step": 7180
    },
    {
      "epoch": 52.481751824817515,
      "grad_norm": 10.318626403808594,
      "learning_rate": 4.737591240875913e-05,
      "loss": 3.0801,
      "step": 7190
    },
    {
      "epoch": 52.55474452554745,
      "grad_norm": 9.387188911437988,
      "learning_rate": 4.737226277372263e-05,
      "loss": 3.1954,
      "step": 7200
    },
    {
      "epoch": 52.627737226277375,
      "grad_norm": 7.7614827156066895,
      "learning_rate": 4.7368613138686136e-05,
      "loss": 3.0758,
      "step": 7210
    },
    {
      "epoch": 52.7007299270073,
      "grad_norm": 7.961213111877441,
      "learning_rate": 4.736496350364964e-05,
      "loss": 3.0884,
      "step": 7220
    },
    {
      "epoch": 52.77372262773723,
      "grad_norm": 8.609980583190918,
      "learning_rate": 4.7361313868613137e-05,
      "loss": 2.8164,
      "step": 7230
    },
    {
      "epoch": 52.846715328467155,
      "grad_norm": 7.64917516708374,
      "learning_rate": 4.7357664233576644e-05,
      "loss": 2.7991,
      "step": 7240
    },
    {
      "epoch": 52.91970802919708,
      "grad_norm": 6.987364292144775,
      "learning_rate": 4.7354014598540144e-05,
      "loss": 2.7689,
      "step": 7250
    },
    {
      "epoch": 52.99270072992701,
      "grad_norm": 9.1091890335083,
      "learning_rate": 4.735036496350365e-05,
      "loss": 3.0655,
      "step": 7260
    },
    {
      "epoch": 53.065693430656935,
      "grad_norm": 8.643256187438965,
      "learning_rate": 4.734671532846716e-05,
      "loss": 3.2334,
      "step": 7270
    },
    {
      "epoch": 53.13868613138686,
      "grad_norm": 8.998845100402832,
      "learning_rate": 4.734306569343066e-05,
      "loss": 2.9809,
      "step": 7280
    },
    {
      "epoch": 53.21167883211679,
      "grad_norm": 7.0025315284729,
      "learning_rate": 4.733941605839417e-05,
      "loss": 2.9119,
      "step": 7290
    },
    {
      "epoch": 53.284671532846716,
      "grad_norm": 8.45015811920166,
      "learning_rate": 4.733576642335767e-05,
      "loss": 3.2102,
      "step": 7300
    },
    {
      "epoch": 53.35766423357664,
      "grad_norm": 7.2112555503845215,
      "learning_rate": 4.733211678832117e-05,
      "loss": 2.77,
      "step": 7310
    },
    {
      "epoch": 53.43065693430657,
      "grad_norm": 6.52118444442749,
      "learning_rate": 4.7328467153284675e-05,
      "loss": 3.0383,
      "step": 7320
    },
    {
      "epoch": 53.503649635036496,
      "grad_norm": 8.562469482421875,
      "learning_rate": 4.7324817518248175e-05,
      "loss": 3.0617,
      "step": 7330
    },
    {
      "epoch": 53.57664233576642,
      "grad_norm": 7.779535293579102,
      "learning_rate": 4.732116788321168e-05,
      "loss": 2.6697,
      "step": 7340
    },
    {
      "epoch": 53.64963503649635,
      "grad_norm": 8.611153602600098,
      "learning_rate": 4.731751824817518e-05,
      "loss": 3.284,
      "step": 7350
    },
    {
      "epoch": 53.722627737226276,
      "grad_norm": 9.611745834350586,
      "learning_rate": 4.731386861313869e-05,
      "loss": 3.1057,
      "step": 7360
    },
    {
      "epoch": 53.7956204379562,
      "grad_norm": 9.208066940307617,
      "learning_rate": 4.731021897810219e-05,
      "loss": 2.9746,
      "step": 7370
    },
    {
      "epoch": 53.86861313868613,
      "grad_norm": 7.771899223327637,
      "learning_rate": 4.730656934306569e-05,
      "loss": 2.9708,
      "step": 7380
    },
    {
      "epoch": 53.941605839416056,
      "grad_norm": 8.10665225982666,
      "learning_rate": 4.73029197080292e-05,
      "loss": 3.2753,
      "step": 7390
    },
    {
      "epoch": 54.01459854014598,
      "grad_norm": 8.1168851852417,
      "learning_rate": 4.72992700729927e-05,
      "loss": 2.8296,
      "step": 7400
    },
    {
      "epoch": 54.08759124087591,
      "grad_norm": 8.270454406738281,
      "learning_rate": 4.7295620437956206e-05,
      "loss": 3.3501,
      "step": 7410
    },
    {
      "epoch": 54.160583941605836,
      "grad_norm": 8.503896713256836,
      "learning_rate": 4.7291970802919714e-05,
      "loss": 3.0997,
      "step": 7420
    },
    {
      "epoch": 54.23357664233577,
      "grad_norm": 8.67824935913086,
      "learning_rate": 4.7288321167883214e-05,
      "loss": 3.0511,
      "step": 7430
    },
    {
      "epoch": 54.306569343065696,
      "grad_norm": 9.383095741271973,
      "learning_rate": 4.728467153284672e-05,
      "loss": 3.1079,
      "step": 7440
    },
    {
      "epoch": 54.37956204379562,
      "grad_norm": 7.855912685394287,
      "learning_rate": 4.728102189781022e-05,
      "loss": 3.3792,
      "step": 7450
    },
    {
      "epoch": 54.45255474452555,
      "grad_norm": 8.577168464660645,
      "learning_rate": 4.727737226277372e-05,
      "loss": 2.7239,
      "step": 7460
    },
    {
      "epoch": 54.52554744525548,
      "grad_norm": 7.604946136474609,
      "learning_rate": 4.727372262773723e-05,
      "loss": 2.919,
      "step": 7470
    },
    {
      "epoch": 54.5985401459854,
      "grad_norm": 9.38797378540039,
      "learning_rate": 4.727007299270073e-05,
      "loss": 2.7006,
      "step": 7480
    },
    {
      "epoch": 54.67153284671533,
      "grad_norm": 7.831418037414551,
      "learning_rate": 4.726642335766424e-05,
      "loss": 2.8255,
      "step": 7490
    },
    {
      "epoch": 54.74452554744526,
      "grad_norm": 8.843036651611328,
      "learning_rate": 4.726277372262774e-05,
      "loss": 2.6586,
      "step": 7500
    },
    {
      "epoch": 54.81751824817518,
      "grad_norm": 9.017900466918945,
      "learning_rate": 4.7259124087591245e-05,
      "loss": 3.0224,
      "step": 7510
    },
    {
      "epoch": 54.89051094890511,
      "grad_norm": 7.649842739105225,
      "learning_rate": 4.7255474452554746e-05,
      "loss": 3.1848,
      "step": 7520
    },
    {
      "epoch": 54.96350364963504,
      "grad_norm": 8.332992553710938,
      "learning_rate": 4.725182481751825e-05,
      "loss": 2.9028,
      "step": 7530
    },
    {
      "epoch": 55.03649635036496,
      "grad_norm": 7.571765899658203,
      "learning_rate": 4.7248175182481754e-05,
      "loss": 3.1115,
      "step": 7540
    },
    {
      "epoch": 55.10948905109489,
      "grad_norm": 8.418352127075195,
      "learning_rate": 4.7244525547445254e-05,
      "loss": 2.7409,
      "step": 7550
    },
    {
      "epoch": 55.18248175182482,
      "grad_norm": 8.395611763000488,
      "learning_rate": 4.724087591240876e-05,
      "loss": 3.094,
      "step": 7560
    },
    {
      "epoch": 55.25547445255474,
      "grad_norm": 8.436349868774414,
      "learning_rate": 4.723722627737226e-05,
      "loss": 2.9138,
      "step": 7570
    },
    {
      "epoch": 55.32846715328467,
      "grad_norm": 8.50199031829834,
      "learning_rate": 4.723357664233577e-05,
      "loss": 2.7833,
      "step": 7580
    },
    {
      "epoch": 55.4014598540146,
      "grad_norm": 9.715718269348145,
      "learning_rate": 4.7229927007299276e-05,
      "loss": 3.2616,
      "step": 7590
    },
    {
      "epoch": 55.47445255474452,
      "grad_norm": 8.50198745727539,
      "learning_rate": 4.722627737226278e-05,
      "loss": 2.8538,
      "step": 7600
    },
    {
      "epoch": 55.54744525547445,
      "grad_norm": 8.080168724060059,
      "learning_rate": 4.722262773722628e-05,
      "loss": 3.1015,
      "step": 7610
    },
    {
      "epoch": 55.62043795620438,
      "grad_norm": 8.738410949707031,
      "learning_rate": 4.7218978102189785e-05,
      "loss": 2.6505,
      "step": 7620
    },
    {
      "epoch": 55.693430656934304,
      "grad_norm": 9.000143051147461,
      "learning_rate": 4.7215328467153285e-05,
      "loss": 3.0646,
      "step": 7630
    },
    {
      "epoch": 55.76642335766423,
      "grad_norm": 8.794135093688965,
      "learning_rate": 4.721167883211679e-05,
      "loss": 2.7595,
      "step": 7640
    },
    {
      "epoch": 55.839416058394164,
      "grad_norm": 8.704475402832031,
      "learning_rate": 4.720802919708029e-05,
      "loss": 2.8167,
      "step": 7650
    },
    {
      "epoch": 55.91240875912409,
      "grad_norm": 7.363265037536621,
      "learning_rate": 4.72043795620438e-05,
      "loss": 3.286,
      "step": 7660
    },
    {
      "epoch": 55.98540145985402,
      "grad_norm": 8.374150276184082,
      "learning_rate": 4.72007299270073e-05,
      "loss": 3.2065,
      "step": 7670
    },
    {
      "epoch": 56.058394160583944,
      "grad_norm": 7.147278308868408,
      "learning_rate": 4.719708029197081e-05,
      "loss": 2.6702,
      "step": 7680
    },
    {
      "epoch": 56.13138686131387,
      "grad_norm": 8.565130233764648,
      "learning_rate": 4.719343065693431e-05,
      "loss": 2.981,
      "step": 7690
    },
    {
      "epoch": 56.2043795620438,
      "grad_norm": 8.724387168884277,
      "learning_rate": 4.718978102189781e-05,
      "loss": 3.1676,
      "step": 7700
    },
    {
      "epoch": 56.277372262773724,
      "grad_norm": 8.873944282531738,
      "learning_rate": 4.7186131386861316e-05,
      "loss": 2.8119,
      "step": 7710
    },
    {
      "epoch": 56.35036496350365,
      "grad_norm": 8.8887939453125,
      "learning_rate": 4.718248175182482e-05,
      "loss": 3.1508,
      "step": 7720
    },
    {
      "epoch": 56.42335766423358,
      "grad_norm": 8.206578254699707,
      "learning_rate": 4.7178832116788324e-05,
      "loss": 3.0256,
      "step": 7730
    },
    {
      "epoch": 56.496350364963504,
      "grad_norm": 9.247247695922852,
      "learning_rate": 4.717518248175183e-05,
      "loss": 2.7172,
      "step": 7740
    },
    {
      "epoch": 56.56934306569343,
      "grad_norm": 7.9568328857421875,
      "learning_rate": 4.717153284671533e-05,
      "loss": 2.95,
      "step": 7750
    },
    {
      "epoch": 56.64233576642336,
      "grad_norm": 8.44224739074707,
      "learning_rate": 4.716788321167883e-05,
      "loss": 2.7138,
      "step": 7760
    },
    {
      "epoch": 56.715328467153284,
      "grad_norm": 8.170133590698242,
      "learning_rate": 4.716423357664233e-05,
      "loss": 3.0865,
      "step": 7770
    },
    {
      "epoch": 56.78832116788321,
      "grad_norm": 9.341192245483398,
      "learning_rate": 4.716058394160584e-05,
      "loss": 2.9064,
      "step": 7780
    },
    {
      "epoch": 56.86131386861314,
      "grad_norm": 8.360554695129395,
      "learning_rate": 4.715693430656935e-05,
      "loss": 2.9896,
      "step": 7790
    },
    {
      "epoch": 56.934306569343065,
      "grad_norm": 9.025805473327637,
      "learning_rate": 4.715328467153285e-05,
      "loss": 3.0861,
      "step": 7800
    },
    {
      "epoch": 57.00729927007299,
      "grad_norm": 8.201424598693848,
      "learning_rate": 4.7149635036496355e-05,
      "loss": 2.8773,
      "step": 7810
    },
    {
      "epoch": 57.08029197080292,
      "grad_norm": 7.339296340942383,
      "learning_rate": 4.7145985401459855e-05,
      "loss": 2.8541,
      "step": 7820
    },
    {
      "epoch": 57.153284671532845,
      "grad_norm": 8.327534675598145,
      "learning_rate": 4.714233576642336e-05,
      "loss": 2.7412,
      "step": 7830
    },
    {
      "epoch": 57.22627737226277,
      "grad_norm": 8.350628852844238,
      "learning_rate": 4.713868613138686e-05,
      "loss": 2.7502,
      "step": 7840
    },
    {
      "epoch": 57.2992700729927,
      "grad_norm": 8.13753890991211,
      "learning_rate": 4.7135036496350364e-05,
      "loss": 2.8844,
      "step": 7850
    },
    {
      "epoch": 57.372262773722625,
      "grad_norm": 8.616318702697754,
      "learning_rate": 4.713138686131387e-05,
      "loss": 2.9623,
      "step": 7860
    },
    {
      "epoch": 57.44525547445255,
      "grad_norm": 10.462739944458008,
      "learning_rate": 4.712773722627737e-05,
      "loss": 2.9576,
      "step": 7870
    },
    {
      "epoch": 57.518248175182485,
      "grad_norm": 8.846748352050781,
      "learning_rate": 4.712408759124088e-05,
      "loss": 2.6251,
      "step": 7880
    },
    {
      "epoch": 57.59124087591241,
      "grad_norm": 8.074651718139648,
      "learning_rate": 4.7120437956204386e-05,
      "loss": 2.7671,
      "step": 7890
    },
    {
      "epoch": 57.66423357664234,
      "grad_norm": 9.134084701538086,
      "learning_rate": 4.7116788321167887e-05,
      "loss": 3.1364,
      "step": 7900
    },
    {
      "epoch": 57.737226277372265,
      "grad_norm": 7.320777893066406,
      "learning_rate": 4.7113138686131394e-05,
      "loss": 2.9796,
      "step": 7910
    },
    {
      "epoch": 57.81021897810219,
      "grad_norm": 8.287715911865234,
      "learning_rate": 4.710948905109489e-05,
      "loss": 2.976,
      "step": 7920
    },
    {
      "epoch": 57.88321167883212,
      "grad_norm": 7.829207420349121,
      "learning_rate": 4.7105839416058395e-05,
      "loss": 2.9708,
      "step": 7930
    },
    {
      "epoch": 57.956204379562045,
      "grad_norm": 9.010668754577637,
      "learning_rate": 4.71021897810219e-05,
      "loss": 3.0183,
      "step": 7940
    },
    {
      "epoch": 58.02919708029197,
      "grad_norm": 9.017006874084473,
      "learning_rate": 4.70985401459854e-05,
      "loss": 3.2393,
      "step": 7950
    },
    {
      "epoch": 58.1021897810219,
      "grad_norm": 8.199143409729004,
      "learning_rate": 4.709489051094891e-05,
      "loss": 2.8878,
      "step": 7960
    },
    {
      "epoch": 58.175182481751825,
      "grad_norm": 8.79704761505127,
      "learning_rate": 4.709124087591241e-05,
      "loss": 2.7013,
      "step": 7970
    },
    {
      "epoch": 58.24817518248175,
      "grad_norm": 7.466066837310791,
      "learning_rate": 4.708759124087592e-05,
      "loss": 2.3587,
      "step": 7980
    },
    {
      "epoch": 58.32116788321168,
      "grad_norm": 8.639784812927246,
      "learning_rate": 4.708394160583942e-05,
      "loss": 2.7224,
      "step": 7990
    },
    {
      "epoch": 58.394160583941606,
      "grad_norm": 7.833232402801514,
      "learning_rate": 4.708029197080292e-05,
      "loss": 2.941,
      "step": 8000
    },
    {
      "epoch": 58.46715328467153,
      "grad_norm": 7.295593738555908,
      "learning_rate": 4.7076642335766426e-05,
      "loss": 3.1675,
      "step": 8010
    },
    {
      "epoch": 58.54014598540146,
      "grad_norm": 8.510235786437988,
      "learning_rate": 4.7072992700729926e-05,
      "loss": 2.8726,
      "step": 8020
    },
    {
      "epoch": 58.613138686131386,
      "grad_norm": 8.183038711547852,
      "learning_rate": 4.7069343065693434e-05,
      "loss": 2.9273,
      "step": 8030
    },
    {
      "epoch": 58.68613138686131,
      "grad_norm": 8.993810653686523,
      "learning_rate": 4.7065693430656934e-05,
      "loss": 2.893,
      "step": 8040
    },
    {
      "epoch": 58.75912408759124,
      "grad_norm": 7.27039909362793,
      "learning_rate": 4.706204379562044e-05,
      "loss": 2.7321,
      "step": 8050
    },
    {
      "epoch": 58.832116788321166,
      "grad_norm": 8.704367637634277,
      "learning_rate": 4.705839416058395e-05,
      "loss": 2.8128,
      "step": 8060
    },
    {
      "epoch": 58.90510948905109,
      "grad_norm": 9.086610794067383,
      "learning_rate": 4.705474452554744e-05,
      "loss": 2.696,
      "step": 8070
    },
    {
      "epoch": 58.97810218978102,
      "grad_norm": 9.242753982543945,
      "learning_rate": 4.705109489051095e-05,
      "loss": 2.9498,
      "step": 8080
    },
    {
      "epoch": 59.051094890510946,
      "grad_norm": 9.885993957519531,
      "learning_rate": 4.704744525547446e-05,
      "loss": 2.679,
      "step": 8090
    },
    {
      "epoch": 59.12408759124087,
      "grad_norm": 9.550552368164062,
      "learning_rate": 4.704379562043796e-05,
      "loss": 2.9837,
      "step": 8100
    },
    {
      "epoch": 59.197080291970806,
      "grad_norm": 7.8576202392578125,
      "learning_rate": 4.7040145985401465e-05,
      "loss": 2.564,
      "step": 8110
    },
    {
      "epoch": 59.27007299270073,
      "grad_norm": 8.289311408996582,
      "learning_rate": 4.7036496350364965e-05,
      "loss": 2.7601,
      "step": 8120
    },
    {
      "epoch": 59.34306569343066,
      "grad_norm": 7.2179646492004395,
      "learning_rate": 4.703284671532847e-05,
      "loss": 2.6517,
      "step": 8130
    },
    {
      "epoch": 59.416058394160586,
      "grad_norm": 9.287817001342773,
      "learning_rate": 4.702919708029197e-05,
      "loss": 3.1439,
      "step": 8140
    },
    {
      "epoch": 59.48905109489051,
      "grad_norm": 9.152212142944336,
      "learning_rate": 4.7025547445255473e-05,
      "loss": 2.8307,
      "step": 8150
    },
    {
      "epoch": 59.56204379562044,
      "grad_norm": 7.792144775390625,
      "learning_rate": 4.702189781021898e-05,
      "loss": 2.8286,
      "step": 8160
    },
    {
      "epoch": 59.63503649635037,
      "grad_norm": 8.374923706054688,
      "learning_rate": 4.701824817518248e-05,
      "loss": 2.4463,
      "step": 8170
    },
    {
      "epoch": 59.70802919708029,
      "grad_norm": 7.935242652893066,
      "learning_rate": 4.701459854014599e-05,
      "loss": 2.7596,
      "step": 8180
    },
    {
      "epoch": 59.78102189781022,
      "grad_norm": 7.760723114013672,
      "learning_rate": 4.701094890510949e-05,
      "loss": 3.0939,
      "step": 8190
    },
    {
      "epoch": 59.85401459854015,
      "grad_norm": 8.32590103149414,
      "learning_rate": 4.7007299270072996e-05,
      "loss": 2.9403,
      "step": 8200
    },
    {
      "epoch": 59.92700729927007,
      "grad_norm": 8.131762504577637,
      "learning_rate": 4.7003649635036504e-05,
      "loss": 3.0918,
      "step": 8210
    },
    {
      "epoch": 60.0,
      "grad_norm": 11.821467399597168,
      "learning_rate": 4.7e-05,
      "loss": 2.9523,
      "step": 8220
    },
    {
      "epoch": 60.07299270072993,
      "grad_norm": 8.292750358581543,
      "learning_rate": 4.6996350364963505e-05,
      "loss": 2.9551,
      "step": 8230
    },
    {
      "epoch": 60.14598540145985,
      "grad_norm": 7.892772197723389,
      "learning_rate": 4.6992700729927005e-05,
      "loss": 2.9837,
      "step": 8240
    },
    {
      "epoch": 60.21897810218978,
      "grad_norm": 9.112010955810547,
      "learning_rate": 4.698905109489051e-05,
      "loss": 2.7244,
      "step": 8250
    },
    {
      "epoch": 60.29197080291971,
      "grad_norm": 8.349325180053711,
      "learning_rate": 4.698540145985402e-05,
      "loss": 2.6624,
      "step": 8260
    },
    {
      "epoch": 60.36496350364963,
      "grad_norm": 9.394890785217285,
      "learning_rate": 4.698175182481752e-05,
      "loss": 2.5129,
      "step": 8270
    },
    {
      "epoch": 60.43795620437956,
      "grad_norm": 9.609156608581543,
      "learning_rate": 4.697810218978103e-05,
      "loss": 2.8211,
      "step": 8280
    },
    {
      "epoch": 60.51094890510949,
      "grad_norm": 8.793676376342773,
      "learning_rate": 4.697445255474453e-05,
      "loss": 2.8699,
      "step": 8290
    },
    {
      "epoch": 60.583941605839414,
      "grad_norm": 7.232791423797607,
      "learning_rate": 4.697080291970803e-05,
      "loss": 2.9645,
      "step": 8300
    },
    {
      "epoch": 60.65693430656934,
      "grad_norm": 7.9908766746521,
      "learning_rate": 4.6967153284671536e-05,
      "loss": 2.3889,
      "step": 8310
    },
    {
      "epoch": 60.72992700729927,
      "grad_norm": 9.698259353637695,
      "learning_rate": 4.6963503649635036e-05,
      "loss": 2.9978,
      "step": 8320
    },
    {
      "epoch": 60.802919708029194,
      "grad_norm": 8.860577583312988,
      "learning_rate": 4.695985401459854e-05,
      "loss": 3.0344,
      "step": 8330
    },
    {
      "epoch": 60.87591240875913,
      "grad_norm": 5.716768741607666,
      "learning_rate": 4.6956204379562044e-05,
      "loss": 2.8991,
      "step": 8340
    },
    {
      "epoch": 60.948905109489054,
      "grad_norm": 8.853567123413086,
      "learning_rate": 4.695255474452555e-05,
      "loss": 2.6418,
      "step": 8350
    },
    {
      "epoch": 61.02189781021898,
      "grad_norm": 6.946231842041016,
      "learning_rate": 4.694890510948906e-05,
      "loss": 3.1194,
      "step": 8360
    },
    {
      "epoch": 61.09489051094891,
      "grad_norm": 8.250520706176758,
      "learning_rate": 4.694525547445256e-05,
      "loss": 2.8662,
      "step": 8370
    },
    {
      "epoch": 61.167883211678834,
      "grad_norm": 8.994917869567871,
      "learning_rate": 4.694160583941606e-05,
      "loss": 2.9904,
      "step": 8380
    },
    {
      "epoch": 61.24087591240876,
      "grad_norm": 8.59715747833252,
      "learning_rate": 4.693795620437956e-05,
      "loss": 2.8215,
      "step": 8390
    },
    {
      "epoch": 61.31386861313869,
      "grad_norm": 8.953641891479492,
      "learning_rate": 4.693430656934307e-05,
      "loss": 2.6428,
      "step": 8400
    },
    {
      "epoch": 61.386861313868614,
      "grad_norm": 7.355389595031738,
      "learning_rate": 4.6930656934306574e-05,
      "loss": 2.6417,
      "step": 8410
    },
    {
      "epoch": 61.45985401459854,
      "grad_norm": 9.190776824951172,
      "learning_rate": 4.6927007299270075e-05,
      "loss": 3.1813,
      "step": 8420
    },
    {
      "epoch": 61.53284671532847,
      "grad_norm": 9.980050086975098,
      "learning_rate": 4.692335766423358e-05,
      "loss": 2.6013,
      "step": 8430
    },
    {
      "epoch": 61.605839416058394,
      "grad_norm": 7.523983955383301,
      "learning_rate": 4.691970802919708e-05,
      "loss": 2.6664,
      "step": 8440
    },
    {
      "epoch": 61.67883211678832,
      "grad_norm": 11.260103225708008,
      "learning_rate": 4.691605839416058e-05,
      "loss": 2.9033,
      "step": 8450
    },
    {
      "epoch": 61.75182481751825,
      "grad_norm": 8.708834648132324,
      "learning_rate": 4.691240875912409e-05,
      "loss": 2.9454,
      "step": 8460
    },
    {
      "epoch": 61.824817518248175,
      "grad_norm": 7.679849147796631,
      "learning_rate": 4.690875912408759e-05,
      "loss": 2.5476,
      "step": 8470
    },
    {
      "epoch": 61.8978102189781,
      "grad_norm": 8.074784278869629,
      "learning_rate": 4.69051094890511e-05,
      "loss": 2.9125,
      "step": 8480
    },
    {
      "epoch": 61.97080291970803,
      "grad_norm": 8.389068603515625,
      "learning_rate": 4.69014598540146e-05,
      "loss": 2.7893,
      "step": 8490
    },
    {
      "epoch": 62.043795620437955,
      "grad_norm": 8.943378448486328,
      "learning_rate": 4.6897810218978106e-05,
      "loss": 2.4483,
      "step": 8500
    },
    {
      "epoch": 62.11678832116788,
      "grad_norm": 8.189798355102539,
      "learning_rate": 4.6894160583941606e-05,
      "loss": 2.471,
      "step": 8510
    },
    {
      "epoch": 62.18978102189781,
      "grad_norm": 8.411944389343262,
      "learning_rate": 4.6890510948905114e-05,
      "loss": 2.873,
      "step": 8520
    },
    {
      "epoch": 62.262773722627735,
      "grad_norm": 6.490377902984619,
      "learning_rate": 4.6886861313868614e-05,
      "loss": 2.9162,
      "step": 8530
    },
    {
      "epoch": 62.33576642335766,
      "grad_norm": 9.668960571289062,
      "learning_rate": 4.6883211678832115e-05,
      "loss": 3.0141,
      "step": 8540
    },
    {
      "epoch": 62.40875912408759,
      "grad_norm": 9.548649787902832,
      "learning_rate": 4.687956204379562e-05,
      "loss": 2.6696,
      "step": 8550
    },
    {
      "epoch": 62.481751824817515,
      "grad_norm": 8.321063041687012,
      "learning_rate": 4.687591240875913e-05,
      "loss": 2.4085,
      "step": 8560
    },
    {
      "epoch": 62.55474452554745,
      "grad_norm": 8.224949836730957,
      "learning_rate": 4.687226277372263e-05,
      "loss": 2.7306,
      "step": 8570
    },
    {
      "epoch": 62.627737226277375,
      "grad_norm": 6.7793073654174805,
      "learning_rate": 4.686861313868614e-05,
      "loss": 2.7359,
      "step": 8580
    },
    {
      "epoch": 62.7007299270073,
      "grad_norm": 9.855649948120117,
      "learning_rate": 4.686496350364964e-05,
      "loss": 2.8018,
      "step": 8590
    },
    {
      "epoch": 62.77372262773723,
      "grad_norm": 7.938188552856445,
      "learning_rate": 4.6861313868613145e-05,
      "loss": 2.7273,
      "step": 8600
    },
    {
      "epoch": 62.846715328467155,
      "grad_norm": 9.900421142578125,
      "learning_rate": 4.6857664233576645e-05,
      "loss": 3.1743,
      "step": 8610
    },
    {
      "epoch": 62.91970802919708,
      "grad_norm": 8.108179092407227,
      "learning_rate": 4.6854014598540146e-05,
      "loss": 2.9895,
      "step": 8620
    },
    {
      "epoch": 62.99270072992701,
      "grad_norm": 8.566247940063477,
      "learning_rate": 4.685036496350365e-05,
      "loss": 2.8191,
      "step": 8630
    },
    {
      "epoch": 63.065693430656935,
      "grad_norm": 8.065897941589355,
      "learning_rate": 4.6846715328467154e-05,
      "loss": 2.6908,
      "step": 8640
    },
    {
      "epoch": 63.13868613138686,
      "grad_norm": 8.559059143066406,
      "learning_rate": 4.684306569343066e-05,
      "loss": 2.9302,
      "step": 8650
    },
    {
      "epoch": 63.21167883211679,
      "grad_norm": 8.663107872009277,
      "learning_rate": 4.683941605839416e-05,
      "loss": 2.6894,
      "step": 8660
    },
    {
      "epoch": 63.284671532846716,
      "grad_norm": 8.652865409851074,
      "learning_rate": 4.683576642335767e-05,
      "loss": 2.773,
      "step": 8670
    },
    {
      "epoch": 63.35766423357664,
      "grad_norm": 8.190279960632324,
      "learning_rate": 4.683211678832117e-05,
      "loss": 2.6977,
      "step": 8680
    },
    {
      "epoch": 63.43065693430657,
      "grad_norm": 10.04646110534668,
      "learning_rate": 4.682846715328467e-05,
      "loss": 2.3093,
      "step": 8690
    },
    {
      "epoch": 63.503649635036496,
      "grad_norm": 10.959410667419434,
      "learning_rate": 4.682481751824818e-05,
      "loss": 2.7788,
      "step": 8700
    },
    {
      "epoch": 63.57664233576642,
      "grad_norm": 10.175189018249512,
      "learning_rate": 4.682116788321168e-05,
      "loss": 3.078,
      "step": 8710
    },
    {
      "epoch": 63.64963503649635,
      "grad_norm": 8.358887672424316,
      "learning_rate": 4.6817518248175185e-05,
      "loss": 2.4771,
      "step": 8720
    },
    {
      "epoch": 63.722627737226276,
      "grad_norm": 7.732770919799805,
      "learning_rate": 4.681386861313869e-05,
      "loss": 2.5291,
      "step": 8730
    },
    {
      "epoch": 63.7956204379562,
      "grad_norm": 8.425557136535645,
      "learning_rate": 4.681021897810219e-05,
      "loss": 2.4089,
      "step": 8740
    },
    {
      "epoch": 63.86861313868613,
      "grad_norm": 7.5812530517578125,
      "learning_rate": 4.68065693430657e-05,
      "loss": 2.6756,
      "step": 8750
    },
    {
      "epoch": 63.941605839416056,
      "grad_norm": 8.398409843444824,
      "learning_rate": 4.68029197080292e-05,
      "loss": 2.8762,
      "step": 8760
    },
    {
      "epoch": 64.01459854014598,
      "grad_norm": 8.62266731262207,
      "learning_rate": 4.67992700729927e-05,
      "loss": 2.9835,
      "step": 8770
    },
    {
      "epoch": 64.08759124087591,
      "grad_norm": 9.050455093383789,
      "learning_rate": 4.679562043795621e-05,
      "loss": 2.5719,
      "step": 8780
    },
    {
      "epoch": 64.16058394160584,
      "grad_norm": 8.370992660522461,
      "learning_rate": 4.679197080291971e-05,
      "loss": 2.9627,
      "step": 8790
    },
    {
      "epoch": 64.23357664233576,
      "grad_norm": 9.38135814666748,
      "learning_rate": 4.6788321167883216e-05,
      "loss": 2.6435,
      "step": 8800
    },
    {
      "epoch": 64.30656934306569,
      "grad_norm": 8.828802108764648,
      "learning_rate": 4.6784671532846716e-05,
      "loss": 2.5475,
      "step": 8810
    },
    {
      "epoch": 64.37956204379562,
      "grad_norm": 8.536206245422363,
      "learning_rate": 4.6781021897810223e-05,
      "loss": 2.7177,
      "step": 8820
    },
    {
      "epoch": 64.45255474452554,
      "grad_norm": 8.531415939331055,
      "learning_rate": 4.677737226277373e-05,
      "loss": 2.5847,
      "step": 8830
    },
    {
      "epoch": 64.52554744525547,
      "grad_norm": 8.072807312011719,
      "learning_rate": 4.6773722627737224e-05,
      "loss": 3.0188,
      "step": 8840
    },
    {
      "epoch": 64.5985401459854,
      "grad_norm": 8.683338165283203,
      "learning_rate": 4.677007299270073e-05,
      "loss": 2.7098,
      "step": 8850
    },
    {
      "epoch": 64.67153284671532,
      "grad_norm": 8.879463195800781,
      "learning_rate": 4.676642335766423e-05,
      "loss": 3.0845,
      "step": 8860
    },
    {
      "epoch": 64.74452554744525,
      "grad_norm": 7.10611629486084,
      "learning_rate": 4.676277372262774e-05,
      "loss": 2.5758,
      "step": 8870
    },
    {
      "epoch": 64.81751824817518,
      "grad_norm": 10.015308380126953,
      "learning_rate": 4.675912408759125e-05,
      "loss": 3.0431,
      "step": 8880
    },
    {
      "epoch": 64.8905109489051,
      "grad_norm": 8.436310768127441,
      "learning_rate": 4.675547445255475e-05,
      "loss": 2.2569,
      "step": 8890
    },
    {
      "epoch": 64.96350364963503,
      "grad_norm": 7.830694198608398,
      "learning_rate": 4.6751824817518254e-05,
      "loss": 2.3973,
      "step": 8900
    },
    {
      "epoch": 65.03649635036497,
      "grad_norm": 10.44150447845459,
      "learning_rate": 4.674817518248175e-05,
      "loss": 2.5493,
      "step": 8910
    },
    {
      "epoch": 65.1094890510949,
      "grad_norm": 7.817025661468506,
      "learning_rate": 4.6744525547445255e-05,
      "loss": 2.9252,
      "step": 8920
    },
    {
      "epoch": 65.18248175182482,
      "grad_norm": 9.465065002441406,
      "learning_rate": 4.674087591240876e-05,
      "loss": 2.7131,
      "step": 8930
    },
    {
      "epoch": 65.25547445255475,
      "grad_norm": 8.466678619384766,
      "learning_rate": 4.673722627737226e-05,
      "loss": 2.689,
      "step": 8940
    },
    {
      "epoch": 65.32846715328468,
      "grad_norm": 9.093271255493164,
      "learning_rate": 4.673357664233577e-05,
      "loss": 2.5382,
      "step": 8950
    },
    {
      "epoch": 65.4014598540146,
      "grad_norm": 7.665717124938965,
      "learning_rate": 4.672992700729927e-05,
      "loss": 2.4015,
      "step": 8960
    },
    {
      "epoch": 65.47445255474453,
      "grad_norm": 7.891382217407227,
      "learning_rate": 4.672627737226278e-05,
      "loss": 2.721,
      "step": 8970
    },
    {
      "epoch": 65.54744525547446,
      "grad_norm": 8.00580883026123,
      "learning_rate": 4.6722627737226286e-05,
      "loss": 2.7995,
      "step": 8980
    },
    {
      "epoch": 65.62043795620438,
      "grad_norm": 8.87135124206543,
      "learning_rate": 4.671897810218978e-05,
      "loss": 2.6694,
      "step": 8990
    },
    {
      "epoch": 65.69343065693431,
      "grad_norm": 7.6214094161987305,
      "learning_rate": 4.6715328467153287e-05,
      "loss": 2.588,
      "step": 9000
    },
    {
      "epoch": 65.76642335766424,
      "grad_norm": 9.055577278137207,
      "learning_rate": 4.671167883211679e-05,
      "loss": 2.6976,
      "step": 9010
    },
    {
      "epoch": 65.83941605839416,
      "grad_norm": 8.542374610900879,
      "learning_rate": 4.6708029197080294e-05,
      "loss": 2.8828,
      "step": 9020
    },
    {
      "epoch": 65.91240875912409,
      "grad_norm": 8.318772315979004,
      "learning_rate": 4.67043795620438e-05,
      "loss": 2.7692,
      "step": 9030
    },
    {
      "epoch": 65.98540145985402,
      "grad_norm": 8.755783081054688,
      "learning_rate": 4.67007299270073e-05,
      "loss": 2.4044,
      "step": 9040
    },
    {
      "epoch": 66.05839416058394,
      "grad_norm": 8.058103561401367,
      "learning_rate": 4.669708029197081e-05,
      "loss": 2.2941,
      "step": 9050
    },
    {
      "epoch": 66.13138686131387,
      "grad_norm": 7.288005828857422,
      "learning_rate": 4.66934306569343e-05,
      "loss": 2.8319,
      "step": 9060
    },
    {
      "epoch": 66.2043795620438,
      "grad_norm": 8.71182918548584,
      "learning_rate": 4.668978102189781e-05,
      "loss": 2.5561,
      "step": 9070
    },
    {
      "epoch": 66.27737226277372,
      "grad_norm": 8.349370002746582,
      "learning_rate": 4.668613138686132e-05,
      "loss": 2.7625,
      "step": 9080
    },
    {
      "epoch": 66.35036496350365,
      "grad_norm": 9.389342308044434,
      "learning_rate": 4.668248175182482e-05,
      "loss": 2.6712,
      "step": 9090
    },
    {
      "epoch": 66.42335766423358,
      "grad_norm": 10.287125587463379,
      "learning_rate": 4.6678832116788325e-05,
      "loss": 2.9278,
      "step": 9100
    },
    {
      "epoch": 66.4963503649635,
      "grad_norm": 9.528912544250488,
      "learning_rate": 4.6675182481751826e-05,
      "loss": 2.6767,
      "step": 9110
    },
    {
      "epoch": 66.56934306569343,
      "grad_norm": 8.065231323242188,
      "learning_rate": 4.667153284671533e-05,
      "loss": 2.4569,
      "step": 9120
    },
    {
      "epoch": 66.64233576642336,
      "grad_norm": 8.72672176361084,
      "learning_rate": 4.6667883211678834e-05,
      "loss": 2.7251,
      "step": 9130
    },
    {
      "epoch": 66.71532846715328,
      "grad_norm": 8.404200553894043,
      "learning_rate": 4.6664233576642334e-05,
      "loss": 2.3766,
      "step": 9140
    },
    {
      "epoch": 66.78832116788321,
      "grad_norm": 8.418856620788574,
      "learning_rate": 4.666058394160584e-05,
      "loss": 2.5966,
      "step": 9150
    },
    {
      "epoch": 66.86131386861314,
      "grad_norm": 9.520618438720703,
      "learning_rate": 4.665693430656934e-05,
      "loss": 2.7477,
      "step": 9160
    },
    {
      "epoch": 66.93430656934306,
      "grad_norm": 8.801376342773438,
      "learning_rate": 4.665328467153285e-05,
      "loss": 2.6904,
      "step": 9170
    },
    {
      "epoch": 67.00729927007299,
      "grad_norm": 9.455183982849121,
      "learning_rate": 4.6649635036496356e-05,
      "loss": 2.7144,
      "step": 9180
    },
    {
      "epoch": 67.08029197080292,
      "grad_norm": 7.718023300170898,
      "learning_rate": 4.664598540145986e-05,
      "loss": 2.5738,
      "step": 9190
    },
    {
      "epoch": 67.15328467153284,
      "grad_norm": 9.15173625946045,
      "learning_rate": 4.6642335766423364e-05,
      "loss": 2.5203,
      "step": 9200
    },
    {
      "epoch": 67.22627737226277,
      "grad_norm": 8.221242904663086,
      "learning_rate": 4.6638686131386865e-05,
      "loss": 2.5858,
      "step": 9210
    },
    {
      "epoch": 67.2992700729927,
      "grad_norm": 10.113749504089355,
      "learning_rate": 4.6635036496350365e-05,
      "loss": 2.8332,
      "step": 9220
    },
    {
      "epoch": 67.37226277372262,
      "grad_norm": 8.421930313110352,
      "learning_rate": 4.663138686131387e-05,
      "loss": 2.6608,
      "step": 9230
    },
    {
      "epoch": 67.44525547445255,
      "grad_norm": 6.653226852416992,
      "learning_rate": 4.662773722627737e-05,
      "loss": 2.631,
      "step": 9240
    },
    {
      "epoch": 67.51824817518248,
      "grad_norm": 8.886222839355469,
      "learning_rate": 4.662408759124088e-05,
      "loss": 2.5884,
      "step": 9250
    },
    {
      "epoch": 67.5912408759124,
      "grad_norm": 10.102092742919922,
      "learning_rate": 4.662043795620438e-05,
      "loss": 2.5662,
      "step": 9260
    },
    {
      "epoch": 67.66423357664233,
      "grad_norm": 8.313273429870605,
      "learning_rate": 4.661678832116789e-05,
      "loss": 2.5264,
      "step": 9270
    },
    {
      "epoch": 67.73722627737226,
      "grad_norm": 9.10920524597168,
      "learning_rate": 4.661313868613139e-05,
      "loss": 2.8695,
      "step": 9280
    },
    {
      "epoch": 67.81021897810218,
      "grad_norm": 9.915353775024414,
      "learning_rate": 4.660948905109489e-05,
      "loss": 2.8,
      "step": 9290
    },
    {
      "epoch": 67.88321167883211,
      "grad_norm": 7.940946578979492,
      "learning_rate": 4.6605839416058396e-05,
      "loss": 2.2731,
      "step": 9300
    },
    {
      "epoch": 67.95620437956204,
      "grad_norm": 9.76718521118164,
      "learning_rate": 4.66021897810219e-05,
      "loss": 2.5711,
      "step": 9310
    },
    {
      "epoch": 68.02919708029196,
      "grad_norm": 7.950852870941162,
      "learning_rate": 4.6598540145985404e-05,
      "loss": 2.797,
      "step": 9320
    },
    {
      "epoch": 68.10218978102189,
      "grad_norm": 8.521410942077637,
      "learning_rate": 4.6594890510948904e-05,
      "loss": 2.806,
      "step": 9330
    },
    {
      "epoch": 68.17518248175182,
      "grad_norm": 8.403189659118652,
      "learning_rate": 4.659124087591241e-05,
      "loss": 2.3593,
      "step": 9340
    },
    {
      "epoch": 68.24817518248175,
      "grad_norm": 8.29212474822998,
      "learning_rate": 4.658759124087592e-05,
      "loss": 2.4267,
      "step": 9350
    },
    {
      "epoch": 68.32116788321167,
      "grad_norm": 9.971334457397461,
      "learning_rate": 4.658394160583942e-05,
      "loss": 2.4592,
      "step": 9360
    },
    {
      "epoch": 68.39416058394161,
      "grad_norm": 9.368919372558594,
      "learning_rate": 4.658029197080292e-05,
      "loss": 2.6759,
      "step": 9370
    },
    {
      "epoch": 68.46715328467154,
      "grad_norm": 8.168248176574707,
      "learning_rate": 4.657664233576643e-05,
      "loss": 2.8163,
      "step": 9380
    },
    {
      "epoch": 68.54014598540147,
      "grad_norm": 9.410228729248047,
      "learning_rate": 4.657299270072993e-05,
      "loss": 2.7689,
      "step": 9390
    },
    {
      "epoch": 68.61313868613139,
      "grad_norm": 8.44469165802002,
      "learning_rate": 4.6569343065693435e-05,
      "loss": 2.7975,
      "step": 9400
    },
    {
      "epoch": 68.68613138686132,
      "grad_norm": 7.655986309051514,
      "learning_rate": 4.6565693430656936e-05,
      "loss": 2.3195,
      "step": 9410
    },
    {
      "epoch": 68.75912408759125,
      "grad_norm": 5.819263458251953,
      "learning_rate": 4.656204379562044e-05,
      "loss": 2.0969,
      "step": 9420
    },
    {
      "epoch": 68.83211678832117,
      "grad_norm": 6.6874189376831055,
      "learning_rate": 4.655839416058394e-05,
      "loss": 2.6084,
      "step": 9430
    },
    {
      "epoch": 68.9051094890511,
      "grad_norm": 8.062403678894043,
      "learning_rate": 4.655474452554745e-05,
      "loss": 2.6403,
      "step": 9440
    },
    {
      "epoch": 68.97810218978103,
      "grad_norm": 8.285534858703613,
      "learning_rate": 4.655109489051095e-05,
      "loss": 2.2746,
      "step": 9450
    },
    {
      "epoch": 69.05109489051095,
      "grad_norm": 9.416319847106934,
      "learning_rate": 4.654744525547445e-05,
      "loss": 2.6501,
      "step": 9460
    },
    {
      "epoch": 69.12408759124088,
      "grad_norm": 7.80089807510376,
      "learning_rate": 4.654379562043796e-05,
      "loss": 2.2259,
      "step": 9470
    },
    {
      "epoch": 69.1970802919708,
      "grad_norm": 9.094663619995117,
      "learning_rate": 4.654014598540146e-05,
      "loss": 2.5138,
      "step": 9480
    },
    {
      "epoch": 69.27007299270073,
      "grad_norm": 8.17952823638916,
      "learning_rate": 4.6536496350364967e-05,
      "loss": 2.711,
      "step": 9490
    },
    {
      "epoch": 69.34306569343066,
      "grad_norm": 6.862319469451904,
      "learning_rate": 4.6532846715328474e-05,
      "loss": 2.3728,
      "step": 9500
    },
    {
      "epoch": 69.41605839416059,
      "grad_norm": 8.877115249633789,
      "learning_rate": 4.6529197080291974e-05,
      "loss": 2.5612,
      "step": 9510
    },
    {
      "epoch": 69.48905109489051,
      "grad_norm": 9.454279899597168,
      "learning_rate": 4.6525547445255475e-05,
      "loss": 2.5154,
      "step": 9520
    },
    {
      "epoch": 69.56204379562044,
      "grad_norm": 9.113628387451172,
      "learning_rate": 4.6521897810218975e-05,
      "loss": 3.17,
      "step": 9530
    },
    {
      "epoch": 69.63503649635037,
      "grad_norm": 9.028993606567383,
      "learning_rate": 4.651824817518248e-05,
      "loss": 2.6785,
      "step": 9540
    },
    {
      "epoch": 69.7080291970803,
      "grad_norm": 8.32353401184082,
      "learning_rate": 4.651459854014599e-05,
      "loss": 2.4674,
      "step": 9550
    },
    {
      "epoch": 69.78102189781022,
      "grad_norm": 7.714119911193848,
      "learning_rate": 4.651094890510949e-05,
      "loss": 2.532,
      "step": 9560
    },
    {
      "epoch": 69.85401459854015,
      "grad_norm": 8.47606086730957,
      "learning_rate": 4.6507299270073e-05,
      "loss": 2.6316,
      "step": 9570
    },
    {
      "epoch": 69.92700729927007,
      "grad_norm": 8.280865669250488,
      "learning_rate": 4.65036496350365e-05,
      "loss": 2.5307,
      "step": 9580
    },
    {
      "epoch": 70.0,
      "grad_norm": 14.285330772399902,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 2.3854,
      "step": 9590
    },
    {
      "epoch": 70.07299270072993,
      "grad_norm": 8.699234962463379,
      "learning_rate": 4.6496350364963506e-05,
      "loss": 2.5269,
      "step": 9600
    },
    {
      "epoch": 70.14598540145985,
      "grad_norm": 8.436446189880371,
      "learning_rate": 4.6492700729927006e-05,
      "loss": 2.6866,
      "step": 9610
    },
    {
      "epoch": 70.21897810218978,
      "grad_norm": 7.5097575187683105,
      "learning_rate": 4.6489051094890514e-05,
      "loss": 2.1848,
      "step": 9620
    },
    {
      "epoch": 70.2919708029197,
      "grad_norm": 7.732736587524414,
      "learning_rate": 4.6485401459854014e-05,
      "loss": 2.4756,
      "step": 9630
    },
    {
      "epoch": 70.36496350364963,
      "grad_norm": 6.661638259887695,
      "learning_rate": 4.648175182481752e-05,
      "loss": 2.4013,
      "step": 9640
    },
    {
      "epoch": 70.43795620437956,
      "grad_norm": 8.948593139648438,
      "learning_rate": 4.647810218978103e-05,
      "loss": 2.6006,
      "step": 9650
    },
    {
      "epoch": 70.51094890510949,
      "grad_norm": 10.963958740234375,
      "learning_rate": 4.647445255474453e-05,
      "loss": 2.6427,
      "step": 9660
    },
    {
      "epoch": 70.58394160583941,
      "grad_norm": 9.39780044555664,
      "learning_rate": 4.6470802919708036e-05,
      "loss": 2.2017,
      "step": 9670
    },
    {
      "epoch": 70.65693430656934,
      "grad_norm": 8.250306129455566,
      "learning_rate": 4.646715328467153e-05,
      "loss": 2.7096,
      "step": 9680
    },
    {
      "epoch": 70.72992700729927,
      "grad_norm": 8.364460945129395,
      "learning_rate": 4.646350364963504e-05,
      "loss": 2.6419,
      "step": 9690
    },
    {
      "epoch": 70.8029197080292,
      "grad_norm": 7.851487636566162,
      "learning_rate": 4.6459854014598545e-05,
      "loss": 2.838,
      "step": 9700
    },
    {
      "epoch": 70.87591240875912,
      "grad_norm": 6.700494766235352,
      "learning_rate": 4.6456204379562045e-05,
      "loss": 2.4826,
      "step": 9710
    },
    {
      "epoch": 70.94890510948905,
      "grad_norm": 7.914392471313477,
      "learning_rate": 4.645255474452555e-05,
      "loss": 2.7925,
      "step": 9720
    },
    {
      "epoch": 71.02189781021897,
      "grad_norm": 9.173160552978516,
      "learning_rate": 4.644890510948905e-05,
      "loss": 2.5689,
      "step": 9730
    },
    {
      "epoch": 71.0948905109489,
      "grad_norm": 9.348103523254395,
      "learning_rate": 4.644525547445256e-05,
      "loss": 2.7323,
      "step": 9740
    },
    {
      "epoch": 71.16788321167883,
      "grad_norm": 9.080575942993164,
      "learning_rate": 4.644160583941606e-05,
      "loss": 2.7482,
      "step": 9750
    },
    {
      "epoch": 71.24087591240875,
      "grad_norm": 9.286197662353516,
      "learning_rate": 4.643795620437956e-05,
      "loss": 2.8211,
      "step": 9760
    },
    {
      "epoch": 71.31386861313868,
      "grad_norm": 9.333728790283203,
      "learning_rate": 4.643430656934307e-05,
      "loss": 2.3504,
      "step": 9770
    },
    {
      "epoch": 71.38686131386861,
      "grad_norm": 9.451054573059082,
      "learning_rate": 4.643065693430657e-05,
      "loss": 2.4909,
      "step": 9780
    },
    {
      "epoch": 71.45985401459853,
      "grad_norm": 8.915358543395996,
      "learning_rate": 4.6427007299270076e-05,
      "loss": 2.4373,
      "step": 9790
    },
    {
      "epoch": 71.53284671532846,
      "grad_norm": 7.645786285400391,
      "learning_rate": 4.642335766423358e-05,
      "loss": 2.3769,
      "step": 9800
    },
    {
      "epoch": 71.60583941605839,
      "grad_norm": 12.280701637268066,
      "learning_rate": 4.6419708029197084e-05,
      "loss": 2.6829,
      "step": 9810
    },
    {
      "epoch": 71.67883211678833,
      "grad_norm": 8.92786979675293,
      "learning_rate": 4.641605839416059e-05,
      "loss": 2.6343,
      "step": 9820
    },
    {
      "epoch": 71.75182481751825,
      "grad_norm": 9.021984100341797,
      "learning_rate": 4.6412408759124085e-05,
      "loss": 2.4728,
      "step": 9830
    },
    {
      "epoch": 71.82481751824818,
      "grad_norm": 9.133907318115234,
      "learning_rate": 4.640875912408759e-05,
      "loss": 2.3227,
      "step": 9840
    },
    {
      "epoch": 71.89781021897811,
      "grad_norm": 9.050518989562988,
      "learning_rate": 4.64051094890511e-05,
      "loss": 2.3814,
      "step": 9850
    },
    {
      "epoch": 71.97080291970804,
      "grad_norm": 9.088743209838867,
      "learning_rate": 4.64014598540146e-05,
      "loss": 2.2475,
      "step": 9860
    },
    {
      "epoch": 72.04379562043796,
      "grad_norm": 9.039204597473145,
      "learning_rate": 4.639781021897811e-05,
      "loss": 2.5598,
      "step": 9870
    },
    {
      "epoch": 72.11678832116789,
      "grad_norm": 6.831589698791504,
      "learning_rate": 4.639416058394161e-05,
      "loss": 2.3786,
      "step": 9880
    },
    {
      "epoch": 72.18978102189782,
      "grad_norm": 6.698596477508545,
      "learning_rate": 4.6390510948905115e-05,
      "loss": 2.5286,
      "step": 9890
    },
    {
      "epoch": 72.26277372262774,
      "grad_norm": 9.482604026794434,
      "learning_rate": 4.6386861313868616e-05,
      "loss": 2.7529,
      "step": 9900
    },
    {
      "epoch": 72.33576642335767,
      "grad_norm": 10.580243110656738,
      "learning_rate": 4.6383211678832116e-05,
      "loss": 2.5823,
      "step": 9910
    },
    {
      "epoch": 72.4087591240876,
      "grad_norm": 7.5115814208984375,
      "learning_rate": 4.637956204379562e-05,
      "loss": 2.743,
      "step": 9920
    },
    {
      "epoch": 72.48175182481752,
      "grad_norm": 7.216279983520508,
      "learning_rate": 4.6375912408759124e-05,
      "loss": 2.0876,
      "step": 9930
    },
    {
      "epoch": 72.55474452554745,
      "grad_norm": 8.095937728881836,
      "learning_rate": 4.637226277372263e-05,
      "loss": 2.4509,
      "step": 9940
    },
    {
      "epoch": 72.62773722627738,
      "grad_norm": 6.506322860717773,
      "learning_rate": 4.636861313868613e-05,
      "loss": 2.2997,
      "step": 9950
    },
    {
      "epoch": 72.7007299270073,
      "grad_norm": 8.143203735351562,
      "learning_rate": 4.636496350364964e-05,
      "loss": 2.2166,
      "step": 9960
    },
    {
      "epoch": 72.77372262773723,
      "grad_norm": 9.014776229858398,
      "learning_rate": 4.6361313868613146e-05,
      "loss": 2.6232,
      "step": 9970
    },
    {
      "epoch": 72.84671532846716,
      "grad_norm": 8.833806991577148,
      "learning_rate": 4.635766423357664e-05,
      "loss": 2.2262,
      "step": 9980
    },
    {
      "epoch": 72.91970802919708,
      "grad_norm": 8.676372528076172,
      "learning_rate": 4.635401459854015e-05,
      "loss": 2.6835,
      "step": 9990
    },
    {
      "epoch": 72.99270072992701,
      "grad_norm": 8.503900527954102,
      "learning_rate": 4.635036496350365e-05,
      "loss": 2.6441,
      "step": 10000
    },
    {
      "epoch": 73.06569343065694,
      "grad_norm": 8.06814193725586,
      "learning_rate": 4.6346715328467155e-05,
      "loss": 2.2676,
      "step": 10010
    },
    {
      "epoch": 73.13868613138686,
      "grad_norm": 9.507509231567383,
      "learning_rate": 4.634306569343066e-05,
      "loss": 2.3914,
      "step": 10020
    },
    {
      "epoch": 73.21167883211679,
      "grad_norm": 7.419965744018555,
      "learning_rate": 4.633941605839416e-05,
      "loss": 2.5734,
      "step": 10030
    },
    {
      "epoch": 73.28467153284672,
      "grad_norm": 9.750085830688477,
      "learning_rate": 4.633576642335767e-05,
      "loss": 2.2096,
      "step": 10040
    },
    {
      "epoch": 73.35766423357664,
      "grad_norm": 10.293025970458984,
      "learning_rate": 4.633211678832117e-05,
      "loss": 2.4686,
      "step": 10050
    },
    {
      "epoch": 73.43065693430657,
      "grad_norm": 10.295361518859863,
      "learning_rate": 4.632846715328467e-05,
      "loss": 2.4623,
      "step": 10060
    },
    {
      "epoch": 73.5036496350365,
      "grad_norm": 8.31441593170166,
      "learning_rate": 4.632481751824818e-05,
      "loss": 2.6754,
      "step": 10070
    },
    {
      "epoch": 73.57664233576642,
      "grad_norm": 9.061057090759277,
      "learning_rate": 4.632116788321168e-05,
      "loss": 2.4102,
      "step": 10080
    },
    {
      "epoch": 73.64963503649635,
      "grad_norm": 8.02424144744873,
      "learning_rate": 4.6317518248175186e-05,
      "loss": 2.41,
      "step": 10090
    },
    {
      "epoch": 73.72262773722628,
      "grad_norm": 8.623151779174805,
      "learning_rate": 4.6313868613138686e-05,
      "loss": 2.4775,
      "step": 10100
    },
    {
      "epoch": 73.7956204379562,
      "grad_norm": 8.549969673156738,
      "learning_rate": 4.6310218978102194e-05,
      "loss": 2.2954,
      "step": 10110
    },
    {
      "epoch": 73.86861313868613,
      "grad_norm": 8.939152717590332,
      "learning_rate": 4.63065693430657e-05,
      "loss": 2.4469,
      "step": 10120
    },
    {
      "epoch": 73.94160583941606,
      "grad_norm": 8.341839790344238,
      "learning_rate": 4.6302919708029195e-05,
      "loss": 2.8351,
      "step": 10130
    },
    {
      "epoch": 74.01459854014598,
      "grad_norm": 8.61984920501709,
      "learning_rate": 4.62992700729927e-05,
      "loss": 2.392,
      "step": 10140
    },
    {
      "epoch": 74.08759124087591,
      "grad_norm": 7.427525520324707,
      "learning_rate": 4.62956204379562e-05,
      "loss": 2.6109,
      "step": 10150
    },
    {
      "epoch": 74.16058394160584,
      "grad_norm": 7.982034683227539,
      "learning_rate": 4.629197080291971e-05,
      "loss": 1.9693,
      "step": 10160
    },
    {
      "epoch": 74.23357664233576,
      "grad_norm": 7.3623433113098145,
      "learning_rate": 4.628832116788322e-05,
      "loss": 2.6749,
      "step": 10170
    },
    {
      "epoch": 74.30656934306569,
      "grad_norm": 8.267638206481934,
      "learning_rate": 4.628467153284672e-05,
      "loss": 2.6385,
      "step": 10180
    },
    {
      "epoch": 74.37956204379562,
      "grad_norm": 8.074139595031738,
      "learning_rate": 4.6281021897810225e-05,
      "loss": 2.503,
      "step": 10190
    },
    {
      "epoch": 74.45255474452554,
      "grad_norm": 7.954578399658203,
      "learning_rate": 4.6277372262773725e-05,
      "loss": 2.6171,
      "step": 10200
    },
    {
      "epoch": 74.52554744525547,
      "grad_norm": 8.696812629699707,
      "learning_rate": 4.6273722627737226e-05,
      "loss": 2.458,
      "step": 10210
    },
    {
      "epoch": 74.5985401459854,
      "grad_norm": 7.6016998291015625,
      "learning_rate": 4.627007299270073e-05,
      "loss": 2.5748,
      "step": 10220
    },
    {
      "epoch": 74.67153284671532,
      "grad_norm": 9.06789779663086,
      "learning_rate": 4.6266423357664234e-05,
      "loss": 2.4848,
      "step": 10230
    },
    {
      "epoch": 74.74452554744525,
      "grad_norm": 9.127803802490234,
      "learning_rate": 4.626277372262774e-05,
      "loss": 2.4284,
      "step": 10240
    },
    {
      "epoch": 74.81751824817518,
      "grad_norm": 8.216216087341309,
      "learning_rate": 4.625912408759124e-05,
      "loss": 2.4765,
      "step": 10250
    },
    {
      "epoch": 74.8905109489051,
      "grad_norm": 7.674044609069824,
      "learning_rate": 4.625547445255475e-05,
      "loss": 2.4371,
      "step": 10260
    },
    {
      "epoch": 74.96350364963503,
      "grad_norm": 8.32175064086914,
      "learning_rate": 4.625182481751825e-05,
      "loss": 2.0415,
      "step": 10270
    },
    {
      "epoch": 75.03649635036497,
      "grad_norm": 8.853251457214355,
      "learning_rate": 4.6248175182481756e-05,
      "loss": 2.4935,
      "step": 10280
    },
    {
      "epoch": 75.1094890510949,
      "grad_norm": 9.131630897521973,
      "learning_rate": 4.624452554744526e-05,
      "loss": 2.4056,
      "step": 10290
    },
    {
      "epoch": 75.18248175182482,
      "grad_norm": 8.375635147094727,
      "learning_rate": 4.624087591240876e-05,
      "loss": 2.6211,
      "step": 10300
    },
    {
      "epoch": 75.25547445255475,
      "grad_norm": 8.137763977050781,
      "learning_rate": 4.6237226277372265e-05,
      "loss": 2.5073,
      "step": 10310
    },
    {
      "epoch": 75.32846715328468,
      "grad_norm": 9.831011772155762,
      "learning_rate": 4.623357664233577e-05,
      "loss": 2.1714,
      "step": 10320
    },
    {
      "epoch": 75.4014598540146,
      "grad_norm": 7.971724987030029,
      "learning_rate": 4.622992700729927e-05,
      "loss": 2.4983,
      "step": 10330
    },
    {
      "epoch": 75.47445255474453,
      "grad_norm": 9.307472229003906,
      "learning_rate": 4.622627737226278e-05,
      "loss": 2.0948,
      "step": 10340
    },
    {
      "epoch": 75.54744525547446,
      "grad_norm": 9.454017639160156,
      "learning_rate": 4.622262773722628e-05,
      "loss": 2.6405,
      "step": 10350
    },
    {
      "epoch": 75.62043795620438,
      "grad_norm": 6.9677042961120605,
      "learning_rate": 4.621897810218978e-05,
      "loss": 2.0913,
      "step": 10360
    },
    {
      "epoch": 75.69343065693431,
      "grad_norm": 8.114607810974121,
      "learning_rate": 4.621532846715329e-05,
      "loss": 2.3099,
      "step": 10370
    },
    {
      "epoch": 75.76642335766424,
      "grad_norm": 9.126616477966309,
      "learning_rate": 4.621167883211679e-05,
      "loss": 2.6148,
      "step": 10380
    },
    {
      "epoch": 75.83941605839416,
      "grad_norm": 8.11962604522705,
      "learning_rate": 4.6208029197080296e-05,
      "loss": 2.5161,
      "step": 10390
    },
    {
      "epoch": 75.91240875912409,
      "grad_norm": 8.224739074707031,
      "learning_rate": 4.6204379562043796e-05,
      "loss": 2.4027,
      "step": 10400
    },
    {
      "epoch": 75.98540145985402,
      "grad_norm": 9.099801063537598,
      "learning_rate": 4.6200729927007303e-05,
      "loss": 2.4564,
      "step": 10410
    },
    {
      "epoch": 76.05839416058394,
      "grad_norm": 7.776366233825684,
      "learning_rate": 4.6197080291970804e-05,
      "loss": 2.3974,
      "step": 10420
    },
    {
      "epoch": 76.13138686131387,
      "grad_norm": 9.815591812133789,
      "learning_rate": 4.619343065693431e-05,
      "loss": 2.4978,
      "step": 10430
    },
    {
      "epoch": 76.2043795620438,
      "grad_norm": 8.432649612426758,
      "learning_rate": 4.618978102189781e-05,
      "loss": 2.5568,
      "step": 10440
    },
    {
      "epoch": 76.27737226277372,
      "grad_norm": 8.143864631652832,
      "learning_rate": 4.618613138686131e-05,
      "loss": 2.316,
      "step": 10450
    },
    {
      "epoch": 76.35036496350365,
      "grad_norm": 9.134722709655762,
      "learning_rate": 4.618248175182482e-05,
      "loss": 2.3984,
      "step": 10460
    },
    {
      "epoch": 76.42335766423358,
      "grad_norm": 9.241286277770996,
      "learning_rate": 4.617883211678832e-05,
      "loss": 2.0747,
      "step": 10470
    },
    {
      "epoch": 76.4963503649635,
      "grad_norm": 8.341778755187988,
      "learning_rate": 4.617518248175183e-05,
      "loss": 2.5322,
      "step": 10480
    },
    {
      "epoch": 76.56934306569343,
      "grad_norm": 7.793670177459717,
      "learning_rate": 4.6171532846715335e-05,
      "loss": 2.2439,
      "step": 10490
    },
    {
      "epoch": 76.64233576642336,
      "grad_norm": 8.35032844543457,
      "learning_rate": 4.6167883211678835e-05,
      "loss": 2.3175,
      "step": 10500
    },
    {
      "epoch": 76.71532846715328,
      "grad_norm": 8.800687789916992,
      "learning_rate": 4.616423357664234e-05,
      "loss": 2.6112,
      "step": 10510
    },
    {
      "epoch": 76.78832116788321,
      "grad_norm": 9.363505363464355,
      "learning_rate": 4.616058394160584e-05,
      "loss": 2.6673,
      "step": 10520
    },
    {
      "epoch": 76.86131386861314,
      "grad_norm": 10.116491317749023,
      "learning_rate": 4.615693430656934e-05,
      "loss": 2.4788,
      "step": 10530
    },
    {
      "epoch": 76.93430656934306,
      "grad_norm": 8.773386001586914,
      "learning_rate": 4.615328467153285e-05,
      "loss": 2.2844,
      "step": 10540
    },
    {
      "epoch": 77.00729927007299,
      "grad_norm": 8.591864585876465,
      "learning_rate": 4.614963503649635e-05,
      "loss": 2.2488,
      "step": 10550
    },
    {
      "epoch": 77.08029197080292,
      "grad_norm": 7.285348892211914,
      "learning_rate": 4.614598540145986e-05,
      "loss": 2.377,
      "step": 10560
    },
    {
      "epoch": 77.15328467153284,
      "grad_norm": 7.928030967712402,
      "learning_rate": 4.614233576642336e-05,
      "loss": 2.5147,
      "step": 10570
    },
    {
      "epoch": 77.22627737226277,
      "grad_norm": 6.703647613525391,
      "learning_rate": 4.6138686131386866e-05,
      "loss": 2.0724,
      "step": 10580
    },
    {
      "epoch": 77.2992700729927,
      "grad_norm": 7.3111958503723145,
      "learning_rate": 4.6135036496350367e-05,
      "loss": 2.3896,
      "step": 10590
    },
    {
      "epoch": 77.37226277372262,
      "grad_norm": 8.701776504516602,
      "learning_rate": 4.613138686131387e-05,
      "loss": 2.8411,
      "step": 10600
    },
    {
      "epoch": 77.44525547445255,
      "grad_norm": 9.613645553588867,
      "learning_rate": 4.6127737226277374e-05,
      "loss": 2.6596,
      "step": 10610
    },
    {
      "epoch": 77.51824817518248,
      "grad_norm": 7.179610252380371,
      "learning_rate": 4.6124087591240875e-05,
      "loss": 2.2699,
      "step": 10620
    },
    {
      "epoch": 77.5912408759124,
      "grad_norm": 7.644958019256592,
      "learning_rate": 4.612043795620438e-05,
      "loss": 1.884,
      "step": 10630
    },
    {
      "epoch": 77.66423357664233,
      "grad_norm": 8.516834259033203,
      "learning_rate": 4.611678832116789e-05,
      "loss": 2.2189,
      "step": 10640
    },
    {
      "epoch": 77.73722627737226,
      "grad_norm": 8.309212684631348,
      "learning_rate": 4.611313868613139e-05,
      "loss": 2.2838,
      "step": 10650
    },
    {
      "epoch": 77.81021897810218,
      "grad_norm": 8.603532791137695,
      "learning_rate": 4.61094890510949e-05,
      "loss": 2.3864,
      "step": 10660
    },
    {
      "epoch": 77.88321167883211,
      "grad_norm": 9.811681747436523,
      "learning_rate": 4.610583941605839e-05,
      "loss": 2.3597,
      "step": 10670
    },
    {
      "epoch": 77.95620437956204,
      "grad_norm": 6.239060401916504,
      "learning_rate": 4.61021897810219e-05,
      "loss": 2.5356,
      "step": 10680
    },
    {
      "epoch": 78.02919708029196,
      "grad_norm": 8.833934783935547,
      "learning_rate": 4.6098540145985405e-05,
      "loss": 2.0885,
      "step": 10690
    },
    {
      "epoch": 78.10218978102189,
      "grad_norm": 6.4574294090271,
      "learning_rate": 4.6094890510948906e-05,
      "loss": 2.14,
      "step": 10700
    },
    {
      "epoch": 78.17518248175182,
      "grad_norm": 8.164152145385742,
      "learning_rate": 4.609124087591241e-05,
      "loss": 2.6927,
      "step": 10710
    },
    {
      "epoch": 78.24817518248175,
      "grad_norm": 7.134696960449219,
      "learning_rate": 4.6087591240875914e-05,
      "loss": 2.0316,
      "step": 10720
    },
    {
      "epoch": 78.32116788321167,
      "grad_norm": 11.650343894958496,
      "learning_rate": 4.608394160583942e-05,
      "loss": 2.7099,
      "step": 10730
    },
    {
      "epoch": 78.39416058394161,
      "grad_norm": 9.655472755432129,
      "learning_rate": 4.608029197080292e-05,
      "loss": 2.3976,
      "step": 10740
    },
    {
      "epoch": 78.46715328467154,
      "grad_norm": 7.612411022186279,
      "learning_rate": 4.607664233576642e-05,
      "loss": 2.4443,
      "step": 10750
    },
    {
      "epoch": 78.54014598540147,
      "grad_norm": 9.566328048706055,
      "learning_rate": 4.607299270072993e-05,
      "loss": 2.3431,
      "step": 10760
    },
    {
      "epoch": 78.61313868613139,
      "grad_norm": 8.369704246520996,
      "learning_rate": 4.606934306569343e-05,
      "loss": 2.563,
      "step": 10770
    },
    {
      "epoch": 78.68613138686132,
      "grad_norm": 8.421335220336914,
      "learning_rate": 4.606569343065694e-05,
      "loss": 2.3083,
      "step": 10780
    },
    {
      "epoch": 78.75912408759125,
      "grad_norm": 8.835753440856934,
      "learning_rate": 4.6062043795620444e-05,
      "loss": 1.982,
      "step": 10790
    },
    {
      "epoch": 78.83211678832117,
      "grad_norm": 8.70273208618164,
      "learning_rate": 4.6058394160583945e-05,
      "loss": 2.1302,
      "step": 10800
    },
    {
      "epoch": 78.9051094890511,
      "grad_norm": 8.991981506347656,
      "learning_rate": 4.605474452554745e-05,
      "loss": 2.3975,
      "step": 10810
    },
    {
      "epoch": 78.97810218978103,
      "grad_norm": 6.369942665100098,
      "learning_rate": 4.6051094890510946e-05,
      "loss": 2.0606,
      "step": 10820
    },
    {
      "epoch": 79.05109489051095,
      "grad_norm": 8.699602127075195,
      "learning_rate": 4.604744525547445e-05,
      "loss": 2.3229,
      "step": 10830
    },
    {
      "epoch": 79.12408759124088,
      "grad_norm": 8.653620719909668,
      "learning_rate": 4.604379562043796e-05,
      "loss": 2.4307,
      "step": 10840
    },
    {
      "epoch": 79.1970802919708,
      "grad_norm": 11.032907485961914,
      "learning_rate": 4.604014598540146e-05,
      "loss": 2.4958,
      "step": 10850
    },
    {
      "epoch": 79.27007299270073,
      "grad_norm": 5.760213851928711,
      "learning_rate": 4.603649635036497e-05,
      "loss": 2.0647,
      "step": 10860
    },
    {
      "epoch": 79.34306569343066,
      "grad_norm": 9.713324546813965,
      "learning_rate": 4.603284671532847e-05,
      "loss": 2.236,
      "step": 10870
    },
    {
      "epoch": 79.41605839416059,
      "grad_norm": 9.075529098510742,
      "learning_rate": 4.6029197080291976e-05,
      "loss": 2.4534,
      "step": 10880
    },
    {
      "epoch": 79.48905109489051,
      "grad_norm": 9.185127258300781,
      "learning_rate": 4.6025547445255476e-05,
      "loss": 2.3561,
      "step": 10890
    },
    {
      "epoch": 79.56204379562044,
      "grad_norm": 10.578099250793457,
      "learning_rate": 4.602189781021898e-05,
      "loss": 2.3224,
      "step": 10900
    },
    {
      "epoch": 79.63503649635037,
      "grad_norm": 7.300014495849609,
      "learning_rate": 4.6018248175182484e-05,
      "loss": 2.2047,
      "step": 10910
    },
    {
      "epoch": 79.7080291970803,
      "grad_norm": 10.208767890930176,
      "learning_rate": 4.6014598540145985e-05,
      "loss": 2.3379,
      "step": 10920
    },
    {
      "epoch": 79.78102189781022,
      "grad_norm": 9.541413307189941,
      "learning_rate": 4.601094890510949e-05,
      "loss": 2.1214,
      "step": 10930
    },
    {
      "epoch": 79.85401459854015,
      "grad_norm": 6.067612648010254,
      "learning_rate": 4.600729927007299e-05,
      "loss": 2.3084,
      "step": 10940
    },
    {
      "epoch": 79.92700729927007,
      "grad_norm": 8.429797172546387,
      "learning_rate": 4.60036496350365e-05,
      "loss": 2.6355,
      "step": 10950
    },
    {
      "epoch": 80.0,
      "grad_norm": 20.344680786132812,
      "learning_rate": 4.600000000000001e-05,
      "loss": 2.3645,
      "step": 10960
    },
    {
      "epoch": 80.07299270072993,
      "grad_norm": 7.1259613037109375,
      "learning_rate": 4.599635036496351e-05,
      "loss": 2.2177,
      "step": 10970
    },
    {
      "epoch": 80.14598540145985,
      "grad_norm": 7.572420120239258,
      "learning_rate": 4.599270072992701e-05,
      "loss": 2.178,
      "step": 10980
    },
    {
      "epoch": 80.21897810218978,
      "grad_norm": 7.258819103240967,
      "learning_rate": 4.5989051094890515e-05,
      "loss": 2.3375,
      "step": 10990
    },
    {
      "epoch": 80.2919708029197,
      "grad_norm": 8.045957565307617,
      "learning_rate": 4.5985401459854016e-05,
      "loss": 2.4032,
      "step": 11000
    },
    {
      "epoch": 80.36496350364963,
      "grad_norm": 6.017981052398682,
      "learning_rate": 4.598175182481752e-05,
      "loss": 2.33,
      "step": 11010
    },
    {
      "epoch": 80.43795620437956,
      "grad_norm": 7.230168342590332,
      "learning_rate": 4.597810218978102e-05,
      "loss": 2.4544,
      "step": 11020
    },
    {
      "epoch": 80.51094890510949,
      "grad_norm": 8.444075584411621,
      "learning_rate": 4.597445255474453e-05,
      "loss": 2.0956,
      "step": 11030
    },
    {
      "epoch": 80.58394160583941,
      "grad_norm": 7.8728556632995605,
      "learning_rate": 4.597080291970803e-05,
      "loss": 2.2438,
      "step": 11040
    },
    {
      "epoch": 80.65693430656934,
      "grad_norm": 6.247605323791504,
      "learning_rate": 4.596715328467153e-05,
      "loss": 2.2712,
      "step": 11050
    },
    {
      "epoch": 80.72992700729927,
      "grad_norm": 7.513851165771484,
      "learning_rate": 4.596350364963504e-05,
      "loss": 2.3441,
      "step": 11060
    },
    {
      "epoch": 80.8029197080292,
      "grad_norm": 6.753681182861328,
      "learning_rate": 4.595985401459854e-05,
      "loss": 2.2191,
      "step": 11070
    },
    {
      "epoch": 80.87591240875912,
      "grad_norm": 8.471134185791016,
      "learning_rate": 4.595620437956205e-05,
      "loss": 2.1893,
      "step": 11080
    },
    {
      "epoch": 80.94890510948905,
      "grad_norm": 10.39369010925293,
      "learning_rate": 4.595255474452555e-05,
      "loss": 2.3658,
      "step": 11090
    },
    {
      "epoch": 81.02189781021897,
      "grad_norm": 9.007689476013184,
      "learning_rate": 4.5948905109489054e-05,
      "loss": 2.6626,
      "step": 11100
    },
    {
      "epoch": 81.0948905109489,
      "grad_norm": 8.108125686645508,
      "learning_rate": 4.594525547445256e-05,
      "loss": 2.3219,
      "step": 11110
    },
    {
      "epoch": 81.16788321167883,
      "grad_norm": 8.775516510009766,
      "learning_rate": 4.594160583941606e-05,
      "loss": 2.3619,
      "step": 11120
    },
    {
      "epoch": 81.24087591240875,
      "grad_norm": 6.913877487182617,
      "learning_rate": 4.593795620437956e-05,
      "loss": 2.075,
      "step": 11130
    },
    {
      "epoch": 81.31386861313868,
      "grad_norm": 9.44472599029541,
      "learning_rate": 4.593430656934306e-05,
      "loss": 2.0214,
      "step": 11140
    },
    {
      "epoch": 81.38686131386861,
      "grad_norm": 8.21399211883545,
      "learning_rate": 4.593065693430657e-05,
      "loss": 1.8923,
      "step": 11150
    },
    {
      "epoch": 81.45985401459853,
      "grad_norm": 9.320032119750977,
      "learning_rate": 4.592700729927008e-05,
      "loss": 2.5596,
      "step": 11160
    },
    {
      "epoch": 81.53284671532846,
      "grad_norm": 7.474761009216309,
      "learning_rate": 4.592335766423358e-05,
      "loss": 2.4552,
      "step": 11170
    },
    {
      "epoch": 81.60583941605839,
      "grad_norm": 6.649792194366455,
      "learning_rate": 4.5919708029197085e-05,
      "loss": 1.9446,
      "step": 11180
    },
    {
      "epoch": 81.67883211678833,
      "grad_norm": 10.515715599060059,
      "learning_rate": 4.5916058394160586e-05,
      "loss": 2.2839,
      "step": 11190
    },
    {
      "epoch": 81.75182481751825,
      "grad_norm": 7.358583927154541,
      "learning_rate": 4.591240875912409e-05,
      "loss": 2.092,
      "step": 11200
    },
    {
      "epoch": 81.82481751824818,
      "grad_norm": 8.30880355834961,
      "learning_rate": 4.5908759124087594e-05,
      "loss": 2.394,
      "step": 11210
    },
    {
      "epoch": 81.89781021897811,
      "grad_norm": 7.425327777862549,
      "learning_rate": 4.5905109489051094e-05,
      "loss": 2.4456,
      "step": 11220
    },
    {
      "epoch": 81.97080291970804,
      "grad_norm": 8.545862197875977,
      "learning_rate": 4.59014598540146e-05,
      "loss": 2.2626,
      "step": 11230
    },
    {
      "epoch": 82.04379562043796,
      "grad_norm": 8.628652572631836,
      "learning_rate": 4.58978102189781e-05,
      "loss": 2.3146,
      "step": 11240
    },
    {
      "epoch": 82.11678832116789,
      "grad_norm": 8.490213394165039,
      "learning_rate": 4.589416058394161e-05,
      "loss": 1.9334,
      "step": 11250
    },
    {
      "epoch": 82.18978102189782,
      "grad_norm": 9.05513858795166,
      "learning_rate": 4.5890510948905117e-05,
      "loss": 2.0044,
      "step": 11260
    },
    {
      "epoch": 82.26277372262774,
      "grad_norm": 9.52308464050293,
      "learning_rate": 4.588686131386862e-05,
      "loss": 2.4525,
      "step": 11270
    },
    {
      "epoch": 82.33576642335767,
      "grad_norm": 9.430709838867188,
      "learning_rate": 4.588321167883212e-05,
      "loss": 2.4927,
      "step": 11280
    },
    {
      "epoch": 82.4087591240876,
      "grad_norm": 8.655454635620117,
      "learning_rate": 4.587956204379562e-05,
      "loss": 2.5284,
      "step": 11290
    },
    {
      "epoch": 82.48175182481752,
      "grad_norm": 7.260451793670654,
      "learning_rate": 4.5875912408759125e-05,
      "loss": 2.209,
      "step": 11300
    },
    {
      "epoch": 82.55474452554745,
      "grad_norm": 8.926838874816895,
      "learning_rate": 4.587226277372263e-05,
      "loss": 2.2051,
      "step": 11310
    },
    {
      "epoch": 82.62773722627738,
      "grad_norm": 8.79376220703125,
      "learning_rate": 4.586861313868613e-05,
      "loss": 2.2076,
      "step": 11320
    },
    {
      "epoch": 82.7007299270073,
      "grad_norm": 9.156394958496094,
      "learning_rate": 4.586496350364964e-05,
      "loss": 2.2446,
      "step": 11330
    },
    {
      "epoch": 82.77372262773723,
      "grad_norm": 8.866315841674805,
      "learning_rate": 4.586131386861314e-05,
      "loss": 2.2085,
      "step": 11340
    },
    {
      "epoch": 82.84671532846716,
      "grad_norm": 8.805964469909668,
      "learning_rate": 4.585766423357665e-05,
      "loss": 2.033,
      "step": 11350
    },
    {
      "epoch": 82.91970802919708,
      "grad_norm": 10.638151168823242,
      "learning_rate": 4.585401459854015e-05,
      "loss": 2.6538,
      "step": 11360
    },
    {
      "epoch": 82.99270072992701,
      "grad_norm": 8.666590690612793,
      "learning_rate": 4.585036496350365e-05,
      "loss": 2.3161,
      "step": 11370
    },
    {
      "epoch": 83.06569343065694,
      "grad_norm": 11.102560997009277,
      "learning_rate": 4.5846715328467156e-05,
      "loss": 2.1499,
      "step": 11380
    },
    {
      "epoch": 83.13868613138686,
      "grad_norm": 7.876651287078857,
      "learning_rate": 4.584306569343066e-05,
      "loss": 1.9797,
      "step": 11390
    },
    {
      "epoch": 83.21167883211679,
      "grad_norm": 6.414863109588623,
      "learning_rate": 4.5839416058394164e-05,
      "loss": 2.5164,
      "step": 11400
    },
    {
      "epoch": 83.28467153284672,
      "grad_norm": 8.253376960754395,
      "learning_rate": 4.5835766423357665e-05,
      "loss": 2.2999,
      "step": 11410
    },
    {
      "epoch": 83.35766423357664,
      "grad_norm": 13.344277381896973,
      "learning_rate": 4.583211678832117e-05,
      "loss": 2.1948,
      "step": 11420
    },
    {
      "epoch": 83.43065693430657,
      "grad_norm": 9.621691703796387,
      "learning_rate": 4.582846715328467e-05,
      "loss": 2.2177,
      "step": 11430
    },
    {
      "epoch": 83.5036496350365,
      "grad_norm": 8.24867057800293,
      "learning_rate": 4.582481751824817e-05,
      "loss": 2.395,
      "step": 11440
    },
    {
      "epoch": 83.57664233576642,
      "grad_norm": 9.280611038208008,
      "learning_rate": 4.582116788321168e-05,
      "loss": 2.0878,
      "step": 11450
    },
    {
      "epoch": 83.64963503649635,
      "grad_norm": 7.748764991760254,
      "learning_rate": 4.581751824817519e-05,
      "loss": 2.0378,
      "step": 11460
    },
    {
      "epoch": 83.72262773722628,
      "grad_norm": 7.163119792938232,
      "learning_rate": 4.581386861313869e-05,
      "loss": 2.2843,
      "step": 11470
    },
    {
      "epoch": 83.7956204379562,
      "grad_norm": 8.872284889221191,
      "learning_rate": 4.5810218978102195e-05,
      "loss": 2.166,
      "step": 11480
    },
    {
      "epoch": 83.86861313868613,
      "grad_norm": 7.46298360824585,
      "learning_rate": 4.5806569343065696e-05,
      "loss": 2.2057,
      "step": 11490
    },
    {
      "epoch": 83.94160583941606,
      "grad_norm": 9.53577995300293,
      "learning_rate": 4.58029197080292e-05,
      "loss": 2.4799,
      "step": 11500
    },
    {
      "epoch": 84.01459854014598,
      "grad_norm": 6.7639970779418945,
      "learning_rate": 4.5799270072992703e-05,
      "loss": 1.9783,
      "step": 11510
    },
    {
      "epoch": 84.08759124087591,
      "grad_norm": 7.279810905456543,
      "learning_rate": 4.5795620437956204e-05,
      "loss": 1.9992,
      "step": 11520
    },
    {
      "epoch": 84.16058394160584,
      "grad_norm": 10.102058410644531,
      "learning_rate": 4.579197080291971e-05,
      "loss": 2.5122,
      "step": 11530
    },
    {
      "epoch": 84.23357664233576,
      "grad_norm": 8.82426929473877,
      "learning_rate": 4.578832116788321e-05,
      "loss": 2.4549,
      "step": 11540
    },
    {
      "epoch": 84.30656934306569,
      "grad_norm": 8.838247299194336,
      "learning_rate": 4.578467153284672e-05,
      "loss": 2.2057,
      "step": 11550
    },
    {
      "epoch": 84.37956204379562,
      "grad_norm": 7.734355926513672,
      "learning_rate": 4.578102189781022e-05,
      "loss": 2.0102,
      "step": 11560
    },
    {
      "epoch": 84.45255474452554,
      "grad_norm": 8.702051162719727,
      "learning_rate": 4.577737226277373e-05,
      "loss": 2.4051,
      "step": 11570
    },
    {
      "epoch": 84.52554744525547,
      "grad_norm": 8.043899536132812,
      "learning_rate": 4.5773722627737234e-05,
      "loss": 2.3124,
      "step": 11580
    },
    {
      "epoch": 84.5985401459854,
      "grad_norm": 5.172885894775391,
      "learning_rate": 4.577007299270073e-05,
      "loss": 2.1595,
      "step": 11590
    },
    {
      "epoch": 84.67153284671532,
      "grad_norm": 7.817770957946777,
      "learning_rate": 4.5766423357664235e-05,
      "loss": 2.0559,
      "step": 11600
    },
    {
      "epoch": 84.74452554744525,
      "grad_norm": 8.95318603515625,
      "learning_rate": 4.5762773722627735e-05,
      "loss": 2.0209,
      "step": 11610
    },
    {
      "epoch": 84.81751824817518,
      "grad_norm": 8.812023162841797,
      "learning_rate": 4.575912408759124e-05,
      "loss": 2.0561,
      "step": 11620
    },
    {
      "epoch": 84.8905109489051,
      "grad_norm": 7.110284805297852,
      "learning_rate": 4.575547445255475e-05,
      "loss": 2.4652,
      "step": 11630
    },
    {
      "epoch": 84.96350364963503,
      "grad_norm": 7.9086689949035645,
      "learning_rate": 4.575182481751825e-05,
      "loss": 2.2033,
      "step": 11640
    },
    {
      "epoch": 85.03649635036497,
      "grad_norm": 8.759615898132324,
      "learning_rate": 4.574817518248176e-05,
      "loss": 2.0874,
      "step": 11650
    },
    {
      "epoch": 85.1094890510949,
      "grad_norm": 10.051961898803711,
      "learning_rate": 4.574452554744526e-05,
      "loss": 2.2252,
      "step": 11660
    },
    {
      "epoch": 85.18248175182482,
      "grad_norm": 7.775115013122559,
      "learning_rate": 4.574087591240876e-05,
      "loss": 2.5514,
      "step": 11670
    },
    {
      "epoch": 85.25547445255475,
      "grad_norm": 10.509012222290039,
      "learning_rate": 4.5737226277372266e-05,
      "loss": 1.9599,
      "step": 11680
    },
    {
      "epoch": 85.32846715328468,
      "grad_norm": 10.676698684692383,
      "learning_rate": 4.5733576642335767e-05,
      "loss": 2.2846,
      "step": 11690
    },
    {
      "epoch": 85.4014598540146,
      "grad_norm": 11.848406791687012,
      "learning_rate": 4.5729927007299274e-05,
      "loss": 1.9965,
      "step": 11700
    },
    {
      "epoch": 85.47445255474453,
      "grad_norm": 7.319695949554443,
      "learning_rate": 4.5726277372262774e-05,
      "loss": 1.9577,
      "step": 11710
    },
    {
      "epoch": 85.54744525547446,
      "grad_norm": 12.374298095703125,
      "learning_rate": 4.572262773722628e-05,
      "loss": 2.109,
      "step": 11720
    },
    {
      "epoch": 85.62043795620438,
      "grad_norm": 9.950313568115234,
      "learning_rate": 4.571897810218979e-05,
      "loss": 2.4133,
      "step": 11730
    },
    {
      "epoch": 85.69343065693431,
      "grad_norm": 7.652458190917969,
      "learning_rate": 4.571532846715328e-05,
      "loss": 1.9038,
      "step": 11740
    },
    {
      "epoch": 85.76642335766424,
      "grad_norm": 8.8052339553833,
      "learning_rate": 4.571167883211679e-05,
      "loss": 1.8722,
      "step": 11750
    },
    {
      "epoch": 85.83941605839416,
      "grad_norm": 5.8906660079956055,
      "learning_rate": 4.570802919708029e-05,
      "loss": 2.2111,
      "step": 11760
    },
    {
      "epoch": 85.91240875912409,
      "grad_norm": 8.751943588256836,
      "learning_rate": 4.57043795620438e-05,
      "loss": 2.3499,
      "step": 11770
    },
    {
      "epoch": 85.98540145985402,
      "grad_norm": 8.545292854309082,
      "learning_rate": 4.5700729927007305e-05,
      "loss": 2.1106,
      "step": 11780
    },
    {
      "epoch": 86.05839416058394,
      "grad_norm": 8.8885498046875,
      "learning_rate": 4.5697080291970805e-05,
      "loss": 2.1717,
      "step": 11790
    },
    {
      "epoch": 86.13138686131387,
      "grad_norm": 7.986952781677246,
      "learning_rate": 4.569343065693431e-05,
      "loss": 2.1528,
      "step": 11800
    },
    {
      "epoch": 86.2043795620438,
      "grad_norm": 9.94591999053955,
      "learning_rate": 4.568978102189781e-05,
      "loss": 2.1448,
      "step": 11810
    },
    {
      "epoch": 86.27737226277372,
      "grad_norm": 9.170019149780273,
      "learning_rate": 4.5686131386861314e-05,
      "loss": 2.1932,
      "step": 11820
    },
    {
      "epoch": 86.35036496350365,
      "grad_norm": 7.918513298034668,
      "learning_rate": 4.568248175182482e-05,
      "loss": 1.8993,
      "step": 11830
    },
    {
      "epoch": 86.42335766423358,
      "grad_norm": 9.329787254333496,
      "learning_rate": 4.567883211678832e-05,
      "loss": 1.9276,
      "step": 11840
    },
    {
      "epoch": 86.4963503649635,
      "grad_norm": 8.578278541564941,
      "learning_rate": 4.567518248175183e-05,
      "loss": 2.0156,
      "step": 11850
    },
    {
      "epoch": 86.56934306569343,
      "grad_norm": 9.290051460266113,
      "learning_rate": 4.567153284671533e-05,
      "loss": 2.3303,
      "step": 11860
    },
    {
      "epoch": 86.64233576642336,
      "grad_norm": 6.344516277313232,
      "learning_rate": 4.5667883211678836e-05,
      "loss": 1.8776,
      "step": 11870
    },
    {
      "epoch": 86.71532846715328,
      "grad_norm": 9.575169563293457,
      "learning_rate": 4.566423357664234e-05,
      "loss": 2.3547,
      "step": 11880
    },
    {
      "epoch": 86.78832116788321,
      "grad_norm": 10.122126579284668,
      "learning_rate": 4.566058394160584e-05,
      "loss": 2.2831,
      "step": 11890
    },
    {
      "epoch": 86.86131386861314,
      "grad_norm": 8.314630508422852,
      "learning_rate": 4.5656934306569345e-05,
      "loss": 2.1451,
      "step": 11900
    },
    {
      "epoch": 86.93430656934306,
      "grad_norm": 8.94879150390625,
      "learning_rate": 4.5653284671532845e-05,
      "loss": 2.4128,
      "step": 11910
    },
    {
      "epoch": 87.00729927007299,
      "grad_norm": 6.8956828117370605,
      "learning_rate": 4.564963503649635e-05,
      "loss": 2.4141,
      "step": 11920
    },
    {
      "epoch": 87.08029197080292,
      "grad_norm": 8.737772941589355,
      "learning_rate": 4.564598540145986e-05,
      "loss": 2.1186,
      "step": 11930
    },
    {
      "epoch": 87.15328467153284,
      "grad_norm": 6.830104827880859,
      "learning_rate": 4.564233576642336e-05,
      "loss": 1.7084,
      "step": 11940
    },
    {
      "epoch": 87.22627737226277,
      "grad_norm": 6.38648796081543,
      "learning_rate": 4.563868613138687e-05,
      "loss": 2.0397,
      "step": 11950
    },
    {
      "epoch": 87.2992700729927,
      "grad_norm": 8.285869598388672,
      "learning_rate": 4.563503649635037e-05,
      "loss": 2.0673,
      "step": 11960
    },
    {
      "epoch": 87.37226277372262,
      "grad_norm": 7.588603496551514,
      "learning_rate": 4.563138686131387e-05,
      "loss": 2.1468,
      "step": 11970
    },
    {
      "epoch": 87.44525547445255,
      "grad_norm": 9.793158531188965,
      "learning_rate": 4.5627737226277376e-05,
      "loss": 1.9617,
      "step": 11980
    },
    {
      "epoch": 87.51824817518248,
      "grad_norm": 8.219175338745117,
      "learning_rate": 4.5624087591240876e-05,
      "loss": 2.0257,
      "step": 11990
    },
    {
      "epoch": 87.5912408759124,
      "grad_norm": 7.779135227203369,
      "learning_rate": 4.5620437956204383e-05,
      "loss": 2.1131,
      "step": 12000
    },
    {
      "epoch": 87.66423357664233,
      "grad_norm": 9.0825777053833,
      "learning_rate": 4.5616788321167884e-05,
      "loss": 2.5815,
      "step": 12010
    },
    {
      "epoch": 87.73722627737226,
      "grad_norm": 7.314243316650391,
      "learning_rate": 4.561313868613139e-05,
      "loss": 2.1217,
      "step": 12020
    },
    {
      "epoch": 87.81021897810218,
      "grad_norm": 6.469011306762695,
      "learning_rate": 4.560948905109489e-05,
      "loss": 2.423,
      "step": 12030
    },
    {
      "epoch": 87.88321167883211,
      "grad_norm": 8.140186309814453,
      "learning_rate": 4.56058394160584e-05,
      "loss": 2.0718,
      "step": 12040
    },
    {
      "epoch": 87.95620437956204,
      "grad_norm": 11.323860168457031,
      "learning_rate": 4.56021897810219e-05,
      "loss": 2.3697,
      "step": 12050
    },
    {
      "epoch": 88.02919708029196,
      "grad_norm": 8.673510551452637,
      "learning_rate": 4.55985401459854e-05,
      "loss": 2.2208,
      "step": 12060
    },
    {
      "epoch": 88.10218978102189,
      "grad_norm": 8.963788986206055,
      "learning_rate": 4.559489051094891e-05,
      "loss": 2.3375,
      "step": 12070
    },
    {
      "epoch": 88.17518248175182,
      "grad_norm": 6.136707782745361,
      "learning_rate": 4.559124087591241e-05,
      "loss": 1.7278,
      "step": 12080
    },
    {
      "epoch": 88.24817518248175,
      "grad_norm": 6.920126438140869,
      "learning_rate": 4.5587591240875915e-05,
      "loss": 2.304,
      "step": 12090
    },
    {
      "epoch": 88.32116788321167,
      "grad_norm": 9.177021980285645,
      "learning_rate": 4.558394160583942e-05,
      "loss": 2.1498,
      "step": 12100
    },
    {
      "epoch": 88.39416058394161,
      "grad_norm": 8.452842712402344,
      "learning_rate": 4.558029197080292e-05,
      "loss": 2.2917,
      "step": 12110
    },
    {
      "epoch": 88.46715328467154,
      "grad_norm": 8.298066139221191,
      "learning_rate": 4.557664233576642e-05,
      "loss": 2.5482,
      "step": 12120
    },
    {
      "epoch": 88.54014598540147,
      "grad_norm": 8.762243270874023,
      "learning_rate": 4.557299270072993e-05,
      "loss": 1.652,
      "step": 12130
    },
    {
      "epoch": 88.61313868613139,
      "grad_norm": 10.181900024414062,
      "learning_rate": 4.556934306569343e-05,
      "loss": 2.1732,
      "step": 12140
    },
    {
      "epoch": 88.68613138686132,
      "grad_norm": 9.192129135131836,
      "learning_rate": 4.556569343065694e-05,
      "loss": 2.0546,
      "step": 12150
    },
    {
      "epoch": 88.75912408759125,
      "grad_norm": 9.648609161376953,
      "learning_rate": 4.556204379562044e-05,
      "loss": 1.9605,
      "step": 12160
    },
    {
      "epoch": 88.83211678832117,
      "grad_norm": 7.057205677032471,
      "learning_rate": 4.5558394160583946e-05,
      "loss": 2.1059,
      "step": 12170
    },
    {
      "epoch": 88.9051094890511,
      "grad_norm": 10.333545684814453,
      "learning_rate": 4.5554744525547447e-05,
      "loss": 2.278,
      "step": 12180
    },
    {
      "epoch": 88.97810218978103,
      "grad_norm": 11.0519437789917,
      "learning_rate": 4.5551094890510954e-05,
      "loss": 2.0644,
      "step": 12190
    },
    {
      "epoch": 89.05109489051095,
      "grad_norm": 8.891258239746094,
      "learning_rate": 4.5547445255474454e-05,
      "loss": 1.8498,
      "step": 12200
    },
    {
      "epoch": 89.12408759124088,
      "grad_norm": 8.335373878479004,
      "learning_rate": 4.5543795620437955e-05,
      "loss": 2.0506,
      "step": 12210
    },
    {
      "epoch": 89.1970802919708,
      "grad_norm": 9.849021911621094,
      "learning_rate": 4.554014598540146e-05,
      "loss": 2.0292,
      "step": 12220
    },
    {
      "epoch": 89.27007299270073,
      "grad_norm": 8.521318435668945,
      "learning_rate": 4.553649635036496e-05,
      "loss": 2.0482,
      "step": 12230
    },
    {
      "epoch": 89.34306569343066,
      "grad_norm": 10.06744384765625,
      "learning_rate": 4.553284671532847e-05,
      "loss": 2.285,
      "step": 12240
    },
    {
      "epoch": 89.41605839416059,
      "grad_norm": 5.52708101272583,
      "learning_rate": 4.552919708029198e-05,
      "loss": 2.0115,
      "step": 12250
    },
    {
      "epoch": 89.48905109489051,
      "grad_norm": 9.517698287963867,
      "learning_rate": 4.552554744525548e-05,
      "loss": 2.0531,
      "step": 12260
    },
    {
      "epoch": 89.56204379562044,
      "grad_norm": 8.151664733886719,
      "learning_rate": 4.5521897810218985e-05,
      "loss": 2.2171,
      "step": 12270
    },
    {
      "epoch": 89.63503649635037,
      "grad_norm": 7.035257339477539,
      "learning_rate": 4.551824817518248e-05,
      "loss": 2.1154,
      "step": 12280
    },
    {
      "epoch": 89.7080291970803,
      "grad_norm": 8.760542869567871,
      "learning_rate": 4.5514598540145986e-05,
      "loss": 2.2411,
      "step": 12290
    },
    {
      "epoch": 89.78102189781022,
      "grad_norm": 7.425405502319336,
      "learning_rate": 4.551094890510949e-05,
      "loss": 2.0349,
      "step": 12300
    },
    {
      "epoch": 89.85401459854015,
      "grad_norm": 6.960668563842773,
      "learning_rate": 4.5507299270072994e-05,
      "loss": 2.1487,
      "step": 12310
    },
    {
      "epoch": 89.92700729927007,
      "grad_norm": 9.0276460647583,
      "learning_rate": 4.55036496350365e-05,
      "loss": 2.2674,
      "step": 12320
    },
    {
      "epoch": 90.0,
      "grad_norm": 12.390395164489746,
      "learning_rate": 4.55e-05,
      "loss": 2.0762,
      "step": 12330
    },
    {
      "epoch": 90.07299270072993,
      "grad_norm": 13.364368438720703,
      "learning_rate": 4.549635036496351e-05,
      "loss": 2.4079,
      "step": 12340
    },
    {
      "epoch": 90.14598540145985,
      "grad_norm": 8.903594970703125,
      "learning_rate": 4.549270072992701e-05,
      "loss": 2.2129,
      "step": 12350
    },
    {
      "epoch": 90.21897810218978,
      "grad_norm": 8.966135025024414,
      "learning_rate": 4.548905109489051e-05,
      "loss": 1.7841,
      "step": 12360
    },
    {
      "epoch": 90.2919708029197,
      "grad_norm": 10.112025260925293,
      "learning_rate": 4.548540145985402e-05,
      "loss": 2.2228,
      "step": 12370
    },
    {
      "epoch": 90.36496350364963,
      "grad_norm": 8.954251289367676,
      "learning_rate": 4.548175182481752e-05,
      "loss": 2.2464,
      "step": 12380
    },
    {
      "epoch": 90.43795620437956,
      "grad_norm": 13.174469947814941,
      "learning_rate": 4.5478102189781025e-05,
      "loss": 2.0939,
      "step": 12390
    },
    {
      "epoch": 90.51094890510949,
      "grad_norm": 9.054685592651367,
      "learning_rate": 4.547445255474453e-05,
      "loss": 2.0766,
      "step": 12400
    },
    {
      "epoch": 90.58394160583941,
      "grad_norm": 7.052023410797119,
      "learning_rate": 4.547080291970803e-05,
      "loss": 1.9742,
      "step": 12410
    },
    {
      "epoch": 90.65693430656934,
      "grad_norm": 9.365327835083008,
      "learning_rate": 4.546715328467154e-05,
      "loss": 2.004,
      "step": 12420
    },
    {
      "epoch": 90.72992700729927,
      "grad_norm": 10.990387916564941,
      "learning_rate": 4.5463503649635033e-05,
      "loss": 2.2032,
      "step": 12430
    },
    {
      "epoch": 90.8029197080292,
      "grad_norm": 8.041733741760254,
      "learning_rate": 4.545985401459854e-05,
      "loss": 1.9916,
      "step": 12440
    },
    {
      "epoch": 90.87591240875912,
      "grad_norm": 8.66248893737793,
      "learning_rate": 4.545620437956205e-05,
      "loss": 1.7686,
      "step": 12450
    },
    {
      "epoch": 90.94890510948905,
      "grad_norm": 9.05783462524414,
      "learning_rate": 4.545255474452555e-05,
      "loss": 2.0577,
      "step": 12460
    },
    {
      "epoch": 91.02189781021897,
      "grad_norm": 7.52478551864624,
      "learning_rate": 4.5448905109489056e-05,
      "loss": 2.1546,
      "step": 12470
    },
    {
      "epoch": 91.0948905109489,
      "grad_norm": 9.432825088500977,
      "learning_rate": 4.5445255474452556e-05,
      "loss": 2.3178,
      "step": 12480
    },
    {
      "epoch": 91.16788321167883,
      "grad_norm": 7.601831436157227,
      "learning_rate": 4.5441605839416064e-05,
      "loss": 1.8213,
      "step": 12490
    },
    {
      "epoch": 91.24087591240875,
      "grad_norm": 8.08014965057373,
      "learning_rate": 4.5437956204379564e-05,
      "loss": 2.2741,
      "step": 12500
    },
    {
      "epoch": 91.31386861313868,
      "grad_norm": 10.162763595581055,
      "learning_rate": 4.5434306569343065e-05,
      "loss": 1.7005,
      "step": 12510
    },
    {
      "epoch": 91.38686131386861,
      "grad_norm": 6.968814373016357,
      "learning_rate": 4.543065693430657e-05,
      "loss": 1.6413,
      "step": 12520
    },
    {
      "epoch": 91.45985401459853,
      "grad_norm": 9.903953552246094,
      "learning_rate": 4.542700729927007e-05,
      "loss": 1.8661,
      "step": 12530
    },
    {
      "epoch": 91.53284671532846,
      "grad_norm": 8.106932640075684,
      "learning_rate": 4.542335766423358e-05,
      "loss": 1.9384,
      "step": 12540
    },
    {
      "epoch": 91.60583941605839,
      "grad_norm": 7.894394874572754,
      "learning_rate": 4.541970802919708e-05,
      "loss": 2.0498,
      "step": 12550
    },
    {
      "epoch": 91.67883211678833,
      "grad_norm": 9.971972465515137,
      "learning_rate": 4.541605839416059e-05,
      "loss": 2.0705,
      "step": 12560
    },
    {
      "epoch": 91.75182481751825,
      "grad_norm": 8.32946491241455,
      "learning_rate": 4.5412408759124095e-05,
      "loss": 2.2691,
      "step": 12570
    },
    {
      "epoch": 91.82481751824818,
      "grad_norm": 8.989662170410156,
      "learning_rate": 4.540875912408759e-05,
      "loss": 2.121,
      "step": 12580
    },
    {
      "epoch": 91.89781021897811,
      "grad_norm": 10.281055450439453,
      "learning_rate": 4.5405109489051096e-05,
      "loss": 2.3734,
      "step": 12590
    },
    {
      "epoch": 91.97080291970804,
      "grad_norm": 8.096772193908691,
      "learning_rate": 4.54014598540146e-05,
      "loss": 2.0986,
      "step": 12600
    },
    {
      "epoch": 92.04379562043796,
      "grad_norm": 8.830144882202148,
      "learning_rate": 4.53978102189781e-05,
      "loss": 2.0679,
      "step": 12610
    },
    {
      "epoch": 92.11678832116789,
      "grad_norm": 7.4677653312683105,
      "learning_rate": 4.539416058394161e-05,
      "loss": 1.5695,
      "step": 12620
    },
    {
      "epoch": 92.18978102189782,
      "grad_norm": 7.816417694091797,
      "learning_rate": 4.539051094890511e-05,
      "loss": 1.8758,
      "step": 12630
    },
    {
      "epoch": 92.26277372262774,
      "grad_norm": 7.193466663360596,
      "learning_rate": 4.538686131386862e-05,
      "loss": 2.1336,
      "step": 12640
    },
    {
      "epoch": 92.33576642335767,
      "grad_norm": 7.246639251708984,
      "learning_rate": 4.538321167883212e-05,
      "loss": 2.1405,
      "step": 12650
    },
    {
      "epoch": 92.4087591240876,
      "grad_norm": 7.942572593688965,
      "learning_rate": 4.537956204379562e-05,
      "loss": 2.2829,
      "step": 12660
    },
    {
      "epoch": 92.48175182481752,
      "grad_norm": 9.854264259338379,
      "learning_rate": 4.537591240875913e-05,
      "loss": 2.106,
      "step": 12670
    },
    {
      "epoch": 92.55474452554745,
      "grad_norm": 7.886901378631592,
      "learning_rate": 4.537226277372263e-05,
      "loss": 1.6475,
      "step": 12680
    },
    {
      "epoch": 92.62773722627738,
      "grad_norm": 7.697049617767334,
      "learning_rate": 4.5368613138686134e-05,
      "loss": 2.2669,
      "step": 12690
    },
    {
      "epoch": 92.7007299270073,
      "grad_norm": 10.09101390838623,
      "learning_rate": 4.5364963503649635e-05,
      "loss": 2.1526,
      "step": 12700
    },
    {
      "epoch": 92.77372262773723,
      "grad_norm": 6.961455821990967,
      "learning_rate": 4.536131386861314e-05,
      "loss": 1.7459,
      "step": 12710
    },
    {
      "epoch": 92.84671532846716,
      "grad_norm": 7.631137847900391,
      "learning_rate": 4.535766423357665e-05,
      "loss": 2.0919,
      "step": 12720
    },
    {
      "epoch": 92.91970802919708,
      "grad_norm": 8.305168151855469,
      "learning_rate": 4.535401459854014e-05,
      "loss": 2.1599,
      "step": 12730
    },
    {
      "epoch": 92.99270072992701,
      "grad_norm": 6.673232555389404,
      "learning_rate": 4.535036496350365e-05,
      "loss": 2.157,
      "step": 12740
    },
    {
      "epoch": 93.06569343065694,
      "grad_norm": 5.869288921356201,
      "learning_rate": 4.534671532846715e-05,
      "loss": 1.8876,
      "step": 12750
    },
    {
      "epoch": 93.13868613138686,
      "grad_norm": 8.266212463378906,
      "learning_rate": 4.534306569343066e-05,
      "loss": 2.344,
      "step": 12760
    },
    {
      "epoch": 93.21167883211679,
      "grad_norm": 7.837944030761719,
      "learning_rate": 4.5339416058394165e-05,
      "loss": 1.9152,
      "step": 12770
    },
    {
      "epoch": 93.28467153284672,
      "grad_norm": 8.89465045928955,
      "learning_rate": 4.5335766423357666e-05,
      "loss": 1.9269,
      "step": 12780
    },
    {
      "epoch": 93.35766423357664,
      "grad_norm": 7.563448905944824,
      "learning_rate": 4.533211678832117e-05,
      "loss": 1.9633,
      "step": 12790
    },
    {
      "epoch": 93.43065693430657,
      "grad_norm": 7.040625095367432,
      "learning_rate": 4.5328467153284674e-05,
      "loss": 1.863,
      "step": 12800
    },
    {
      "epoch": 93.5036496350365,
      "grad_norm": 7.765994071960449,
      "learning_rate": 4.5324817518248174e-05,
      "loss": 1.7313,
      "step": 12810
    },
    {
      "epoch": 93.57664233576642,
      "grad_norm": 8.750750541687012,
      "learning_rate": 4.532116788321168e-05,
      "loss": 1.9139,
      "step": 12820
    },
    {
      "epoch": 93.64963503649635,
      "grad_norm": 8.697046279907227,
      "learning_rate": 4.531751824817518e-05,
      "loss": 1.9216,
      "step": 12830
    },
    {
      "epoch": 93.72262773722628,
      "grad_norm": 9.685698509216309,
      "learning_rate": 4.531386861313869e-05,
      "loss": 2.5072,
      "step": 12840
    },
    {
      "epoch": 93.7956204379562,
      "grad_norm": 8.187887191772461,
      "learning_rate": 4.531021897810219e-05,
      "loss": 1.7816,
      "step": 12850
    },
    {
      "epoch": 93.86861313868613,
      "grad_norm": 7.135890960693359,
      "learning_rate": 4.53065693430657e-05,
      "loss": 2.0539,
      "step": 12860
    },
    {
      "epoch": 93.94160583941606,
      "grad_norm": 8.297340393066406,
      "learning_rate": 4.5302919708029204e-05,
      "loss": 2.1077,
      "step": 12870
    },
    {
      "epoch": 94.01459854014598,
      "grad_norm": 7.615033149719238,
      "learning_rate": 4.5299270072992705e-05,
      "loss": 1.9023,
      "step": 12880
    },
    {
      "epoch": 94.08759124087591,
      "grad_norm": 8.803805351257324,
      "learning_rate": 4.5295620437956205e-05,
      "loss": 1.8285,
      "step": 12890
    },
    {
      "epoch": 94.16058394160584,
      "grad_norm": 8.387718200683594,
      "learning_rate": 4.5291970802919706e-05,
      "loss": 2.0018,
      "step": 12900
    },
    {
      "epoch": 94.23357664233576,
      "grad_norm": 7.699875831604004,
      "learning_rate": 4.528832116788321e-05,
      "loss": 1.7023,
      "step": 12910
    },
    {
      "epoch": 94.30656934306569,
      "grad_norm": 8.682129859924316,
      "learning_rate": 4.528467153284672e-05,
      "loss": 1.8382,
      "step": 12920
    },
    {
      "epoch": 94.37956204379562,
      "grad_norm": 7.671107292175293,
      "learning_rate": 4.528102189781022e-05,
      "loss": 2.0381,
      "step": 12930
    },
    {
      "epoch": 94.45255474452554,
      "grad_norm": 10.936332702636719,
      "learning_rate": 4.527737226277373e-05,
      "loss": 2.3472,
      "step": 12940
    },
    {
      "epoch": 94.52554744525547,
      "grad_norm": 10.694669723510742,
      "learning_rate": 4.527372262773723e-05,
      "loss": 2.1385,
      "step": 12950
    },
    {
      "epoch": 94.5985401459854,
      "grad_norm": 8.216022491455078,
      "learning_rate": 4.527007299270073e-05,
      "loss": 2.2397,
      "step": 12960
    },
    {
      "epoch": 94.67153284671532,
      "grad_norm": 7.9856061935424805,
      "learning_rate": 4.5266423357664236e-05,
      "loss": 2.09,
      "step": 12970
    },
    {
      "epoch": 94.74452554744525,
      "grad_norm": 8.749197959899902,
      "learning_rate": 4.526277372262774e-05,
      "loss": 1.7253,
      "step": 12980
    },
    {
      "epoch": 94.81751824817518,
      "grad_norm": 8.77181339263916,
      "learning_rate": 4.5259124087591244e-05,
      "loss": 2.0574,
      "step": 12990
    },
    {
      "epoch": 94.8905109489051,
      "grad_norm": 9.147290229797363,
      "learning_rate": 4.5255474452554745e-05,
      "loss": 2.0834,
      "step": 13000
    },
    {
      "epoch": 94.96350364963503,
      "grad_norm": 8.856572151184082,
      "learning_rate": 4.525182481751825e-05,
      "loss": 1.9508,
      "step": 13010
    },
    {
      "epoch": 95.03649635036497,
      "grad_norm": 6.758163928985596,
      "learning_rate": 4.524817518248175e-05,
      "loss": 1.5637,
      "step": 13020
    },
    {
      "epoch": 95.1094890510949,
      "grad_norm": 9.974635124206543,
      "learning_rate": 4.524452554744526e-05,
      "loss": 1.9354,
      "step": 13030
    },
    {
      "epoch": 95.18248175182482,
      "grad_norm": 8.750662803649902,
      "learning_rate": 4.524087591240876e-05,
      "loss": 2.09,
      "step": 13040
    },
    {
      "epoch": 95.25547445255475,
      "grad_norm": 7.364860534667969,
      "learning_rate": 4.523722627737226e-05,
      "loss": 1.8594,
      "step": 13050
    },
    {
      "epoch": 95.32846715328468,
      "grad_norm": 7.515598297119141,
      "learning_rate": 4.523357664233577e-05,
      "loss": 1.5749,
      "step": 13060
    },
    {
      "epoch": 95.4014598540146,
      "grad_norm": 7.4591898918151855,
      "learning_rate": 4.5229927007299275e-05,
      "loss": 1.8328,
      "step": 13070
    },
    {
      "epoch": 95.47445255474453,
      "grad_norm": 10.477828979492188,
      "learning_rate": 4.5226277372262776e-05,
      "loss": 1.8346,
      "step": 13080
    },
    {
      "epoch": 95.54744525547446,
      "grad_norm": 9.295780181884766,
      "learning_rate": 4.522262773722628e-05,
      "loss": 2.2094,
      "step": 13090
    },
    {
      "epoch": 95.62043795620438,
      "grad_norm": 9.384648323059082,
      "learning_rate": 4.5218978102189783e-05,
      "loss": 1.9896,
      "step": 13100
    },
    {
      "epoch": 95.69343065693431,
      "grad_norm": 8.99341869354248,
      "learning_rate": 4.521532846715329e-05,
      "loss": 2.0644,
      "step": 13110
    },
    {
      "epoch": 95.76642335766424,
      "grad_norm": 8.994447708129883,
      "learning_rate": 4.521167883211679e-05,
      "loss": 1.7128,
      "step": 13120
    },
    {
      "epoch": 95.83941605839416,
      "grad_norm": 9.599687576293945,
      "learning_rate": 4.520802919708029e-05,
      "loss": 2.0859,
      "step": 13130
    },
    {
      "epoch": 95.91240875912409,
      "grad_norm": 6.600605487823486,
      "learning_rate": 4.52043795620438e-05,
      "loss": 2.0939,
      "step": 13140
    },
    {
      "epoch": 95.98540145985402,
      "grad_norm": 7.730823040008545,
      "learning_rate": 4.52007299270073e-05,
      "loss": 2.1353,
      "step": 13150
    },
    {
      "epoch": 96.05839416058394,
      "grad_norm": 10.857349395751953,
      "learning_rate": 4.519708029197081e-05,
      "loss": 2.2083,
      "step": 13160
    },
    {
      "epoch": 96.13138686131387,
      "grad_norm": 5.587101936340332,
      "learning_rate": 4.519343065693431e-05,
      "loss": 2.1609,
      "step": 13170
    },
    {
      "epoch": 96.2043795620438,
      "grad_norm": 11.014532089233398,
      "learning_rate": 4.5189781021897815e-05,
      "loss": 1.767,
      "step": 13180
    },
    {
      "epoch": 96.27737226277372,
      "grad_norm": 13.483600616455078,
      "learning_rate": 4.5186131386861315e-05,
      "loss": 1.8247,
      "step": 13190
    },
    {
      "epoch": 96.35036496350365,
      "grad_norm": 6.908823490142822,
      "learning_rate": 4.5182481751824815e-05,
      "loss": 1.829,
      "step": 13200
    },
    {
      "epoch": 96.42335766423358,
      "grad_norm": 8.737764358520508,
      "learning_rate": 4.517883211678832e-05,
      "loss": 1.9662,
      "step": 13210
    },
    {
      "epoch": 96.4963503649635,
      "grad_norm": 7.304510116577148,
      "learning_rate": 4.517518248175182e-05,
      "loss": 2.2284,
      "step": 13220
    },
    {
      "epoch": 96.56934306569343,
      "grad_norm": 8.627935409545898,
      "learning_rate": 4.517153284671533e-05,
      "loss": 1.9954,
      "step": 13230
    },
    {
      "epoch": 96.64233576642336,
      "grad_norm": 9.227214813232422,
      "learning_rate": 4.516788321167884e-05,
      "loss": 2.0508,
      "step": 13240
    },
    {
      "epoch": 96.71532846715328,
      "grad_norm": 6.9181389808654785,
      "learning_rate": 4.516423357664234e-05,
      "loss": 1.9903,
      "step": 13250
    },
    {
      "epoch": 96.78832116788321,
      "grad_norm": 6.830414295196533,
      "learning_rate": 4.5160583941605846e-05,
      "loss": 1.9129,
      "step": 13260
    },
    {
      "epoch": 96.86131386861314,
      "grad_norm": 9.526597023010254,
      "learning_rate": 4.5156934306569346e-05,
      "loss": 1.6603,
      "step": 13270
    },
    {
      "epoch": 96.93430656934306,
      "grad_norm": 10.659797668457031,
      "learning_rate": 4.5153284671532847e-05,
      "loss": 2.0841,
      "step": 13280
    },
    {
      "epoch": 97.00729927007299,
      "grad_norm": 8.294817924499512,
      "learning_rate": 4.5149635036496354e-05,
      "loss": 2.4467,
      "step": 13290
    },
    {
      "epoch": 97.08029197080292,
      "grad_norm": 10.838866233825684,
      "learning_rate": 4.5145985401459854e-05,
      "loss": 2.1384,
      "step": 13300
    },
    {
      "epoch": 97.15328467153284,
      "grad_norm": 7.410917282104492,
      "learning_rate": 4.514233576642336e-05,
      "loss": 1.6258,
      "step": 13310
    },
    {
      "epoch": 97.22627737226277,
      "grad_norm": 12.46248722076416,
      "learning_rate": 4.513868613138686e-05,
      "loss": 1.8294,
      "step": 13320
    },
    {
      "epoch": 97.2992700729927,
      "grad_norm": 7.670097351074219,
      "learning_rate": 4.513503649635037e-05,
      "loss": 1.8068,
      "step": 13330
    },
    {
      "epoch": 97.37226277372262,
      "grad_norm": 9.323479652404785,
      "learning_rate": 4.513138686131388e-05,
      "loss": 2.1339,
      "step": 13340
    },
    {
      "epoch": 97.44525547445255,
      "grad_norm": 6.298277378082275,
      "learning_rate": 4.512773722627737e-05,
      "loss": 1.8755,
      "step": 13350
    },
    {
      "epoch": 97.51824817518248,
      "grad_norm": 11.412766456604004,
      "learning_rate": 4.512408759124088e-05,
      "loss": 1.7446,
      "step": 13360
    },
    {
      "epoch": 97.5912408759124,
      "grad_norm": 10.339167594909668,
      "learning_rate": 4.512043795620438e-05,
      "loss": 1.8526,
      "step": 13370
    },
    {
      "epoch": 97.66423357664233,
      "grad_norm": 8.909931182861328,
      "learning_rate": 4.5116788321167885e-05,
      "loss": 2.2022,
      "step": 13380
    },
    {
      "epoch": 97.73722627737226,
      "grad_norm": 7.802490234375,
      "learning_rate": 4.511313868613139e-05,
      "loss": 1.8157,
      "step": 13390
    },
    {
      "epoch": 97.81021897810218,
      "grad_norm": 8.610011100769043,
      "learning_rate": 4.510948905109489e-05,
      "loss": 1.9744,
      "step": 13400
    },
    {
      "epoch": 97.88321167883211,
      "grad_norm": 7.428300380706787,
      "learning_rate": 4.51058394160584e-05,
      "loss": 2.2488,
      "step": 13410
    },
    {
      "epoch": 97.95620437956204,
      "grad_norm": 9.285191535949707,
      "learning_rate": 4.5102189781021894e-05,
      "loss": 2.0215,
      "step": 13420
    },
    {
      "epoch": 98.02919708029196,
      "grad_norm": 8.017374992370605,
      "learning_rate": 4.50985401459854e-05,
      "loss": 1.7753,
      "step": 13430
    },
    {
      "epoch": 98.10218978102189,
      "grad_norm": 11.182353973388672,
      "learning_rate": 4.509489051094891e-05,
      "loss": 2.0412,
      "step": 13440
    },
    {
      "epoch": 98.17518248175182,
      "grad_norm": 12.503250122070312,
      "learning_rate": 4.509124087591241e-05,
      "loss": 2.0027,
      "step": 13450
    },
    {
      "epoch": 98.24817518248175,
      "grad_norm": 7.6887526512146,
      "learning_rate": 4.5087591240875916e-05,
      "loss": 1.8407,
      "step": 13460
    },
    {
      "epoch": 98.32116788321167,
      "grad_norm": 8.160327911376953,
      "learning_rate": 4.508394160583942e-05,
      "loss": 1.9848,
      "step": 13470
    },
    {
      "epoch": 98.39416058394161,
      "grad_norm": 8.514440536499023,
      "learning_rate": 4.5080291970802924e-05,
      "loss": 1.8198,
      "step": 13480
    },
    {
      "epoch": 98.46715328467154,
      "grad_norm": 9.263225555419922,
      "learning_rate": 4.5076642335766425e-05,
      "loss": 2.0105,
      "step": 13490
    },
    {
      "epoch": 98.54014598540147,
      "grad_norm": 8.232451438903809,
      "learning_rate": 4.5072992700729925e-05,
      "loss": 1.6923,
      "step": 13500
    },
    {
      "epoch": 98.61313868613139,
      "grad_norm": 10.138444900512695,
      "learning_rate": 4.506934306569343e-05,
      "loss": 2.0695,
      "step": 13510
    },
    {
      "epoch": 98.68613138686132,
      "grad_norm": 6.4130024909973145,
      "learning_rate": 4.506569343065693e-05,
      "loss": 1.7212,
      "step": 13520
    },
    {
      "epoch": 98.75912408759125,
      "grad_norm": 8.908543586730957,
      "learning_rate": 4.506204379562044e-05,
      "loss": 2.278,
      "step": 13530
    },
    {
      "epoch": 98.83211678832117,
      "grad_norm": 9.458405494689941,
      "learning_rate": 4.505839416058395e-05,
      "loss": 1.9888,
      "step": 13540
    },
    {
      "epoch": 98.9051094890511,
      "grad_norm": 10.12101936340332,
      "learning_rate": 4.505474452554745e-05,
      "loss": 1.8723,
      "step": 13550
    },
    {
      "epoch": 98.97810218978103,
      "grad_norm": 8.59935474395752,
      "learning_rate": 4.5051094890510955e-05,
      "loss": 1.8024,
      "step": 13560
    },
    {
      "epoch": 99.05109489051095,
      "grad_norm": 11.525803565979004,
      "learning_rate": 4.5047445255474456e-05,
      "loss": 1.801,
      "step": 13570
    },
    {
      "epoch": 99.12408759124088,
      "grad_norm": 11.0022611618042,
      "learning_rate": 4.5043795620437956e-05,
      "loss": 1.9505,
      "step": 13580
    },
    {
      "epoch": 99.1970802919708,
      "grad_norm": 9.600676536560059,
      "learning_rate": 4.5040145985401464e-05,
      "loss": 2.0658,
      "step": 13590
    },
    {
      "epoch": 99.27007299270073,
      "grad_norm": 11.148439407348633,
      "learning_rate": 4.5036496350364964e-05,
      "loss": 1.9809,
      "step": 13600
    },
    {
      "epoch": 99.34306569343066,
      "grad_norm": 7.352169990539551,
      "learning_rate": 4.503284671532847e-05,
      "loss": 1.9263,
      "step": 13610
    },
    {
      "epoch": 99.41605839416059,
      "grad_norm": 9.202115058898926,
      "learning_rate": 4.502919708029197e-05,
      "loss": 1.8052,
      "step": 13620
    },
    {
      "epoch": 99.48905109489051,
      "grad_norm": 7.819957256317139,
      "learning_rate": 4.502554744525548e-05,
      "loss": 1.8097,
      "step": 13630
    },
    {
      "epoch": 99.56204379562044,
      "grad_norm": 7.932864189147949,
      "learning_rate": 4.502189781021898e-05,
      "loss": 2.1242,
      "step": 13640
    },
    {
      "epoch": 99.63503649635037,
      "grad_norm": 7.762574672698975,
      "learning_rate": 4.501824817518248e-05,
      "loss": 1.7005,
      "step": 13650
    },
    {
      "epoch": 99.7080291970803,
      "grad_norm": 6.118496894836426,
      "learning_rate": 4.501459854014599e-05,
      "loss": 1.8335,
      "step": 13660
    },
    {
      "epoch": 99.78102189781022,
      "grad_norm": 11.126506805419922,
      "learning_rate": 4.501094890510949e-05,
      "loss": 1.7733,
      "step": 13670
    },
    {
      "epoch": 99.85401459854015,
      "grad_norm": 8.673826217651367,
      "learning_rate": 4.5007299270072995e-05,
      "loss": 1.6788,
      "step": 13680
    },
    {
      "epoch": 99.92700729927007,
      "grad_norm": 8.860477447509766,
      "learning_rate": 4.5003649635036496e-05,
      "loss": 2.0857,
      "step": 13690
    },
    {
      "epoch": 100.0,
      "grad_norm": 13.346271514892578,
      "learning_rate": 4.5e-05,
      "loss": 2.1134,
      "step": 13700
    },
    {
      "epoch": 100.07299270072993,
      "grad_norm": 6.17230224609375,
      "learning_rate": 4.499635036496351e-05,
      "loss": 1.7388,
      "step": 13710
    },
    {
      "epoch": 100.14598540145985,
      "grad_norm": 9.586466789245605,
      "learning_rate": 4.499270072992701e-05,
      "loss": 1.9037,
      "step": 13720
    },
    {
      "epoch": 100.21897810218978,
      "grad_norm": 8.53075122833252,
      "learning_rate": 4.498905109489051e-05,
      "loss": 1.6794,
      "step": 13730
    },
    {
      "epoch": 100.2919708029197,
      "grad_norm": 10.50554370880127,
      "learning_rate": 4.498540145985402e-05,
      "loss": 1.6511,
      "step": 13740
    },
    {
      "epoch": 100.36496350364963,
      "grad_norm": 7.44916296005249,
      "learning_rate": 4.498175182481752e-05,
      "loss": 1.9184,
      "step": 13750
    },
    {
      "epoch": 100.43795620437956,
      "grad_norm": 12.591179847717285,
      "learning_rate": 4.4978102189781026e-05,
      "loss": 2.0739,
      "step": 13760
    },
    {
      "epoch": 100.51094890510949,
      "grad_norm": 7.376893520355225,
      "learning_rate": 4.497445255474453e-05,
      "loss": 1.8424,
      "step": 13770
    },
    {
      "epoch": 100.58394160583941,
      "grad_norm": 7.964250564575195,
      "learning_rate": 4.4970802919708034e-05,
      "loss": 2.2291,
      "step": 13780
    },
    {
      "epoch": 100.65693430656934,
      "grad_norm": 8.905165672302246,
      "learning_rate": 4.4967153284671534e-05,
      "loss": 1.8952,
      "step": 13790
    },
    {
      "epoch": 100.72992700729927,
      "grad_norm": 9.140936851501465,
      "learning_rate": 4.4963503649635035e-05,
      "loss": 1.7991,
      "step": 13800
    },
    {
      "epoch": 100.8029197080292,
      "grad_norm": 9.141054153442383,
      "learning_rate": 4.495985401459854e-05,
      "loss": 1.7136,
      "step": 13810
    },
    {
      "epoch": 100.87591240875912,
      "grad_norm": 9.295843124389648,
      "learning_rate": 4.495620437956204e-05,
      "loss": 2.1771,
      "step": 13820
    },
    {
      "epoch": 100.94890510948905,
      "grad_norm": 10.002490043640137,
      "learning_rate": 4.495255474452555e-05,
      "loss": 1.6886,
      "step": 13830
    },
    {
      "epoch": 101.02189781021897,
      "grad_norm": 8.691564559936523,
      "learning_rate": 4.494890510948905e-05,
      "loss": 1.7799,
      "step": 13840
    },
    {
      "epoch": 101.0948905109489,
      "grad_norm": 8.297396659851074,
      "learning_rate": 4.494525547445256e-05,
      "loss": 1.9655,
      "step": 13850
    },
    {
      "epoch": 101.16788321167883,
      "grad_norm": 9.41586971282959,
      "learning_rate": 4.4941605839416065e-05,
      "loss": 1.7182,
      "step": 13860
    },
    {
      "epoch": 101.24087591240875,
      "grad_norm": 6.913358688354492,
      "learning_rate": 4.4937956204379565e-05,
      "loss": 1.5379,
      "step": 13870
    },
    {
      "epoch": 101.31386861313868,
      "grad_norm": 9.062111854553223,
      "learning_rate": 4.4934306569343066e-05,
      "loss": 1.9368,
      "step": 13880
    },
    {
      "epoch": 101.38686131386861,
      "grad_norm": 8.760687828063965,
      "learning_rate": 4.4930656934306566e-05,
      "loss": 1.5885,
      "step": 13890
    },
    {
      "epoch": 101.45985401459853,
      "grad_norm": 9.395820617675781,
      "learning_rate": 4.4927007299270074e-05,
      "loss": 1.8477,
      "step": 13900
    },
    {
      "epoch": 101.53284671532846,
      "grad_norm": 10.441774368286133,
      "learning_rate": 4.492335766423358e-05,
      "loss": 2.2206,
      "step": 13910
    },
    {
      "epoch": 101.60583941605839,
      "grad_norm": 9.876200675964355,
      "learning_rate": 4.491970802919708e-05,
      "loss": 1.9035,
      "step": 13920
    },
    {
      "epoch": 101.67883211678833,
      "grad_norm": 11.875004768371582,
      "learning_rate": 4.491605839416059e-05,
      "loss": 1.79,
      "step": 13930
    },
    {
      "epoch": 101.75182481751825,
      "grad_norm": 10.392016410827637,
      "learning_rate": 4.491240875912409e-05,
      "loss": 1.908,
      "step": 13940
    },
    {
      "epoch": 101.82481751824818,
      "grad_norm": 9.771927833557129,
      "learning_rate": 4.4908759124087597e-05,
      "loss": 1.945,
      "step": 13950
    },
    {
      "epoch": 101.89781021897811,
      "grad_norm": 7.294882774353027,
      "learning_rate": 4.49051094890511e-05,
      "loss": 2.1187,
      "step": 13960
    },
    {
      "epoch": 101.97080291970804,
      "grad_norm": 12.268325805664062,
      "learning_rate": 4.49014598540146e-05,
      "loss": 2.1733,
      "step": 13970
    },
    {
      "epoch": 102.04379562043796,
      "grad_norm": 9.721158981323242,
      "learning_rate": 4.4897810218978105e-05,
      "loss": 1.7213,
      "step": 13980
    },
    {
      "epoch": 102.11678832116789,
      "grad_norm": 7.8062310218811035,
      "learning_rate": 4.4894160583941605e-05,
      "loss": 1.4605,
      "step": 13990
    },
    {
      "epoch": 102.18978102189782,
      "grad_norm": 8.946623802185059,
      "learning_rate": 4.489051094890511e-05,
      "loss": 1.752,
      "step": 14000
    },
    {
      "epoch": 102.26277372262774,
      "grad_norm": 7.86249303817749,
      "learning_rate": 4.488686131386862e-05,
      "loss": 1.9933,
      "step": 14010
    },
    {
      "epoch": 102.33576642335767,
      "grad_norm": 8.240409851074219,
      "learning_rate": 4.488321167883212e-05,
      "loss": 2.0094,
      "step": 14020
    },
    {
      "epoch": 102.4087591240876,
      "grad_norm": 8.779581069946289,
      "learning_rate": 4.487956204379562e-05,
      "loss": 2.1819,
      "step": 14030
    },
    {
      "epoch": 102.48175182481752,
      "grad_norm": 10.154329299926758,
      "learning_rate": 4.487591240875912e-05,
      "loss": 1.8872,
      "step": 14040
    },
    {
      "epoch": 102.55474452554745,
      "grad_norm": 7.527287006378174,
      "learning_rate": 4.487226277372263e-05,
      "loss": 1.7751,
      "step": 14050
    },
    {
      "epoch": 102.62773722627738,
      "grad_norm": 7.0948166847229,
      "learning_rate": 4.4868613138686136e-05,
      "loss": 1.8475,
      "step": 14060
    },
    {
      "epoch": 102.7007299270073,
      "grad_norm": 8.758254051208496,
      "learning_rate": 4.4864963503649636e-05,
      "loss": 2.0396,
      "step": 14070
    },
    {
      "epoch": 102.77372262773723,
      "grad_norm": 8.359235763549805,
      "learning_rate": 4.4861313868613144e-05,
      "loss": 1.7636,
      "step": 14080
    },
    {
      "epoch": 102.84671532846716,
      "grad_norm": 9.304901123046875,
      "learning_rate": 4.4857664233576644e-05,
      "loss": 2.0696,
      "step": 14090
    },
    {
      "epoch": 102.91970802919708,
      "grad_norm": 9.314889907836914,
      "learning_rate": 4.485401459854015e-05,
      "loss": 1.5584,
      "step": 14100
    },
    {
      "epoch": 102.99270072992701,
      "grad_norm": 8.4660005569458,
      "learning_rate": 4.485036496350365e-05,
      "loss": 1.8159,
      "step": 14110
    },
    {
      "epoch": 103.06569343065694,
      "grad_norm": 4.975253582000732,
      "learning_rate": 4.484671532846715e-05,
      "loss": 1.8338,
      "step": 14120
    },
    {
      "epoch": 103.13868613138686,
      "grad_norm": 7.970245361328125,
      "learning_rate": 4.484306569343066e-05,
      "loss": 1.8118,
      "step": 14130
    },
    {
      "epoch": 103.21167883211679,
      "grad_norm": 9.371699333190918,
      "learning_rate": 4.483941605839416e-05,
      "loss": 2.2124,
      "step": 14140
    },
    {
      "epoch": 103.28467153284672,
      "grad_norm": 11.19807243347168,
      "learning_rate": 4.483576642335767e-05,
      "loss": 1.5926,
      "step": 14150
    },
    {
      "epoch": 103.35766423357664,
      "grad_norm": 8.562725067138672,
      "learning_rate": 4.483211678832117e-05,
      "loss": 2.0104,
      "step": 14160
    },
    {
      "epoch": 103.43065693430657,
      "grad_norm": 7.285326957702637,
      "learning_rate": 4.4828467153284675e-05,
      "loss": 1.6547,
      "step": 14170
    },
    {
      "epoch": 103.5036496350365,
      "grad_norm": 7.383859634399414,
      "learning_rate": 4.482481751824818e-05,
      "loss": 1.8962,
      "step": 14180
    },
    {
      "epoch": 103.57664233576642,
      "grad_norm": 9.269935607910156,
      "learning_rate": 4.4821167883211676e-05,
      "loss": 1.9304,
      "step": 14190
    },
    {
      "epoch": 103.64963503649635,
      "grad_norm": 6.945948600769043,
      "learning_rate": 4.4817518248175183e-05,
      "loss": 1.7178,
      "step": 14200
    },
    {
      "epoch": 103.72262773722628,
      "grad_norm": 7.2307915687561035,
      "learning_rate": 4.481386861313869e-05,
      "loss": 1.6657,
      "step": 14210
    },
    {
      "epoch": 103.7956204379562,
      "grad_norm": 5.812793254852295,
      "learning_rate": 4.481021897810219e-05,
      "loss": 1.321,
      "step": 14220
    },
    {
      "epoch": 103.86861313868613,
      "grad_norm": 7.528883457183838,
      "learning_rate": 4.48065693430657e-05,
      "loss": 1.8473,
      "step": 14230
    },
    {
      "epoch": 103.94160583941606,
      "grad_norm": 10.762690544128418,
      "learning_rate": 4.48029197080292e-05,
      "loss": 2.2027,
      "step": 14240
    },
    {
      "epoch": 104.01459854014598,
      "grad_norm": 9.123373031616211,
      "learning_rate": 4.4799270072992706e-05,
      "loss": 2.0484,
      "step": 14250
    },
    {
      "epoch": 104.08759124087591,
      "grad_norm": 7.144225120544434,
      "learning_rate": 4.479562043795621e-05,
      "loss": 1.6068,
      "step": 14260
    },
    {
      "epoch": 104.16058394160584,
      "grad_norm": 9.457913398742676,
      "learning_rate": 4.479197080291971e-05,
      "loss": 1.7217,
      "step": 14270
    },
    {
      "epoch": 104.23357664233576,
      "grad_norm": 8.000064849853516,
      "learning_rate": 4.4788321167883214e-05,
      "loss": 1.7071,
      "step": 14280
    },
    {
      "epoch": 104.30656934306569,
      "grad_norm": 7.967243671417236,
      "learning_rate": 4.4784671532846715e-05,
      "loss": 1.6257,
      "step": 14290
    },
    {
      "epoch": 104.37956204379562,
      "grad_norm": 5.5991034507751465,
      "learning_rate": 4.478102189781022e-05,
      "loss": 1.7831,
      "step": 14300
    },
    {
      "epoch": 104.45255474452554,
      "grad_norm": 7.934085845947266,
      "learning_rate": 4.477737226277372e-05,
      "loss": 1.7969,
      "step": 14310
    },
    {
      "epoch": 104.52554744525547,
      "grad_norm": 13.10136890411377,
      "learning_rate": 4.477372262773723e-05,
      "loss": 2.2596,
      "step": 14320
    },
    {
      "epoch": 104.5985401459854,
      "grad_norm": 8.202381134033203,
      "learning_rate": 4.477007299270074e-05,
      "loss": 1.6867,
      "step": 14330
    },
    {
      "epoch": 104.67153284671532,
      "grad_norm": 17.358623504638672,
      "learning_rate": 4.476642335766423e-05,
      "loss": 2.0635,
      "step": 14340
    },
    {
      "epoch": 104.74452554744525,
      "grad_norm": 8.408138275146484,
      "learning_rate": 4.476277372262774e-05,
      "loss": 1.6479,
      "step": 14350
    },
    {
      "epoch": 104.81751824817518,
      "grad_norm": 9.692716598510742,
      "learning_rate": 4.475912408759124e-05,
      "loss": 1.8006,
      "step": 14360
    },
    {
      "epoch": 104.8905109489051,
      "grad_norm": 9.145973205566406,
      "learning_rate": 4.4755474452554746e-05,
      "loss": 1.9253,
      "step": 14370
    },
    {
      "epoch": 104.96350364963503,
      "grad_norm": 8.713685035705566,
      "learning_rate": 4.475182481751825e-05,
      "loss": 1.7613,
      "step": 14380
    },
    {
      "epoch": 105.03649635036497,
      "grad_norm": 11.388829231262207,
      "learning_rate": 4.4748175182481754e-05,
      "loss": 2.0728,
      "step": 14390
    },
    {
      "epoch": 105.1094890510949,
      "grad_norm": 7.3456878662109375,
      "learning_rate": 4.474452554744526e-05,
      "loss": 1.4713,
      "step": 14400
    },
    {
      "epoch": 105.18248175182482,
      "grad_norm": 7.9025187492370605,
      "learning_rate": 4.474087591240876e-05,
      "loss": 1.4855,
      "step": 14410
    },
    {
      "epoch": 105.25547445255475,
      "grad_norm": 8.485091209411621,
      "learning_rate": 4.473722627737226e-05,
      "loss": 2.0001,
      "step": 14420
    },
    {
      "epoch": 105.32846715328468,
      "grad_norm": 10.655559539794922,
      "learning_rate": 4.473357664233577e-05,
      "loss": 1.9782,
      "step": 14430
    },
    {
      "epoch": 105.4014598540146,
      "grad_norm": 7.625552654266357,
      "learning_rate": 4.472992700729927e-05,
      "loss": 1.6201,
      "step": 14440
    },
    {
      "epoch": 105.47445255474453,
      "grad_norm": 9.215340614318848,
      "learning_rate": 4.472627737226278e-05,
      "loss": 1.8005,
      "step": 14450
    },
    {
      "epoch": 105.54744525547446,
      "grad_norm": 10.0902099609375,
      "learning_rate": 4.472262773722628e-05,
      "loss": 1.7284,
      "step": 14460
    },
    {
      "epoch": 105.62043795620438,
      "grad_norm": 7.372303009033203,
      "learning_rate": 4.4718978102189785e-05,
      "loss": 1.8323,
      "step": 14470
    },
    {
      "epoch": 105.69343065693431,
      "grad_norm": 7.371627330780029,
      "learning_rate": 4.471532846715329e-05,
      "loss": 1.6894,
      "step": 14480
    },
    {
      "epoch": 105.76642335766424,
      "grad_norm": 7.523766994476318,
      "learning_rate": 4.4711678832116786e-05,
      "loss": 1.4074,
      "step": 14490
    },
    {
      "epoch": 105.83941605839416,
      "grad_norm": 7.93372917175293,
      "learning_rate": 4.470802919708029e-05,
      "loss": 2.1119,
      "step": 14500
    },
    {
      "epoch": 105.91240875912409,
      "grad_norm": 9.478010177612305,
      "learning_rate": 4.4704379562043794e-05,
      "loss": 2.114,
      "step": 14510
    },
    {
      "epoch": 105.98540145985402,
      "grad_norm": 8.114380836486816,
      "learning_rate": 4.47007299270073e-05,
      "loss": 2.1174,
      "step": 14520
    },
    {
      "epoch": 106.05839416058394,
      "grad_norm": 8.32519245147705,
      "learning_rate": 4.469708029197081e-05,
      "loss": 1.9296,
      "step": 14530
    },
    {
      "epoch": 106.13138686131387,
      "grad_norm": 11.74579906463623,
      "learning_rate": 4.469343065693431e-05,
      "loss": 1.8892,
      "step": 14540
    },
    {
      "epoch": 106.2043795620438,
      "grad_norm": 11.752994537353516,
      "learning_rate": 4.4689781021897816e-05,
      "loss": 1.9244,
      "step": 14550
    },
    {
      "epoch": 106.27737226277372,
      "grad_norm": 9.765296936035156,
      "learning_rate": 4.4686131386861316e-05,
      "loss": 1.6883,
      "step": 14560
    },
    {
      "epoch": 106.35036496350365,
      "grad_norm": 10.259687423706055,
      "learning_rate": 4.468248175182482e-05,
      "loss": 1.8179,
      "step": 14570
    },
    {
      "epoch": 106.42335766423358,
      "grad_norm": 8.952404975891113,
      "learning_rate": 4.4678832116788324e-05,
      "loss": 1.5996,
      "step": 14580
    },
    {
      "epoch": 106.4963503649635,
      "grad_norm": 7.221264839172363,
      "learning_rate": 4.4675182481751825e-05,
      "loss": 1.8679,
      "step": 14590
    },
    {
      "epoch": 106.56934306569343,
      "grad_norm": 6.172835826873779,
      "learning_rate": 4.467153284671533e-05,
      "loss": 1.7881,
      "step": 14600
    },
    {
      "epoch": 106.64233576642336,
      "grad_norm": 9.400410652160645,
      "learning_rate": 4.466788321167883e-05,
      "loss": 1.3566,
      "step": 14610
    },
    {
      "epoch": 106.71532846715328,
      "grad_norm": 9.670136451721191,
      "learning_rate": 4.466423357664234e-05,
      "loss": 1.961,
      "step": 14620
    },
    {
      "epoch": 106.78832116788321,
      "grad_norm": 10.591642379760742,
      "learning_rate": 4.466058394160585e-05,
      "loss": 2.0231,
      "step": 14630
    },
    {
      "epoch": 106.86131386861314,
      "grad_norm": 7.887257099151611,
      "learning_rate": 4.465693430656935e-05,
      "loss": 1.729,
      "step": 14640
    },
    {
      "epoch": 106.93430656934306,
      "grad_norm": 11.550492286682129,
      "learning_rate": 4.465328467153285e-05,
      "loss": 1.738,
      "step": 14650
    },
    {
      "epoch": 107.00729927007299,
      "grad_norm": 9.761780738830566,
      "learning_rate": 4.464963503649635e-05,
      "loss": 1.7551,
      "step": 14660
    },
    {
      "epoch": 107.08029197080292,
      "grad_norm": 7.79520845413208,
      "learning_rate": 4.4645985401459856e-05,
      "loss": 1.6973,
      "step": 14670
    },
    {
      "epoch": 107.15328467153284,
      "grad_norm": 9.097628593444824,
      "learning_rate": 4.464233576642336e-05,
      "loss": 1.8347,
      "step": 14680
    },
    {
      "epoch": 107.22627737226277,
      "grad_norm": 8.575913429260254,
      "learning_rate": 4.4638686131386863e-05,
      "loss": 1.8846,
      "step": 14690
    },
    {
      "epoch": 107.2992700729927,
      "grad_norm": 11.171202659606934,
      "learning_rate": 4.463503649635037e-05,
      "loss": 1.8475,
      "step": 14700
    },
    {
      "epoch": 107.37226277372262,
      "grad_norm": 9.54670524597168,
      "learning_rate": 4.463138686131387e-05,
      "loss": 1.7676,
      "step": 14710
    },
    {
      "epoch": 107.44525547445255,
      "grad_norm": 6.705810070037842,
      "learning_rate": 4.462773722627737e-05,
      "loss": 1.5983,
      "step": 14720
    },
    {
      "epoch": 107.51824817518248,
      "grad_norm": 10.426926612854004,
      "learning_rate": 4.462408759124088e-05,
      "loss": 1.9962,
      "step": 14730
    },
    {
      "epoch": 107.5912408759124,
      "grad_norm": 8.461385726928711,
      "learning_rate": 4.462043795620438e-05,
      "loss": 1.6324,
      "step": 14740
    },
    {
      "epoch": 107.66423357664233,
      "grad_norm": 8.539246559143066,
      "learning_rate": 4.461678832116789e-05,
      "loss": 1.3378,
      "step": 14750
    },
    {
      "epoch": 107.73722627737226,
      "grad_norm": 13.995347023010254,
      "learning_rate": 4.461313868613139e-05,
      "loss": 1.7971,
      "step": 14760
    },
    {
      "epoch": 107.81021897810218,
      "grad_norm": 6.617910385131836,
      "learning_rate": 4.4609489051094895e-05,
      "loss": 1.7506,
      "step": 14770
    },
    {
      "epoch": 107.88321167883211,
      "grad_norm": 13.425054550170898,
      "learning_rate": 4.4605839416058395e-05,
      "loss": 1.4768,
      "step": 14780
    },
    {
      "epoch": 107.95620437956204,
      "grad_norm": 8.286999702453613,
      "learning_rate": 4.46021897810219e-05,
      "loss": 2.0289,
      "step": 14790
    },
    {
      "epoch": 108.02919708029196,
      "grad_norm": 9.641839981079102,
      "learning_rate": 4.45985401459854e-05,
      "loss": 1.7579,
      "step": 14800
    },
    {
      "epoch": 108.10218978102189,
      "grad_norm": 8.578736305236816,
      "learning_rate": 4.45948905109489e-05,
      "loss": 1.8662,
      "step": 14810
    },
    {
      "epoch": 108.17518248175182,
      "grad_norm": 7.9721360206604,
      "learning_rate": 4.459124087591241e-05,
      "loss": 1.2258,
      "step": 14820
    },
    {
      "epoch": 108.24817518248175,
      "grad_norm": 6.447525978088379,
      "learning_rate": 4.458759124087591e-05,
      "loss": 1.7982,
      "step": 14830
    },
    {
      "epoch": 108.32116788321167,
      "grad_norm": 7.7821173667907715,
      "learning_rate": 4.458394160583942e-05,
      "loss": 1.684,
      "step": 14840
    },
    {
      "epoch": 108.39416058394161,
      "grad_norm": 11.052127838134766,
      "learning_rate": 4.4580291970802926e-05,
      "loss": 1.6128,
      "step": 14850
    },
    {
      "epoch": 108.46715328467154,
      "grad_norm": 6.926581859588623,
      "learning_rate": 4.4576642335766426e-05,
      "loss": 1.8872,
      "step": 14860
    },
    {
      "epoch": 108.54014598540147,
      "grad_norm": 8.49096965789795,
      "learning_rate": 4.4572992700729927e-05,
      "loss": 1.818,
      "step": 14870
    },
    {
      "epoch": 108.61313868613139,
      "grad_norm": 8.066861152648926,
      "learning_rate": 4.4569343065693434e-05,
      "loss": 1.7384,
      "step": 14880
    },
    {
      "epoch": 108.68613138686132,
      "grad_norm": 8.059194564819336,
      "learning_rate": 4.4565693430656934e-05,
      "loss": 1.4908,
      "step": 14890
    },
    {
      "epoch": 108.75912408759125,
      "grad_norm": 9.382063865661621,
      "learning_rate": 4.456204379562044e-05,
      "loss": 1.8308,
      "step": 14900
    },
    {
      "epoch": 108.83211678832117,
      "grad_norm": 6.976299285888672,
      "learning_rate": 4.455839416058394e-05,
      "loss": 1.9165,
      "step": 14910
    },
    {
      "epoch": 108.9051094890511,
      "grad_norm": 8.883035659790039,
      "learning_rate": 4.455474452554745e-05,
      "loss": 1.8194,
      "step": 14920
    },
    {
      "epoch": 108.97810218978103,
      "grad_norm": 10.717549324035645,
      "learning_rate": 4.455109489051095e-05,
      "loss": 1.6963,
      "step": 14930
    },
    {
      "epoch": 109.05109489051095,
      "grad_norm": 7.638457775115967,
      "learning_rate": 4.454744525547446e-05,
      "loss": 1.5118,
      "step": 14940
    },
    {
      "epoch": 109.12408759124088,
      "grad_norm": 8.027828216552734,
      "learning_rate": 4.454379562043796e-05,
      "loss": 1.9566,
      "step": 14950
    },
    {
      "epoch": 109.1970802919708,
      "grad_norm": 6.814172744750977,
      "learning_rate": 4.454014598540146e-05,
      "loss": 1.7581,
      "step": 14960
    },
    {
      "epoch": 109.27007299270073,
      "grad_norm": 13.01601505279541,
      "learning_rate": 4.4536496350364965e-05,
      "loss": 1.7111,
      "step": 14970
    },
    {
      "epoch": 109.34306569343066,
      "grad_norm": 8.046052932739258,
      "learning_rate": 4.4532846715328466e-05,
      "loss": 1.4215,
      "step": 14980
    },
    {
      "epoch": 109.41605839416059,
      "grad_norm": 10.65967082977295,
      "learning_rate": 4.452919708029197e-05,
      "loss": 1.6555,
      "step": 14990
    },
    {
      "epoch": 109.48905109489051,
      "grad_norm": 7.870738506317139,
      "learning_rate": 4.452554744525548e-05,
      "loss": 1.7294,
      "step": 15000
    },
    {
      "epoch": 109.56204379562044,
      "grad_norm": 8.888495445251465,
      "learning_rate": 4.452189781021898e-05,
      "loss": 1.5013,
      "step": 15010
    },
    {
      "epoch": 109.63503649635037,
      "grad_norm": 7.680357933044434,
      "learning_rate": 4.451824817518249e-05,
      "loss": 2.2028,
      "step": 15020
    },
    {
      "epoch": 109.7080291970803,
      "grad_norm": 7.215332984924316,
      "learning_rate": 4.451459854014598e-05,
      "loss": 1.8213,
      "step": 15030
    },
    {
      "epoch": 109.78102189781022,
      "grad_norm": 8.016324043273926,
      "learning_rate": 4.451094890510949e-05,
      "loss": 1.6285,
      "step": 15040
    },
    {
      "epoch": 109.85401459854015,
      "grad_norm": 7.504595756530762,
      "learning_rate": 4.4507299270072996e-05,
      "loss": 1.7805,
      "step": 15050
    },
    {
      "epoch": 109.92700729927007,
      "grad_norm": 8.650070190429688,
      "learning_rate": 4.45036496350365e-05,
      "loss": 1.6314,
      "step": 15060
    },
    {
      "epoch": 110.0,
      "grad_norm": 5.197783470153809,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.6366,
      "step": 15070
    },
    {
      "epoch": 110.07299270072993,
      "grad_norm": 7.7306060791015625,
      "learning_rate": 4.4496350364963505e-05,
      "loss": 1.5938,
      "step": 15080
    },
    {
      "epoch": 110.14598540145985,
      "grad_norm": 10.172487258911133,
      "learning_rate": 4.449270072992701e-05,
      "loss": 1.8733,
      "step": 15090
    },
    {
      "epoch": 110.21897810218978,
      "grad_norm": 7.48293399810791,
      "learning_rate": 4.448905109489051e-05,
      "loss": 1.8072,
      "step": 15100
    },
    {
      "epoch": 110.2919708029197,
      "grad_norm": 7.092833518981934,
      "learning_rate": 4.448540145985401e-05,
      "loss": 1.6684,
      "step": 15110
    },
    {
      "epoch": 110.36496350364963,
      "grad_norm": 9.203411102294922,
      "learning_rate": 4.448175182481752e-05,
      "loss": 2.0316,
      "step": 15120
    },
    {
      "epoch": 110.43795620437956,
      "grad_norm": 6.18344783782959,
      "learning_rate": 4.447810218978102e-05,
      "loss": 1.8543,
      "step": 15130
    },
    {
      "epoch": 110.51094890510949,
      "grad_norm": 8.172477722167969,
      "learning_rate": 4.447445255474453e-05,
      "loss": 1.6847,
      "step": 15140
    },
    {
      "epoch": 110.58394160583941,
      "grad_norm": 7.896505355834961,
      "learning_rate": 4.4470802919708035e-05,
      "loss": 2.1398,
      "step": 15150
    },
    {
      "epoch": 110.65693430656934,
      "grad_norm": 6.589114189147949,
      "learning_rate": 4.4467153284671536e-05,
      "loss": 1.6724,
      "step": 15160
    },
    {
      "epoch": 110.72992700729927,
      "grad_norm": 6.675487041473389,
      "learning_rate": 4.446350364963504e-05,
      "loss": 1.6137,
      "step": 15170
    },
    {
      "epoch": 110.8029197080292,
      "grad_norm": 9.248998641967773,
      "learning_rate": 4.445985401459854e-05,
      "loss": 1.6811,
      "step": 15180
    },
    {
      "epoch": 110.87591240875912,
      "grad_norm": 7.066266059875488,
      "learning_rate": 4.4456204379562044e-05,
      "loss": 1.2619,
      "step": 15190
    },
    {
      "epoch": 110.94890510948905,
      "grad_norm": 7.598845958709717,
      "learning_rate": 4.445255474452555e-05,
      "loss": 1.5377,
      "step": 15200
    },
    {
      "epoch": 111.02189781021897,
      "grad_norm": 8.137667655944824,
      "learning_rate": 4.444890510948905e-05,
      "loss": 1.4616,
      "step": 15210
    },
    {
      "epoch": 111.0948905109489,
      "grad_norm": 11.320157051086426,
      "learning_rate": 4.444525547445256e-05,
      "loss": 1.6644,
      "step": 15220
    },
    {
      "epoch": 111.16788321167883,
      "grad_norm": 10.847804069519043,
      "learning_rate": 4.444160583941606e-05,
      "loss": 1.7167,
      "step": 15230
    },
    {
      "epoch": 111.24087591240875,
      "grad_norm": 7.194634437561035,
      "learning_rate": 4.443795620437957e-05,
      "loss": 1.4926,
      "step": 15240
    },
    {
      "epoch": 111.31386861313868,
      "grad_norm": 6.657051086425781,
      "learning_rate": 4.443430656934307e-05,
      "loss": 1.5093,
      "step": 15250
    },
    {
      "epoch": 111.38686131386861,
      "grad_norm": 11.687232971191406,
      "learning_rate": 4.443065693430657e-05,
      "loss": 2.0182,
      "step": 15260
    },
    {
      "epoch": 111.45985401459853,
      "grad_norm": 6.7442169189453125,
      "learning_rate": 4.4427007299270075e-05,
      "loss": 1.9388,
      "step": 15270
    },
    {
      "epoch": 111.53284671532846,
      "grad_norm": 9.24905776977539,
      "learning_rate": 4.4423357664233576e-05,
      "loss": 2.0026,
      "step": 15280
    },
    {
      "epoch": 111.60583941605839,
      "grad_norm": 7.995850086212158,
      "learning_rate": 4.441970802919708e-05,
      "loss": 1.4429,
      "step": 15290
    },
    {
      "epoch": 111.67883211678833,
      "grad_norm": 9.69937515258789,
      "learning_rate": 4.441605839416059e-05,
      "loss": 1.4868,
      "step": 15300
    },
    {
      "epoch": 111.75182481751825,
      "grad_norm": 8.576991081237793,
      "learning_rate": 4.441240875912409e-05,
      "loss": 1.7859,
      "step": 15310
    },
    {
      "epoch": 111.82481751824818,
      "grad_norm": 8.593217849731445,
      "learning_rate": 4.44087591240876e-05,
      "loss": 1.8614,
      "step": 15320
    },
    {
      "epoch": 111.89781021897811,
      "grad_norm": 7.896897792816162,
      "learning_rate": 4.440510948905109e-05,
      "loss": 1.2809,
      "step": 15330
    },
    {
      "epoch": 111.97080291970804,
      "grad_norm": 8.311001777648926,
      "learning_rate": 4.44014598540146e-05,
      "loss": 1.8596,
      "step": 15340
    },
    {
      "epoch": 112.04379562043796,
      "grad_norm": 9.90404224395752,
      "learning_rate": 4.4397810218978106e-05,
      "loss": 1.9809,
      "step": 15350
    },
    {
      "epoch": 112.11678832116789,
      "grad_norm": 8.66572093963623,
      "learning_rate": 4.439416058394161e-05,
      "loss": 1.8722,
      "step": 15360
    },
    {
      "epoch": 112.18978102189782,
      "grad_norm": 7.466742038726807,
      "learning_rate": 4.4390510948905114e-05,
      "loss": 1.3717,
      "step": 15370
    },
    {
      "epoch": 112.26277372262774,
      "grad_norm": 7.155414581298828,
      "learning_rate": 4.4386861313868614e-05,
      "loss": 1.5486,
      "step": 15380
    },
    {
      "epoch": 112.33576642335767,
      "grad_norm": 7.125380039215088,
      "learning_rate": 4.438321167883212e-05,
      "loss": 1.4539,
      "step": 15390
    },
    {
      "epoch": 112.4087591240876,
      "grad_norm": 8.98560619354248,
      "learning_rate": 4.437956204379562e-05,
      "loss": 1.3718,
      "step": 15400
    },
    {
      "epoch": 112.48175182481752,
      "grad_norm": 10.23505973815918,
      "learning_rate": 4.437591240875912e-05,
      "loss": 1.6807,
      "step": 15410
    },
    {
      "epoch": 112.55474452554745,
      "grad_norm": 8.456011772155762,
      "learning_rate": 4.437226277372263e-05,
      "loss": 1.738,
      "step": 15420
    },
    {
      "epoch": 112.62773722627738,
      "grad_norm": 10.874242782592773,
      "learning_rate": 4.436861313868613e-05,
      "loss": 1.8338,
      "step": 15430
    },
    {
      "epoch": 112.7007299270073,
      "grad_norm": 12.848003387451172,
      "learning_rate": 4.436496350364964e-05,
      "loss": 1.6579,
      "step": 15440
    },
    {
      "epoch": 112.77372262773723,
      "grad_norm": 12.540519714355469,
      "learning_rate": 4.436131386861314e-05,
      "loss": 2.006,
      "step": 15450
    },
    {
      "epoch": 112.84671532846716,
      "grad_norm": 9.795267105102539,
      "learning_rate": 4.4357664233576645e-05,
      "loss": 1.7564,
      "step": 15460
    },
    {
      "epoch": 112.91970802919708,
      "grad_norm": 9.595829010009766,
      "learning_rate": 4.435401459854015e-05,
      "loss": 1.7245,
      "step": 15470
    },
    {
      "epoch": 112.99270072992701,
      "grad_norm": 11.355622291564941,
      "learning_rate": 4.435036496350365e-05,
      "loss": 1.7348,
      "step": 15480
    },
    {
      "epoch": 113.06569343065694,
      "grad_norm": 7.391946792602539,
      "learning_rate": 4.4346715328467154e-05,
      "loss": 1.7386,
      "step": 15490
    },
    {
      "epoch": 113.13868613138686,
      "grad_norm": 9.788451194763184,
      "learning_rate": 4.434306569343066e-05,
      "loss": 1.6851,
      "step": 15500
    },
    {
      "epoch": 113.21167883211679,
      "grad_norm": 7.44010066986084,
      "learning_rate": 4.433941605839416e-05,
      "loss": 1.8455,
      "step": 15510
    },
    {
      "epoch": 113.28467153284672,
      "grad_norm": 7.0964460372924805,
      "learning_rate": 4.433576642335767e-05,
      "loss": 1.5585,
      "step": 15520
    },
    {
      "epoch": 113.35766423357664,
      "grad_norm": 7.064988136291504,
      "learning_rate": 4.433211678832117e-05,
      "loss": 1.4073,
      "step": 15530
    },
    {
      "epoch": 113.43065693430657,
      "grad_norm": 7.244340896606445,
      "learning_rate": 4.4328467153284677e-05,
      "loss": 1.5618,
      "step": 15540
    },
    {
      "epoch": 113.5036496350365,
      "grad_norm": 8.123464584350586,
      "learning_rate": 4.432481751824818e-05,
      "loss": 1.9651,
      "step": 15550
    },
    {
      "epoch": 113.57664233576642,
      "grad_norm": 8.693683624267578,
      "learning_rate": 4.432116788321168e-05,
      "loss": 1.388,
      "step": 15560
    },
    {
      "epoch": 113.64963503649635,
      "grad_norm": 15.414015769958496,
      "learning_rate": 4.4317518248175185e-05,
      "loss": 1.5732,
      "step": 15570
    },
    {
      "epoch": 113.72262773722628,
      "grad_norm": 10.672205924987793,
      "learning_rate": 4.4313868613138685e-05,
      "loss": 1.9818,
      "step": 15580
    },
    {
      "epoch": 113.7956204379562,
      "grad_norm": 6.966463565826416,
      "learning_rate": 4.431021897810219e-05,
      "loss": 1.5119,
      "step": 15590
    },
    {
      "epoch": 113.86861313868613,
      "grad_norm": 16.944650650024414,
      "learning_rate": 4.430656934306569e-05,
      "loss": 1.8351,
      "step": 15600
    },
    {
      "epoch": 113.94160583941606,
      "grad_norm": 7.045033931732178,
      "learning_rate": 4.43029197080292e-05,
      "loss": 1.4166,
      "step": 15610
    },
    {
      "epoch": 114.01459854014598,
      "grad_norm": 8.984197616577148,
      "learning_rate": 4.429927007299271e-05,
      "loss": 1.8918,
      "step": 15620
    },
    {
      "epoch": 114.08759124087591,
      "grad_norm": 7.269629001617432,
      "learning_rate": 4.429562043795621e-05,
      "loss": 1.5506,
      "step": 15630
    },
    {
      "epoch": 114.16058394160584,
      "grad_norm": 8.630200386047363,
      "learning_rate": 4.429197080291971e-05,
      "loss": 1.8709,
      "step": 15640
    },
    {
      "epoch": 114.23357664233576,
      "grad_norm": 10.388851165771484,
      "learning_rate": 4.428832116788321e-05,
      "loss": 1.588,
      "step": 15650
    },
    {
      "epoch": 114.30656934306569,
      "grad_norm": 6.013396263122559,
      "learning_rate": 4.4284671532846716e-05,
      "loss": 1.7175,
      "step": 15660
    },
    {
      "epoch": 114.37956204379562,
      "grad_norm": 6.137138843536377,
      "learning_rate": 4.4281021897810224e-05,
      "loss": 1.4525,
      "step": 15670
    },
    {
      "epoch": 114.45255474452554,
      "grad_norm": 6.487769603729248,
      "learning_rate": 4.4277372262773724e-05,
      "loss": 1.6353,
      "step": 15680
    },
    {
      "epoch": 114.52554744525547,
      "grad_norm": 7.402571201324463,
      "learning_rate": 4.427372262773723e-05,
      "loss": 1.6526,
      "step": 15690
    },
    {
      "epoch": 114.5985401459854,
      "grad_norm": 10.196539878845215,
      "learning_rate": 4.427007299270073e-05,
      "loss": 1.8629,
      "step": 15700
    },
    {
      "epoch": 114.67153284671532,
      "grad_norm": 8.042014122009277,
      "learning_rate": 4.426642335766424e-05,
      "loss": 1.5389,
      "step": 15710
    },
    {
      "epoch": 114.74452554744525,
      "grad_norm": 7.355345249176025,
      "learning_rate": 4.426277372262774e-05,
      "loss": 1.7042,
      "step": 15720
    },
    {
      "epoch": 114.81751824817518,
      "grad_norm": 9.6629638671875,
      "learning_rate": 4.425912408759124e-05,
      "loss": 1.2704,
      "step": 15730
    },
    {
      "epoch": 114.8905109489051,
      "grad_norm": 14.393441200256348,
      "learning_rate": 4.425547445255475e-05,
      "loss": 1.8195,
      "step": 15740
    },
    {
      "epoch": 114.96350364963503,
      "grad_norm": 7.358463287353516,
      "learning_rate": 4.425182481751825e-05,
      "loss": 1.7068,
      "step": 15750
    },
    {
      "epoch": 115.03649635036497,
      "grad_norm": 9.844039916992188,
      "learning_rate": 4.4248175182481755e-05,
      "loss": 1.7116,
      "step": 15760
    },
    {
      "epoch": 115.1094890510949,
      "grad_norm": 7.069134712219238,
      "learning_rate": 4.424452554744526e-05,
      "loss": 1.3797,
      "step": 15770
    },
    {
      "epoch": 115.18248175182482,
      "grad_norm": 7.798666000366211,
      "learning_rate": 4.424087591240876e-05,
      "loss": 1.2422,
      "step": 15780
    },
    {
      "epoch": 115.25547445255475,
      "grad_norm": 6.495977878570557,
      "learning_rate": 4.4237226277372263e-05,
      "loss": 1.8628,
      "step": 15790
    },
    {
      "epoch": 115.32846715328468,
      "grad_norm": 7.77605676651001,
      "learning_rate": 4.4233576642335764e-05,
      "loss": 1.5572,
      "step": 15800
    },
    {
      "epoch": 115.4014598540146,
      "grad_norm": 10.702628135681152,
      "learning_rate": 4.422992700729927e-05,
      "loss": 1.929,
      "step": 15810
    },
    {
      "epoch": 115.47445255474453,
      "grad_norm": 8.251404762268066,
      "learning_rate": 4.422627737226278e-05,
      "loss": 1.6208,
      "step": 15820
    },
    {
      "epoch": 115.54744525547446,
      "grad_norm": 7.295353889465332,
      "learning_rate": 4.422262773722628e-05,
      "loss": 1.7801,
      "step": 15830
    },
    {
      "epoch": 115.62043795620438,
      "grad_norm": 6.58648157119751,
      "learning_rate": 4.4218978102189786e-05,
      "loss": 1.5924,
      "step": 15840
    },
    {
      "epoch": 115.69343065693431,
      "grad_norm": 7.347164154052734,
      "learning_rate": 4.421532846715329e-05,
      "loss": 1.5268,
      "step": 15850
    },
    {
      "epoch": 115.76642335766424,
      "grad_norm": 7.439708709716797,
      "learning_rate": 4.4211678832116794e-05,
      "loss": 1.5529,
      "step": 15860
    },
    {
      "epoch": 115.83941605839416,
      "grad_norm": 7.58172607421875,
      "learning_rate": 4.4208029197080294e-05,
      "loss": 1.5416,
      "step": 15870
    },
    {
      "epoch": 115.91240875912409,
      "grad_norm": 7.662848472595215,
      "learning_rate": 4.4204379562043795e-05,
      "loss": 1.8837,
      "step": 15880
    },
    {
      "epoch": 115.98540145985402,
      "grad_norm": 12.939644813537598,
      "learning_rate": 4.42007299270073e-05,
      "loss": 1.4984,
      "step": 15890
    },
    {
      "epoch": 116.05839416058394,
      "grad_norm": 10.216479301452637,
      "learning_rate": 4.41970802919708e-05,
      "loss": 2.1121,
      "step": 15900
    },
    {
      "epoch": 116.13138686131387,
      "grad_norm": 8.012547492980957,
      "learning_rate": 4.419343065693431e-05,
      "loss": 1.3915,
      "step": 15910
    },
    {
      "epoch": 116.2043795620438,
      "grad_norm": 6.568687438964844,
      "learning_rate": 4.418978102189781e-05,
      "loss": 1.6522,
      "step": 15920
    },
    {
      "epoch": 116.27737226277372,
      "grad_norm": 9.899147033691406,
      "learning_rate": 4.418613138686132e-05,
      "loss": 1.6747,
      "step": 15930
    },
    {
      "epoch": 116.35036496350365,
      "grad_norm": 9.049147605895996,
      "learning_rate": 4.4182481751824825e-05,
      "loss": 1.5043,
      "step": 15940
    },
    {
      "epoch": 116.42335766423358,
      "grad_norm": 9.525497436523438,
      "learning_rate": 4.417883211678832e-05,
      "loss": 1.5171,
      "step": 15950
    },
    {
      "epoch": 116.4963503649635,
      "grad_norm": 9.195012092590332,
      "learning_rate": 4.4175182481751826e-05,
      "loss": 1.4333,
      "step": 15960
    },
    {
      "epoch": 116.56934306569343,
      "grad_norm": 6.877308368682861,
      "learning_rate": 4.417153284671533e-05,
      "loss": 1.3843,
      "step": 15970
    },
    {
      "epoch": 116.64233576642336,
      "grad_norm": 6.873545169830322,
      "learning_rate": 4.4167883211678834e-05,
      "loss": 1.6198,
      "step": 15980
    },
    {
      "epoch": 116.71532846715328,
      "grad_norm": 13.036638259887695,
      "learning_rate": 4.416423357664234e-05,
      "loss": 1.6985,
      "step": 15990
    },
    {
      "epoch": 116.78832116788321,
      "grad_norm": 10.275901794433594,
      "learning_rate": 4.416058394160584e-05,
      "loss": 1.2379,
      "step": 16000
    },
    {
      "epoch": 116.86131386861314,
      "grad_norm": 14.364154815673828,
      "learning_rate": 4.415693430656935e-05,
      "loss": 1.65,
      "step": 16010
    },
    {
      "epoch": 116.93430656934306,
      "grad_norm": 6.003392219543457,
      "learning_rate": 4.415328467153285e-05,
      "loss": 1.8246,
      "step": 16020
    },
    {
      "epoch": 117.00729927007299,
      "grad_norm": 9.082921981811523,
      "learning_rate": 4.414963503649635e-05,
      "loss": 2.0035,
      "step": 16030
    },
    {
      "epoch": 117.08029197080292,
      "grad_norm": 8.611350059509277,
      "learning_rate": 4.414598540145986e-05,
      "loss": 1.5971,
      "step": 16040
    },
    {
      "epoch": 117.15328467153284,
      "grad_norm": 13.819690704345703,
      "learning_rate": 4.414233576642336e-05,
      "loss": 1.7994,
      "step": 16050
    },
    {
      "epoch": 117.22627737226277,
      "grad_norm": 7.331334590911865,
      "learning_rate": 4.4138686131386865e-05,
      "loss": 1.8312,
      "step": 16060
    },
    {
      "epoch": 117.2992700729927,
      "grad_norm": 5.071183204650879,
      "learning_rate": 4.4135036496350365e-05,
      "loss": 1.2855,
      "step": 16070
    },
    {
      "epoch": 117.37226277372262,
      "grad_norm": 7.334725379943848,
      "learning_rate": 4.413138686131387e-05,
      "loss": 1.3142,
      "step": 16080
    },
    {
      "epoch": 117.44525547445255,
      "grad_norm": 7.183288097381592,
      "learning_rate": 4.412773722627738e-05,
      "loss": 1.6171,
      "step": 16090
    },
    {
      "epoch": 117.51824817518248,
      "grad_norm": 9.063834190368652,
      "learning_rate": 4.4124087591240874e-05,
      "loss": 1.567,
      "step": 16100
    },
    {
      "epoch": 117.5912408759124,
      "grad_norm": 8.289584159851074,
      "learning_rate": 4.412043795620438e-05,
      "loss": 1.8919,
      "step": 16110
    },
    {
      "epoch": 117.66423357664233,
      "grad_norm": 10.089837074279785,
      "learning_rate": 4.411678832116788e-05,
      "loss": 1.7513,
      "step": 16120
    },
    {
      "epoch": 117.73722627737226,
      "grad_norm": 6.354298114776611,
      "learning_rate": 4.411313868613139e-05,
      "loss": 1.6392,
      "step": 16130
    },
    {
      "epoch": 117.81021897810218,
      "grad_norm": 12.51050090789795,
      "learning_rate": 4.4109489051094896e-05,
      "loss": 1.3382,
      "step": 16140
    },
    {
      "epoch": 117.88321167883211,
      "grad_norm": 8.111146926879883,
      "learning_rate": 4.4105839416058396e-05,
      "loss": 1.4167,
      "step": 16150
    },
    {
      "epoch": 117.95620437956204,
      "grad_norm": 9.63278865814209,
      "learning_rate": 4.4102189781021904e-05,
      "loss": 1.4657,
      "step": 16160
    },
    {
      "epoch": 118.02919708029196,
      "grad_norm": 6.924086570739746,
      "learning_rate": 4.4098540145985404e-05,
      "loss": 1.4582,
      "step": 16170
    },
    {
      "epoch": 118.10218978102189,
      "grad_norm": 13.003229141235352,
      "learning_rate": 4.4094890510948905e-05,
      "loss": 1.8617,
      "step": 16180
    },
    {
      "epoch": 118.17518248175182,
      "grad_norm": 6.469235420227051,
      "learning_rate": 4.409124087591241e-05,
      "loss": 1.489,
      "step": 16190
    },
    {
      "epoch": 118.24817518248175,
      "grad_norm": 12.944036483764648,
      "learning_rate": 4.408759124087591e-05,
      "loss": 1.8248,
      "step": 16200
    },
    {
      "epoch": 118.32116788321167,
      "grad_norm": 7.217850685119629,
      "learning_rate": 4.408394160583942e-05,
      "loss": 1.3807,
      "step": 16210
    },
    {
      "epoch": 118.39416058394161,
      "grad_norm": 10.471753120422363,
      "learning_rate": 4.408029197080292e-05,
      "loss": 1.4314,
      "step": 16220
    },
    {
      "epoch": 118.46715328467154,
      "grad_norm": 5.891303539276123,
      "learning_rate": 4.407664233576643e-05,
      "loss": 1.2648,
      "step": 16230
    },
    {
      "epoch": 118.54014598540147,
      "grad_norm": 9.906776428222656,
      "learning_rate": 4.4072992700729935e-05,
      "loss": 1.9486,
      "step": 16240
    },
    {
      "epoch": 118.61313868613139,
      "grad_norm": 9.665369987487793,
      "learning_rate": 4.406934306569343e-05,
      "loss": 1.5875,
      "step": 16250
    },
    {
      "epoch": 118.68613138686132,
      "grad_norm": 8.184490203857422,
      "learning_rate": 4.4065693430656936e-05,
      "loss": 1.656,
      "step": 16260
    },
    {
      "epoch": 118.75912408759125,
      "grad_norm": 2.8522820472717285,
      "learning_rate": 4.4062043795620436e-05,
      "loss": 1.7921,
      "step": 16270
    },
    {
      "epoch": 118.83211678832117,
      "grad_norm": 8.706132888793945,
      "learning_rate": 4.4058394160583944e-05,
      "loss": 1.5651,
      "step": 16280
    },
    {
      "epoch": 118.9051094890511,
      "grad_norm": 7.47614049911499,
      "learning_rate": 4.405474452554745e-05,
      "loss": 1.4403,
      "step": 16290
    },
    {
      "epoch": 118.97810218978103,
      "grad_norm": 15.72224235534668,
      "learning_rate": 4.405109489051095e-05,
      "loss": 1.7249,
      "step": 16300
    },
    {
      "epoch": 119.05109489051095,
      "grad_norm": 5.710715293884277,
      "learning_rate": 4.404744525547446e-05,
      "loss": 1.5466,
      "step": 16310
    },
    {
      "epoch": 119.12408759124088,
      "grad_norm": 7.950319290161133,
      "learning_rate": 4.404379562043796e-05,
      "loss": 1.6726,
      "step": 16320
    },
    {
      "epoch": 119.1970802919708,
      "grad_norm": 5.324032306671143,
      "learning_rate": 4.404014598540146e-05,
      "loss": 1.4448,
      "step": 16330
    },
    {
      "epoch": 119.27007299270073,
      "grad_norm": 10.812843322753906,
      "learning_rate": 4.403649635036497e-05,
      "loss": 1.4929,
      "step": 16340
    },
    {
      "epoch": 119.34306569343066,
      "grad_norm": 7.456628322601318,
      "learning_rate": 4.403284671532847e-05,
      "loss": 1.7091,
      "step": 16350
    },
    {
      "epoch": 119.41605839416059,
      "grad_norm": 7.540661811828613,
      "learning_rate": 4.4029197080291975e-05,
      "loss": 1.3704,
      "step": 16360
    },
    {
      "epoch": 119.48905109489051,
      "grad_norm": 7.566046237945557,
      "learning_rate": 4.4025547445255475e-05,
      "loss": 1.6632,
      "step": 16370
    },
    {
      "epoch": 119.56204379562044,
      "grad_norm": 6.494356155395508,
      "learning_rate": 4.402189781021898e-05,
      "loss": 1.5663,
      "step": 16380
    },
    {
      "epoch": 119.63503649635037,
      "grad_norm": 8.191946029663086,
      "learning_rate": 4.401824817518248e-05,
      "loss": 1.4089,
      "step": 16390
    },
    {
      "epoch": 119.7080291970803,
      "grad_norm": 8.39284610748291,
      "learning_rate": 4.401459854014598e-05,
      "loss": 1.4792,
      "step": 16400
    },
    {
      "epoch": 119.78102189781022,
      "grad_norm": 8.776568412780762,
      "learning_rate": 4.401094890510949e-05,
      "loss": 1.6866,
      "step": 16410
    },
    {
      "epoch": 119.85401459854015,
      "grad_norm": 10.249983787536621,
      "learning_rate": 4.400729927007299e-05,
      "loss": 1.9024,
      "step": 16420
    },
    {
      "epoch": 119.92700729927007,
      "grad_norm": 8.954532623291016,
      "learning_rate": 4.40036496350365e-05,
      "loss": 1.7043,
      "step": 16430
    },
    {
      "epoch": 120.0,
      "grad_norm": 16.347267150878906,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.7269,
      "step": 16440
    },
    {
      "epoch": 120.07299270072993,
      "grad_norm": 11.700413703918457,
      "learning_rate": 4.3996350364963506e-05,
      "loss": 1.8095,
      "step": 16450
    },
    {
      "epoch": 120.14598540145985,
      "grad_norm": 8.949897766113281,
      "learning_rate": 4.3992700729927013e-05,
      "loss": 1.5578,
      "step": 16460
    },
    {
      "epoch": 120.21897810218978,
      "grad_norm": 5.997267723083496,
      "learning_rate": 4.3989051094890514e-05,
      "loss": 1.5635,
      "step": 16470
    },
    {
      "epoch": 120.2919708029197,
      "grad_norm": 7.885794639587402,
      "learning_rate": 4.3985401459854014e-05,
      "loss": 1.6629,
      "step": 16480
    },
    {
      "epoch": 120.36496350364963,
      "grad_norm": 8.905851364135742,
      "learning_rate": 4.398175182481752e-05,
      "loss": 1.5519,
      "step": 16490
    },
    {
      "epoch": 120.43795620437956,
      "grad_norm": 7.091686248779297,
      "learning_rate": 4.397810218978102e-05,
      "loss": 1.2713,
      "step": 16500
    },
    {
      "epoch": 120.51094890510949,
      "grad_norm": 7.053647994995117,
      "learning_rate": 4.397445255474453e-05,
      "loss": 1.8613,
      "step": 16510
    },
    {
      "epoch": 120.58394160583941,
      "grad_norm": 8.458623886108398,
      "learning_rate": 4.397080291970803e-05,
      "loss": 1.4332,
      "step": 16520
    },
    {
      "epoch": 120.65693430656934,
      "grad_norm": 8.98057746887207,
      "learning_rate": 4.396715328467154e-05,
      "loss": 1.425,
      "step": 16530
    },
    {
      "epoch": 120.72992700729927,
      "grad_norm": 7.783954620361328,
      "learning_rate": 4.396350364963504e-05,
      "loss": 1.4447,
      "step": 16540
    },
    {
      "epoch": 120.8029197080292,
      "grad_norm": 7.250265121459961,
      "learning_rate": 4.3959854014598545e-05,
      "loss": 1.3042,
      "step": 16550
    },
    {
      "epoch": 120.87591240875912,
      "grad_norm": 8.904407501220703,
      "learning_rate": 4.3956204379562045e-05,
      "loss": 1.5001,
      "step": 16560
    },
    {
      "epoch": 120.94890510948905,
      "grad_norm": 6.336430072784424,
      "learning_rate": 4.3952554744525546e-05,
      "loss": 1.8019,
      "step": 16570
    },
    {
      "epoch": 121.02189781021897,
      "grad_norm": 6.147923469543457,
      "learning_rate": 4.394890510948905e-05,
      "loss": 1.4922,
      "step": 16580
    },
    {
      "epoch": 121.0948905109489,
      "grad_norm": 9.957376480102539,
      "learning_rate": 4.3945255474452554e-05,
      "loss": 1.5799,
      "step": 16590
    },
    {
      "epoch": 121.16788321167883,
      "grad_norm": 7.024807453155518,
      "learning_rate": 4.394160583941606e-05,
      "loss": 1.5599,
      "step": 16600
    },
    {
      "epoch": 121.24087591240875,
      "grad_norm": 8.48979377746582,
      "learning_rate": 4.393795620437957e-05,
      "loss": 1.3696,
      "step": 16610
    },
    {
      "epoch": 121.31386861313868,
      "grad_norm": 9.370275497436523,
      "learning_rate": 4.393430656934307e-05,
      "loss": 1.6505,
      "step": 16620
    },
    {
      "epoch": 121.38686131386861,
      "grad_norm": 8.859766006469727,
      "learning_rate": 4.393065693430657e-05,
      "loss": 1.4722,
      "step": 16630
    },
    {
      "epoch": 121.45985401459853,
      "grad_norm": 6.3081278800964355,
      "learning_rate": 4.3927007299270077e-05,
      "loss": 1.4572,
      "step": 16640
    },
    {
      "epoch": 121.53284671532846,
      "grad_norm": 9.113651275634766,
      "learning_rate": 4.392335766423358e-05,
      "loss": 1.6051,
      "step": 16650
    },
    {
      "epoch": 121.60583941605839,
      "grad_norm": 6.55104398727417,
      "learning_rate": 4.3919708029197084e-05,
      "loss": 1.7734,
      "step": 16660
    },
    {
      "epoch": 121.67883211678833,
      "grad_norm": 7.76767110824585,
      "learning_rate": 4.3916058394160585e-05,
      "loss": 1.9235,
      "step": 16670
    },
    {
      "epoch": 121.75182481751825,
      "grad_norm": 9.524958610534668,
      "learning_rate": 4.391240875912409e-05,
      "loss": 1.5518,
      "step": 16680
    },
    {
      "epoch": 121.82481751824818,
      "grad_norm": 7.2967634201049805,
      "learning_rate": 4.390875912408759e-05,
      "loss": 1.5262,
      "step": 16690
    },
    {
      "epoch": 121.89781021897811,
      "grad_norm": 9.23688793182373,
      "learning_rate": 4.39051094890511e-05,
      "loss": 1.3187,
      "step": 16700
    },
    {
      "epoch": 121.97080291970804,
      "grad_norm": 6.536318778991699,
      "learning_rate": 4.39014598540146e-05,
      "loss": 1.4052,
      "step": 16710
    },
    {
      "epoch": 122.04379562043796,
      "grad_norm": 7.113826274871826,
      "learning_rate": 4.38978102189781e-05,
      "loss": 1.6088,
      "step": 16720
    },
    {
      "epoch": 122.11678832116789,
      "grad_norm": 9.195883750915527,
      "learning_rate": 4.389416058394161e-05,
      "loss": 1.4419,
      "step": 16730
    },
    {
      "epoch": 122.18978102189782,
      "grad_norm": 7.8714776039123535,
      "learning_rate": 4.389051094890511e-05,
      "loss": 1.6329,
      "step": 16740
    },
    {
      "epoch": 122.26277372262774,
      "grad_norm": 8.039840698242188,
      "learning_rate": 4.3886861313868616e-05,
      "loss": 1.6176,
      "step": 16750
    },
    {
      "epoch": 122.33576642335767,
      "grad_norm": 3.6457040309906006,
      "learning_rate": 4.388321167883212e-05,
      "loss": 1.5381,
      "step": 16760
    },
    {
      "epoch": 122.4087591240876,
      "grad_norm": 5.959100723266602,
      "learning_rate": 4.3879562043795624e-05,
      "loss": 1.4869,
      "step": 16770
    },
    {
      "epoch": 122.48175182481752,
      "grad_norm": 9.27052116394043,
      "learning_rate": 4.387591240875913e-05,
      "loss": 1.9121,
      "step": 16780
    },
    {
      "epoch": 122.55474452554745,
      "grad_norm": 10.187047004699707,
      "learning_rate": 4.3872262773722625e-05,
      "loss": 1.3932,
      "step": 16790
    },
    {
      "epoch": 122.62773722627738,
      "grad_norm": 6.802284240722656,
      "learning_rate": 4.386861313868613e-05,
      "loss": 1.5122,
      "step": 16800
    },
    {
      "epoch": 122.7007299270073,
      "grad_norm": 7.948598384857178,
      "learning_rate": 4.386496350364964e-05,
      "loss": 1.5421,
      "step": 16810
    },
    {
      "epoch": 122.77372262773723,
      "grad_norm": 10.74998950958252,
      "learning_rate": 4.386131386861314e-05,
      "loss": 1.4569,
      "step": 16820
    },
    {
      "epoch": 122.84671532846716,
      "grad_norm": 5.196231365203857,
      "learning_rate": 4.385766423357665e-05,
      "loss": 1.499,
      "step": 16830
    },
    {
      "epoch": 122.91970802919708,
      "grad_norm": 7.6161603927612305,
      "learning_rate": 4.385401459854015e-05,
      "loss": 1.4453,
      "step": 16840
    },
    {
      "epoch": 122.99270072992701,
      "grad_norm": 9.429839134216309,
      "learning_rate": 4.3850364963503655e-05,
      "loss": 1.5403,
      "step": 16850
    },
    {
      "epoch": 123.06569343065694,
      "grad_norm": 10.24146556854248,
      "learning_rate": 4.3846715328467155e-05,
      "loss": 1.7199,
      "step": 16860
    },
    {
      "epoch": 123.13868613138686,
      "grad_norm": 5.744823455810547,
      "learning_rate": 4.3843065693430656e-05,
      "loss": 1.4441,
      "step": 16870
    },
    {
      "epoch": 123.21167883211679,
      "grad_norm": 5.259166717529297,
      "learning_rate": 4.383941605839416e-05,
      "loss": 1.2786,
      "step": 16880
    },
    {
      "epoch": 123.28467153284672,
      "grad_norm": 10.417327880859375,
      "learning_rate": 4.3835766423357663e-05,
      "loss": 1.4603,
      "step": 16890
    },
    {
      "epoch": 123.35766423357664,
      "grad_norm": 7.4047136306762695,
      "learning_rate": 4.383211678832117e-05,
      "loss": 1.5085,
      "step": 16900
    },
    {
      "epoch": 123.43065693430657,
      "grad_norm": 8.796968460083008,
      "learning_rate": 4.382846715328468e-05,
      "loss": 1.6124,
      "step": 16910
    },
    {
      "epoch": 123.5036496350365,
      "grad_norm": 15.350290298461914,
      "learning_rate": 4.382481751824818e-05,
      "loss": 1.9113,
      "step": 16920
    },
    {
      "epoch": 123.57664233576642,
      "grad_norm": 10.024744987487793,
      "learning_rate": 4.3821167883211686e-05,
      "loss": 1.3118,
      "step": 16930
    },
    {
      "epoch": 123.64963503649635,
      "grad_norm": 9.017050743103027,
      "learning_rate": 4.381751824817518e-05,
      "loss": 1.3249,
      "step": 16940
    },
    {
      "epoch": 123.72262773722628,
      "grad_norm": 9.08228874206543,
      "learning_rate": 4.381386861313869e-05,
      "loss": 1.3901,
      "step": 16950
    },
    {
      "epoch": 123.7956204379562,
      "grad_norm": 8.586355209350586,
      "learning_rate": 4.3810218978102194e-05,
      "loss": 1.7013,
      "step": 16960
    },
    {
      "epoch": 123.86861313868613,
      "grad_norm": 12.650081634521484,
      "learning_rate": 4.3806569343065694e-05,
      "loss": 1.7804,
      "step": 16970
    },
    {
      "epoch": 123.94160583941606,
      "grad_norm": 6.851284503936768,
      "learning_rate": 4.38029197080292e-05,
      "loss": 1.7866,
      "step": 16980
    },
    {
      "epoch": 124.01459854014598,
      "grad_norm": 10.43588638305664,
      "learning_rate": 4.37992700729927e-05,
      "loss": 1.4859,
      "step": 16990
    },
    {
      "epoch": 124.08759124087591,
      "grad_norm": 9.907036781311035,
      "learning_rate": 4.379562043795621e-05,
      "loss": 1.4787,
      "step": 17000
    },
    {
      "epoch": 124.16058394160584,
      "grad_norm": 7.013283729553223,
      "learning_rate": 4.379197080291971e-05,
      "loss": 1.6798,
      "step": 17010
    },
    {
      "epoch": 124.23357664233576,
      "grad_norm": 12.506123542785645,
      "learning_rate": 4.378832116788321e-05,
      "loss": 1.457,
      "step": 17020
    },
    {
      "epoch": 124.30656934306569,
      "grad_norm": 12.508466720581055,
      "learning_rate": 4.378467153284672e-05,
      "loss": 1.5504,
      "step": 17030
    },
    {
      "epoch": 124.37956204379562,
      "grad_norm": 8.968981742858887,
      "learning_rate": 4.378102189781022e-05,
      "loss": 1.135,
      "step": 17040
    },
    {
      "epoch": 124.45255474452554,
      "grad_norm": 12.918011665344238,
      "learning_rate": 4.3777372262773726e-05,
      "loss": 1.6347,
      "step": 17050
    },
    {
      "epoch": 124.52554744525547,
      "grad_norm": 5.217225551605225,
      "learning_rate": 4.3773722627737226e-05,
      "loss": 1.2803,
      "step": 17060
    },
    {
      "epoch": 124.5985401459854,
      "grad_norm": 9.738814353942871,
      "learning_rate": 4.377007299270073e-05,
      "loss": 1.4746,
      "step": 17070
    },
    {
      "epoch": 124.67153284671532,
      "grad_norm": 14.036070823669434,
      "learning_rate": 4.376642335766424e-05,
      "loss": 1.6763,
      "step": 17080
    },
    {
      "epoch": 124.74452554744525,
      "grad_norm": 7.538975715637207,
      "learning_rate": 4.3762773722627734e-05,
      "loss": 1.1459,
      "step": 17090
    },
    {
      "epoch": 124.81751824817518,
      "grad_norm": 10.496341705322266,
      "learning_rate": 4.375912408759124e-05,
      "loss": 2.2042,
      "step": 17100
    },
    {
      "epoch": 124.8905109489051,
      "grad_norm": 9.258624076843262,
      "learning_rate": 4.375547445255475e-05,
      "loss": 1.2281,
      "step": 17110
    },
    {
      "epoch": 124.96350364963503,
      "grad_norm": 8.68409252166748,
      "learning_rate": 4.375182481751825e-05,
      "loss": 1.6995,
      "step": 17120
    },
    {
      "epoch": 125.03649635036497,
      "grad_norm": 8.29478645324707,
      "learning_rate": 4.3748175182481757e-05,
      "loss": 1.684,
      "step": 17130
    },
    {
      "epoch": 125.1094890510949,
      "grad_norm": 7.6039228439331055,
      "learning_rate": 4.374452554744526e-05,
      "loss": 1.3724,
      "step": 17140
    },
    {
      "epoch": 125.18248175182482,
      "grad_norm": 8.054693222045898,
      "learning_rate": 4.3740875912408764e-05,
      "loss": 1.5669,
      "step": 17150
    },
    {
      "epoch": 125.25547445255475,
      "grad_norm": 7.501577377319336,
      "learning_rate": 4.3737226277372265e-05,
      "loss": 1.442,
      "step": 17160
    },
    {
      "epoch": 125.32846715328468,
      "grad_norm": 8.646188735961914,
      "learning_rate": 4.3733576642335765e-05,
      "loss": 1.3205,
      "step": 17170
    },
    {
      "epoch": 125.4014598540146,
      "grad_norm": 8.979560852050781,
      "learning_rate": 4.372992700729927e-05,
      "loss": 1.3747,
      "step": 17180
    },
    {
      "epoch": 125.47445255474453,
      "grad_norm": 8.71618938446045,
      "learning_rate": 4.372627737226277e-05,
      "loss": 1.6595,
      "step": 17190
    },
    {
      "epoch": 125.54744525547446,
      "grad_norm": 14.46216106414795,
      "learning_rate": 4.372262773722628e-05,
      "loss": 1.4766,
      "step": 17200
    },
    {
      "epoch": 125.62043795620438,
      "grad_norm": 9.849017143249512,
      "learning_rate": 4.371897810218978e-05,
      "loss": 1.4443,
      "step": 17210
    },
    {
      "epoch": 125.69343065693431,
      "grad_norm": 7.065585613250732,
      "learning_rate": 4.371532846715329e-05,
      "loss": 1.6715,
      "step": 17220
    },
    {
      "epoch": 125.76642335766424,
      "grad_norm": 7.262433052062988,
      "learning_rate": 4.3711678832116795e-05,
      "loss": 1.6035,
      "step": 17230
    },
    {
      "epoch": 125.83941605839416,
      "grad_norm": 12.417479515075684,
      "learning_rate": 4.370802919708029e-05,
      "loss": 1.5513,
      "step": 17240
    },
    {
      "epoch": 125.91240875912409,
      "grad_norm": 8.049226760864258,
      "learning_rate": 4.3704379562043796e-05,
      "loss": 1.6586,
      "step": 17250
    },
    {
      "epoch": 125.98540145985402,
      "grad_norm": 8.155348777770996,
      "learning_rate": 4.37007299270073e-05,
      "loss": 1.3983,
      "step": 17260
    },
    {
      "epoch": 126.05839416058394,
      "grad_norm": 5.929600715637207,
      "learning_rate": 4.3697080291970804e-05,
      "loss": 1.2009,
      "step": 17270
    },
    {
      "epoch": 126.13138686131387,
      "grad_norm": 8.66657543182373,
      "learning_rate": 4.369343065693431e-05,
      "loss": 1.6862,
      "step": 17280
    },
    {
      "epoch": 126.2043795620438,
      "grad_norm": 9.100458145141602,
      "learning_rate": 4.368978102189781e-05,
      "loss": 1.6304,
      "step": 17290
    },
    {
      "epoch": 126.27737226277372,
      "grad_norm": 8.69874382019043,
      "learning_rate": 4.368613138686132e-05,
      "loss": 1.6092,
      "step": 17300
    },
    {
      "epoch": 126.35036496350365,
      "grad_norm": 10.807585716247559,
      "learning_rate": 4.368248175182482e-05,
      "loss": 1.1962,
      "step": 17310
    },
    {
      "epoch": 126.42335766423358,
      "grad_norm": 11.570398330688477,
      "learning_rate": 4.367883211678832e-05,
      "loss": 1.3813,
      "step": 17320
    },
    {
      "epoch": 126.4963503649635,
      "grad_norm": 7.2299065589904785,
      "learning_rate": 4.367518248175183e-05,
      "loss": 1.6935,
      "step": 17330
    },
    {
      "epoch": 126.56934306569343,
      "grad_norm": 10.637557983398438,
      "learning_rate": 4.367153284671533e-05,
      "loss": 1.3515,
      "step": 17340
    },
    {
      "epoch": 126.64233576642336,
      "grad_norm": 8.69655704498291,
      "learning_rate": 4.3667883211678835e-05,
      "loss": 1.9835,
      "step": 17350
    },
    {
      "epoch": 126.71532846715328,
      "grad_norm": 9.5890531539917,
      "learning_rate": 4.3664233576642336e-05,
      "loss": 1.2488,
      "step": 17360
    },
    {
      "epoch": 126.78832116788321,
      "grad_norm": 6.9921698570251465,
      "learning_rate": 4.366058394160584e-05,
      "loss": 1.2113,
      "step": 17370
    },
    {
      "epoch": 126.86131386861314,
      "grad_norm": 6.0799665451049805,
      "learning_rate": 4.365693430656935e-05,
      "loss": 1.4315,
      "step": 17380
    },
    {
      "epoch": 126.93430656934306,
      "grad_norm": 12.353032112121582,
      "learning_rate": 4.365328467153285e-05,
      "loss": 1.6522,
      "step": 17390
    },
    {
      "epoch": 127.00729927007299,
      "grad_norm": 10.214818954467773,
      "learning_rate": 4.364963503649635e-05,
      "loss": 1.4636,
      "step": 17400
    },
    {
      "epoch": 127.08029197080292,
      "grad_norm": 10.76453971862793,
      "learning_rate": 4.364598540145985e-05,
      "loss": 1.6279,
      "step": 17410
    },
    {
      "epoch": 127.15328467153284,
      "grad_norm": 8.680436134338379,
      "learning_rate": 4.364233576642336e-05,
      "loss": 1.363,
      "step": 17420
    },
    {
      "epoch": 127.22627737226277,
      "grad_norm": 5.465325355529785,
      "learning_rate": 4.3638686131386866e-05,
      "loss": 1.335,
      "step": 17430
    },
    {
      "epoch": 127.2992700729927,
      "grad_norm": 7.0186076164245605,
      "learning_rate": 4.363503649635037e-05,
      "loss": 1.566,
      "step": 17440
    },
    {
      "epoch": 127.37226277372262,
      "grad_norm": 5.979953289031982,
      "learning_rate": 4.3631386861313874e-05,
      "loss": 1.6337,
      "step": 17450
    },
    {
      "epoch": 127.44525547445255,
      "grad_norm": 7.263072490692139,
      "learning_rate": 4.3627737226277375e-05,
      "loss": 1.3857,
      "step": 17460
    },
    {
      "epoch": 127.51824817518248,
      "grad_norm": 6.510002136230469,
      "learning_rate": 4.3624087591240875e-05,
      "loss": 1.6774,
      "step": 17470
    },
    {
      "epoch": 127.5912408759124,
      "grad_norm": 7.7294230461120605,
      "learning_rate": 4.362043795620438e-05,
      "loss": 2.012,
      "step": 17480
    },
    {
      "epoch": 127.66423357664233,
      "grad_norm": 7.44177770614624,
      "learning_rate": 4.361678832116788e-05,
      "loss": 1.326,
      "step": 17490
    },
    {
      "epoch": 127.73722627737226,
      "grad_norm": 4.855976581573486,
      "learning_rate": 4.361313868613139e-05,
      "loss": 1.4056,
      "step": 17500
    },
    {
      "epoch": 127.81021897810218,
      "grad_norm": 7.458724498748779,
      "learning_rate": 4.360948905109489e-05,
      "loss": 1.8636,
      "step": 17510
    },
    {
      "epoch": 127.88321167883211,
      "grad_norm": 6.380349636077881,
      "learning_rate": 4.36058394160584e-05,
      "loss": 1.4011,
      "step": 17520
    },
    {
      "epoch": 127.95620437956204,
      "grad_norm": 8.267613410949707,
      "learning_rate": 4.36021897810219e-05,
      "loss": 1.1127,
      "step": 17530
    },
    {
      "epoch": 128.02919708029196,
      "grad_norm": 5.967607021331787,
      "learning_rate": 4.3598540145985406e-05,
      "loss": 1.4943,
      "step": 17540
    },
    {
      "epoch": 128.1021897810219,
      "grad_norm": 11.9129056930542,
      "learning_rate": 4.3594890510948906e-05,
      "loss": 1.5985,
      "step": 17550
    },
    {
      "epoch": 128.17518248175182,
      "grad_norm": 9.384061813354492,
      "learning_rate": 4.3591240875912407e-05,
      "loss": 1.3279,
      "step": 17560
    },
    {
      "epoch": 128.24817518248176,
      "grad_norm": 9.148177146911621,
      "learning_rate": 4.3587591240875914e-05,
      "loss": 1.7478,
      "step": 17570
    },
    {
      "epoch": 128.32116788321167,
      "grad_norm": 11.589197158813477,
      "learning_rate": 4.358394160583942e-05,
      "loss": 1.3372,
      "step": 17580
    },
    {
      "epoch": 128.3941605839416,
      "grad_norm": 6.365081787109375,
      "learning_rate": 4.358029197080292e-05,
      "loss": 1.6963,
      "step": 17590
    },
    {
      "epoch": 128.46715328467153,
      "grad_norm": 5.788754463195801,
      "learning_rate": 4.357664233576643e-05,
      "loss": 1.165,
      "step": 17600
    },
    {
      "epoch": 128.54014598540147,
      "grad_norm": 2.6891520023345947,
      "learning_rate": 4.357299270072993e-05,
      "loss": 1.1535,
      "step": 17610
    },
    {
      "epoch": 128.61313868613138,
      "grad_norm": 7.061149597167969,
      "learning_rate": 4.356934306569344e-05,
      "loss": 1.3676,
      "step": 17620
    },
    {
      "epoch": 128.68613138686132,
      "grad_norm": 9.593463897705078,
      "learning_rate": 4.356569343065694e-05,
      "loss": 1.8313,
      "step": 17630
    },
    {
      "epoch": 128.75912408759123,
      "grad_norm": 8.853918075561523,
      "learning_rate": 4.356204379562044e-05,
      "loss": 1.5088,
      "step": 17640
    },
    {
      "epoch": 128.83211678832117,
      "grad_norm": 8.236272811889648,
      "learning_rate": 4.3558394160583945e-05,
      "loss": 1.4383,
      "step": 17650
    },
    {
      "epoch": 128.90510948905109,
      "grad_norm": 11.207520484924316,
      "learning_rate": 4.3554744525547445e-05,
      "loss": 1.2788,
      "step": 17660
    },
    {
      "epoch": 128.97810218978103,
      "grad_norm": 8.483753204345703,
      "learning_rate": 4.355109489051095e-05,
      "loss": 1.4858,
      "step": 17670
    },
    {
      "epoch": 129.05109489051094,
      "grad_norm": 4.9008002281188965,
      "learning_rate": 4.354744525547445e-05,
      "loss": 1.4613,
      "step": 17680
    },
    {
      "epoch": 129.12408759124088,
      "grad_norm": 6.793809413909912,
      "learning_rate": 4.354379562043796e-05,
      "loss": 1.1597,
      "step": 17690
    },
    {
      "epoch": 129.1970802919708,
      "grad_norm": 11.164796829223633,
      "learning_rate": 4.354014598540146e-05,
      "loss": 1.609,
      "step": 17700
    },
    {
      "epoch": 129.27007299270073,
      "grad_norm": 11.581619262695312,
      "learning_rate": 4.353649635036496e-05,
      "loss": 1.6781,
      "step": 17710
    },
    {
      "epoch": 129.34306569343065,
      "grad_norm": 9.122026443481445,
      "learning_rate": 4.353284671532847e-05,
      "loss": 1.7504,
      "step": 17720
    },
    {
      "epoch": 129.4160583941606,
      "grad_norm": 8.978997230529785,
      "learning_rate": 4.352919708029197e-05,
      "loss": 1.6091,
      "step": 17730
    },
    {
      "epoch": 129.4890510948905,
      "grad_norm": 9.554841995239258,
      "learning_rate": 4.3525547445255476e-05,
      "loss": 1.4725,
      "step": 17740
    },
    {
      "epoch": 129.56204379562044,
      "grad_norm": 7.055891990661621,
      "learning_rate": 4.3521897810218984e-05,
      "loss": 1.5128,
      "step": 17750
    },
    {
      "epoch": 129.63503649635035,
      "grad_norm": 7.5946221351623535,
      "learning_rate": 4.3518248175182484e-05,
      "loss": 1.4535,
      "step": 17760
    },
    {
      "epoch": 129.7080291970803,
      "grad_norm": 14.552244186401367,
      "learning_rate": 4.351459854014599e-05,
      "loss": 1.6443,
      "step": 17770
    },
    {
      "epoch": 129.7810218978102,
      "grad_norm": 5.896969795227051,
      "learning_rate": 4.351094890510949e-05,
      "loss": 1.347,
      "step": 17780
    },
    {
      "epoch": 129.85401459854015,
      "grad_norm": 12.522459983825684,
      "learning_rate": 4.350729927007299e-05,
      "loss": 1.4527,
      "step": 17790
    },
    {
      "epoch": 129.92700729927006,
      "grad_norm": 11.037109375,
      "learning_rate": 4.35036496350365e-05,
      "loss": 1.0902,
      "step": 17800
    },
    {
      "epoch": 130.0,
      "grad_norm": 12.343327522277832,
      "learning_rate": 4.35e-05,
      "loss": 1.3948,
      "step": 17810
    },
    {
      "epoch": 130.07299270072994,
      "grad_norm": 6.94193696975708,
      "learning_rate": 4.349635036496351e-05,
      "loss": 1.7792,
      "step": 17820
    },
    {
      "epoch": 130.14598540145985,
      "grad_norm": 14.016014099121094,
      "learning_rate": 4.349270072992701e-05,
      "loss": 1.4937,
      "step": 17830
    },
    {
      "epoch": 130.2189781021898,
      "grad_norm": 4.85932731628418,
      "learning_rate": 4.3489051094890515e-05,
      "loss": 1.5934,
      "step": 17840
    },
    {
      "epoch": 130.2919708029197,
      "grad_norm": 4.665006637573242,
      "learning_rate": 4.348540145985402e-05,
      "loss": 1.2957,
      "step": 17850
    },
    {
      "epoch": 130.36496350364965,
      "grad_norm": 6.231540679931641,
      "learning_rate": 4.3481751824817516e-05,
      "loss": 1.1793,
      "step": 17860
    },
    {
      "epoch": 130.43795620437956,
      "grad_norm": 9.355978012084961,
      "learning_rate": 4.3478102189781024e-05,
      "loss": 1.5985,
      "step": 17870
    },
    {
      "epoch": 130.5109489051095,
      "grad_norm": 9.916555404663086,
      "learning_rate": 4.3474452554744524e-05,
      "loss": 1.614,
      "step": 17880
    },
    {
      "epoch": 130.5839416058394,
      "grad_norm": 9.272551536560059,
      "learning_rate": 4.347080291970803e-05,
      "loss": 1.5369,
      "step": 17890
    },
    {
      "epoch": 130.65693430656935,
      "grad_norm": 4.970005989074707,
      "learning_rate": 4.346715328467154e-05,
      "loss": 1.3002,
      "step": 17900
    },
    {
      "epoch": 130.72992700729927,
      "grad_norm": 11.24009895324707,
      "learning_rate": 4.346350364963504e-05,
      "loss": 1.4381,
      "step": 17910
    },
    {
      "epoch": 130.8029197080292,
      "grad_norm": 5.0775580406188965,
      "learning_rate": 4.3459854014598546e-05,
      "loss": 1.1768,
      "step": 17920
    },
    {
      "epoch": 130.87591240875912,
      "grad_norm": 8.88525390625,
      "learning_rate": 4.345620437956204e-05,
      "loss": 1.3761,
      "step": 17930
    },
    {
      "epoch": 130.94890510948906,
      "grad_norm": 10.112015724182129,
      "learning_rate": 4.345255474452555e-05,
      "loss": 1.3169,
      "step": 17940
    },
    {
      "epoch": 131.02189781021897,
      "grad_norm": 12.932169914245605,
      "learning_rate": 4.3448905109489055e-05,
      "loss": 1.0954,
      "step": 17950
    },
    {
      "epoch": 131.09489051094891,
      "grad_norm": 8.36438274383545,
      "learning_rate": 4.3445255474452555e-05,
      "loss": 1.6146,
      "step": 17960
    },
    {
      "epoch": 131.16788321167883,
      "grad_norm": 9.085102081298828,
      "learning_rate": 4.344160583941606e-05,
      "loss": 1.6018,
      "step": 17970
    },
    {
      "epoch": 131.24087591240877,
      "grad_norm": 4.649982452392578,
      "learning_rate": 4.343795620437956e-05,
      "loss": 1.0597,
      "step": 17980
    },
    {
      "epoch": 131.31386861313868,
      "grad_norm": 9.509594917297363,
      "learning_rate": 4.343430656934307e-05,
      "loss": 1.7922,
      "step": 17990
    },
    {
      "epoch": 131.38686131386862,
      "grad_norm": 10.653407096862793,
      "learning_rate": 4.343065693430657e-05,
      "loss": 1.6394,
      "step": 18000
    },
    {
      "epoch": 131.45985401459853,
      "grad_norm": 9.686018943786621,
      "learning_rate": 4.342700729927007e-05,
      "loss": 1.3941,
      "step": 18010
    },
    {
      "epoch": 131.53284671532847,
      "grad_norm": 6.017792224884033,
      "learning_rate": 4.342335766423358e-05,
      "loss": 1.6157,
      "step": 18020
    },
    {
      "epoch": 131.6058394160584,
      "grad_norm": 5.482326030731201,
      "learning_rate": 4.341970802919708e-05,
      "loss": 1.1956,
      "step": 18030
    },
    {
      "epoch": 131.67883211678833,
      "grad_norm": 8.673927307128906,
      "learning_rate": 4.3416058394160586e-05,
      "loss": 1.4081,
      "step": 18040
    },
    {
      "epoch": 131.75182481751824,
      "grad_norm": 3.9069344997406006,
      "learning_rate": 4.3412408759124093e-05,
      "loss": 1.2476,
      "step": 18050
    },
    {
      "epoch": 131.82481751824818,
      "grad_norm": 9.457472801208496,
      "learning_rate": 4.3408759124087594e-05,
      "loss": 1.1441,
      "step": 18060
    },
    {
      "epoch": 131.8978102189781,
      "grad_norm": 8.012370109558105,
      "learning_rate": 4.34051094890511e-05,
      "loss": 1.623,
      "step": 18070
    },
    {
      "epoch": 131.97080291970804,
      "grad_norm": 1.8778949975967407,
      "learning_rate": 4.34014598540146e-05,
      "loss": 1.4041,
      "step": 18080
    },
    {
      "epoch": 132.04379562043795,
      "grad_norm": 6.679173946380615,
      "learning_rate": 4.33978102189781e-05,
      "loss": 1.3966,
      "step": 18090
    },
    {
      "epoch": 132.1167883211679,
      "grad_norm": 2.0034143924713135,
      "learning_rate": 4.339416058394161e-05,
      "loss": 1.072,
      "step": 18100
    },
    {
      "epoch": 132.1897810218978,
      "grad_norm": 10.533907890319824,
      "learning_rate": 4.339051094890511e-05,
      "loss": 1.4904,
      "step": 18110
    },
    {
      "epoch": 132.26277372262774,
      "grad_norm": 9.164508819580078,
      "learning_rate": 4.338686131386862e-05,
      "loss": 1.534,
      "step": 18120
    },
    {
      "epoch": 132.33576642335765,
      "grad_norm": 8.940553665161133,
      "learning_rate": 4.338321167883212e-05,
      "loss": 1.53,
      "step": 18130
    },
    {
      "epoch": 132.4087591240876,
      "grad_norm": 13.656606674194336,
      "learning_rate": 4.3379562043795625e-05,
      "loss": 1.4408,
      "step": 18140
    },
    {
      "epoch": 132.4817518248175,
      "grad_norm": 7.974906921386719,
      "learning_rate": 4.3375912408759125e-05,
      "loss": 1.3982,
      "step": 18150
    },
    {
      "epoch": 132.55474452554745,
      "grad_norm": 6.940765380859375,
      "learning_rate": 4.3372262773722626e-05,
      "loss": 1.3272,
      "step": 18160
    },
    {
      "epoch": 132.62773722627736,
      "grad_norm": 3.2507705688476562,
      "learning_rate": 4.336861313868613e-05,
      "loss": 1.1804,
      "step": 18170
    },
    {
      "epoch": 132.7007299270073,
      "grad_norm": 8.559430122375488,
      "learning_rate": 4.3364963503649634e-05,
      "loss": 1.4604,
      "step": 18180
    },
    {
      "epoch": 132.77372262773721,
      "grad_norm": 3.9808847904205322,
      "learning_rate": 4.336131386861314e-05,
      "loss": 1.1915,
      "step": 18190
    },
    {
      "epoch": 132.84671532846716,
      "grad_norm": 10.63377571105957,
      "learning_rate": 4.335766423357664e-05,
      "loss": 1.5505,
      "step": 18200
    },
    {
      "epoch": 132.91970802919707,
      "grad_norm": 5.513287544250488,
      "learning_rate": 4.335401459854015e-05,
      "loss": 1.5118,
      "step": 18210
    },
    {
      "epoch": 132.992700729927,
      "grad_norm": 7.328207015991211,
      "learning_rate": 4.3350364963503656e-05,
      "loss": 1.5432,
      "step": 18220
    },
    {
      "epoch": 133.06569343065692,
      "grad_norm": 7.640615463256836,
      "learning_rate": 4.3346715328467157e-05,
      "loss": 1.7021,
      "step": 18230
    },
    {
      "epoch": 133.13868613138686,
      "grad_norm": 13.565571784973145,
      "learning_rate": 4.334306569343066e-05,
      "loss": 1.3042,
      "step": 18240
    },
    {
      "epoch": 133.21167883211677,
      "grad_norm": 6.610926151275635,
      "learning_rate": 4.3339416058394164e-05,
      "loss": 1.4337,
      "step": 18250
    },
    {
      "epoch": 133.28467153284672,
      "grad_norm": 6.068715572357178,
      "learning_rate": 4.3335766423357665e-05,
      "loss": 1.6106,
      "step": 18260
    },
    {
      "epoch": 133.35766423357666,
      "grad_norm": 12.533513069152832,
      "learning_rate": 4.333211678832117e-05,
      "loss": 1.4231,
      "step": 18270
    },
    {
      "epoch": 133.43065693430657,
      "grad_norm": 8.50301742553711,
      "learning_rate": 4.332846715328467e-05,
      "loss": 1.2975,
      "step": 18280
    },
    {
      "epoch": 133.5036496350365,
      "grad_norm": 11.093443870544434,
      "learning_rate": 4.332481751824818e-05,
      "loss": 1.3158,
      "step": 18290
    },
    {
      "epoch": 133.57664233576642,
      "grad_norm": 3.6104650497436523,
      "learning_rate": 4.332116788321168e-05,
      "loss": 1.024,
      "step": 18300
    },
    {
      "epoch": 133.64963503649636,
      "grad_norm": 5.2922773361206055,
      "learning_rate": 4.331751824817518e-05,
      "loss": 1.4824,
      "step": 18310
    },
    {
      "epoch": 133.72262773722628,
      "grad_norm": 7.875799179077148,
      "learning_rate": 4.331386861313869e-05,
      "loss": 1.4518,
      "step": 18320
    },
    {
      "epoch": 133.79562043795622,
      "grad_norm": 8.189244270324707,
      "learning_rate": 4.331021897810219e-05,
      "loss": 1.3568,
      "step": 18330
    },
    {
      "epoch": 133.86861313868613,
      "grad_norm": 10.2977294921875,
      "learning_rate": 4.3306569343065696e-05,
      "loss": 1.5293,
      "step": 18340
    },
    {
      "epoch": 133.94160583941607,
      "grad_norm": 10.923537254333496,
      "learning_rate": 4.3302919708029196e-05,
      "loss": 1.3694,
      "step": 18350
    },
    {
      "epoch": 134.01459854014598,
      "grad_norm": 7.584399700164795,
      "learning_rate": 4.3299270072992704e-05,
      "loss": 1.2296,
      "step": 18360
    },
    {
      "epoch": 134.08759124087592,
      "grad_norm": 11.139450073242188,
      "learning_rate": 4.329562043795621e-05,
      "loss": 1.3469,
      "step": 18370
    },
    {
      "epoch": 134.16058394160584,
      "grad_norm": 5.083632469177246,
      "learning_rate": 4.329197080291971e-05,
      "loss": 1.2941,
      "step": 18380
    },
    {
      "epoch": 134.23357664233578,
      "grad_norm": 9.52773666381836,
      "learning_rate": 4.328832116788321e-05,
      "loss": 1.3096,
      "step": 18390
    },
    {
      "epoch": 134.3065693430657,
      "grad_norm": 12.532258033752441,
      "learning_rate": 4.328467153284671e-05,
      "loss": 1.4595,
      "step": 18400
    },
    {
      "epoch": 134.37956204379563,
      "grad_norm": 15.246814727783203,
      "learning_rate": 4.328102189781022e-05,
      "loss": 1.7125,
      "step": 18410
    },
    {
      "epoch": 134.45255474452554,
      "grad_norm": 8.213542938232422,
      "learning_rate": 4.327737226277373e-05,
      "loss": 1.6337,
      "step": 18420
    },
    {
      "epoch": 134.52554744525548,
      "grad_norm": 10.33651351928711,
      "learning_rate": 4.327372262773723e-05,
      "loss": 1.3527,
      "step": 18430
    },
    {
      "epoch": 134.5985401459854,
      "grad_norm": 6.545255661010742,
      "learning_rate": 4.3270072992700735e-05,
      "loss": 1.403,
      "step": 18440
    },
    {
      "epoch": 134.67153284671534,
      "grad_norm": 6.539196014404297,
      "learning_rate": 4.3266423357664235e-05,
      "loss": 1.2788,
      "step": 18450
    },
    {
      "epoch": 134.74452554744525,
      "grad_norm": 5.895935535430908,
      "learning_rate": 4.326277372262774e-05,
      "loss": 1.7587,
      "step": 18460
    },
    {
      "epoch": 134.8175182481752,
      "grad_norm": 6.21536922454834,
      "learning_rate": 4.325912408759124e-05,
      "loss": 0.9895,
      "step": 18470
    },
    {
      "epoch": 134.8905109489051,
      "grad_norm": 5.8216328620910645,
      "learning_rate": 4.3255474452554743e-05,
      "loss": 1.1957,
      "step": 18480
    },
    {
      "epoch": 134.96350364963504,
      "grad_norm": 7.438533782958984,
      "learning_rate": 4.325182481751825e-05,
      "loss": 1.5669,
      "step": 18490
    },
    {
      "epoch": 135.03649635036496,
      "grad_norm": 7.247239112854004,
      "learning_rate": 4.324817518248175e-05,
      "loss": 1.3207,
      "step": 18500
    },
    {
      "epoch": 135.1094890510949,
      "grad_norm": 4.860044002532959,
      "learning_rate": 4.324452554744526e-05,
      "loss": 1.4146,
      "step": 18510
    },
    {
      "epoch": 135.1824817518248,
      "grad_norm": 7.8157958984375,
      "learning_rate": 4.3240875912408766e-05,
      "loss": 1.4166,
      "step": 18520
    },
    {
      "epoch": 135.25547445255475,
      "grad_norm": 5.757903575897217,
      "learning_rate": 4.3237226277372266e-05,
      "loss": 1.3256,
      "step": 18530
    },
    {
      "epoch": 135.32846715328466,
      "grad_norm": 8.766945838928223,
      "learning_rate": 4.323357664233577e-05,
      "loss": 1.6287,
      "step": 18540
    },
    {
      "epoch": 135.4014598540146,
      "grad_norm": 12.605944633483887,
      "learning_rate": 4.322992700729927e-05,
      "loss": 1.5459,
      "step": 18550
    },
    {
      "epoch": 135.47445255474452,
      "grad_norm": 11.279940605163574,
      "learning_rate": 4.3226277372262774e-05,
      "loss": 1.4575,
      "step": 18560
    },
    {
      "epoch": 135.54744525547446,
      "grad_norm": 4.448019504547119,
      "learning_rate": 4.322262773722628e-05,
      "loss": 1.3579,
      "step": 18570
    },
    {
      "epoch": 135.62043795620437,
      "grad_norm": 11.35698413848877,
      "learning_rate": 4.321897810218978e-05,
      "loss": 1.2314,
      "step": 18580
    },
    {
      "epoch": 135.6934306569343,
      "grad_norm": 10.098395347595215,
      "learning_rate": 4.321532846715329e-05,
      "loss": 1.36,
      "step": 18590
    },
    {
      "epoch": 135.76642335766422,
      "grad_norm": 12.963188171386719,
      "learning_rate": 4.321167883211679e-05,
      "loss": 1.2268,
      "step": 18600
    },
    {
      "epoch": 135.83941605839416,
      "grad_norm": 8.501324653625488,
      "learning_rate": 4.32080291970803e-05,
      "loss": 1.6185,
      "step": 18610
    },
    {
      "epoch": 135.91240875912408,
      "grad_norm": 12.253329277038574,
      "learning_rate": 4.32043795620438e-05,
      "loss": 1.1766,
      "step": 18620
    },
    {
      "epoch": 135.98540145985402,
      "grad_norm": 11.811473846435547,
      "learning_rate": 4.32007299270073e-05,
      "loss": 1.4325,
      "step": 18630
    },
    {
      "epoch": 136.05839416058393,
      "grad_norm": 6.4793381690979,
      "learning_rate": 4.3197080291970806e-05,
      "loss": 1.2355,
      "step": 18640
    },
    {
      "epoch": 136.13138686131387,
      "grad_norm": 5.092779636383057,
      "learning_rate": 4.3193430656934306e-05,
      "loss": 1.1075,
      "step": 18650
    },
    {
      "epoch": 136.20437956204378,
      "grad_norm": 7.388583183288574,
      "learning_rate": 4.318978102189781e-05,
      "loss": 0.9666,
      "step": 18660
    },
    {
      "epoch": 136.27737226277372,
      "grad_norm": 5.420618534088135,
      "learning_rate": 4.3186131386861314e-05,
      "loss": 1.1328,
      "step": 18670
    },
    {
      "epoch": 136.35036496350364,
      "grad_norm": 8.527416229248047,
      "learning_rate": 4.318248175182482e-05,
      "loss": 1.3208,
      "step": 18680
    },
    {
      "epoch": 136.42335766423358,
      "grad_norm": 6.288177967071533,
      "learning_rate": 4.317883211678833e-05,
      "loss": 1.328,
      "step": 18690
    },
    {
      "epoch": 136.4963503649635,
      "grad_norm": 5.784770488739014,
      "learning_rate": 4.317518248175182e-05,
      "loss": 1.7976,
      "step": 18700
    },
    {
      "epoch": 136.56934306569343,
      "grad_norm": 7.247549533843994,
      "learning_rate": 4.317153284671533e-05,
      "loss": 1.1853,
      "step": 18710
    },
    {
      "epoch": 136.64233576642334,
      "grad_norm": 6.18763542175293,
      "learning_rate": 4.316788321167884e-05,
      "loss": 1.1027,
      "step": 18720
    },
    {
      "epoch": 136.71532846715328,
      "grad_norm": 10.192784309387207,
      "learning_rate": 4.316423357664234e-05,
      "loss": 1.8108,
      "step": 18730
    },
    {
      "epoch": 136.78832116788323,
      "grad_norm": 6.4306206703186035,
      "learning_rate": 4.3160583941605844e-05,
      "loss": 1.4463,
      "step": 18740
    },
    {
      "epoch": 136.86131386861314,
      "grad_norm": 8.85124683380127,
      "learning_rate": 4.3156934306569345e-05,
      "loss": 1.3033,
      "step": 18750
    },
    {
      "epoch": 136.93430656934308,
      "grad_norm": 10.037795066833496,
      "learning_rate": 4.315328467153285e-05,
      "loss": 1.6984,
      "step": 18760
    },
    {
      "epoch": 137.007299270073,
      "grad_norm": 7.412909984588623,
      "learning_rate": 4.314963503649635e-05,
      "loss": 1.5297,
      "step": 18770
    },
    {
      "epoch": 137.08029197080293,
      "grad_norm": 7.287370204925537,
      "learning_rate": 4.314598540145985e-05,
      "loss": 1.117,
      "step": 18780
    },
    {
      "epoch": 137.15328467153284,
      "grad_norm": 8.617700576782227,
      "learning_rate": 4.314233576642336e-05,
      "loss": 1.5059,
      "step": 18790
    },
    {
      "epoch": 137.22627737226279,
      "grad_norm": 9.522468566894531,
      "learning_rate": 4.313868613138686e-05,
      "loss": 1.4003,
      "step": 18800
    },
    {
      "epoch": 137.2992700729927,
      "grad_norm": 13.24104118347168,
      "learning_rate": 4.313503649635037e-05,
      "loss": 1.3543,
      "step": 18810
    },
    {
      "epoch": 137.37226277372264,
      "grad_norm": 6.46738338470459,
      "learning_rate": 4.313138686131387e-05,
      "loss": 1.3111,
      "step": 18820
    },
    {
      "epoch": 137.44525547445255,
      "grad_norm": 3.630730628967285,
      "learning_rate": 4.3127737226277376e-05,
      "loss": 1.2993,
      "step": 18830
    },
    {
      "epoch": 137.5182481751825,
      "grad_norm": 10.509791374206543,
      "learning_rate": 4.312408759124088e-05,
      "loss": 1.5025,
      "step": 18840
    },
    {
      "epoch": 137.5912408759124,
      "grad_norm": 8.058316230773926,
      "learning_rate": 4.312043795620438e-05,
      "loss": 1.6901,
      "step": 18850
    },
    {
      "epoch": 137.66423357664235,
      "grad_norm": 7.799978733062744,
      "learning_rate": 4.3116788321167884e-05,
      "loss": 1.224,
      "step": 18860
    },
    {
      "epoch": 137.73722627737226,
      "grad_norm": 10.969893455505371,
      "learning_rate": 4.3113138686131385e-05,
      "loss": 1.4121,
      "step": 18870
    },
    {
      "epoch": 137.8102189781022,
      "grad_norm": 7.911336421966553,
      "learning_rate": 4.310948905109489e-05,
      "loss": 1.1442,
      "step": 18880
    },
    {
      "epoch": 137.8832116788321,
      "grad_norm": 7.9951324462890625,
      "learning_rate": 4.31058394160584e-05,
      "loss": 1.2506,
      "step": 18890
    },
    {
      "epoch": 137.95620437956205,
      "grad_norm": 9.699145317077637,
      "learning_rate": 4.31021897810219e-05,
      "loss": 1.4909,
      "step": 18900
    },
    {
      "epoch": 138.02919708029196,
      "grad_norm": 9.485210418701172,
      "learning_rate": 4.309854014598541e-05,
      "loss": 1.4572,
      "step": 18910
    },
    {
      "epoch": 138.1021897810219,
      "grad_norm": 8.789347648620605,
      "learning_rate": 4.309489051094891e-05,
      "loss": 1.1803,
      "step": 18920
    },
    {
      "epoch": 138.17518248175182,
      "grad_norm": 3.914372682571411,
      "learning_rate": 4.309124087591241e-05,
      "loss": 1.1377,
      "step": 18930
    },
    {
      "epoch": 138.24817518248176,
      "grad_norm": 9.800630569458008,
      "learning_rate": 4.3087591240875915e-05,
      "loss": 1.7496,
      "step": 18940
    },
    {
      "epoch": 138.32116788321167,
      "grad_norm": 3.1970901489257812,
      "learning_rate": 4.3083941605839416e-05,
      "loss": 1.0865,
      "step": 18950
    },
    {
      "epoch": 138.3941605839416,
      "grad_norm": 6.37898063659668,
      "learning_rate": 4.308029197080292e-05,
      "loss": 1.3317,
      "step": 18960
    },
    {
      "epoch": 138.46715328467153,
      "grad_norm": 7.374994277954102,
      "learning_rate": 4.3076642335766424e-05,
      "loss": 1.1285,
      "step": 18970
    },
    {
      "epoch": 138.54014598540147,
      "grad_norm": 15.161067008972168,
      "learning_rate": 4.307299270072993e-05,
      "loss": 1.4576,
      "step": 18980
    },
    {
      "epoch": 138.61313868613138,
      "grad_norm": 6.8161702156066895,
      "learning_rate": 4.306934306569344e-05,
      "loss": 1.5053,
      "step": 18990
    },
    {
      "epoch": 138.68613138686132,
      "grad_norm": 9.117392539978027,
      "learning_rate": 4.306569343065693e-05,
      "loss": 1.3272,
      "step": 19000
    },
    {
      "epoch": 138.75912408759123,
      "grad_norm": 7.771666526794434,
      "learning_rate": 4.306204379562044e-05,
      "loss": 1.4689,
      "step": 19010
    },
    {
      "epoch": 138.83211678832117,
      "grad_norm": 8.69596004486084,
      "learning_rate": 4.305839416058394e-05,
      "loss": 1.3603,
      "step": 19020
    },
    {
      "epoch": 138.90510948905109,
      "grad_norm": 8.611213684082031,
      "learning_rate": 4.305474452554745e-05,
      "loss": 1.4137,
      "step": 19030
    },
    {
      "epoch": 138.97810218978103,
      "grad_norm": 9.217584609985352,
      "learning_rate": 4.3051094890510954e-05,
      "loss": 1.5203,
      "step": 19040
    },
    {
      "epoch": 139.05109489051094,
      "grad_norm": 11.72304630279541,
      "learning_rate": 4.3047445255474455e-05,
      "loss": 1.1563,
      "step": 19050
    },
    {
      "epoch": 139.12408759124088,
      "grad_norm": 7.434545516967773,
      "learning_rate": 4.304379562043796e-05,
      "loss": 1.4809,
      "step": 19060
    },
    {
      "epoch": 139.1970802919708,
      "grad_norm": 4.773085117340088,
      "learning_rate": 4.304014598540146e-05,
      "loss": 1.0785,
      "step": 19070
    },
    {
      "epoch": 139.27007299270073,
      "grad_norm": 12.266768455505371,
      "learning_rate": 4.303649635036496e-05,
      "loss": 1.3524,
      "step": 19080
    },
    {
      "epoch": 139.34306569343065,
      "grad_norm": 11.812087059020996,
      "learning_rate": 4.303284671532847e-05,
      "loss": 1.5227,
      "step": 19090
    },
    {
      "epoch": 139.4160583941606,
      "grad_norm": 16.40105438232422,
      "learning_rate": 4.302919708029197e-05,
      "loss": 1.8951,
      "step": 19100
    },
    {
      "epoch": 139.4890510948905,
      "grad_norm": 5.452486515045166,
      "learning_rate": 4.302554744525548e-05,
      "loss": 1.2365,
      "step": 19110
    },
    {
      "epoch": 139.56204379562044,
      "grad_norm": 5.055355548858643,
      "learning_rate": 4.302189781021898e-05,
      "loss": 1.1113,
      "step": 19120
    },
    {
      "epoch": 139.63503649635035,
      "grad_norm": 7.601513385772705,
      "learning_rate": 4.3018248175182486e-05,
      "loss": 1.445,
      "step": 19130
    },
    {
      "epoch": 139.7080291970803,
      "grad_norm": 2.761281728744507,
      "learning_rate": 4.3014598540145986e-05,
      "loss": 1.3772,
      "step": 19140
    },
    {
      "epoch": 139.7810218978102,
      "grad_norm": 5.766668319702148,
      "learning_rate": 4.3010948905109493e-05,
      "loss": 1.4249,
      "step": 19150
    },
    {
      "epoch": 139.85401459854015,
      "grad_norm": 10.883845329284668,
      "learning_rate": 4.3007299270072994e-05,
      "loss": 1.4384,
      "step": 19160
    },
    {
      "epoch": 139.92700729927006,
      "grad_norm": 5.702410697937012,
      "learning_rate": 4.3003649635036494e-05,
      "loss": 1.2126,
      "step": 19170
    },
    {
      "epoch": 140.0,
      "grad_norm": 10.744924545288086,
      "learning_rate": 4.3e-05,
      "loss": 1.2759,
      "step": 19180
    },
    {
      "epoch": 140.07299270072994,
      "grad_norm": 6.589254379272461,
      "learning_rate": 4.299635036496351e-05,
      "loss": 1.4774,
      "step": 19190
    },
    {
      "epoch": 140.14598540145985,
      "grad_norm": 3.86979079246521,
      "learning_rate": 4.299270072992701e-05,
      "loss": 1.4379,
      "step": 19200
    },
    {
      "epoch": 140.2189781021898,
      "grad_norm": 7.242737293243408,
      "learning_rate": 4.298905109489052e-05,
      "loss": 1.0898,
      "step": 19210
    },
    {
      "epoch": 140.2919708029197,
      "grad_norm": 7.148710250854492,
      "learning_rate": 4.298540145985402e-05,
      "loss": 1.4785,
      "step": 19220
    },
    {
      "epoch": 140.36496350364965,
      "grad_norm": 10.74510383605957,
      "learning_rate": 4.298175182481752e-05,
      "loss": 1.2205,
      "step": 19230
    },
    {
      "epoch": 140.43795620437956,
      "grad_norm": 6.874517440795898,
      "learning_rate": 4.2978102189781025e-05,
      "loss": 1.4028,
      "step": 19240
    },
    {
      "epoch": 140.5109489051095,
      "grad_norm": 6.725882530212402,
      "learning_rate": 4.2974452554744525e-05,
      "loss": 1.255,
      "step": 19250
    },
    {
      "epoch": 140.5839416058394,
      "grad_norm": 9.621070861816406,
      "learning_rate": 4.297080291970803e-05,
      "loss": 1.2563,
      "step": 19260
    },
    {
      "epoch": 140.65693430656935,
      "grad_norm": 4.908504486083984,
      "learning_rate": 4.296715328467153e-05,
      "loss": 0.9887,
      "step": 19270
    },
    {
      "epoch": 140.72992700729927,
      "grad_norm": 4.912956237792969,
      "learning_rate": 4.296350364963504e-05,
      "loss": 1.435,
      "step": 19280
    },
    {
      "epoch": 140.8029197080292,
      "grad_norm": 11.487340927124023,
      "learning_rate": 4.295985401459854e-05,
      "loss": 1.3858,
      "step": 19290
    },
    {
      "epoch": 140.87591240875912,
      "grad_norm": 15.939202308654785,
      "learning_rate": 4.295620437956205e-05,
      "loss": 1.2247,
      "step": 19300
    },
    {
      "epoch": 140.94890510948906,
      "grad_norm": 3.9889068603515625,
      "learning_rate": 4.295255474452555e-05,
      "loss": 1.5123,
      "step": 19310
    },
    {
      "epoch": 141.02189781021897,
      "grad_norm": 3.420717716217041,
      "learning_rate": 4.294890510948905e-05,
      "loss": 1.6142,
      "step": 19320
    },
    {
      "epoch": 141.09489051094891,
      "grad_norm": 7.756557464599609,
      "learning_rate": 4.2945255474452557e-05,
      "loss": 1.2163,
      "step": 19330
    },
    {
      "epoch": 141.16788321167883,
      "grad_norm": 7.852048397064209,
      "learning_rate": 4.294160583941606e-05,
      "loss": 1.1113,
      "step": 19340
    },
    {
      "epoch": 141.24087591240877,
      "grad_norm": 8.810816764831543,
      "learning_rate": 4.2937956204379564e-05,
      "loss": 1.2708,
      "step": 19350
    },
    {
      "epoch": 141.31386861313868,
      "grad_norm": 5.585315227508545,
      "learning_rate": 4.293430656934307e-05,
      "loss": 1.6434,
      "step": 19360
    },
    {
      "epoch": 141.38686131386862,
      "grad_norm": 9.305874824523926,
      "learning_rate": 4.293065693430657e-05,
      "loss": 1.0037,
      "step": 19370
    },
    {
      "epoch": 141.45985401459853,
      "grad_norm": 6.288941383361816,
      "learning_rate": 4.292700729927008e-05,
      "loss": 1.2046,
      "step": 19380
    },
    {
      "epoch": 141.53284671532847,
      "grad_norm": 5.3597307205200195,
      "learning_rate": 4.292335766423358e-05,
      "loss": 1.4855,
      "step": 19390
    },
    {
      "epoch": 141.6058394160584,
      "grad_norm": 7.005001068115234,
      "learning_rate": 4.291970802919708e-05,
      "loss": 1.0016,
      "step": 19400
    },
    {
      "epoch": 141.67883211678833,
      "grad_norm": 7.48416805267334,
      "learning_rate": 4.291605839416059e-05,
      "loss": 1.6536,
      "step": 19410
    },
    {
      "epoch": 141.75182481751824,
      "grad_norm": 9.383573532104492,
      "learning_rate": 4.291240875912409e-05,
      "loss": 1.1442,
      "step": 19420
    },
    {
      "epoch": 141.82481751824818,
      "grad_norm": 6.140207290649414,
      "learning_rate": 4.2908759124087595e-05,
      "loss": 1.2483,
      "step": 19430
    },
    {
      "epoch": 141.8978102189781,
      "grad_norm": 7.24576473236084,
      "learning_rate": 4.2905109489051096e-05,
      "loss": 1.5968,
      "step": 19440
    },
    {
      "epoch": 141.97080291970804,
      "grad_norm": 9.860665321350098,
      "learning_rate": 4.29014598540146e-05,
      "loss": 1.6474,
      "step": 19450
    },
    {
      "epoch": 142.04379562043795,
      "grad_norm": 7.497220516204834,
      "learning_rate": 4.2897810218978104e-05,
      "loss": 1.1625,
      "step": 19460
    },
    {
      "epoch": 142.1167883211679,
      "grad_norm": 8.64724349975586,
      "learning_rate": 4.2894160583941604e-05,
      "loss": 1.4596,
      "step": 19470
    },
    {
      "epoch": 142.1897810218978,
      "grad_norm": 7.595546245574951,
      "learning_rate": 4.289051094890511e-05,
      "loss": 1.4287,
      "step": 19480
    },
    {
      "epoch": 142.26277372262774,
      "grad_norm": 5.896034240722656,
      "learning_rate": 4.288686131386861e-05,
      "loss": 1.241,
      "step": 19490
    },
    {
      "epoch": 142.33576642335765,
      "grad_norm": 8.703298568725586,
      "learning_rate": 4.288321167883212e-05,
      "loss": 1.805,
      "step": 19500
    },
    {
      "epoch": 142.4087591240876,
      "grad_norm": 9.289847373962402,
      "learning_rate": 4.2879562043795626e-05,
      "loss": 1.1461,
      "step": 19510
    },
    {
      "epoch": 142.4817518248175,
      "grad_norm": 11.950377464294434,
      "learning_rate": 4.287591240875913e-05,
      "loss": 1.3331,
      "step": 19520
    },
    {
      "epoch": 142.55474452554745,
      "grad_norm": 3.5764312744140625,
      "learning_rate": 4.2872262773722634e-05,
      "loss": 1.3138,
      "step": 19530
    },
    {
      "epoch": 142.62773722627736,
      "grad_norm": 8.92115592956543,
      "learning_rate": 4.286861313868613e-05,
      "loss": 1.3847,
      "step": 19540
    },
    {
      "epoch": 142.7007299270073,
      "grad_norm": 11.552342414855957,
      "learning_rate": 4.2864963503649635e-05,
      "loss": 1.3389,
      "step": 19550
    },
    {
      "epoch": 142.77372262773721,
      "grad_norm": 4.955410003662109,
      "learning_rate": 4.286131386861314e-05,
      "loss": 1.1393,
      "step": 19560
    },
    {
      "epoch": 142.84671532846716,
      "grad_norm": 6.728199481964111,
      "learning_rate": 4.285766423357664e-05,
      "loss": 1.4162,
      "step": 19570
    },
    {
      "epoch": 142.91970802919707,
      "grad_norm": 13.46131420135498,
      "learning_rate": 4.285401459854015e-05,
      "loss": 1.1613,
      "step": 19580
    },
    {
      "epoch": 142.992700729927,
      "grad_norm": 8.300626754760742,
      "learning_rate": 4.285036496350365e-05,
      "loss": 1.3045,
      "step": 19590
    },
    {
      "epoch": 143.06569343065692,
      "grad_norm": 10.0095853805542,
      "learning_rate": 4.284671532846716e-05,
      "loss": 1.4676,
      "step": 19600
    },
    {
      "epoch": 143.13868613138686,
      "grad_norm": 8.046622276306152,
      "learning_rate": 4.284306569343066e-05,
      "loss": 1.3751,
      "step": 19610
    },
    {
      "epoch": 143.21167883211677,
      "grad_norm": 7.2149977684021,
      "learning_rate": 4.283941605839416e-05,
      "loss": 1.3293,
      "step": 19620
    },
    {
      "epoch": 143.28467153284672,
      "grad_norm": 6.936398983001709,
      "learning_rate": 4.2835766423357666e-05,
      "loss": 1.1157,
      "step": 19630
    },
    {
      "epoch": 143.35766423357666,
      "grad_norm": 2.977221727371216,
      "learning_rate": 4.283211678832117e-05,
      "loss": 0.8347,
      "step": 19640
    },
    {
      "epoch": 143.43065693430657,
      "grad_norm": 7.130316257476807,
      "learning_rate": 4.2828467153284674e-05,
      "loss": 1.112,
      "step": 19650
    },
    {
      "epoch": 143.5036496350365,
      "grad_norm": 8.471450805664062,
      "learning_rate": 4.282481751824818e-05,
      "loss": 1.4634,
      "step": 19660
    },
    {
      "epoch": 143.57664233576642,
      "grad_norm": 6.397084712982178,
      "learning_rate": 4.282116788321168e-05,
      "loss": 1.4521,
      "step": 19670
    },
    {
      "epoch": 143.64963503649636,
      "grad_norm": 11.52154541015625,
      "learning_rate": 4.281751824817519e-05,
      "loss": 1.4299,
      "step": 19680
    },
    {
      "epoch": 143.72262773722628,
      "grad_norm": 10.818573951721191,
      "learning_rate": 4.281386861313868e-05,
      "loss": 1.3941,
      "step": 19690
    },
    {
      "epoch": 143.79562043795622,
      "grad_norm": 2.3348546028137207,
      "learning_rate": 4.281021897810219e-05,
      "loss": 1.3433,
      "step": 19700
    },
    {
      "epoch": 143.86861313868613,
      "grad_norm": 6.939274311065674,
      "learning_rate": 4.28065693430657e-05,
      "loss": 1.369,
      "step": 19710
    },
    {
      "epoch": 143.94160583941607,
      "grad_norm": 7.381412506103516,
      "learning_rate": 4.28029197080292e-05,
      "loss": 1.2584,
      "step": 19720
    },
    {
      "epoch": 144.01459854014598,
      "grad_norm": 6.930603504180908,
      "learning_rate": 4.2799270072992705e-05,
      "loss": 1.2493,
      "step": 19730
    },
    {
      "epoch": 144.08759124087592,
      "grad_norm": 12.887569427490234,
      "learning_rate": 4.2795620437956206e-05,
      "loss": 1.4363,
      "step": 19740
    },
    {
      "epoch": 144.16058394160584,
      "grad_norm": 10.273863792419434,
      "learning_rate": 4.279197080291971e-05,
      "loss": 1.5012,
      "step": 19750
    },
    {
      "epoch": 144.23357664233578,
      "grad_norm": 10.135222434997559,
      "learning_rate": 4.278832116788321e-05,
      "loss": 1.2242,
      "step": 19760
    },
    {
      "epoch": 144.3065693430657,
      "grad_norm": 11.728250503540039,
      "learning_rate": 4.2784671532846714e-05,
      "loss": 1.2642,
      "step": 19770
    },
    {
      "epoch": 144.37956204379563,
      "grad_norm": 6.970035552978516,
      "learning_rate": 4.278102189781022e-05,
      "loss": 1.136,
      "step": 19780
    },
    {
      "epoch": 144.45255474452554,
      "grad_norm": 6.894661903381348,
      "learning_rate": 4.277737226277372e-05,
      "loss": 1.4168,
      "step": 19790
    },
    {
      "epoch": 144.52554744525548,
      "grad_norm": 8.249645233154297,
      "learning_rate": 4.277372262773723e-05,
      "loss": 1.2158,
      "step": 19800
    },
    {
      "epoch": 144.5985401459854,
      "grad_norm": 8.395763397216797,
      "learning_rate": 4.277007299270073e-05,
      "loss": 1.0946,
      "step": 19810
    },
    {
      "epoch": 144.67153284671534,
      "grad_norm": 9.02692985534668,
      "learning_rate": 4.2766423357664237e-05,
      "loss": 1.4015,
      "step": 19820
    },
    {
      "epoch": 144.74452554744525,
      "grad_norm": 6.709798812866211,
      "learning_rate": 4.2762773722627744e-05,
      "loss": 1.0605,
      "step": 19830
    },
    {
      "epoch": 144.8175182481752,
      "grad_norm": 9.413745880126953,
      "learning_rate": 4.275912408759124e-05,
      "loss": 1.4244,
      "step": 19840
    },
    {
      "epoch": 144.8905109489051,
      "grad_norm": 8.203540802001953,
      "learning_rate": 4.2755474452554745e-05,
      "loss": 1.3729,
      "step": 19850
    },
    {
      "epoch": 144.96350364963504,
      "grad_norm": 7.496875762939453,
      "learning_rate": 4.275182481751825e-05,
      "loss": 1.4471,
      "step": 19860
    },
    {
      "epoch": 145.03649635036496,
      "grad_norm": 6.9959235191345215,
      "learning_rate": 4.274817518248175e-05,
      "loss": 1.5703,
      "step": 19870
    },
    {
      "epoch": 145.1094890510949,
      "grad_norm": 8.61933422088623,
      "learning_rate": 4.274452554744526e-05,
      "loss": 1.5217,
      "step": 19880
    },
    {
      "epoch": 145.1824817518248,
      "grad_norm": 7.526329517364502,
      "learning_rate": 4.274087591240876e-05,
      "loss": 1.5164,
      "step": 19890
    },
    {
      "epoch": 145.25547445255475,
      "grad_norm": 6.29282283782959,
      "learning_rate": 4.273722627737227e-05,
      "loss": 1.1064,
      "step": 19900
    },
    {
      "epoch": 145.32846715328466,
      "grad_norm": 6.022654056549072,
      "learning_rate": 4.273357664233577e-05,
      "loss": 1.2292,
      "step": 19910
    },
    {
      "epoch": 145.4014598540146,
      "grad_norm": 12.041536331176758,
      "learning_rate": 4.272992700729927e-05,
      "loss": 1.6053,
      "step": 19920
    },
    {
      "epoch": 145.47445255474452,
      "grad_norm": 10.447397232055664,
      "learning_rate": 4.2726277372262776e-05,
      "loss": 1.5818,
      "step": 19930
    },
    {
      "epoch": 145.54744525547446,
      "grad_norm": 4.328502178192139,
      "learning_rate": 4.2722627737226276e-05,
      "loss": 1.0499,
      "step": 19940
    },
    {
      "epoch": 145.62043795620437,
      "grad_norm": 6.538865089416504,
      "learning_rate": 4.2718978102189784e-05,
      "loss": 1.4841,
      "step": 19950
    },
    {
      "epoch": 145.6934306569343,
      "grad_norm": 9.667201042175293,
      "learning_rate": 4.2715328467153284e-05,
      "loss": 1.4102,
      "step": 19960
    },
    {
      "epoch": 145.76642335766422,
      "grad_norm": 3.8358545303344727,
      "learning_rate": 4.271167883211679e-05,
      "loss": 0.9049,
      "step": 19970
    },
    {
      "epoch": 145.83941605839416,
      "grad_norm": 4.595722675323486,
      "learning_rate": 4.27080291970803e-05,
      "loss": 1.2475,
      "step": 19980
    },
    {
      "epoch": 145.91240875912408,
      "grad_norm": 5.96580696105957,
      "learning_rate": 4.27043795620438e-05,
      "loss": 0.999,
      "step": 19990
    },
    {
      "epoch": 145.98540145985402,
      "grad_norm": 6.712390422821045,
      "learning_rate": 4.27007299270073e-05,
      "loss": 1.2781,
      "step": 20000
    },
    {
      "epoch": 146.05839416058393,
      "grad_norm": 11.596929550170898,
      "learning_rate": 4.26970802919708e-05,
      "loss": 1.4685,
      "step": 20010
    },
    {
      "epoch": 146.13138686131387,
      "grad_norm": 1.9239627122879028,
      "learning_rate": 4.269343065693431e-05,
      "loss": 1.2636,
      "step": 20020
    },
    {
      "epoch": 146.20437956204378,
      "grad_norm": 9.148489952087402,
      "learning_rate": 4.2689781021897815e-05,
      "loss": 1.0608,
      "step": 20030
    },
    {
      "epoch": 146.27737226277372,
      "grad_norm": 12.462112426757812,
      "learning_rate": 4.2686131386861315e-05,
      "loss": 1.3426,
      "step": 20040
    },
    {
      "epoch": 146.35036496350364,
      "grad_norm": 9.272125244140625,
      "learning_rate": 4.268248175182482e-05,
      "loss": 1.0976,
      "step": 20050
    },
    {
      "epoch": 146.42335766423358,
      "grad_norm": 11.595162391662598,
      "learning_rate": 4.267883211678832e-05,
      "loss": 1.0848,
      "step": 20060
    },
    {
      "epoch": 146.4963503649635,
      "grad_norm": 8.303450584411621,
      "learning_rate": 4.2675182481751823e-05,
      "loss": 1.3477,
      "step": 20070
    },
    {
      "epoch": 146.56934306569343,
      "grad_norm": 9.387304306030273,
      "learning_rate": 4.267153284671533e-05,
      "loss": 1.5296,
      "step": 20080
    },
    {
      "epoch": 146.64233576642334,
      "grad_norm": 7.08400297164917,
      "learning_rate": 4.266788321167883e-05,
      "loss": 1.7088,
      "step": 20090
    },
    {
      "epoch": 146.71532846715328,
      "grad_norm": 13.165305137634277,
      "learning_rate": 4.266423357664234e-05,
      "loss": 1.3649,
      "step": 20100
    },
    {
      "epoch": 146.78832116788323,
      "grad_norm": 13.81842041015625,
      "learning_rate": 4.266058394160584e-05,
      "loss": 1.253,
      "step": 20110
    },
    {
      "epoch": 146.86131386861314,
      "grad_norm": 9.140515327453613,
      "learning_rate": 4.2656934306569346e-05,
      "loss": 1.2601,
      "step": 20120
    },
    {
      "epoch": 146.93430656934308,
      "grad_norm": 12.091176986694336,
      "learning_rate": 4.2653284671532854e-05,
      "loss": 1.0327,
      "step": 20130
    },
    {
      "epoch": 147.007299270073,
      "grad_norm": 7.391693592071533,
      "learning_rate": 4.2649635036496354e-05,
      "loss": 1.4749,
      "step": 20140
    },
    {
      "epoch": 147.08029197080293,
      "grad_norm": 8.746590614318848,
      "learning_rate": 4.2645985401459855e-05,
      "loss": 1.5906,
      "step": 20150
    },
    {
      "epoch": 147.15328467153284,
      "grad_norm": 8.493154525756836,
      "learning_rate": 4.2642335766423355e-05,
      "loss": 1.0628,
      "step": 20160
    },
    {
      "epoch": 147.22627737226279,
      "grad_norm": 8.115143775939941,
      "learning_rate": 4.263868613138686e-05,
      "loss": 1.4347,
      "step": 20170
    },
    {
      "epoch": 147.2992700729927,
      "grad_norm": 7.028315544128418,
      "learning_rate": 4.263503649635037e-05,
      "loss": 1.2557,
      "step": 20180
    },
    {
      "epoch": 147.37226277372264,
      "grad_norm": 8.135947227478027,
      "learning_rate": 4.263138686131387e-05,
      "loss": 1.1806,
      "step": 20190
    },
    {
      "epoch": 147.44525547445255,
      "grad_norm": 2.285234212875366,
      "learning_rate": 4.262773722627738e-05,
      "loss": 1.2022,
      "step": 20200
    },
    {
      "epoch": 147.5182481751825,
      "grad_norm": 12.398832321166992,
      "learning_rate": 4.262408759124088e-05,
      "loss": 1.2246,
      "step": 20210
    },
    {
      "epoch": 147.5912408759124,
      "grad_norm": 13.471467971801758,
      "learning_rate": 4.2620437956204385e-05,
      "loss": 1.4464,
      "step": 20220
    },
    {
      "epoch": 147.66423357664235,
      "grad_norm": 7.519443511962891,
      "learning_rate": 4.2616788321167886e-05,
      "loss": 1.1214,
      "step": 20230
    },
    {
      "epoch": 147.73722627737226,
      "grad_norm": 5.42017936706543,
      "learning_rate": 4.2613138686131386e-05,
      "loss": 1.3221,
      "step": 20240
    },
    {
      "epoch": 147.8102189781022,
      "grad_norm": 4.287572860717773,
      "learning_rate": 4.260948905109489e-05,
      "loss": 1.0942,
      "step": 20250
    },
    {
      "epoch": 147.8832116788321,
      "grad_norm": 10.229798316955566,
      "learning_rate": 4.2605839416058394e-05,
      "loss": 1.4322,
      "step": 20260
    },
    {
      "epoch": 147.95620437956205,
      "grad_norm": 13.04385757446289,
      "learning_rate": 4.26021897810219e-05,
      "loss": 1.204,
      "step": 20270
    },
    {
      "epoch": 148.02919708029196,
      "grad_norm": 3.5754384994506836,
      "learning_rate": 4.25985401459854e-05,
      "loss": 1.1047,
      "step": 20280
    },
    {
      "epoch": 148.1021897810219,
      "grad_norm": 10.09145450592041,
      "learning_rate": 4.259489051094891e-05,
      "loss": 1.2368,
      "step": 20290
    },
    {
      "epoch": 148.17518248175182,
      "grad_norm": 2.750020742416382,
      "learning_rate": 4.259124087591241e-05,
      "loss": 1.2065,
      "step": 20300
    },
    {
      "epoch": 148.24817518248176,
      "grad_norm": 14.011116981506348,
      "learning_rate": 4.258759124087591e-05,
      "loss": 1.4252,
      "step": 20310
    },
    {
      "epoch": 148.32116788321167,
      "grad_norm": 11.739117622375488,
      "learning_rate": 4.258394160583942e-05,
      "loss": 1.3321,
      "step": 20320
    },
    {
      "epoch": 148.3941605839416,
      "grad_norm": 7.798165321350098,
      "learning_rate": 4.2580291970802924e-05,
      "loss": 1.3439,
      "step": 20330
    },
    {
      "epoch": 148.46715328467153,
      "grad_norm": 3.5263442993164062,
      "learning_rate": 4.2576642335766425e-05,
      "loss": 0.933,
      "step": 20340
    },
    {
      "epoch": 148.54014598540147,
      "grad_norm": 7.1445746421813965,
      "learning_rate": 4.257299270072993e-05,
      "loss": 0.9972,
      "step": 20350
    },
    {
      "epoch": 148.61313868613138,
      "grad_norm": 6.902267932891846,
      "learning_rate": 4.256934306569343e-05,
      "loss": 1.4702,
      "step": 20360
    },
    {
      "epoch": 148.68613138686132,
      "grad_norm": 4.73543119430542,
      "learning_rate": 4.256569343065694e-05,
      "loss": 1.3526,
      "step": 20370
    },
    {
      "epoch": 148.75912408759123,
      "grad_norm": 6.157570838928223,
      "learning_rate": 4.256204379562044e-05,
      "loss": 1.3257,
      "step": 20380
    },
    {
      "epoch": 148.83211678832117,
      "grad_norm": 11.648691177368164,
      "learning_rate": 4.255839416058394e-05,
      "loss": 1.2191,
      "step": 20390
    },
    {
      "epoch": 148.90510948905109,
      "grad_norm": 10.047513961791992,
      "learning_rate": 4.255474452554745e-05,
      "loss": 1.2692,
      "step": 20400
    },
    {
      "epoch": 148.97810218978103,
      "grad_norm": 4.846639633178711,
      "learning_rate": 4.255109489051095e-05,
      "loss": 1.5753,
      "step": 20410
    },
    {
      "epoch": 149.05109489051094,
      "grad_norm": 8.841049194335938,
      "learning_rate": 4.2547445255474456e-05,
      "loss": 1.4508,
      "step": 20420
    },
    {
      "epoch": 149.12408759124088,
      "grad_norm": 9.56186580657959,
      "learning_rate": 4.2543795620437956e-05,
      "loss": 1.173,
      "step": 20430
    },
    {
      "epoch": 149.1970802919708,
      "grad_norm": 8.38294506072998,
      "learning_rate": 4.2540145985401464e-05,
      "loss": 1.2186,
      "step": 20440
    },
    {
      "epoch": 149.27007299270073,
      "grad_norm": 9.2018404006958,
      "learning_rate": 4.253649635036497e-05,
      "loss": 1.1635,
      "step": 20450
    },
    {
      "epoch": 149.34306569343065,
      "grad_norm": 11.681488990783691,
      "learning_rate": 4.2532846715328465e-05,
      "loss": 1.4969,
      "step": 20460
    },
    {
      "epoch": 149.4160583941606,
      "grad_norm": 7.43720817565918,
      "learning_rate": 4.252919708029197e-05,
      "loss": 1.0031,
      "step": 20470
    },
    {
      "epoch": 149.4890510948905,
      "grad_norm": 9.202557563781738,
      "learning_rate": 4.252554744525547e-05,
      "loss": 1.1799,
      "step": 20480
    },
    {
      "epoch": 149.56204379562044,
      "grad_norm": 10.748114585876465,
      "learning_rate": 4.252189781021898e-05,
      "loss": 0.8955,
      "step": 20490
    },
    {
      "epoch": 149.63503649635035,
      "grad_norm": 4.9273271560668945,
      "learning_rate": 4.251824817518249e-05,
      "loss": 1.4254,
      "step": 20500
    },
    {
      "epoch": 149.7080291970803,
      "grad_norm": 3.4148733615875244,
      "learning_rate": 4.251459854014599e-05,
      "loss": 1.1281,
      "step": 20510
    },
    {
      "epoch": 149.7810218978102,
      "grad_norm": 3.4779818058013916,
      "learning_rate": 4.2510948905109495e-05,
      "loss": 1.2473,
      "step": 20520
    },
    {
      "epoch": 149.85401459854015,
      "grad_norm": 10.389599800109863,
      "learning_rate": 4.2507299270072995e-05,
      "loss": 1.5507,
      "step": 20530
    },
    {
      "epoch": 149.92700729927006,
      "grad_norm": 6.4060492515563965,
      "learning_rate": 4.2503649635036496e-05,
      "loss": 1.3204,
      "step": 20540
    },
    {
      "epoch": 150.0,
      "grad_norm": 6.635114669799805,
      "learning_rate": 4.25e-05,
      "loss": 1.7127,
      "step": 20550
    },
    {
      "epoch": 150.07299270072994,
      "grad_norm": 6.249308109283447,
      "learning_rate": 4.2496350364963504e-05,
      "loss": 1.0495,
      "step": 20560
    },
    {
      "epoch": 150.14598540145985,
      "grad_norm": 2.3351359367370605,
      "learning_rate": 4.249270072992701e-05,
      "loss": 1.1561,
      "step": 20570
    },
    {
      "epoch": 150.2189781021898,
      "grad_norm": 10.808212280273438,
      "learning_rate": 4.248905109489051e-05,
      "loss": 1.2899,
      "step": 20580
    },
    {
      "epoch": 150.2919708029197,
      "grad_norm": 7.028171062469482,
      "learning_rate": 4.248540145985402e-05,
      "loss": 1.1221,
      "step": 20590
    },
    {
      "epoch": 150.36496350364965,
      "grad_norm": 12.318568229675293,
      "learning_rate": 4.2481751824817526e-05,
      "loss": 1.3726,
      "step": 20600
    },
    {
      "epoch": 150.43795620437956,
      "grad_norm": 7.476924419403076,
      "learning_rate": 4.247810218978102e-05,
      "loss": 1.1401,
      "step": 20610
    },
    {
      "epoch": 150.5109489051095,
      "grad_norm": 7.122749328613281,
      "learning_rate": 4.247445255474453e-05,
      "loss": 0.9888,
      "step": 20620
    },
    {
      "epoch": 150.5839416058394,
      "grad_norm": 8.885366439819336,
      "learning_rate": 4.247080291970803e-05,
      "loss": 1.32,
      "step": 20630
    },
    {
      "epoch": 150.65693430656935,
      "grad_norm": 8.696792602539062,
      "learning_rate": 4.2467153284671535e-05,
      "loss": 1.5251,
      "step": 20640
    },
    {
      "epoch": 150.72992700729927,
      "grad_norm": 5.353273391723633,
      "learning_rate": 4.246350364963504e-05,
      "loss": 1.4877,
      "step": 20650
    },
    {
      "epoch": 150.8029197080292,
      "grad_norm": 10.961183547973633,
      "learning_rate": 4.245985401459854e-05,
      "loss": 1.3275,
      "step": 20660
    },
    {
      "epoch": 150.87591240875912,
      "grad_norm": 10.994126319885254,
      "learning_rate": 4.245620437956205e-05,
      "loss": 1.1402,
      "step": 20670
    },
    {
      "epoch": 150.94890510948906,
      "grad_norm": 12.084004402160645,
      "learning_rate": 4.245255474452554e-05,
      "loss": 1.5143,
      "step": 20680
    },
    {
      "epoch": 151.02189781021897,
      "grad_norm": 9.613435745239258,
      "learning_rate": 4.244890510948905e-05,
      "loss": 1.5344,
      "step": 20690
    },
    {
      "epoch": 151.09489051094891,
      "grad_norm": 2.2836008071899414,
      "learning_rate": 4.244525547445256e-05,
      "loss": 1.1618,
      "step": 20700
    },
    {
      "epoch": 151.16788321167883,
      "grad_norm": 3.0874826908111572,
      "learning_rate": 4.244160583941606e-05,
      "loss": 0.8211,
      "step": 20710
    },
    {
      "epoch": 151.24087591240877,
      "grad_norm": 3.551917552947998,
      "learning_rate": 4.2437956204379566e-05,
      "loss": 1.2042,
      "step": 20720
    },
    {
      "epoch": 151.31386861313868,
      "grad_norm": 9.385675430297852,
      "learning_rate": 4.2434306569343066e-05,
      "loss": 1.5152,
      "step": 20730
    },
    {
      "epoch": 151.38686131386862,
      "grad_norm": 6.876314163208008,
      "learning_rate": 4.2430656934306573e-05,
      "loss": 1.1106,
      "step": 20740
    },
    {
      "epoch": 151.45985401459853,
      "grad_norm": 6.764479160308838,
      "learning_rate": 4.242700729927008e-05,
      "loss": 1.2925,
      "step": 20750
    },
    {
      "epoch": 151.53284671532847,
      "grad_norm": 4.513572692871094,
      "learning_rate": 4.2423357664233574e-05,
      "loss": 0.7918,
      "step": 20760
    },
    {
      "epoch": 151.6058394160584,
      "grad_norm": 6.375625133514404,
      "learning_rate": 4.241970802919708e-05,
      "loss": 1.3117,
      "step": 20770
    },
    {
      "epoch": 151.67883211678833,
      "grad_norm": 6.875290870666504,
      "learning_rate": 4.241605839416058e-05,
      "loss": 1.486,
      "step": 20780
    },
    {
      "epoch": 151.75182481751824,
      "grad_norm": 7.008708477020264,
      "learning_rate": 4.241240875912409e-05,
      "loss": 1.7828,
      "step": 20790
    },
    {
      "epoch": 151.82481751824818,
      "grad_norm": 4.158120155334473,
      "learning_rate": 4.24087591240876e-05,
      "loss": 1.0415,
      "step": 20800
    },
    {
      "epoch": 151.8978102189781,
      "grad_norm": 11.107112884521484,
      "learning_rate": 4.24051094890511e-05,
      "loss": 1.3275,
      "step": 20810
    },
    {
      "epoch": 151.97080291970804,
      "grad_norm": 7.937274932861328,
      "learning_rate": 4.2401459854014604e-05,
      "loss": 1.5421,
      "step": 20820
    },
    {
      "epoch": 152.04379562043795,
      "grad_norm": 6.824107646942139,
      "learning_rate": 4.2397810218978105e-05,
      "loss": 0.9471,
      "step": 20830
    },
    {
      "epoch": 152.1167883211679,
      "grad_norm": 7.03945255279541,
      "learning_rate": 4.2394160583941605e-05,
      "loss": 1.5829,
      "step": 20840
    },
    {
      "epoch": 152.1897810218978,
      "grad_norm": 9.402965545654297,
      "learning_rate": 4.239051094890511e-05,
      "loss": 1.1304,
      "step": 20850
    },
    {
      "epoch": 152.26277372262774,
      "grad_norm": 8.800856590270996,
      "learning_rate": 4.238686131386861e-05,
      "loss": 1.1708,
      "step": 20860
    },
    {
      "epoch": 152.33576642335765,
      "grad_norm": 11.422669410705566,
      "learning_rate": 4.238321167883212e-05,
      "loss": 1.3779,
      "step": 20870
    },
    {
      "epoch": 152.4087591240876,
      "grad_norm": 2.9186692237854004,
      "learning_rate": 4.237956204379562e-05,
      "loss": 0.9929,
      "step": 20880
    },
    {
      "epoch": 152.4817518248175,
      "grad_norm": 14.448153495788574,
      "learning_rate": 4.237591240875913e-05,
      "loss": 1.369,
      "step": 20890
    },
    {
      "epoch": 152.55474452554745,
      "grad_norm": 6.690126419067383,
      "learning_rate": 4.237226277372263e-05,
      "loss": 1.0487,
      "step": 20900
    },
    {
      "epoch": 152.62773722627736,
      "grad_norm": 11.57304573059082,
      "learning_rate": 4.236861313868613e-05,
      "loss": 0.8351,
      "step": 20910
    },
    {
      "epoch": 152.7007299270073,
      "grad_norm": 4.4258713722229,
      "learning_rate": 4.2364963503649637e-05,
      "loss": 1.22,
      "step": 20920
    },
    {
      "epoch": 152.77372262773721,
      "grad_norm": 9.143600463867188,
      "learning_rate": 4.236131386861314e-05,
      "loss": 1.561,
      "step": 20930
    },
    {
      "epoch": 152.84671532846716,
      "grad_norm": 2.5250587463378906,
      "learning_rate": 4.2357664233576644e-05,
      "loss": 1.2972,
      "step": 20940
    },
    {
      "epoch": 152.91970802919707,
      "grad_norm": 5.208895206451416,
      "learning_rate": 4.235401459854015e-05,
      "loss": 1.4097,
      "step": 20950
    },
    {
      "epoch": 152.992700729927,
      "grad_norm": 8.167686462402344,
      "learning_rate": 4.235036496350365e-05,
      "loss": 1.4054,
      "step": 20960
    },
    {
      "epoch": 153.06569343065692,
      "grad_norm": 8.201573371887207,
      "learning_rate": 4.234671532846716e-05,
      "loss": 1.3129,
      "step": 20970
    },
    {
      "epoch": 153.13868613138686,
      "grad_norm": 14.738357543945312,
      "learning_rate": 4.234306569343066e-05,
      "loss": 1.274,
      "step": 20980
    },
    {
      "epoch": 153.21167883211677,
      "grad_norm": 8.96413516998291,
      "learning_rate": 4.233941605839416e-05,
      "loss": 1.0427,
      "step": 20990
    },
    {
      "epoch": 153.28467153284672,
      "grad_norm": 3.799078941345215,
      "learning_rate": 4.233576642335767e-05,
      "loss": 1.1493,
      "step": 21000
    },
    {
      "epoch": 153.35766423357666,
      "grad_norm": 10.279011726379395,
      "learning_rate": 4.233211678832117e-05,
      "loss": 1.5187,
      "step": 21010
    },
    {
      "epoch": 153.43065693430657,
      "grad_norm": 8.086869239807129,
      "learning_rate": 4.2328467153284675e-05,
      "loss": 1.129,
      "step": 21020
    },
    {
      "epoch": 153.5036496350365,
      "grad_norm": 11.947393417358398,
      "learning_rate": 4.2324817518248176e-05,
      "loss": 1.0765,
      "step": 21030
    },
    {
      "epoch": 153.57664233576642,
      "grad_norm": 6.645598411560059,
      "learning_rate": 4.232116788321168e-05,
      "loss": 1.4017,
      "step": 21040
    },
    {
      "epoch": 153.64963503649636,
      "grad_norm": 13.278623580932617,
      "learning_rate": 4.2317518248175184e-05,
      "loss": 1.1145,
      "step": 21050
    },
    {
      "epoch": 153.72262773722628,
      "grad_norm": 4.037060737609863,
      "learning_rate": 4.231386861313869e-05,
      "loss": 1.0853,
      "step": 21060
    },
    {
      "epoch": 153.79562043795622,
      "grad_norm": 5.6422834396362305,
      "learning_rate": 4.231021897810219e-05,
      "loss": 1.2999,
      "step": 21070
    },
    {
      "epoch": 153.86861313868613,
      "grad_norm": 6.986506938934326,
      "learning_rate": 4.230656934306569e-05,
      "loss": 1.264,
      "step": 21080
    },
    {
      "epoch": 153.94160583941607,
      "grad_norm": 6.654682159423828,
      "learning_rate": 4.23029197080292e-05,
      "loss": 1.3774,
      "step": 21090
    },
    {
      "epoch": 154.01459854014598,
      "grad_norm": 3.9411468505859375,
      "learning_rate": 4.22992700729927e-05,
      "loss": 1.3314,
      "step": 21100
    },
    {
      "epoch": 154.08759124087592,
      "grad_norm": 8.804550170898438,
      "learning_rate": 4.229562043795621e-05,
      "loss": 1.1376,
      "step": 21110
    },
    {
      "epoch": 154.16058394160584,
      "grad_norm": 8.264755249023438,
      "learning_rate": 4.2291970802919714e-05,
      "loss": 1.1627,
      "step": 21120
    },
    {
      "epoch": 154.23357664233578,
      "grad_norm": 7.792499542236328,
      "learning_rate": 4.2288321167883215e-05,
      "loss": 0.9964,
      "step": 21130
    },
    {
      "epoch": 154.3065693430657,
      "grad_norm": 7.465267658233643,
      "learning_rate": 4.2284671532846715e-05,
      "loss": 1.2465,
      "step": 21140
    },
    {
      "epoch": 154.37956204379563,
      "grad_norm": 6.585792541503906,
      "learning_rate": 4.2281021897810216e-05,
      "loss": 0.7854,
      "step": 21150
    },
    {
      "epoch": 154.45255474452554,
      "grad_norm": 9.908955574035645,
      "learning_rate": 4.227737226277372e-05,
      "loss": 1.6352,
      "step": 21160
    },
    {
      "epoch": 154.52554744525548,
      "grad_norm": 14.026823997497559,
      "learning_rate": 4.227372262773723e-05,
      "loss": 1.7005,
      "step": 21170
    },
    {
      "epoch": 154.5985401459854,
      "grad_norm": 3.4933369159698486,
      "learning_rate": 4.227007299270073e-05,
      "loss": 1.1757,
      "step": 21180
    },
    {
      "epoch": 154.67153284671534,
      "grad_norm": 9.5314359664917,
      "learning_rate": 4.226642335766424e-05,
      "loss": 1.1634,
      "step": 21190
    },
    {
      "epoch": 154.74452554744525,
      "grad_norm": 18.93465805053711,
      "learning_rate": 4.226277372262774e-05,
      "loss": 1.1519,
      "step": 21200
    },
    {
      "epoch": 154.8175182481752,
      "grad_norm": 10.970735549926758,
      "learning_rate": 4.2259124087591246e-05,
      "loss": 1.5707,
      "step": 21210
    },
    {
      "epoch": 154.8905109489051,
      "grad_norm": 5.5546112060546875,
      "learning_rate": 4.2255474452554746e-05,
      "loss": 1.0338,
      "step": 21220
    },
    {
      "epoch": 154.96350364963504,
      "grad_norm": 10.20831298828125,
      "learning_rate": 4.225182481751825e-05,
      "loss": 1.4884,
      "step": 21230
    },
    {
      "epoch": 155.03649635036496,
      "grad_norm": 6.144556522369385,
      "learning_rate": 4.2248175182481754e-05,
      "loss": 1.2918,
      "step": 21240
    },
    {
      "epoch": 155.1094890510949,
      "grad_norm": 11.246146202087402,
      "learning_rate": 4.2244525547445254e-05,
      "loss": 1.0054,
      "step": 21250
    },
    {
      "epoch": 155.1824817518248,
      "grad_norm": 10.23721694946289,
      "learning_rate": 4.224087591240876e-05,
      "loss": 1.4511,
      "step": 21260
    },
    {
      "epoch": 155.25547445255475,
      "grad_norm": 15.699407577514648,
      "learning_rate": 4.223722627737227e-05,
      "loss": 0.8967,
      "step": 21270
    },
    {
      "epoch": 155.32846715328466,
      "grad_norm": 6.584080696105957,
      "learning_rate": 4.223357664233577e-05,
      "loss": 0.7925,
      "step": 21280
    },
    {
      "epoch": 155.4014598540146,
      "grad_norm": 8.274665832519531,
      "learning_rate": 4.222992700729928e-05,
      "loss": 1.5119,
      "step": 21290
    },
    {
      "epoch": 155.47445255474452,
      "grad_norm": 6.424304962158203,
      "learning_rate": 4.222627737226277e-05,
      "loss": 1.4881,
      "step": 21300
    },
    {
      "epoch": 155.54744525547446,
      "grad_norm": 2.394563913345337,
      "learning_rate": 4.222262773722628e-05,
      "loss": 1.025,
      "step": 21310
    },
    {
      "epoch": 155.62043795620437,
      "grad_norm": 11.945626258850098,
      "learning_rate": 4.2218978102189785e-05,
      "loss": 1.417,
      "step": 21320
    },
    {
      "epoch": 155.6934306569343,
      "grad_norm": 3.591966152191162,
      "learning_rate": 4.2215328467153286e-05,
      "loss": 1.2209,
      "step": 21330
    },
    {
      "epoch": 155.76642335766422,
      "grad_norm": 10.95345401763916,
      "learning_rate": 4.221167883211679e-05,
      "loss": 1.4665,
      "step": 21340
    },
    {
      "epoch": 155.83941605839416,
      "grad_norm": 11.100177764892578,
      "learning_rate": 4.220802919708029e-05,
      "loss": 1.444,
      "step": 21350
    },
    {
      "epoch": 155.91240875912408,
      "grad_norm": 13.757341384887695,
      "learning_rate": 4.22043795620438e-05,
      "loss": 1.1115,
      "step": 21360
    },
    {
      "epoch": 155.98540145985402,
      "grad_norm": 11.898886680603027,
      "learning_rate": 4.22007299270073e-05,
      "loss": 0.954,
      "step": 21370
    },
    {
      "epoch": 156.05839416058393,
      "grad_norm": 13.116474151611328,
      "learning_rate": 4.21970802919708e-05,
      "loss": 1.2668,
      "step": 21380
    },
    {
      "epoch": 156.13138686131387,
      "grad_norm": 7.917607307434082,
      "learning_rate": 4.219343065693431e-05,
      "loss": 1.1285,
      "step": 21390
    },
    {
      "epoch": 156.20437956204378,
      "grad_norm": 6.278014183044434,
      "learning_rate": 4.218978102189781e-05,
      "loss": 1.2296,
      "step": 21400
    },
    {
      "epoch": 156.27737226277372,
      "grad_norm": 6.164242744445801,
      "learning_rate": 4.218613138686132e-05,
      "loss": 1.623,
      "step": 21410
    },
    {
      "epoch": 156.35036496350364,
      "grad_norm": 7.420700550079346,
      "learning_rate": 4.2182481751824824e-05,
      "loss": 1.1409,
      "step": 21420
    },
    {
      "epoch": 156.42335766423358,
      "grad_norm": 7.386558532714844,
      "learning_rate": 4.2178832116788324e-05,
      "loss": 1.341,
      "step": 21430
    },
    {
      "epoch": 156.4963503649635,
      "grad_norm": 6.852940082550049,
      "learning_rate": 4.217518248175183e-05,
      "loss": 0.9104,
      "step": 21440
    },
    {
      "epoch": 156.56934306569343,
      "grad_norm": 11.694686889648438,
      "learning_rate": 4.2171532846715325e-05,
      "loss": 1.7833,
      "step": 21450
    },
    {
      "epoch": 156.64233576642334,
      "grad_norm": 8.998286247253418,
      "learning_rate": 4.216788321167883e-05,
      "loss": 1.0563,
      "step": 21460
    },
    {
      "epoch": 156.71532846715328,
      "grad_norm": 3.403223752975464,
      "learning_rate": 4.216423357664234e-05,
      "loss": 1.0992,
      "step": 21470
    },
    {
      "epoch": 156.78832116788323,
      "grad_norm": 11.835628509521484,
      "learning_rate": 4.216058394160584e-05,
      "loss": 1.1143,
      "step": 21480
    },
    {
      "epoch": 156.86131386861314,
      "grad_norm": 8.713363647460938,
      "learning_rate": 4.215693430656935e-05,
      "loss": 1.3164,
      "step": 21490
    },
    {
      "epoch": 156.93430656934308,
      "grad_norm": 8.818490982055664,
      "learning_rate": 4.215328467153285e-05,
      "loss": 1.1009,
      "step": 21500
    },
    {
      "epoch": 157.007299270073,
      "grad_norm": 8.192460060119629,
      "learning_rate": 4.2149635036496355e-05,
      "loss": 1.2857,
      "step": 21510
    },
    {
      "epoch": 157.08029197080293,
      "grad_norm": 4.794474124908447,
      "learning_rate": 4.2145985401459856e-05,
      "loss": 1.1755,
      "step": 21520
    },
    {
      "epoch": 157.15328467153284,
      "grad_norm": 13.11224365234375,
      "learning_rate": 4.2142335766423356e-05,
      "loss": 1.4458,
      "step": 21530
    },
    {
      "epoch": 157.22627737226279,
      "grad_norm": 8.442904472351074,
      "learning_rate": 4.2138686131386864e-05,
      "loss": 1.0671,
      "step": 21540
    },
    {
      "epoch": 157.2992700729927,
      "grad_norm": 2.869824171066284,
      "learning_rate": 4.2135036496350364e-05,
      "loss": 1.0745,
      "step": 21550
    },
    {
      "epoch": 157.37226277372264,
      "grad_norm": 4.209914684295654,
      "learning_rate": 4.213138686131387e-05,
      "loss": 1.3781,
      "step": 21560
    },
    {
      "epoch": 157.44525547445255,
      "grad_norm": 9.99070930480957,
      "learning_rate": 4.212773722627737e-05,
      "loss": 1.1267,
      "step": 21570
    },
    {
      "epoch": 157.5182481751825,
      "grad_norm": 8.203956604003906,
      "learning_rate": 4.212408759124088e-05,
      "loss": 1.2565,
      "step": 21580
    },
    {
      "epoch": 157.5912408759124,
      "grad_norm": 11.30607795715332,
      "learning_rate": 4.2120437956204386e-05,
      "loss": 1.3592,
      "step": 21590
    },
    {
      "epoch": 157.66423357664235,
      "grad_norm": 1.5194748640060425,
      "learning_rate": 4.211678832116788e-05,
      "loss": 1.0008,
      "step": 21600
    },
    {
      "epoch": 157.73722627737226,
      "grad_norm": 15.190140724182129,
      "learning_rate": 4.211313868613139e-05,
      "loss": 1.3597,
      "step": 21610
    },
    {
      "epoch": 157.8102189781022,
      "grad_norm": 4.008851528167725,
      "learning_rate": 4.2109489051094895e-05,
      "loss": 1.2521,
      "step": 21620
    },
    {
      "epoch": 157.8832116788321,
      "grad_norm": 8.288522720336914,
      "learning_rate": 4.2105839416058395e-05,
      "loss": 1.2957,
      "step": 21630
    },
    {
      "epoch": 157.95620437956205,
      "grad_norm": 6.513716220855713,
      "learning_rate": 4.21021897810219e-05,
      "loss": 1.0233,
      "step": 21640
    },
    {
      "epoch": 158.02919708029196,
      "grad_norm": 9.86156940460205,
      "learning_rate": 4.20985401459854e-05,
      "loss": 1.4814,
      "step": 21650
    },
    {
      "epoch": 158.1021897810219,
      "grad_norm": 6.250804901123047,
      "learning_rate": 4.209489051094891e-05,
      "loss": 0.9651,
      "step": 21660
    },
    {
      "epoch": 158.17518248175182,
      "grad_norm": 8.54723834991455,
      "learning_rate": 4.209124087591241e-05,
      "loss": 1.1551,
      "step": 21670
    },
    {
      "epoch": 158.24817518248176,
      "grad_norm": 2.545736789703369,
      "learning_rate": 4.208759124087591e-05,
      "loss": 0.8685,
      "step": 21680
    },
    {
      "epoch": 158.32116788321167,
      "grad_norm": 6.714073657989502,
      "learning_rate": 4.208394160583942e-05,
      "loss": 1.2103,
      "step": 21690
    },
    {
      "epoch": 158.3941605839416,
      "grad_norm": 7.014136791229248,
      "learning_rate": 4.208029197080292e-05,
      "loss": 0.7983,
      "step": 21700
    },
    {
      "epoch": 158.46715328467153,
      "grad_norm": 3.962033271789551,
      "learning_rate": 4.2076642335766426e-05,
      "loss": 1.3811,
      "step": 21710
    },
    {
      "epoch": 158.54014598540147,
      "grad_norm": 7.8167829513549805,
      "learning_rate": 4.207299270072993e-05,
      "loss": 1.6414,
      "step": 21720
    },
    {
      "epoch": 158.61313868613138,
      "grad_norm": 4.989095211029053,
      "learning_rate": 4.2069343065693434e-05,
      "loss": 1.1592,
      "step": 21730
    },
    {
      "epoch": 158.68613138686132,
      "grad_norm": 9.50385570526123,
      "learning_rate": 4.206569343065694e-05,
      "loss": 1.3743,
      "step": 21740
    },
    {
      "epoch": 158.75912408759123,
      "grad_norm": 2.5738346576690674,
      "learning_rate": 4.206204379562044e-05,
      "loss": 0.9596,
      "step": 21750
    },
    {
      "epoch": 158.83211678832117,
      "grad_norm": 4.1771016120910645,
      "learning_rate": 4.205839416058394e-05,
      "loss": 1.3562,
      "step": 21760
    },
    {
      "epoch": 158.90510948905109,
      "grad_norm": 12.2823486328125,
      "learning_rate": 4.205474452554744e-05,
      "loss": 1.4767,
      "step": 21770
    },
    {
      "epoch": 158.97810218978103,
      "grad_norm": 4.097815036773682,
      "learning_rate": 4.205109489051095e-05,
      "loss": 1.2805,
      "step": 21780
    },
    {
      "epoch": 159.05109489051094,
      "grad_norm": 6.283990383148193,
      "learning_rate": 4.204744525547446e-05,
      "loss": 0.8601,
      "step": 21790
    },
    {
      "epoch": 159.12408759124088,
      "grad_norm": 12.756368637084961,
      "learning_rate": 4.204379562043796e-05,
      "loss": 1.404,
      "step": 21800
    },
    {
      "epoch": 159.1970802919708,
      "grad_norm": 7.4232258796691895,
      "learning_rate": 4.2040145985401465e-05,
      "loss": 1.1207,
      "step": 21810
    },
    {
      "epoch": 159.27007299270073,
      "grad_norm": 9.731280326843262,
      "learning_rate": 4.2036496350364966e-05,
      "loss": 1.0262,
      "step": 21820
    },
    {
      "epoch": 159.34306569343065,
      "grad_norm": 4.102999210357666,
      "learning_rate": 4.2032846715328466e-05,
      "loss": 0.9815,
      "step": 21830
    },
    {
      "epoch": 159.4160583941606,
      "grad_norm": 9.630457878112793,
      "learning_rate": 4.2029197080291973e-05,
      "loss": 1.2742,
      "step": 21840
    },
    {
      "epoch": 159.4890510948905,
      "grad_norm": 15.439375877380371,
      "learning_rate": 4.2025547445255474e-05,
      "loss": 1.0814,
      "step": 21850
    },
    {
      "epoch": 159.56204379562044,
      "grad_norm": 11.63277816772461,
      "learning_rate": 4.202189781021898e-05,
      "loss": 1.2935,
      "step": 21860
    },
    {
      "epoch": 159.63503649635035,
      "grad_norm": 3.6559793949127197,
      "learning_rate": 4.201824817518248e-05,
      "loss": 1.0366,
      "step": 21870
    },
    {
      "epoch": 159.7080291970803,
      "grad_norm": 11.303220748901367,
      "learning_rate": 4.201459854014599e-05,
      "loss": 1.3125,
      "step": 21880
    },
    {
      "epoch": 159.7810218978102,
      "grad_norm": 9.031944274902344,
      "learning_rate": 4.2010948905109496e-05,
      "loss": 1.4372,
      "step": 21890
    },
    {
      "epoch": 159.85401459854015,
      "grad_norm": 8.92077922821045,
      "learning_rate": 4.2007299270073e-05,
      "loss": 1.4532,
      "step": 21900
    },
    {
      "epoch": 159.92700729927006,
      "grad_norm": 2.881387710571289,
      "learning_rate": 4.20036496350365e-05,
      "loss": 1.1093,
      "step": 21910
    },
    {
      "epoch": 160.0,
      "grad_norm": 19.573328018188477,
      "learning_rate": 4.2e-05,
      "loss": 1.3161,
      "step": 21920
    },
    {
      "epoch": 160.07299270072994,
      "grad_norm": 11.843707084655762,
      "learning_rate": 4.1996350364963505e-05,
      "loss": 1.4434,
      "step": 21930
    },
    {
      "epoch": 160.14598540145985,
      "grad_norm": 1.864278793334961,
      "learning_rate": 4.199270072992701e-05,
      "loss": 1.0742,
      "step": 21940
    },
    {
      "epoch": 160.2189781021898,
      "grad_norm": 1.144395351409912,
      "learning_rate": 4.198905109489051e-05,
      "loss": 1.2198,
      "step": 21950
    },
    {
      "epoch": 160.2919708029197,
      "grad_norm": 1.5986354351043701,
      "learning_rate": 4.198540145985402e-05,
      "loss": 0.9608,
      "step": 21960
    },
    {
      "epoch": 160.36496350364965,
      "grad_norm": 2.1539740562438965,
      "learning_rate": 4.198175182481752e-05,
      "loss": 1.0462,
      "step": 21970
    },
    {
      "epoch": 160.43795620437956,
      "grad_norm": 11.446677207946777,
      "learning_rate": 4.197810218978102e-05,
      "loss": 0.9646,
      "step": 21980
    },
    {
      "epoch": 160.5109489051095,
      "grad_norm": 7.986482620239258,
      "learning_rate": 4.197445255474453e-05,
      "loss": 1.4097,
      "step": 21990
    },
    {
      "epoch": 160.5839416058394,
      "grad_norm": 10.831809043884277,
      "learning_rate": 4.197080291970803e-05,
      "loss": 0.9929,
      "step": 22000
    },
    {
      "epoch": 160.65693430656935,
      "grad_norm": 8.079483985900879,
      "learning_rate": 4.1967153284671536e-05,
      "loss": 1.5059,
      "step": 22010
    },
    {
      "epoch": 160.72992700729927,
      "grad_norm": 10.000288963317871,
      "learning_rate": 4.1963503649635037e-05,
      "loss": 1.0877,
      "step": 22020
    },
    {
      "epoch": 160.8029197080292,
      "grad_norm": 4.500598430633545,
      "learning_rate": 4.1959854014598544e-05,
      "loss": 1.3628,
      "step": 22030
    },
    {
      "epoch": 160.87591240875912,
      "grad_norm": 3.0339176654815674,
      "learning_rate": 4.1956204379562044e-05,
      "loss": 1.2286,
      "step": 22040
    },
    {
      "epoch": 160.94890510948906,
      "grad_norm": 6.7878618240356445,
      "learning_rate": 4.195255474452555e-05,
      "loss": 1.2315,
      "step": 22050
    },
    {
      "epoch": 161.02189781021897,
      "grad_norm": 6.092592239379883,
      "learning_rate": 4.194890510948905e-05,
      "loss": 1.1283,
      "step": 22060
    },
    {
      "epoch": 161.09489051094891,
      "grad_norm": 15.884838104248047,
      "learning_rate": 4.194525547445255e-05,
      "loss": 1.2581,
      "step": 22070
    },
    {
      "epoch": 161.16788321167883,
      "grad_norm": 12.1726713180542,
      "learning_rate": 4.194160583941606e-05,
      "loss": 1.3504,
      "step": 22080
    },
    {
      "epoch": 161.24087591240877,
      "grad_norm": 4.906444549560547,
      "learning_rate": 4.193795620437957e-05,
      "loss": 1.2549,
      "step": 22090
    },
    {
      "epoch": 161.31386861313868,
      "grad_norm": 6.824007511138916,
      "learning_rate": 4.193430656934307e-05,
      "loss": 1.3619,
      "step": 22100
    },
    {
      "epoch": 161.38686131386862,
      "grad_norm": 10.031750679016113,
      "learning_rate": 4.1930656934306575e-05,
      "loss": 1.0978,
      "step": 22110
    },
    {
      "epoch": 161.45985401459853,
      "grad_norm": 10.45382022857666,
      "learning_rate": 4.1927007299270075e-05,
      "loss": 1.2212,
      "step": 22120
    },
    {
      "epoch": 161.53284671532847,
      "grad_norm": 9.247496604919434,
      "learning_rate": 4.192335766423358e-05,
      "loss": 1.3234,
      "step": 22130
    },
    {
      "epoch": 161.6058394160584,
      "grad_norm": 5.192567825317383,
      "learning_rate": 4.191970802919708e-05,
      "loss": 1.496,
      "step": 22140
    },
    {
      "epoch": 161.67883211678833,
      "grad_norm": 2.6211118698120117,
      "learning_rate": 4.1916058394160584e-05,
      "loss": 1.3703,
      "step": 22150
    },
    {
      "epoch": 161.75182481751824,
      "grad_norm": 1.49770987033844,
      "learning_rate": 4.191240875912409e-05,
      "loss": 1.1515,
      "step": 22160
    },
    {
      "epoch": 161.82481751824818,
      "grad_norm": 8.838563919067383,
      "learning_rate": 4.190875912408759e-05,
      "loss": 0.8784,
      "step": 22170
    },
    {
      "epoch": 161.8978102189781,
      "grad_norm": 3.829664707183838,
      "learning_rate": 4.19051094890511e-05,
      "loss": 1.2858,
      "step": 22180
    },
    {
      "epoch": 161.97080291970804,
      "grad_norm": 7.440489768981934,
      "learning_rate": 4.19014598540146e-05,
      "loss": 1.046,
      "step": 22190
    },
    {
      "epoch": 162.04379562043795,
      "grad_norm": 6.108567714691162,
      "learning_rate": 4.1897810218978106e-05,
      "loss": 0.84,
      "step": 22200
    },
    {
      "epoch": 162.1167883211679,
      "grad_norm": 12.03344440460205,
      "learning_rate": 4.189416058394161e-05,
      "loss": 0.6768,
      "step": 22210
    },
    {
      "epoch": 162.1897810218978,
      "grad_norm": 9.973516464233398,
      "learning_rate": 4.189051094890511e-05,
      "loss": 1.2325,
      "step": 22220
    },
    {
      "epoch": 162.26277372262774,
      "grad_norm": 6.86489200592041,
      "learning_rate": 4.1886861313868615e-05,
      "loss": 0.9832,
      "step": 22230
    },
    {
      "epoch": 162.33576642335765,
      "grad_norm": 6.515214443206787,
      "learning_rate": 4.1883211678832115e-05,
      "loss": 1.416,
      "step": 22240
    },
    {
      "epoch": 162.4087591240876,
      "grad_norm": 12.285300254821777,
      "learning_rate": 4.187956204379562e-05,
      "loss": 1.4929,
      "step": 22250
    },
    {
      "epoch": 162.4817518248175,
      "grad_norm": 4.190080165863037,
      "learning_rate": 4.187591240875913e-05,
      "loss": 1.076,
      "step": 22260
    },
    {
      "epoch": 162.55474452554745,
      "grad_norm": 7.725800514221191,
      "learning_rate": 4.187226277372263e-05,
      "loss": 1.6964,
      "step": 22270
    },
    {
      "epoch": 162.62773722627736,
      "grad_norm": 7.309006214141846,
      "learning_rate": 4.186861313868614e-05,
      "loss": 1.5289,
      "step": 22280
    },
    {
      "epoch": 162.7007299270073,
      "grad_norm": 8.0730562210083,
      "learning_rate": 4.186496350364964e-05,
      "loss": 1.1028,
      "step": 22290
    },
    {
      "epoch": 162.77372262773721,
      "grad_norm": 8.54093074798584,
      "learning_rate": 4.186131386861314e-05,
      "loss": 0.9739,
      "step": 22300
    },
    {
      "epoch": 162.84671532846716,
      "grad_norm": 5.009984016418457,
      "learning_rate": 4.1857664233576646e-05,
      "loss": 1.0945,
      "step": 22310
    },
    {
      "epoch": 162.91970802919707,
      "grad_norm": 9.067193984985352,
      "learning_rate": 4.1854014598540146e-05,
      "loss": 1.2702,
      "step": 22320
    },
    {
      "epoch": 162.992700729927,
      "grad_norm": 17.749948501586914,
      "learning_rate": 4.1850364963503653e-05,
      "loss": 1.0678,
      "step": 22330
    },
    {
      "epoch": 163.06569343065692,
      "grad_norm": 11.672189712524414,
      "learning_rate": 4.1846715328467154e-05,
      "loss": 1.0715,
      "step": 22340
    },
    {
      "epoch": 163.13868613138686,
      "grad_norm": 3.536256790161133,
      "learning_rate": 4.184306569343066e-05,
      "loss": 0.9635,
      "step": 22350
    },
    {
      "epoch": 163.21167883211677,
      "grad_norm": 8.773045539855957,
      "learning_rate": 4.183941605839417e-05,
      "loss": 1.2923,
      "step": 22360
    },
    {
      "epoch": 163.28467153284672,
      "grad_norm": 5.768494606018066,
      "learning_rate": 4.183576642335766e-05,
      "loss": 1.1206,
      "step": 22370
    },
    {
      "epoch": 163.35766423357666,
      "grad_norm": 16.49396514892578,
      "learning_rate": 4.183211678832117e-05,
      "loss": 1.1728,
      "step": 22380
    },
    {
      "epoch": 163.43065693430657,
      "grad_norm": 12.932106971740723,
      "learning_rate": 4.182846715328467e-05,
      "loss": 1.293,
      "step": 22390
    },
    {
      "epoch": 163.5036496350365,
      "grad_norm": 6.8530755043029785,
      "learning_rate": 4.182481751824818e-05,
      "loss": 1.2592,
      "step": 22400
    },
    {
      "epoch": 163.57664233576642,
      "grad_norm": 6.82990026473999,
      "learning_rate": 4.1821167883211685e-05,
      "loss": 1.1702,
      "step": 22410
    },
    {
      "epoch": 163.64963503649636,
      "grad_norm": 6.775900840759277,
      "learning_rate": 4.1817518248175185e-05,
      "loss": 1.2761,
      "step": 22420
    },
    {
      "epoch": 163.72262773722628,
      "grad_norm": 5.528038501739502,
      "learning_rate": 4.181386861313869e-05,
      "loss": 1.0339,
      "step": 22430
    },
    {
      "epoch": 163.79562043795622,
      "grad_norm": 6.70438814163208,
      "learning_rate": 4.1810218978102186e-05,
      "loss": 0.9041,
      "step": 22440
    },
    {
      "epoch": 163.86861313868613,
      "grad_norm": 10.139211654663086,
      "learning_rate": 4.180656934306569e-05,
      "loss": 1.3698,
      "step": 22450
    },
    {
      "epoch": 163.94160583941607,
      "grad_norm": 6.730995178222656,
      "learning_rate": 4.18029197080292e-05,
      "loss": 1.1096,
      "step": 22460
    },
    {
      "epoch": 164.01459854014598,
      "grad_norm": 8.910213470458984,
      "learning_rate": 4.17992700729927e-05,
      "loss": 1.6184,
      "step": 22470
    },
    {
      "epoch": 164.08759124087592,
      "grad_norm": 8.128172874450684,
      "learning_rate": 4.179562043795621e-05,
      "loss": 0.7521,
      "step": 22480
    },
    {
      "epoch": 164.16058394160584,
      "grad_norm": 10.853560447692871,
      "learning_rate": 4.179197080291971e-05,
      "loss": 1.5241,
      "step": 22490
    },
    {
      "epoch": 164.23357664233578,
      "grad_norm": 10.111607551574707,
      "learning_rate": 4.1788321167883216e-05,
      "loss": 1.2481,
      "step": 22500
    },
    {
      "epoch": 164.3065693430657,
      "grad_norm": 6.9969563484191895,
      "learning_rate": 4.1784671532846717e-05,
      "loss": 0.8386,
      "step": 22510
    },
    {
      "epoch": 164.37956204379563,
      "grad_norm": 3.796086072921753,
      "learning_rate": 4.178102189781022e-05,
      "loss": 1.0959,
      "step": 22520
    },
    {
      "epoch": 164.45255474452554,
      "grad_norm": 13.935714721679688,
      "learning_rate": 4.1777372262773724e-05,
      "loss": 1.3773,
      "step": 22530
    },
    {
      "epoch": 164.52554744525548,
      "grad_norm": 7.094386100769043,
      "learning_rate": 4.1773722627737225e-05,
      "loss": 1.2275,
      "step": 22540
    },
    {
      "epoch": 164.5985401459854,
      "grad_norm": 8.642463684082031,
      "learning_rate": 4.177007299270073e-05,
      "loss": 1.1317,
      "step": 22550
    },
    {
      "epoch": 164.67153284671534,
      "grad_norm": 8.717851638793945,
      "learning_rate": 4.176642335766424e-05,
      "loss": 1.0217,
      "step": 22560
    },
    {
      "epoch": 164.74452554744525,
      "grad_norm": 9.45380973815918,
      "learning_rate": 4.176277372262774e-05,
      "loss": 1.3455,
      "step": 22570
    },
    {
      "epoch": 164.8175182481752,
      "grad_norm": 7.046368598937988,
      "learning_rate": 4.175912408759125e-05,
      "loss": 1.2312,
      "step": 22580
    },
    {
      "epoch": 164.8905109489051,
      "grad_norm": 8.906288146972656,
      "learning_rate": 4.175547445255475e-05,
      "loss": 1.2481,
      "step": 22590
    },
    {
      "epoch": 164.96350364963504,
      "grad_norm": 11.28148365020752,
      "learning_rate": 4.175182481751825e-05,
      "loss": 1.1725,
      "step": 22600
    },
    {
      "epoch": 165.03649635036496,
      "grad_norm": 9.609509468078613,
      "learning_rate": 4.1748175182481755e-05,
      "loss": 1.1924,
      "step": 22610
    },
    {
      "epoch": 165.1094890510949,
      "grad_norm": 7.760013103485107,
      "learning_rate": 4.1744525547445256e-05,
      "loss": 1.2336,
      "step": 22620
    },
    {
      "epoch": 165.1824817518248,
      "grad_norm": 12.290571212768555,
      "learning_rate": 4.174087591240876e-05,
      "loss": 1.355,
      "step": 22630
    },
    {
      "epoch": 165.25547445255475,
      "grad_norm": 10.299871444702148,
      "learning_rate": 4.1737226277372264e-05,
      "loss": 0.9644,
      "step": 22640
    },
    {
      "epoch": 165.32846715328466,
      "grad_norm": 10.965211868286133,
      "learning_rate": 4.173357664233577e-05,
      "loss": 1.0391,
      "step": 22650
    },
    {
      "epoch": 165.4014598540146,
      "grad_norm": 10.829794883728027,
      "learning_rate": 4.172992700729927e-05,
      "loss": 1.3034,
      "step": 22660
    },
    {
      "epoch": 165.47445255474452,
      "grad_norm": 5.870571136474609,
      "learning_rate": 4.172627737226277e-05,
      "loss": 1.5767,
      "step": 22670
    },
    {
      "epoch": 165.54744525547446,
      "grad_norm": 2.8523621559143066,
      "learning_rate": 4.172262773722628e-05,
      "loss": 0.8494,
      "step": 22680
    },
    {
      "epoch": 165.62043795620437,
      "grad_norm": 4.466474533081055,
      "learning_rate": 4.171897810218978e-05,
      "loss": 1.2909,
      "step": 22690
    },
    {
      "epoch": 165.6934306569343,
      "grad_norm": 8.5802001953125,
      "learning_rate": 4.171532846715329e-05,
      "loss": 1.2735,
      "step": 22700
    },
    {
      "epoch": 165.76642335766422,
      "grad_norm": 7.287238597869873,
      "learning_rate": 4.171167883211679e-05,
      "loss": 0.9602,
      "step": 22710
    },
    {
      "epoch": 165.83941605839416,
      "grad_norm": 8.544413566589355,
      "learning_rate": 4.1708029197080295e-05,
      "loss": 1.1338,
      "step": 22720
    },
    {
      "epoch": 165.91240875912408,
      "grad_norm": 6.334545612335205,
      "learning_rate": 4.17043795620438e-05,
      "loss": 1.1926,
      "step": 22730
    },
    {
      "epoch": 165.98540145985402,
      "grad_norm": 6.944371700286865,
      "learning_rate": 4.17007299270073e-05,
      "loss": 1.3516,
      "step": 22740
    },
    {
      "epoch": 166.05839416058393,
      "grad_norm": 16.206567764282227,
      "learning_rate": 4.16970802919708e-05,
      "loss": 1.1997,
      "step": 22750
    },
    {
      "epoch": 166.13138686131387,
      "grad_norm": 7.5782999992370605,
      "learning_rate": 4.169343065693431e-05,
      "loss": 1.0427,
      "step": 22760
    },
    {
      "epoch": 166.20437956204378,
      "grad_norm": 4.362742900848389,
      "learning_rate": 4.168978102189781e-05,
      "loss": 0.8602,
      "step": 22770
    },
    {
      "epoch": 166.27737226277372,
      "grad_norm": 1.7508597373962402,
      "learning_rate": 4.168613138686132e-05,
      "loss": 1.4949,
      "step": 22780
    },
    {
      "epoch": 166.35036496350364,
      "grad_norm": 8.962414741516113,
      "learning_rate": 4.168248175182482e-05,
      "loss": 1.2569,
      "step": 22790
    },
    {
      "epoch": 166.42335766423358,
      "grad_norm": 1.5501325130462646,
      "learning_rate": 4.1678832116788326e-05,
      "loss": 1.262,
      "step": 22800
    },
    {
      "epoch": 166.4963503649635,
      "grad_norm": 8.955522537231445,
      "learning_rate": 4.1675182481751826e-05,
      "loss": 0.7223,
      "step": 22810
    },
    {
      "epoch": 166.56934306569343,
      "grad_norm": 6.8445329666137695,
      "learning_rate": 4.1671532846715334e-05,
      "loss": 0.8914,
      "step": 22820
    },
    {
      "epoch": 166.64233576642334,
      "grad_norm": 9.326085090637207,
      "learning_rate": 4.1667883211678834e-05,
      "loss": 1.3324,
      "step": 22830
    },
    {
      "epoch": 166.71532846715328,
      "grad_norm": 9.077937126159668,
      "learning_rate": 4.1664233576642335e-05,
      "loss": 1.1126,
      "step": 22840
    },
    {
      "epoch": 166.78832116788323,
      "grad_norm": 9.452512741088867,
      "learning_rate": 4.166058394160584e-05,
      "loss": 1.3372,
      "step": 22850
    },
    {
      "epoch": 166.86131386861314,
      "grad_norm": 2.484379529953003,
      "learning_rate": 4.165693430656934e-05,
      "loss": 1.1171,
      "step": 22860
    },
    {
      "epoch": 166.93430656934308,
      "grad_norm": 6.252804756164551,
      "learning_rate": 4.165328467153285e-05,
      "loss": 1.2431,
      "step": 22870
    },
    {
      "epoch": 167.007299270073,
      "grad_norm": 8.200474739074707,
      "learning_rate": 4.164963503649636e-05,
      "loss": 1.0895,
      "step": 22880
    },
    {
      "epoch": 167.08029197080293,
      "grad_norm": 7.043573379516602,
      "learning_rate": 4.164598540145986e-05,
      "loss": 1.2109,
      "step": 22890
    },
    {
      "epoch": 167.15328467153284,
      "grad_norm": 7.8776774406433105,
      "learning_rate": 4.164233576642336e-05,
      "loss": 0.9755,
      "step": 22900
    },
    {
      "epoch": 167.22627737226279,
      "grad_norm": 11.709721565246582,
      "learning_rate": 4.163868613138686e-05,
      "loss": 0.8253,
      "step": 22910
    },
    {
      "epoch": 167.2992700729927,
      "grad_norm": 11.502388954162598,
      "learning_rate": 4.1635036496350366e-05,
      "loss": 1.4108,
      "step": 22920
    },
    {
      "epoch": 167.37226277372264,
      "grad_norm": 13.55377197265625,
      "learning_rate": 4.163138686131387e-05,
      "loss": 1.0323,
      "step": 22930
    },
    {
      "epoch": 167.44525547445255,
      "grad_norm": 12.899338722229004,
      "learning_rate": 4.162773722627737e-05,
      "loss": 1.1619,
      "step": 22940
    },
    {
      "epoch": 167.5182481751825,
      "grad_norm": 6.041660785675049,
      "learning_rate": 4.162408759124088e-05,
      "loss": 1.2331,
      "step": 22950
    },
    {
      "epoch": 167.5912408759124,
      "grad_norm": 7.7276177406311035,
      "learning_rate": 4.162043795620438e-05,
      "loss": 1.085,
      "step": 22960
    },
    {
      "epoch": 167.66423357664235,
      "grad_norm": 8.350939750671387,
      "learning_rate": 4.161678832116789e-05,
      "loss": 1.386,
      "step": 22970
    },
    {
      "epoch": 167.73722627737226,
      "grad_norm": 7.5020751953125,
      "learning_rate": 4.161313868613139e-05,
      "loss": 1.6403,
      "step": 22980
    },
    {
      "epoch": 167.8102189781022,
      "grad_norm": 7.1212687492370605,
      "learning_rate": 4.160948905109489e-05,
      "loss": 0.6376,
      "step": 22990
    },
    {
      "epoch": 167.8832116788321,
      "grad_norm": 8.575136184692383,
      "learning_rate": 4.16058394160584e-05,
      "loss": 1.0521,
      "step": 23000
    },
    {
      "epoch": 167.95620437956205,
      "grad_norm": 6.823816299438477,
      "learning_rate": 4.16021897810219e-05,
      "loss": 1.2918,
      "step": 23010
    },
    {
      "epoch": 168.02919708029196,
      "grad_norm": 11.371930122375488,
      "learning_rate": 4.1598540145985404e-05,
      "loss": 1.0834,
      "step": 23020
    },
    {
      "epoch": 168.1021897810219,
      "grad_norm": 2.882333993911743,
      "learning_rate": 4.159489051094891e-05,
      "loss": 0.9699,
      "step": 23030
    },
    {
      "epoch": 168.17518248175182,
      "grad_norm": 9.346498489379883,
      "learning_rate": 4.159124087591241e-05,
      "loss": 1.2462,
      "step": 23040
    },
    {
      "epoch": 168.24817518248176,
      "grad_norm": 13.543067932128906,
      "learning_rate": 4.158759124087591e-05,
      "loss": 1.1399,
      "step": 23050
    },
    {
      "epoch": 168.32116788321167,
      "grad_norm": 1.9207695722579956,
      "learning_rate": 4.158394160583941e-05,
      "loss": 1.0708,
      "step": 23060
    },
    {
      "epoch": 168.3941605839416,
      "grad_norm": 6.418662071228027,
      "learning_rate": 4.158029197080292e-05,
      "loss": 1.3738,
      "step": 23070
    },
    {
      "epoch": 168.46715328467153,
      "grad_norm": 6.866926670074463,
      "learning_rate": 4.157664233576643e-05,
      "loss": 1.0799,
      "step": 23080
    },
    {
      "epoch": 168.54014598540147,
      "grad_norm": 6.4658708572387695,
      "learning_rate": 4.157299270072993e-05,
      "loss": 1.1957,
      "step": 23090
    },
    {
      "epoch": 168.61313868613138,
      "grad_norm": 5.177879333496094,
      "learning_rate": 4.1569343065693435e-05,
      "loss": 1.2417,
      "step": 23100
    },
    {
      "epoch": 168.68613138686132,
      "grad_norm": 12.698760032653809,
      "learning_rate": 4.1565693430656936e-05,
      "loss": 1.3077,
      "step": 23110
    },
    {
      "epoch": 168.75912408759123,
      "grad_norm": 6.36598539352417,
      "learning_rate": 4.156204379562044e-05,
      "loss": 1.121,
      "step": 23120
    },
    {
      "epoch": 168.83211678832117,
      "grad_norm": 5.356926441192627,
      "learning_rate": 4.1558394160583944e-05,
      "loss": 1.097,
      "step": 23130
    },
    {
      "epoch": 168.90510948905109,
      "grad_norm": 11.609149932861328,
      "learning_rate": 4.1554744525547444e-05,
      "loss": 0.9167,
      "step": 23140
    },
    {
      "epoch": 168.97810218978103,
      "grad_norm": 8.213324546813965,
      "learning_rate": 4.155109489051095e-05,
      "loss": 0.9667,
      "step": 23150
    },
    {
      "epoch": 169.05109489051094,
      "grad_norm": 1.7360912561416626,
      "learning_rate": 4.154744525547445e-05,
      "loss": 1.15,
      "step": 23160
    },
    {
      "epoch": 169.12408759124088,
      "grad_norm": 3.3673148155212402,
      "learning_rate": 4.154379562043796e-05,
      "loss": 0.4068,
      "step": 23170
    },
    {
      "epoch": 169.1970802919708,
      "grad_norm": 10.62928581237793,
      "learning_rate": 4.154014598540146e-05,
      "loss": 0.7535,
      "step": 23180
    },
    {
      "epoch": 169.27007299270073,
      "grad_norm": 13.285396575927734,
      "learning_rate": 4.153649635036497e-05,
      "loss": 0.9709,
      "step": 23190
    },
    {
      "epoch": 169.34306569343065,
      "grad_norm": 10.496259689331055,
      "learning_rate": 4.1532846715328474e-05,
      "loss": 1.4967,
      "step": 23200
    },
    {
      "epoch": 169.4160583941606,
      "grad_norm": 2.3135390281677246,
      "learning_rate": 4.152919708029197e-05,
      "loss": 1.1688,
      "step": 23210
    },
    {
      "epoch": 169.4890510948905,
      "grad_norm": 8.656107902526855,
      "learning_rate": 4.1525547445255475e-05,
      "loss": 1.2531,
      "step": 23220
    },
    {
      "epoch": 169.56204379562044,
      "grad_norm": 12.584280014038086,
      "learning_rate": 4.152189781021898e-05,
      "loss": 1.4469,
      "step": 23230
    },
    {
      "epoch": 169.63503649635035,
      "grad_norm": 6.008350372314453,
      "learning_rate": 4.151824817518248e-05,
      "loss": 0.839,
      "step": 23240
    },
    {
      "epoch": 169.7080291970803,
      "grad_norm": 2.0638210773468018,
      "learning_rate": 4.151459854014599e-05,
      "loss": 1.0218,
      "step": 23250
    },
    {
      "epoch": 169.7810218978102,
      "grad_norm": 7.1855926513671875,
      "learning_rate": 4.151094890510949e-05,
      "loss": 1.2607,
      "step": 23260
    },
    {
      "epoch": 169.85401459854015,
      "grad_norm": 10.808188438415527,
      "learning_rate": 4.1507299270073e-05,
      "loss": 1.48,
      "step": 23270
    },
    {
      "epoch": 169.92700729927006,
      "grad_norm": 4.875879764556885,
      "learning_rate": 4.15036496350365e-05,
      "loss": 1.2992,
      "step": 23280
    },
    {
      "epoch": 170.0,
      "grad_norm": 14.543740272521973,
      "learning_rate": 4.15e-05,
      "loss": 1.4428,
      "step": 23290
    },
    {
      "epoch": 170.07299270072994,
      "grad_norm": 4.491449356079102,
      "learning_rate": 4.1496350364963506e-05,
      "loss": 1.0968,
      "step": 23300
    },
    {
      "epoch": 170.14598540145985,
      "grad_norm": 7.12906551361084,
      "learning_rate": 4.149270072992701e-05,
      "loss": 1.01,
      "step": 23310
    },
    {
      "epoch": 170.2189781021898,
      "grad_norm": 8.30903148651123,
      "learning_rate": 4.1489051094890514e-05,
      "loss": 1.0018,
      "step": 23320
    },
    {
      "epoch": 170.2919708029197,
      "grad_norm": 7.890915870666504,
      "learning_rate": 4.1485401459854015e-05,
      "loss": 1.1127,
      "step": 23330
    },
    {
      "epoch": 170.36496350364965,
      "grad_norm": 5.94583797454834,
      "learning_rate": 4.148175182481752e-05,
      "loss": 1.1136,
      "step": 23340
    },
    {
      "epoch": 170.43795620437956,
      "grad_norm": 9.713613510131836,
      "learning_rate": 4.147810218978103e-05,
      "loss": 1.4776,
      "step": 23350
    },
    {
      "epoch": 170.5109489051095,
      "grad_norm": 8.326203346252441,
      "learning_rate": 4.147445255474452e-05,
      "loss": 1.3632,
      "step": 23360
    },
    {
      "epoch": 170.5839416058394,
      "grad_norm": 9.646645545959473,
      "learning_rate": 4.147080291970803e-05,
      "loss": 1.5689,
      "step": 23370
    },
    {
      "epoch": 170.65693430656935,
      "grad_norm": 3.016162395477295,
      "learning_rate": 4.146715328467153e-05,
      "loss": 1.0546,
      "step": 23380
    },
    {
      "epoch": 170.72992700729927,
      "grad_norm": 3.7970359325408936,
      "learning_rate": 4.146350364963504e-05,
      "loss": 1.2389,
      "step": 23390
    },
    {
      "epoch": 170.8029197080292,
      "grad_norm": 7.664740562438965,
      "learning_rate": 4.1459854014598545e-05,
      "loss": 1.0099,
      "step": 23400
    },
    {
      "epoch": 170.87591240875912,
      "grad_norm": 2.2554845809936523,
      "learning_rate": 4.1456204379562046e-05,
      "loss": 0.8032,
      "step": 23410
    },
    {
      "epoch": 170.94890510948906,
      "grad_norm": 9.60813045501709,
      "learning_rate": 4.145255474452555e-05,
      "loss": 1.167,
      "step": 23420
    },
    {
      "epoch": 171.02189781021897,
      "grad_norm": 3.6253960132598877,
      "learning_rate": 4.1448905109489053e-05,
      "loss": 0.9929,
      "step": 23430
    },
    {
      "epoch": 171.09489051094891,
      "grad_norm": 9.897841453552246,
      "learning_rate": 4.1445255474452554e-05,
      "loss": 1.0475,
      "step": 23440
    },
    {
      "epoch": 171.16788321167883,
      "grad_norm": 3.7700014114379883,
      "learning_rate": 4.144160583941606e-05,
      "loss": 1.3121,
      "step": 23450
    },
    {
      "epoch": 171.24087591240877,
      "grad_norm": 3.273562431335449,
      "learning_rate": 4.143795620437956e-05,
      "loss": 0.939,
      "step": 23460
    },
    {
      "epoch": 171.31386861313868,
      "grad_norm": 7.068398952484131,
      "learning_rate": 4.143430656934307e-05,
      "loss": 0.8203,
      "step": 23470
    },
    {
      "epoch": 171.38686131386862,
      "grad_norm": 7.673141002655029,
      "learning_rate": 4.143065693430657e-05,
      "loss": 0.9885,
      "step": 23480
    },
    {
      "epoch": 171.45985401459853,
      "grad_norm": 7.302479267120361,
      "learning_rate": 4.142700729927008e-05,
      "loss": 0.909,
      "step": 23490
    },
    {
      "epoch": 171.53284671532847,
      "grad_norm": 3.778602123260498,
      "learning_rate": 4.1423357664233584e-05,
      "loss": 1.1366,
      "step": 23500
    },
    {
      "epoch": 171.6058394160584,
      "grad_norm": 9.01824951171875,
      "learning_rate": 4.141970802919708e-05,
      "loss": 1.4251,
      "step": 23510
    },
    {
      "epoch": 171.67883211678833,
      "grad_norm": 9.500924110412598,
      "learning_rate": 4.1416058394160585e-05,
      "loss": 1.2251,
      "step": 23520
    },
    {
      "epoch": 171.75182481751824,
      "grad_norm": 12.497147560119629,
      "learning_rate": 4.1412408759124085e-05,
      "loss": 1.2305,
      "step": 23530
    },
    {
      "epoch": 171.82481751824818,
      "grad_norm": 9.215289115905762,
      "learning_rate": 4.140875912408759e-05,
      "loss": 1.7078,
      "step": 23540
    },
    {
      "epoch": 171.8978102189781,
      "grad_norm": 11.66903305053711,
      "learning_rate": 4.14051094890511e-05,
      "loss": 1.4624,
      "step": 23550
    },
    {
      "epoch": 171.97080291970804,
      "grad_norm": 8.876933097839355,
      "learning_rate": 4.14014598540146e-05,
      "loss": 0.9922,
      "step": 23560
    },
    {
      "epoch": 172.04379562043795,
      "grad_norm": 16.59025001525879,
      "learning_rate": 4.139781021897811e-05,
      "loss": 1.2827,
      "step": 23570
    },
    {
      "epoch": 172.1167883211679,
      "grad_norm": 2.876833915710449,
      "learning_rate": 4.139416058394161e-05,
      "loss": 0.9686,
      "step": 23580
    },
    {
      "epoch": 172.1897810218978,
      "grad_norm": 11.976428031921387,
      "learning_rate": 4.139051094890511e-05,
      "loss": 1.5403,
      "step": 23590
    },
    {
      "epoch": 172.26277372262774,
      "grad_norm": 1.5104436874389648,
      "learning_rate": 4.1386861313868616e-05,
      "loss": 0.7162,
      "step": 23600
    },
    {
      "epoch": 172.33576642335765,
      "grad_norm": 8.188660621643066,
      "learning_rate": 4.1383211678832117e-05,
      "loss": 1.3837,
      "step": 23610
    },
    {
      "epoch": 172.4087591240876,
      "grad_norm": 7.297947883605957,
      "learning_rate": 4.1379562043795624e-05,
      "loss": 1.0436,
      "step": 23620
    },
    {
      "epoch": 172.4817518248175,
      "grad_norm": 5.959310054779053,
      "learning_rate": 4.1375912408759124e-05,
      "loss": 0.8638,
      "step": 23630
    },
    {
      "epoch": 172.55474452554745,
      "grad_norm": 1.4950484037399292,
      "learning_rate": 4.137226277372263e-05,
      "loss": 0.9337,
      "step": 23640
    },
    {
      "epoch": 172.62773722627736,
      "grad_norm": 13.542266845703125,
      "learning_rate": 4.136861313868613e-05,
      "loss": 1.4216,
      "step": 23650
    },
    {
      "epoch": 172.7007299270073,
      "grad_norm": 9.31158447265625,
      "learning_rate": 4.136496350364964e-05,
      "loss": 1.1124,
      "step": 23660
    },
    {
      "epoch": 172.77372262773721,
      "grad_norm": 4.4130659103393555,
      "learning_rate": 4.136131386861314e-05,
      "loss": 1.0443,
      "step": 23670
    },
    {
      "epoch": 172.84671532846716,
      "grad_norm": 9.797874450683594,
      "learning_rate": 4.135766423357664e-05,
      "loss": 1.1504,
      "step": 23680
    },
    {
      "epoch": 172.91970802919707,
      "grad_norm": 6.93976354598999,
      "learning_rate": 4.135401459854015e-05,
      "loss": 0.9233,
      "step": 23690
    },
    {
      "epoch": 172.992700729927,
      "grad_norm": 7.241426467895508,
      "learning_rate": 4.1350364963503655e-05,
      "loss": 1.5631,
      "step": 23700
    },
    {
      "epoch": 173.06569343065692,
      "grad_norm": 1.8577566146850586,
      "learning_rate": 4.1346715328467155e-05,
      "loss": 1.1944,
      "step": 23710
    },
    {
      "epoch": 173.13868613138686,
      "grad_norm": 6.675162315368652,
      "learning_rate": 4.134306569343066e-05,
      "loss": 0.9186,
      "step": 23720
    },
    {
      "epoch": 173.21167883211677,
      "grad_norm": 12.308002471923828,
      "learning_rate": 4.133941605839416e-05,
      "loss": 1.2212,
      "step": 23730
    },
    {
      "epoch": 173.28467153284672,
      "grad_norm": 9.081140518188477,
      "learning_rate": 4.1335766423357664e-05,
      "loss": 1.1014,
      "step": 23740
    },
    {
      "epoch": 173.35766423357666,
      "grad_norm": 7.075845241546631,
      "learning_rate": 4.133211678832117e-05,
      "loss": 1.0518,
      "step": 23750
    },
    {
      "epoch": 173.43065693430657,
      "grad_norm": 2.2433454990386963,
      "learning_rate": 4.132846715328467e-05,
      "loss": 1.136,
      "step": 23760
    },
    {
      "epoch": 173.5036496350365,
      "grad_norm": 7.639805316925049,
      "learning_rate": 4.132481751824818e-05,
      "loss": 0.8199,
      "step": 23770
    },
    {
      "epoch": 173.57664233576642,
      "grad_norm": 11.252026557922363,
      "learning_rate": 4.132116788321168e-05,
      "loss": 1.0661,
      "step": 23780
    },
    {
      "epoch": 173.64963503649636,
      "grad_norm": 13.66894817352295,
      "learning_rate": 4.1317518248175186e-05,
      "loss": 1.6498,
      "step": 23790
    },
    {
      "epoch": 173.72262773722628,
      "grad_norm": 1.384366512298584,
      "learning_rate": 4.131386861313869e-05,
      "loss": 0.9918,
      "step": 23800
    },
    {
      "epoch": 173.79562043795622,
      "grad_norm": 7.148906707763672,
      "learning_rate": 4.1310218978102194e-05,
      "loss": 1.0592,
      "step": 23810
    },
    {
      "epoch": 173.86861313868613,
      "grad_norm": 8.900999069213867,
      "learning_rate": 4.1306569343065695e-05,
      "loss": 1.1669,
      "step": 23820
    },
    {
      "epoch": 173.94160583941607,
      "grad_norm": 11.283773422241211,
      "learning_rate": 4.1302919708029195e-05,
      "loss": 1.1111,
      "step": 23830
    },
    {
      "epoch": 174.01459854014598,
      "grad_norm": 9.156046867370605,
      "learning_rate": 4.12992700729927e-05,
      "loss": 1.5192,
      "step": 23840
    },
    {
      "epoch": 174.08759124087592,
      "grad_norm": 8.955870628356934,
      "learning_rate": 4.12956204379562e-05,
      "loss": 1.236,
      "step": 23850
    },
    {
      "epoch": 174.16058394160584,
      "grad_norm": 7.947558403015137,
      "learning_rate": 4.129197080291971e-05,
      "loss": 0.9738,
      "step": 23860
    },
    {
      "epoch": 174.23357664233578,
      "grad_norm": 6.803461074829102,
      "learning_rate": 4.128832116788322e-05,
      "loss": 1.102,
      "step": 23870
    },
    {
      "epoch": 174.3065693430657,
      "grad_norm": 6.290055274963379,
      "learning_rate": 4.128467153284672e-05,
      "loss": 1.1533,
      "step": 23880
    },
    {
      "epoch": 174.37956204379563,
      "grad_norm": 8.836292266845703,
      "learning_rate": 4.1281021897810225e-05,
      "loss": 1.1991,
      "step": 23890
    },
    {
      "epoch": 174.45255474452554,
      "grad_norm": 6.61489200592041,
      "learning_rate": 4.1277372262773726e-05,
      "loss": 1.4227,
      "step": 23900
    },
    {
      "epoch": 174.52554744525548,
      "grad_norm": 5.305811405181885,
      "learning_rate": 4.1273722627737226e-05,
      "loss": 1.1366,
      "step": 23910
    },
    {
      "epoch": 174.5985401459854,
      "grad_norm": 11.198973655700684,
      "learning_rate": 4.1270072992700734e-05,
      "loss": 1.0673,
      "step": 23920
    },
    {
      "epoch": 174.67153284671534,
      "grad_norm": 12.1754150390625,
      "learning_rate": 4.1266423357664234e-05,
      "loss": 1.201,
      "step": 23930
    },
    {
      "epoch": 174.74452554744525,
      "grad_norm": 2.1190834045410156,
      "learning_rate": 4.126277372262774e-05,
      "loss": 0.9962,
      "step": 23940
    },
    {
      "epoch": 174.8175182481752,
      "grad_norm": 10.212579727172852,
      "learning_rate": 4.125912408759124e-05,
      "loss": 1.1826,
      "step": 23950
    },
    {
      "epoch": 174.8905109489051,
      "grad_norm": 5.884193420410156,
      "learning_rate": 4.125547445255475e-05,
      "loss": 1.0369,
      "step": 23960
    },
    {
      "epoch": 174.96350364963504,
      "grad_norm": 13.732327461242676,
      "learning_rate": 4.125182481751825e-05,
      "loss": 1.1624,
      "step": 23970
    },
    {
      "epoch": 175.03649635036496,
      "grad_norm": 13.30137825012207,
      "learning_rate": 4.124817518248175e-05,
      "loss": 1.2722,
      "step": 23980
    },
    {
      "epoch": 175.1094890510949,
      "grad_norm": 8.668303489685059,
      "learning_rate": 4.124452554744526e-05,
      "loss": 1.4652,
      "step": 23990
    },
    {
      "epoch": 175.1824817518248,
      "grad_norm": 10.580533027648926,
      "learning_rate": 4.124087591240876e-05,
      "loss": 1.2821,
      "step": 24000
    },
    {
      "epoch": 175.25547445255475,
      "grad_norm": 7.680495262145996,
      "learning_rate": 4.1237226277372265e-05,
      "loss": 1.3696,
      "step": 24010
    },
    {
      "epoch": 175.32846715328466,
      "grad_norm": 13.463762283325195,
      "learning_rate": 4.123357664233577e-05,
      "loss": 0.8469,
      "step": 24020
    },
    {
      "epoch": 175.4014598540146,
      "grad_norm": 6.0785088539123535,
      "learning_rate": 4.122992700729927e-05,
      "loss": 0.8166,
      "step": 24030
    },
    {
      "epoch": 175.47445255474452,
      "grad_norm": 2.9215691089630127,
      "learning_rate": 4.122627737226278e-05,
      "loss": 1.2092,
      "step": 24040
    },
    {
      "epoch": 175.54744525547446,
      "grad_norm": 6.3664231300354,
      "learning_rate": 4.1222627737226274e-05,
      "loss": 1.0699,
      "step": 24050
    },
    {
      "epoch": 175.62043795620437,
      "grad_norm": 2.8681797981262207,
      "learning_rate": 4.121897810218978e-05,
      "loss": 0.7881,
      "step": 24060
    },
    {
      "epoch": 175.6934306569343,
      "grad_norm": 7.020412445068359,
      "learning_rate": 4.121532846715329e-05,
      "loss": 0.9117,
      "step": 24070
    },
    {
      "epoch": 175.76642335766422,
      "grad_norm": 11.796268463134766,
      "learning_rate": 4.121167883211679e-05,
      "loss": 1.1438,
      "step": 24080
    },
    {
      "epoch": 175.83941605839416,
      "grad_norm": 13.414026260375977,
      "learning_rate": 4.1208029197080296e-05,
      "loss": 1.1145,
      "step": 24090
    },
    {
      "epoch": 175.91240875912408,
      "grad_norm": 1.0271046161651611,
      "learning_rate": 4.1204379562043797e-05,
      "loss": 1.0795,
      "step": 24100
    },
    {
      "epoch": 175.98540145985402,
      "grad_norm": 11.74804401397705,
      "learning_rate": 4.1200729927007304e-05,
      "loss": 1.241,
      "step": 24110
    },
    {
      "epoch": 176.05839416058393,
      "grad_norm": 14.934503555297852,
      "learning_rate": 4.1197080291970804e-05,
      "loss": 1.0076,
      "step": 24120
    },
    {
      "epoch": 176.13138686131387,
      "grad_norm": 11.996600151062012,
      "learning_rate": 4.1193430656934305e-05,
      "loss": 1.1436,
      "step": 24130
    },
    {
      "epoch": 176.20437956204378,
      "grad_norm": 13.34771728515625,
      "learning_rate": 4.118978102189781e-05,
      "loss": 1.1312,
      "step": 24140
    },
    {
      "epoch": 176.27737226277372,
      "grad_norm": 11.49886703491211,
      "learning_rate": 4.118613138686131e-05,
      "loss": 1.3261,
      "step": 24150
    },
    {
      "epoch": 176.35036496350364,
      "grad_norm": 5.664985179901123,
      "learning_rate": 4.118248175182482e-05,
      "loss": 0.8943,
      "step": 24160
    },
    {
      "epoch": 176.42335766423358,
      "grad_norm": 9.609953880310059,
      "learning_rate": 4.117883211678833e-05,
      "loss": 0.8637,
      "step": 24170
    },
    {
      "epoch": 176.4963503649635,
      "grad_norm": 6.456666946411133,
      "learning_rate": 4.117518248175183e-05,
      "loss": 1.4112,
      "step": 24180
    },
    {
      "epoch": 176.56934306569343,
      "grad_norm": 1.9249011278152466,
      "learning_rate": 4.1171532846715335e-05,
      "loss": 1.0768,
      "step": 24190
    },
    {
      "epoch": 176.64233576642334,
      "grad_norm": 5.0601372718811035,
      "learning_rate": 4.116788321167883e-05,
      "loss": 0.9992,
      "step": 24200
    },
    {
      "epoch": 176.71532846715328,
      "grad_norm": 5.509751319885254,
      "learning_rate": 4.1164233576642336e-05,
      "loss": 1.045,
      "step": 24210
    },
    {
      "epoch": 176.78832116788323,
      "grad_norm": 17.349109649658203,
      "learning_rate": 4.116058394160584e-05,
      "loss": 1.3588,
      "step": 24220
    },
    {
      "epoch": 176.86131386861314,
      "grad_norm": 1.3408652544021606,
      "learning_rate": 4.1156934306569344e-05,
      "loss": 1.2075,
      "step": 24230
    },
    {
      "epoch": 176.93430656934308,
      "grad_norm": 9.555717468261719,
      "learning_rate": 4.115328467153285e-05,
      "loss": 1.1059,
      "step": 24240
    },
    {
      "epoch": 177.007299270073,
      "grad_norm": 7.947714328765869,
      "learning_rate": 4.114963503649635e-05,
      "loss": 1.0006,
      "step": 24250
    },
    {
      "epoch": 177.08029197080293,
      "grad_norm": 10.552885055541992,
      "learning_rate": 4.114598540145986e-05,
      "loss": 1.0494,
      "step": 24260
    },
    {
      "epoch": 177.15328467153284,
      "grad_norm": 16.532978057861328,
      "learning_rate": 4.114233576642336e-05,
      "loss": 1.357,
      "step": 24270
    },
    {
      "epoch": 177.22627737226279,
      "grad_norm": 7.1438798904418945,
      "learning_rate": 4.113868613138686e-05,
      "loss": 1.0021,
      "step": 24280
    },
    {
      "epoch": 177.2992700729927,
      "grad_norm": 12.915457725524902,
      "learning_rate": 4.113503649635037e-05,
      "loss": 1.4315,
      "step": 24290
    },
    {
      "epoch": 177.37226277372264,
      "grad_norm": 11.34627628326416,
      "learning_rate": 4.113138686131387e-05,
      "loss": 1.1205,
      "step": 24300
    },
    {
      "epoch": 177.44525547445255,
      "grad_norm": 8.640776634216309,
      "learning_rate": 4.1127737226277375e-05,
      "loss": 1.1073,
      "step": 24310
    },
    {
      "epoch": 177.5182481751825,
      "grad_norm": 5.1701531410217285,
      "learning_rate": 4.1124087591240875e-05,
      "loss": 0.9044,
      "step": 24320
    },
    {
      "epoch": 177.5912408759124,
      "grad_norm": 6.441155910491943,
      "learning_rate": 4.112043795620438e-05,
      "loss": 0.8278,
      "step": 24330
    },
    {
      "epoch": 177.66423357664235,
      "grad_norm": 7.448686599731445,
      "learning_rate": 4.111678832116789e-05,
      "loss": 1.3069,
      "step": 24340
    },
    {
      "epoch": 177.73722627737226,
      "grad_norm": 11.735824584960938,
      "learning_rate": 4.1113138686131384e-05,
      "loss": 0.828,
      "step": 24350
    },
    {
      "epoch": 177.8102189781022,
      "grad_norm": 12.920852661132812,
      "learning_rate": 4.110948905109489e-05,
      "loss": 1.1324,
      "step": 24360
    },
    {
      "epoch": 177.8832116788321,
      "grad_norm": 2.550393581390381,
      "learning_rate": 4.11058394160584e-05,
      "loss": 1.1251,
      "step": 24370
    },
    {
      "epoch": 177.95620437956205,
      "grad_norm": 6.045400619506836,
      "learning_rate": 4.11021897810219e-05,
      "loss": 0.7457,
      "step": 24380
    },
    {
      "epoch": 178.02919708029196,
      "grad_norm": 2.1956169605255127,
      "learning_rate": 4.1098540145985406e-05,
      "loss": 1.4693,
      "step": 24390
    },
    {
      "epoch": 178.1021897810219,
      "grad_norm": 12.97983169555664,
      "learning_rate": 4.1094890510948906e-05,
      "loss": 1.2091,
      "step": 24400
    },
    {
      "epoch": 178.17518248175182,
      "grad_norm": 5.58955192565918,
      "learning_rate": 4.1091240875912414e-05,
      "loss": 0.8247,
      "step": 24410
    },
    {
      "epoch": 178.24817518248176,
      "grad_norm": 6.851790428161621,
      "learning_rate": 4.1087591240875914e-05,
      "loss": 1.5526,
      "step": 24420
    },
    {
      "epoch": 178.32116788321167,
      "grad_norm": 10.562833786010742,
      "learning_rate": 4.1083941605839415e-05,
      "loss": 0.9022,
      "step": 24430
    },
    {
      "epoch": 178.3941605839416,
      "grad_norm": 13.854037284851074,
      "learning_rate": 4.108029197080292e-05,
      "loss": 1.4102,
      "step": 24440
    },
    {
      "epoch": 178.46715328467153,
      "grad_norm": 4.692591190338135,
      "learning_rate": 4.107664233576642e-05,
      "loss": 0.8448,
      "step": 24450
    },
    {
      "epoch": 178.54014598540147,
      "grad_norm": 9.948473930358887,
      "learning_rate": 4.107299270072993e-05,
      "loss": 1.059,
      "step": 24460
    },
    {
      "epoch": 178.61313868613138,
      "grad_norm": 15.401398658752441,
      "learning_rate": 4.106934306569343e-05,
      "loss": 1.2354,
      "step": 24470
    },
    {
      "epoch": 178.68613138686132,
      "grad_norm": 7.463216781616211,
      "learning_rate": 4.106569343065694e-05,
      "loss": 1.0234,
      "step": 24480
    },
    {
      "epoch": 178.75912408759123,
      "grad_norm": 7.0857672691345215,
      "learning_rate": 4.1062043795620445e-05,
      "loss": 0.9486,
      "step": 24490
    },
    {
      "epoch": 178.83211678832117,
      "grad_norm": 5.885159492492676,
      "learning_rate": 4.1058394160583945e-05,
      "loss": 1.1254,
      "step": 24500
    },
    {
      "epoch": 178.90510948905109,
      "grad_norm": 14.474239349365234,
      "learning_rate": 4.1054744525547446e-05,
      "loss": 1.2644,
      "step": 24510
    },
    {
      "epoch": 178.97810218978103,
      "grad_norm": 8.839920043945312,
      "learning_rate": 4.1051094890510946e-05,
      "loss": 1.3088,
      "step": 24520
    },
    {
      "epoch": 179.05109489051094,
      "grad_norm": 8.546231269836426,
      "learning_rate": 4.1047445255474453e-05,
      "loss": 1.1228,
      "step": 24530
    },
    {
      "epoch": 179.12408759124088,
      "grad_norm": 2.204033374786377,
      "learning_rate": 4.104379562043796e-05,
      "loss": 0.9454,
      "step": 24540
    },
    {
      "epoch": 179.1970802919708,
      "grad_norm": 6.80962610244751,
      "learning_rate": 4.104014598540146e-05,
      "loss": 1.3284,
      "step": 24550
    },
    {
      "epoch": 179.27007299270073,
      "grad_norm": 11.228325843811035,
      "learning_rate": 4.103649635036497e-05,
      "loss": 1.36,
      "step": 24560
    },
    {
      "epoch": 179.34306569343065,
      "grad_norm": 4.889215469360352,
      "learning_rate": 4.103284671532847e-05,
      "loss": 0.8777,
      "step": 24570
    },
    {
      "epoch": 179.4160583941606,
      "grad_norm": 10.889947891235352,
      "learning_rate": 4.102919708029197e-05,
      "loss": 0.8771,
      "step": 24580
    },
    {
      "epoch": 179.4890510948905,
      "grad_norm": 2.059075355529785,
      "learning_rate": 4.102554744525548e-05,
      "loss": 1.1258,
      "step": 24590
    },
    {
      "epoch": 179.56204379562044,
      "grad_norm": 11.291738510131836,
      "learning_rate": 4.102189781021898e-05,
      "loss": 1.2008,
      "step": 24600
    },
    {
      "epoch": 179.63503649635035,
      "grad_norm": 2.5296664237976074,
      "learning_rate": 4.1018248175182484e-05,
      "loss": 1.3068,
      "step": 24610
    },
    {
      "epoch": 179.7080291970803,
      "grad_norm": 7.5737833976745605,
      "learning_rate": 4.1014598540145985e-05,
      "loss": 1.0608,
      "step": 24620
    },
    {
      "epoch": 179.7810218978102,
      "grad_norm": 7.559995651245117,
      "learning_rate": 4.101094890510949e-05,
      "loss": 1.1628,
      "step": 24630
    },
    {
      "epoch": 179.85401459854015,
      "grad_norm": 9.497934341430664,
      "learning_rate": 4.1007299270073e-05,
      "loss": 1.1983,
      "step": 24640
    },
    {
      "epoch": 179.92700729927006,
      "grad_norm": 7.773800373077393,
      "learning_rate": 4.10036496350365e-05,
      "loss": 1.0953,
      "step": 24650
    },
    {
      "epoch": 180.0,
      "grad_norm": 16.589879989624023,
      "learning_rate": 4.1e-05,
      "loss": 1.1217,
      "step": 24660
    },
    {
      "epoch": 180.07299270072994,
      "grad_norm": 1.331458330154419,
      "learning_rate": 4.09963503649635e-05,
      "loss": 0.8588,
      "step": 24670
    },
    {
      "epoch": 180.14598540145985,
      "grad_norm": 5.642516136169434,
      "learning_rate": 4.099270072992701e-05,
      "loss": 1.3962,
      "step": 24680
    },
    {
      "epoch": 180.2189781021898,
      "grad_norm": 17.693458557128906,
      "learning_rate": 4.0989051094890516e-05,
      "loss": 1.3028,
      "step": 24690
    },
    {
      "epoch": 180.2919708029197,
      "grad_norm": 13.821516036987305,
      "learning_rate": 4.0985401459854016e-05,
      "loss": 1.3577,
      "step": 24700
    },
    {
      "epoch": 180.36496350364965,
      "grad_norm": 10.157657623291016,
      "learning_rate": 4.098175182481752e-05,
      "loss": 0.898,
      "step": 24710
    },
    {
      "epoch": 180.43795620437956,
      "grad_norm": 10.933088302612305,
      "learning_rate": 4.0978102189781024e-05,
      "loss": 1.1742,
      "step": 24720
    },
    {
      "epoch": 180.5109489051095,
      "grad_norm": 1.3724192380905151,
      "learning_rate": 4.097445255474453e-05,
      "loss": 0.9992,
      "step": 24730
    },
    {
      "epoch": 180.5839416058394,
      "grad_norm": 13.204846382141113,
      "learning_rate": 4.097080291970803e-05,
      "loss": 0.9801,
      "step": 24740
    },
    {
      "epoch": 180.65693430656935,
      "grad_norm": 11.9722900390625,
      "learning_rate": 4.096715328467153e-05,
      "loss": 1.2836,
      "step": 24750
    },
    {
      "epoch": 180.72992700729927,
      "grad_norm": 2.090597152709961,
      "learning_rate": 4.096350364963504e-05,
      "loss": 0.6638,
      "step": 24760
    },
    {
      "epoch": 180.8029197080292,
      "grad_norm": 7.103545665740967,
      "learning_rate": 4.095985401459854e-05,
      "loss": 1.1234,
      "step": 24770
    },
    {
      "epoch": 180.87591240875912,
      "grad_norm": 3.1985578536987305,
      "learning_rate": 4.095620437956205e-05,
      "loss": 1.1838,
      "step": 24780
    },
    {
      "epoch": 180.94890510948906,
      "grad_norm": 9.467806816101074,
      "learning_rate": 4.095255474452555e-05,
      "loss": 1.1384,
      "step": 24790
    },
    {
      "epoch": 181.02189781021897,
      "grad_norm": 9.413795471191406,
      "learning_rate": 4.0948905109489055e-05,
      "loss": 1.107,
      "step": 24800
    },
    {
      "epoch": 181.09489051094891,
      "grad_norm": 12.815211296081543,
      "learning_rate": 4.0945255474452555e-05,
      "loss": 1.2762,
      "step": 24810
    },
    {
      "epoch": 181.16788321167883,
      "grad_norm": 9.527695655822754,
      "learning_rate": 4.0941605839416056e-05,
      "loss": 1.0478,
      "step": 24820
    },
    {
      "epoch": 181.24087591240877,
      "grad_norm": 14.021317481994629,
      "learning_rate": 4.093795620437956e-05,
      "loss": 1.304,
      "step": 24830
    },
    {
      "epoch": 181.31386861313868,
      "grad_norm": 12.441640853881836,
      "learning_rate": 4.093430656934307e-05,
      "loss": 1.2102,
      "step": 24840
    },
    {
      "epoch": 181.38686131386862,
      "grad_norm": 12.720730781555176,
      "learning_rate": 4.093065693430657e-05,
      "loss": 1.4014,
      "step": 24850
    },
    {
      "epoch": 181.45985401459853,
      "grad_norm": 9.303115844726562,
      "learning_rate": 4.092700729927008e-05,
      "loss": 1.2026,
      "step": 24860
    },
    {
      "epoch": 181.53284671532847,
      "grad_norm": 14.041722297668457,
      "learning_rate": 4.092335766423358e-05,
      "loss": 1.2207,
      "step": 24870
    },
    {
      "epoch": 181.6058394160584,
      "grad_norm": 6.445216178894043,
      "learning_rate": 4.0919708029197086e-05,
      "loss": 1.1609,
      "step": 24880
    },
    {
      "epoch": 181.67883211678833,
      "grad_norm": 6.733865737915039,
      "learning_rate": 4.0916058394160586e-05,
      "loss": 1.0723,
      "step": 24890
    },
    {
      "epoch": 181.75182481751824,
      "grad_norm": 13.956648826599121,
      "learning_rate": 4.091240875912409e-05,
      "loss": 0.6085,
      "step": 24900
    },
    {
      "epoch": 181.82481751824818,
      "grad_norm": 9.989701271057129,
      "learning_rate": 4.0908759124087594e-05,
      "loss": 1.0655,
      "step": 24910
    },
    {
      "epoch": 181.8978102189781,
      "grad_norm": 10.70348072052002,
      "learning_rate": 4.0905109489051095e-05,
      "loss": 1.1019,
      "step": 24920
    },
    {
      "epoch": 181.97080291970804,
      "grad_norm": 8.141983032226562,
      "learning_rate": 4.09014598540146e-05,
      "loss": 0.8198,
      "step": 24930
    },
    {
      "epoch": 182.04379562043795,
      "grad_norm": 10.13211727142334,
      "learning_rate": 4.08978102189781e-05,
      "loss": 1.0453,
      "step": 24940
    },
    {
      "epoch": 182.1167883211679,
      "grad_norm": 8.725831985473633,
      "learning_rate": 4.089416058394161e-05,
      "loss": 1.1498,
      "step": 24950
    },
    {
      "epoch": 182.1897810218978,
      "grad_norm": 10.040489196777344,
      "learning_rate": 4.089051094890512e-05,
      "loss": 1.0236,
      "step": 24960
    },
    {
      "epoch": 182.26277372262774,
      "grad_norm": 3.2798991203308105,
      "learning_rate": 4.088686131386861e-05,
      "loss": 0.7736,
      "step": 24970
    },
    {
      "epoch": 182.33576642335765,
      "grad_norm": 9.743446350097656,
      "learning_rate": 4.088321167883212e-05,
      "loss": 0.6844,
      "step": 24980
    },
    {
      "epoch": 182.4087591240876,
      "grad_norm": 14.504179000854492,
      "learning_rate": 4.087956204379562e-05,
      "loss": 1.0419,
      "step": 24990
    },
    {
      "epoch": 182.4817518248175,
      "grad_norm": 12.173783302307129,
      "learning_rate": 4.0875912408759126e-05,
      "loss": 1.0155,
      "step": 25000
    },
    {
      "epoch": 182.55474452554745,
      "grad_norm": 9.881020545959473,
      "learning_rate": 4.087226277372263e-05,
      "loss": 1.2511,
      "step": 25010
    },
    {
      "epoch": 182.62773722627736,
      "grad_norm": 15.296256065368652,
      "learning_rate": 4.0868613138686133e-05,
      "loss": 0.9396,
      "step": 25020
    },
    {
      "epoch": 182.7007299270073,
      "grad_norm": 6.902202606201172,
      "learning_rate": 4.086496350364964e-05,
      "loss": 1.1701,
      "step": 25030
    },
    {
      "epoch": 182.77372262773721,
      "grad_norm": 13.94985294342041,
      "learning_rate": 4.086131386861314e-05,
      "loss": 1.7617,
      "step": 25040
    },
    {
      "epoch": 182.84671532846716,
      "grad_norm": 6.360294342041016,
      "learning_rate": 4.085766423357664e-05,
      "loss": 1.0358,
      "step": 25050
    },
    {
      "epoch": 182.91970802919707,
      "grad_norm": 8.3905668258667,
      "learning_rate": 4.085401459854015e-05,
      "loss": 1.3002,
      "step": 25060
    },
    {
      "epoch": 182.992700729927,
      "grad_norm": 10.671235084533691,
      "learning_rate": 4.085036496350365e-05,
      "loss": 1.1549,
      "step": 25070
    },
    {
      "epoch": 183.06569343065692,
      "grad_norm": 8.60986614227295,
      "learning_rate": 4.084671532846716e-05,
      "loss": 1.1201,
      "step": 25080
    },
    {
      "epoch": 183.13868613138686,
      "grad_norm": 7.1601152420043945,
      "learning_rate": 4.084306569343066e-05,
      "loss": 1.2407,
      "step": 25090
    },
    {
      "epoch": 183.21167883211677,
      "grad_norm": 10.163667678833008,
      "learning_rate": 4.0839416058394165e-05,
      "loss": 1.0828,
      "step": 25100
    },
    {
      "epoch": 183.28467153284672,
      "grad_norm": 9.557555198669434,
      "learning_rate": 4.083576642335767e-05,
      "loss": 1.1052,
      "step": 25110
    },
    {
      "epoch": 183.35766423357666,
      "grad_norm": 13.897575378417969,
      "learning_rate": 4.0832116788321166e-05,
      "loss": 0.9212,
      "step": 25120
    },
    {
      "epoch": 183.43065693430657,
      "grad_norm": 8.680129051208496,
      "learning_rate": 4.082846715328467e-05,
      "loss": 1.2658,
      "step": 25130
    },
    {
      "epoch": 183.5036496350365,
      "grad_norm": 12.576940536499023,
      "learning_rate": 4.082481751824817e-05,
      "loss": 0.7394,
      "step": 25140
    },
    {
      "epoch": 183.57664233576642,
      "grad_norm": 6.389052391052246,
      "learning_rate": 4.082116788321168e-05,
      "loss": 1.0678,
      "step": 25150
    },
    {
      "epoch": 183.64963503649636,
      "grad_norm": 7.947514057159424,
      "learning_rate": 4.081751824817519e-05,
      "loss": 0.8489,
      "step": 25160
    },
    {
      "epoch": 183.72262773722628,
      "grad_norm": 15.388116836547852,
      "learning_rate": 4.081386861313869e-05,
      "loss": 1.5004,
      "step": 25170
    },
    {
      "epoch": 183.79562043795622,
      "grad_norm": 8.068384170532227,
      "learning_rate": 4.0810218978102196e-05,
      "loss": 0.8239,
      "step": 25180
    },
    {
      "epoch": 183.86861313868613,
      "grad_norm": 14.210463523864746,
      "learning_rate": 4.0806569343065696e-05,
      "loss": 1.4178,
      "step": 25190
    },
    {
      "epoch": 183.94160583941607,
      "grad_norm": 6.585742473602295,
      "learning_rate": 4.0802919708029197e-05,
      "loss": 1.0277,
      "step": 25200
    },
    {
      "epoch": 184.01459854014598,
      "grad_norm": 12.906656265258789,
      "learning_rate": 4.0799270072992704e-05,
      "loss": 1.361,
      "step": 25210
    },
    {
      "epoch": 184.08759124087592,
      "grad_norm": 7.312990188598633,
      "learning_rate": 4.0795620437956204e-05,
      "loss": 1.0565,
      "step": 25220
    },
    {
      "epoch": 184.16058394160584,
      "grad_norm": 10.545969009399414,
      "learning_rate": 4.079197080291971e-05,
      "loss": 0.9695,
      "step": 25230
    },
    {
      "epoch": 184.23357664233578,
      "grad_norm": 8.229345321655273,
      "learning_rate": 4.078832116788321e-05,
      "loss": 1.2239,
      "step": 25240
    },
    {
      "epoch": 184.3065693430657,
      "grad_norm": 4.8644280433654785,
      "learning_rate": 4.078467153284672e-05,
      "loss": 0.995,
      "step": 25250
    },
    {
      "epoch": 184.37956204379563,
      "grad_norm": 2.195415735244751,
      "learning_rate": 4.078102189781022e-05,
      "loss": 0.8893,
      "step": 25260
    },
    {
      "epoch": 184.45255474452554,
      "grad_norm": 2.226318120956421,
      "learning_rate": 4.077737226277372e-05,
      "loss": 1.3626,
      "step": 25270
    },
    {
      "epoch": 184.52554744525548,
      "grad_norm": 8.908583641052246,
      "learning_rate": 4.077372262773723e-05,
      "loss": 0.8926,
      "step": 25280
    },
    {
      "epoch": 184.5985401459854,
      "grad_norm": 13.251474380493164,
      "learning_rate": 4.077007299270073e-05,
      "loss": 1.3118,
      "step": 25290
    },
    {
      "epoch": 184.67153284671534,
      "grad_norm": 9.740251541137695,
      "learning_rate": 4.0766423357664235e-05,
      "loss": 0.9611,
      "step": 25300
    },
    {
      "epoch": 184.74452554744525,
      "grad_norm": 2.2546699047088623,
      "learning_rate": 4.076277372262774e-05,
      "loss": 0.9818,
      "step": 25310
    },
    {
      "epoch": 184.8175182481752,
      "grad_norm": 4.029632568359375,
      "learning_rate": 4.075912408759124e-05,
      "loss": 0.9284,
      "step": 25320
    },
    {
      "epoch": 184.8905109489051,
      "grad_norm": 6.787962913513184,
      "learning_rate": 4.075547445255475e-05,
      "loss": 0.9166,
      "step": 25330
    },
    {
      "epoch": 184.96350364963504,
      "grad_norm": 7.57718563079834,
      "learning_rate": 4.075182481751825e-05,
      "loss": 1.4481,
      "step": 25340
    },
    {
      "epoch": 185.03649635036496,
      "grad_norm": 2.854783058166504,
      "learning_rate": 4.074817518248175e-05,
      "loss": 0.9512,
      "step": 25350
    },
    {
      "epoch": 185.1094890510949,
      "grad_norm": 6.960669994354248,
      "learning_rate": 4.074452554744526e-05,
      "loss": 0.9759,
      "step": 25360
    },
    {
      "epoch": 185.1824817518248,
      "grad_norm": 10.373061180114746,
      "learning_rate": 4.074087591240876e-05,
      "loss": 1.2763,
      "step": 25370
    },
    {
      "epoch": 185.25547445255475,
      "grad_norm": 6.396564483642578,
      "learning_rate": 4.0737226277372266e-05,
      "loss": 0.8535,
      "step": 25380
    },
    {
      "epoch": 185.32846715328466,
      "grad_norm": 6.646600246429443,
      "learning_rate": 4.073357664233577e-05,
      "loss": 1.0808,
      "step": 25390
    },
    {
      "epoch": 185.4014598540146,
      "grad_norm": 8.061197280883789,
      "learning_rate": 4.0729927007299274e-05,
      "loss": 1.5562,
      "step": 25400
    },
    {
      "epoch": 185.47445255474452,
      "grad_norm": 2.5472939014434814,
      "learning_rate": 4.0726277372262775e-05,
      "loss": 1.3549,
      "step": 25410
    },
    {
      "epoch": 185.54744525547446,
      "grad_norm": 1.7440232038497925,
      "learning_rate": 4.0722627737226275e-05,
      "loss": 1.0365,
      "step": 25420
    },
    {
      "epoch": 185.62043795620437,
      "grad_norm": 13.410750389099121,
      "learning_rate": 4.071897810218978e-05,
      "loss": 1.1831,
      "step": 25430
    },
    {
      "epoch": 185.6934306569343,
      "grad_norm": 6.705428600311279,
      "learning_rate": 4.071532846715328e-05,
      "loss": 1.0714,
      "step": 25440
    },
    {
      "epoch": 185.76642335766422,
      "grad_norm": 9.910443305969238,
      "learning_rate": 4.071167883211679e-05,
      "loss": 1.0876,
      "step": 25450
    },
    {
      "epoch": 185.83941605839416,
      "grad_norm": 7.225433826446533,
      "learning_rate": 4.070802919708029e-05,
      "loss": 1.2341,
      "step": 25460
    },
    {
      "epoch": 185.91240875912408,
      "grad_norm": 6.375142574310303,
      "learning_rate": 4.07043795620438e-05,
      "loss": 0.9013,
      "step": 25470
    },
    {
      "epoch": 185.98540145985402,
      "grad_norm": 10.547429084777832,
      "learning_rate": 4.0700729927007305e-05,
      "loss": 0.9773,
      "step": 25480
    },
    {
      "epoch": 186.05839416058393,
      "grad_norm": 8.905847549438477,
      "learning_rate": 4.0697080291970806e-05,
      "loss": 0.8166,
      "step": 25490
    },
    {
      "epoch": 186.13138686131387,
      "grad_norm": 7.553030490875244,
      "learning_rate": 4.0693430656934306e-05,
      "loss": 0.9771,
      "step": 25500
    },
    {
      "epoch": 186.20437956204378,
      "grad_norm": 10.638493537902832,
      "learning_rate": 4.0689781021897814e-05,
      "loss": 1.7274,
      "step": 25510
    },
    {
      "epoch": 186.27737226277372,
      "grad_norm": 6.116581439971924,
      "learning_rate": 4.0686131386861314e-05,
      "loss": 0.8195,
      "step": 25520
    },
    {
      "epoch": 186.35036496350364,
      "grad_norm": 15.021236419677734,
      "learning_rate": 4.068248175182482e-05,
      "loss": 0.9904,
      "step": 25530
    },
    {
      "epoch": 186.42335766423358,
      "grad_norm": 3.0518922805786133,
      "learning_rate": 4.067883211678832e-05,
      "loss": 1.0051,
      "step": 25540
    },
    {
      "epoch": 186.4963503649635,
      "grad_norm": 13.897873878479004,
      "learning_rate": 4.067518248175183e-05,
      "loss": 1.0884,
      "step": 25550
    },
    {
      "epoch": 186.56934306569343,
      "grad_norm": 10.051271438598633,
      "learning_rate": 4.067153284671533e-05,
      "loss": 0.9,
      "step": 25560
    },
    {
      "epoch": 186.64233576642334,
      "grad_norm": 1.1210283041000366,
      "learning_rate": 4.066788321167884e-05,
      "loss": 1.0675,
      "step": 25570
    },
    {
      "epoch": 186.71532846715328,
      "grad_norm": 8.122300148010254,
      "learning_rate": 4.066423357664234e-05,
      "loss": 1.1551,
      "step": 25580
    },
    {
      "epoch": 186.78832116788323,
      "grad_norm": 5.327926158905029,
      "learning_rate": 4.066058394160584e-05,
      "loss": 0.9532,
      "step": 25590
    },
    {
      "epoch": 186.86131386861314,
      "grad_norm": 11.962353706359863,
      "learning_rate": 4.0656934306569345e-05,
      "loss": 1.3082,
      "step": 25600
    },
    {
      "epoch": 186.93430656934308,
      "grad_norm": 10.818848609924316,
      "learning_rate": 4.0653284671532846e-05,
      "loss": 0.9772,
      "step": 25610
    },
    {
      "epoch": 187.007299270073,
      "grad_norm": 7.958013534545898,
      "learning_rate": 4.064963503649635e-05,
      "loss": 1.2771,
      "step": 25620
    },
    {
      "epoch": 187.08029197080293,
      "grad_norm": 7.890718460083008,
      "learning_rate": 4.064598540145986e-05,
      "loss": 1.1014,
      "step": 25630
    },
    {
      "epoch": 187.15328467153284,
      "grad_norm": 5.619480609893799,
      "learning_rate": 4.064233576642336e-05,
      "loss": 1.1967,
      "step": 25640
    },
    {
      "epoch": 187.22627737226279,
      "grad_norm": 10.421466827392578,
      "learning_rate": 4.063868613138686e-05,
      "loss": 0.9334,
      "step": 25650
    },
    {
      "epoch": 187.2992700729927,
      "grad_norm": 8.623122215270996,
      "learning_rate": 4.063503649635036e-05,
      "loss": 1.3137,
      "step": 25660
    },
    {
      "epoch": 187.37226277372264,
      "grad_norm": 11.199579238891602,
      "learning_rate": 4.063138686131387e-05,
      "loss": 1.0042,
      "step": 25670
    },
    {
      "epoch": 187.44525547445255,
      "grad_norm": 8.680785179138184,
      "learning_rate": 4.0627737226277376e-05,
      "loss": 1.0153,
      "step": 25680
    },
    {
      "epoch": 187.5182481751825,
      "grad_norm": 6.926765441894531,
      "learning_rate": 4.062408759124088e-05,
      "loss": 0.8533,
      "step": 25690
    },
    {
      "epoch": 187.5912408759124,
      "grad_norm": 8.954485893249512,
      "learning_rate": 4.0620437956204384e-05,
      "loss": 1.2686,
      "step": 25700
    },
    {
      "epoch": 187.66423357664235,
      "grad_norm": 8.855838775634766,
      "learning_rate": 4.0616788321167884e-05,
      "loss": 0.9798,
      "step": 25710
    },
    {
      "epoch": 187.73722627737226,
      "grad_norm": 11.206361770629883,
      "learning_rate": 4.061313868613139e-05,
      "loss": 1.2959,
      "step": 25720
    },
    {
      "epoch": 187.8102189781022,
      "grad_norm": 1.9319838285446167,
      "learning_rate": 4.060948905109489e-05,
      "loss": 1.1702,
      "step": 25730
    },
    {
      "epoch": 187.8832116788321,
      "grad_norm": 7.3544158935546875,
      "learning_rate": 4.060583941605839e-05,
      "loss": 0.9364,
      "step": 25740
    },
    {
      "epoch": 187.95620437956205,
      "grad_norm": 8.79274845123291,
      "learning_rate": 4.06021897810219e-05,
      "loss": 1.4244,
      "step": 25750
    },
    {
      "epoch": 188.02919708029196,
      "grad_norm": 10.359992980957031,
      "learning_rate": 4.05985401459854e-05,
      "loss": 1.21,
      "step": 25760
    },
    {
      "epoch": 188.1021897810219,
      "grad_norm": 5.679933547973633,
      "learning_rate": 4.059489051094891e-05,
      "loss": 0.8394,
      "step": 25770
    },
    {
      "epoch": 188.17518248175182,
      "grad_norm": 9.692869186401367,
      "learning_rate": 4.0591240875912415e-05,
      "loss": 1.0444,
      "step": 25780
    },
    {
      "epoch": 188.24817518248176,
      "grad_norm": 11.325566291809082,
      "learning_rate": 4.0587591240875915e-05,
      "loss": 0.6588,
      "step": 25790
    },
    {
      "epoch": 188.32116788321167,
      "grad_norm": 9.033682823181152,
      "learning_rate": 4.058394160583942e-05,
      "loss": 1.4889,
      "step": 25800
    },
    {
      "epoch": 188.3941605839416,
      "grad_norm": 2.4581480026245117,
      "learning_rate": 4.0580291970802916e-05,
      "loss": 1.2029,
      "step": 25810
    },
    {
      "epoch": 188.46715328467153,
      "grad_norm": 2.2216227054595947,
      "learning_rate": 4.0576642335766424e-05,
      "loss": 0.7715,
      "step": 25820
    },
    {
      "epoch": 188.54014598540147,
      "grad_norm": 17.281343460083008,
      "learning_rate": 4.057299270072993e-05,
      "loss": 1.3398,
      "step": 25830
    },
    {
      "epoch": 188.61313868613138,
      "grad_norm": 16.067392349243164,
      "learning_rate": 4.056934306569343e-05,
      "loss": 1.1647,
      "step": 25840
    },
    {
      "epoch": 188.68613138686132,
      "grad_norm": 10.450793266296387,
      "learning_rate": 4.056569343065694e-05,
      "loss": 1.4706,
      "step": 25850
    },
    {
      "epoch": 188.75912408759123,
      "grad_norm": 0.9063763618469238,
      "learning_rate": 4.056204379562044e-05,
      "loss": 0.6987,
      "step": 25860
    },
    {
      "epoch": 188.83211678832117,
      "grad_norm": 8.617563247680664,
      "learning_rate": 4.0558394160583947e-05,
      "loss": 1.2136,
      "step": 25870
    },
    {
      "epoch": 188.90510948905109,
      "grad_norm": 6.53123140335083,
      "learning_rate": 4.055474452554745e-05,
      "loss": 1.1637,
      "step": 25880
    },
    {
      "epoch": 188.97810218978103,
      "grad_norm": 9.073200225830078,
      "learning_rate": 4.055109489051095e-05,
      "loss": 1.2829,
      "step": 25890
    },
    {
      "epoch": 189.05109489051094,
      "grad_norm": 10.881353378295898,
      "learning_rate": 4.0547445255474455e-05,
      "loss": 0.9867,
      "step": 25900
    },
    {
      "epoch": 189.12408759124088,
      "grad_norm": 13.964140892028809,
      "learning_rate": 4.0543795620437955e-05,
      "loss": 1.4017,
      "step": 25910
    },
    {
      "epoch": 189.1970802919708,
      "grad_norm": 13.288200378417969,
      "learning_rate": 4.054014598540146e-05,
      "loss": 1.1107,
      "step": 25920
    },
    {
      "epoch": 189.27007299270073,
      "grad_norm": 8.096809387207031,
      "learning_rate": 4.053649635036496e-05,
      "loss": 1.4045,
      "step": 25930
    },
    {
      "epoch": 189.34306569343065,
      "grad_norm": 1.9548803567886353,
      "learning_rate": 4.053284671532847e-05,
      "loss": 0.8718,
      "step": 25940
    },
    {
      "epoch": 189.4160583941606,
      "grad_norm": 2.1769275665283203,
      "learning_rate": 4.052919708029198e-05,
      "loss": 0.8338,
      "step": 25950
    },
    {
      "epoch": 189.4890510948905,
      "grad_norm": 9.252861976623535,
      "learning_rate": 4.052554744525547e-05,
      "loss": 0.8934,
      "step": 25960
    },
    {
      "epoch": 189.56204379562044,
      "grad_norm": 11.458840370178223,
      "learning_rate": 4.052189781021898e-05,
      "loss": 1.3336,
      "step": 25970
    },
    {
      "epoch": 189.63503649635035,
      "grad_norm": 7.020871162414551,
      "learning_rate": 4.0518248175182486e-05,
      "loss": 0.8609,
      "step": 25980
    },
    {
      "epoch": 189.7080291970803,
      "grad_norm": 7.653862953186035,
      "learning_rate": 4.0514598540145986e-05,
      "loss": 1.0581,
      "step": 25990
    },
    {
      "epoch": 189.7810218978102,
      "grad_norm": 1.2486077547073364,
      "learning_rate": 4.0510948905109494e-05,
      "loss": 1.2129,
      "step": 26000
    },
    {
      "epoch": 189.85401459854015,
      "grad_norm": 6.609033584594727,
      "learning_rate": 4.0507299270072994e-05,
      "loss": 1.3903,
      "step": 26010
    },
    {
      "epoch": 189.92700729927006,
      "grad_norm": 1.207680583000183,
      "learning_rate": 4.05036496350365e-05,
      "loss": 0.8281,
      "step": 26020
    },
    {
      "epoch": 190.0,
      "grad_norm": 20.882469177246094,
      "learning_rate": 4.05e-05,
      "loss": 1.0858,
      "step": 26030
    },
    {
      "epoch": 190.07299270072994,
      "grad_norm": 1.9375711679458618,
      "learning_rate": 4.04963503649635e-05,
      "loss": 1.3551,
      "step": 26040
    },
    {
      "epoch": 190.14598540145985,
      "grad_norm": 3.063788890838623,
      "learning_rate": 4.049270072992701e-05,
      "loss": 1.1619,
      "step": 26050
    },
    {
      "epoch": 190.2189781021898,
      "grad_norm": 4.887618541717529,
      "learning_rate": 4.048905109489051e-05,
      "loss": 0.6752,
      "step": 26060
    },
    {
      "epoch": 190.2919708029197,
      "grad_norm": 6.0828328132629395,
      "learning_rate": 4.048540145985402e-05,
      "loss": 1.3433,
      "step": 26070
    },
    {
      "epoch": 190.36496350364965,
      "grad_norm": 10.952014923095703,
      "learning_rate": 4.048175182481752e-05,
      "loss": 1.0758,
      "step": 26080
    },
    {
      "epoch": 190.43795620437956,
      "grad_norm": 5.989253044128418,
      "learning_rate": 4.0478102189781025e-05,
      "loss": 0.7041,
      "step": 26090
    },
    {
      "epoch": 190.5109489051095,
      "grad_norm": 2.3341357707977295,
      "learning_rate": 4.047445255474453e-05,
      "loss": 1.1532,
      "step": 26100
    },
    {
      "epoch": 190.5839416058394,
      "grad_norm": 7.223617076873779,
      "learning_rate": 4.0470802919708026e-05,
      "loss": 1.1156,
      "step": 26110
    },
    {
      "epoch": 190.65693430656935,
      "grad_norm": 16.283580780029297,
      "learning_rate": 4.0467153284671533e-05,
      "loss": 0.973,
      "step": 26120
    },
    {
      "epoch": 190.72992700729927,
      "grad_norm": 7.995548725128174,
      "learning_rate": 4.0463503649635034e-05,
      "loss": 1.4781,
      "step": 26130
    },
    {
      "epoch": 190.8029197080292,
      "grad_norm": 9.491165161132812,
      "learning_rate": 4.045985401459854e-05,
      "loss": 1.0587,
      "step": 26140
    },
    {
      "epoch": 190.87591240875912,
      "grad_norm": 8.918795585632324,
      "learning_rate": 4.045620437956205e-05,
      "loss": 1.131,
      "step": 26150
    },
    {
      "epoch": 190.94890510948906,
      "grad_norm": 0.9254016280174255,
      "learning_rate": 4.045255474452555e-05,
      "loss": 1.0468,
      "step": 26160
    },
    {
      "epoch": 191.02189781021897,
      "grad_norm": 9.527305603027344,
      "learning_rate": 4.0448905109489056e-05,
      "loss": 1.0017,
      "step": 26170
    },
    {
      "epoch": 191.09489051094891,
      "grad_norm": 1.2165123224258423,
      "learning_rate": 4.044525547445256e-05,
      "loss": 0.7404,
      "step": 26180
    },
    {
      "epoch": 191.16788321167883,
      "grad_norm": 13.441070556640625,
      "learning_rate": 4.044160583941606e-05,
      "loss": 1.0056,
      "step": 26190
    },
    {
      "epoch": 191.24087591240877,
      "grad_norm": 7.37565803527832,
      "learning_rate": 4.0437956204379564e-05,
      "loss": 1.1176,
      "step": 26200
    },
    {
      "epoch": 191.31386861313868,
      "grad_norm": 1.6477441787719727,
      "learning_rate": 4.0434306569343065e-05,
      "loss": 1.5704,
      "step": 26210
    },
    {
      "epoch": 191.38686131386862,
      "grad_norm": 8.50163745880127,
      "learning_rate": 4.043065693430657e-05,
      "loss": 1.0121,
      "step": 26220
    },
    {
      "epoch": 191.45985401459853,
      "grad_norm": 12.97445297241211,
      "learning_rate": 4.042700729927007e-05,
      "loss": 1.3355,
      "step": 26230
    },
    {
      "epoch": 191.53284671532847,
      "grad_norm": 9.102862358093262,
      "learning_rate": 4.042335766423358e-05,
      "loss": 0.6192,
      "step": 26240
    },
    {
      "epoch": 191.6058394160584,
      "grad_norm": 7.920401573181152,
      "learning_rate": 4.041970802919709e-05,
      "loss": 0.8985,
      "step": 26250
    },
    {
      "epoch": 191.67883211678833,
      "grad_norm": 9.162022590637207,
      "learning_rate": 4.041605839416059e-05,
      "loss": 0.9673,
      "step": 26260
    },
    {
      "epoch": 191.75182481751824,
      "grad_norm": 6.09879207611084,
      "learning_rate": 4.041240875912409e-05,
      "loss": 1.1465,
      "step": 26270
    },
    {
      "epoch": 191.82481751824818,
      "grad_norm": 11.583163261413574,
      "learning_rate": 4.040875912408759e-05,
      "loss": 1.5974,
      "step": 26280
    },
    {
      "epoch": 191.8978102189781,
      "grad_norm": 12.182036399841309,
      "learning_rate": 4.0405109489051096e-05,
      "loss": 0.9201,
      "step": 26290
    },
    {
      "epoch": 191.97080291970804,
      "grad_norm": 10.834325790405273,
      "learning_rate": 4.04014598540146e-05,
      "loss": 1.1436,
      "step": 26300
    },
    {
      "epoch": 192.04379562043795,
      "grad_norm": 7.175045490264893,
      "learning_rate": 4.0397810218978104e-05,
      "loss": 1.197,
      "step": 26310
    },
    {
      "epoch": 192.1167883211679,
      "grad_norm": 8.714317321777344,
      "learning_rate": 4.039416058394161e-05,
      "loss": 0.9326,
      "step": 26320
    },
    {
      "epoch": 192.1897810218978,
      "grad_norm": 1.7703388929367065,
      "learning_rate": 4.039051094890511e-05,
      "loss": 1.0565,
      "step": 26330
    },
    {
      "epoch": 192.26277372262774,
      "grad_norm": 11.11028003692627,
      "learning_rate": 4.038686131386861e-05,
      "loss": 0.9233,
      "step": 26340
    },
    {
      "epoch": 192.33576642335765,
      "grad_norm": 11.91104793548584,
      "learning_rate": 4.038321167883212e-05,
      "loss": 1.1465,
      "step": 26350
    },
    {
      "epoch": 192.4087591240876,
      "grad_norm": 1.4749380350112915,
      "learning_rate": 4.037956204379562e-05,
      "loss": 0.7682,
      "step": 26360
    },
    {
      "epoch": 192.4817518248175,
      "grad_norm": 6.954788684844971,
      "learning_rate": 4.037591240875913e-05,
      "loss": 1.2788,
      "step": 26370
    },
    {
      "epoch": 192.55474452554745,
      "grad_norm": 14.369871139526367,
      "learning_rate": 4.037226277372263e-05,
      "loss": 1.3823,
      "step": 26380
    },
    {
      "epoch": 192.62773722627736,
      "grad_norm": 10.188803672790527,
      "learning_rate": 4.0368613138686135e-05,
      "loss": 1.1327,
      "step": 26390
    },
    {
      "epoch": 192.7007299270073,
      "grad_norm": 13.440879821777344,
      "learning_rate": 4.0364963503649635e-05,
      "loss": 0.9942,
      "step": 26400
    },
    {
      "epoch": 192.77372262773721,
      "grad_norm": 15.816669464111328,
      "learning_rate": 4.036131386861314e-05,
      "loss": 1.42,
      "step": 26410
    },
    {
      "epoch": 192.84671532846716,
      "grad_norm": 8.998337745666504,
      "learning_rate": 4.035766423357664e-05,
      "loss": 1.1966,
      "step": 26420
    },
    {
      "epoch": 192.91970802919707,
      "grad_norm": 7.334927558898926,
      "learning_rate": 4.0354014598540144e-05,
      "loss": 0.7731,
      "step": 26430
    },
    {
      "epoch": 192.992700729927,
      "grad_norm": 5.1004638671875,
      "learning_rate": 4.035036496350365e-05,
      "loss": 0.7898,
      "step": 26440
    },
    {
      "epoch": 193.06569343065692,
      "grad_norm": 6.873323917388916,
      "learning_rate": 4.034671532846716e-05,
      "loss": 1.0676,
      "step": 26450
    },
    {
      "epoch": 193.13868613138686,
      "grad_norm": 8.674152374267578,
      "learning_rate": 4.034306569343066e-05,
      "loss": 1.0913,
      "step": 26460
    },
    {
      "epoch": 193.21167883211677,
      "grad_norm": 13.698609352111816,
      "learning_rate": 4.0339416058394166e-05,
      "loss": 1.2289,
      "step": 26470
    },
    {
      "epoch": 193.28467153284672,
      "grad_norm": 8.263959884643555,
      "learning_rate": 4.0335766423357666e-05,
      "loss": 1.7492,
      "step": 26480
    },
    {
      "epoch": 193.35766423357666,
      "grad_norm": 9.936014175415039,
      "learning_rate": 4.0332116788321174e-05,
      "loss": 0.844,
      "step": 26490
    },
    {
      "epoch": 193.43065693430657,
      "grad_norm": 9.814785957336426,
      "learning_rate": 4.0328467153284674e-05,
      "loss": 0.9743,
      "step": 26500
    },
    {
      "epoch": 193.5036496350365,
      "grad_norm": 11.451057434082031,
      "learning_rate": 4.0324817518248175e-05,
      "loss": 0.902,
      "step": 26510
    },
    {
      "epoch": 193.57664233576642,
      "grad_norm": 9.314242362976074,
      "learning_rate": 4.032116788321168e-05,
      "loss": 0.76,
      "step": 26520
    },
    {
      "epoch": 193.64963503649636,
      "grad_norm": 5.055178642272949,
      "learning_rate": 4.031751824817518e-05,
      "loss": 0.8989,
      "step": 26530
    },
    {
      "epoch": 193.72262773722628,
      "grad_norm": 1.1437925100326538,
      "learning_rate": 4.031386861313869e-05,
      "loss": 0.6812,
      "step": 26540
    },
    {
      "epoch": 193.79562043795622,
      "grad_norm": 15.18101692199707,
      "learning_rate": 4.031021897810219e-05,
      "loss": 1.2992,
      "step": 26550
    },
    {
      "epoch": 193.86861313868613,
      "grad_norm": 1.5934189558029175,
      "learning_rate": 4.03065693430657e-05,
      "loss": 1.0424,
      "step": 26560
    },
    {
      "epoch": 193.94160583941607,
      "grad_norm": 7.029477596282959,
      "learning_rate": 4.03029197080292e-05,
      "loss": 1.0922,
      "step": 26570
    },
    {
      "epoch": 194.01459854014598,
      "grad_norm": 2.5358023643493652,
      "learning_rate": 4.02992700729927e-05,
      "loss": 1.0735,
      "step": 26580
    },
    {
      "epoch": 194.08759124087592,
      "grad_norm": 8.853580474853516,
      "learning_rate": 4.0295620437956206e-05,
      "loss": 1.2876,
      "step": 26590
    },
    {
      "epoch": 194.16058394160584,
      "grad_norm": 7.810516834259033,
      "learning_rate": 4.0291970802919706e-05,
      "loss": 0.5641,
      "step": 26600
    },
    {
      "epoch": 194.23357664233578,
      "grad_norm": 6.393554210662842,
      "learning_rate": 4.0288321167883214e-05,
      "loss": 0.9787,
      "step": 26610
    },
    {
      "epoch": 194.3065693430657,
      "grad_norm": 15.239716529846191,
      "learning_rate": 4.028467153284672e-05,
      "loss": 1.2036,
      "step": 26620
    },
    {
      "epoch": 194.37956204379563,
      "grad_norm": 4.745543956756592,
      "learning_rate": 4.028102189781022e-05,
      "loss": 1.3547,
      "step": 26630
    },
    {
      "epoch": 194.45255474452554,
      "grad_norm": 12.517425537109375,
      "learning_rate": 4.027737226277373e-05,
      "loss": 0.9626,
      "step": 26640
    },
    {
      "epoch": 194.52554744525548,
      "grad_norm": 6.904541969299316,
      "learning_rate": 4.027372262773723e-05,
      "loss": 1.2164,
      "step": 26650
    },
    {
      "epoch": 194.5985401459854,
      "grad_norm": 7.50414514541626,
      "learning_rate": 4.027007299270073e-05,
      "loss": 0.7766,
      "step": 26660
    },
    {
      "epoch": 194.67153284671534,
      "grad_norm": 12.505439758300781,
      "learning_rate": 4.026642335766424e-05,
      "loss": 1.2755,
      "step": 26670
    },
    {
      "epoch": 194.74452554744525,
      "grad_norm": 8.034143447875977,
      "learning_rate": 4.026277372262774e-05,
      "loss": 1.3162,
      "step": 26680
    },
    {
      "epoch": 194.8175182481752,
      "grad_norm": 1.8341416120529175,
      "learning_rate": 4.0259124087591245e-05,
      "loss": 1.1135,
      "step": 26690
    },
    {
      "epoch": 194.8905109489051,
      "grad_norm": 8.800915718078613,
      "learning_rate": 4.0255474452554745e-05,
      "loss": 0.7367,
      "step": 26700
    },
    {
      "epoch": 194.96350364963504,
      "grad_norm": 9.941007614135742,
      "learning_rate": 4.025182481751825e-05,
      "loss": 1.4264,
      "step": 26710
    },
    {
      "epoch": 195.03649635036496,
      "grad_norm": 1.5991147756576538,
      "learning_rate": 4.024817518248175e-05,
      "loss": 0.9852,
      "step": 26720
    },
    {
      "epoch": 195.1094890510949,
      "grad_norm": 11.138566017150879,
      "learning_rate": 4.024452554744525e-05,
      "loss": 0.8766,
      "step": 26730
    },
    {
      "epoch": 195.1824817518248,
      "grad_norm": 10.76145076751709,
      "learning_rate": 4.024087591240876e-05,
      "loss": 1.2665,
      "step": 26740
    },
    {
      "epoch": 195.25547445255475,
      "grad_norm": 7.398346424102783,
      "learning_rate": 4.023722627737226e-05,
      "loss": 1.1041,
      "step": 26750
    },
    {
      "epoch": 195.32846715328466,
      "grad_norm": 7.494792461395264,
      "learning_rate": 4.023357664233577e-05,
      "loss": 1.0804,
      "step": 26760
    },
    {
      "epoch": 195.4014598540146,
      "grad_norm": 1.0709221363067627,
      "learning_rate": 4.0229927007299276e-05,
      "loss": 1.0475,
      "step": 26770
    },
    {
      "epoch": 195.47445255474452,
      "grad_norm": 4.653322219848633,
      "learning_rate": 4.0226277372262776e-05,
      "loss": 0.9552,
      "step": 26780
    },
    {
      "epoch": 195.54744525547446,
      "grad_norm": 2.79520320892334,
      "learning_rate": 4.022262773722628e-05,
      "loss": 1.3349,
      "step": 26790
    },
    {
      "epoch": 195.62043795620437,
      "grad_norm": 6.269035816192627,
      "learning_rate": 4.021897810218978e-05,
      "loss": 1.1117,
      "step": 26800
    },
    {
      "epoch": 195.6934306569343,
      "grad_norm": 13.583353042602539,
      "learning_rate": 4.0215328467153284e-05,
      "loss": 0.9083,
      "step": 26810
    },
    {
      "epoch": 195.76642335766422,
      "grad_norm": 12.149138450622559,
      "learning_rate": 4.021167883211679e-05,
      "loss": 0.8934,
      "step": 26820
    },
    {
      "epoch": 195.83941605839416,
      "grad_norm": 7.512701034545898,
      "learning_rate": 4.020802919708029e-05,
      "loss": 0.7898,
      "step": 26830
    },
    {
      "epoch": 195.91240875912408,
      "grad_norm": 11.282472610473633,
      "learning_rate": 4.02043795620438e-05,
      "loss": 1.1923,
      "step": 26840
    },
    {
      "epoch": 195.98540145985402,
      "grad_norm": 10.069764137268066,
      "learning_rate": 4.02007299270073e-05,
      "loss": 1.1348,
      "step": 26850
    },
    {
      "epoch": 196.05839416058393,
      "grad_norm": 9.000307083129883,
      "learning_rate": 4.019708029197081e-05,
      "loss": 1.2999,
      "step": 26860
    },
    {
      "epoch": 196.13138686131387,
      "grad_norm": 11.13125228881836,
      "learning_rate": 4.0193430656934314e-05,
      "loss": 0.996,
      "step": 26870
    },
    {
      "epoch": 196.20437956204378,
      "grad_norm": 5.434731483459473,
      "learning_rate": 4.018978102189781e-05,
      "loss": 1.093,
      "step": 26880
    },
    {
      "epoch": 196.27737226277372,
      "grad_norm": 13.022539138793945,
      "learning_rate": 4.0186131386861315e-05,
      "loss": 1.1617,
      "step": 26890
    },
    {
      "epoch": 196.35036496350364,
      "grad_norm": 1.1370083093643188,
      "learning_rate": 4.0182481751824816e-05,
      "loss": 0.6633,
      "step": 26900
    },
    {
      "epoch": 196.42335766423358,
      "grad_norm": 0.7050814032554626,
      "learning_rate": 4.017883211678832e-05,
      "loss": 0.4967,
      "step": 26910
    },
    {
      "epoch": 196.4963503649635,
      "grad_norm": 7.567772388458252,
      "learning_rate": 4.017518248175183e-05,
      "loss": 1.0936,
      "step": 26920
    },
    {
      "epoch": 196.56934306569343,
      "grad_norm": 10.448469161987305,
      "learning_rate": 4.017153284671533e-05,
      "loss": 1.3896,
      "step": 26930
    },
    {
      "epoch": 196.64233576642334,
      "grad_norm": 8.976631164550781,
      "learning_rate": 4.016788321167884e-05,
      "loss": 1.018,
      "step": 26940
    },
    {
      "epoch": 196.71532846715328,
      "grad_norm": 9.949341773986816,
      "learning_rate": 4.016423357664233e-05,
      "loss": 1.2084,
      "step": 26950
    },
    {
      "epoch": 196.78832116788323,
      "grad_norm": 8.080120086669922,
      "learning_rate": 4.016058394160584e-05,
      "loss": 1.2671,
      "step": 26960
    },
    {
      "epoch": 196.86131386861314,
      "grad_norm": 13.31860065460205,
      "learning_rate": 4.0156934306569346e-05,
      "loss": 1.2283,
      "step": 26970
    },
    {
      "epoch": 196.93430656934308,
      "grad_norm": 7.232324600219727,
      "learning_rate": 4.015328467153285e-05,
      "loss": 0.6409,
      "step": 26980
    },
    {
      "epoch": 197.007299270073,
      "grad_norm": 1.1329985857009888,
      "learning_rate": 4.0149635036496354e-05,
      "loss": 1.1198,
      "step": 26990
    },
    {
      "epoch": 197.08029197080293,
      "grad_norm": 4.271699905395508,
      "learning_rate": 4.0145985401459855e-05,
      "loss": 0.6832,
      "step": 27000
    },
    {
      "epoch": 197.15328467153284,
      "grad_norm": 9.551200866699219,
      "learning_rate": 4.014233576642336e-05,
      "loss": 1.3867,
      "step": 27010
    },
    {
      "epoch": 197.22627737226279,
      "grad_norm": 17.54088020324707,
      "learning_rate": 4.013868613138686e-05,
      "loss": 1.2416,
      "step": 27020
    },
    {
      "epoch": 197.2992700729927,
      "grad_norm": 11.344995498657227,
      "learning_rate": 4.013503649635036e-05,
      "loss": 1.0763,
      "step": 27030
    },
    {
      "epoch": 197.37226277372264,
      "grad_norm": 2.4243736267089844,
      "learning_rate": 4.013138686131387e-05,
      "loss": 0.9561,
      "step": 27040
    },
    {
      "epoch": 197.44525547445255,
      "grad_norm": 6.983693599700928,
      "learning_rate": 4.012773722627737e-05,
      "loss": 1.0864,
      "step": 27050
    },
    {
      "epoch": 197.5182481751825,
      "grad_norm": 5.001173973083496,
      "learning_rate": 4.012408759124088e-05,
      "loss": 0.7922,
      "step": 27060
    },
    {
      "epoch": 197.5912408759124,
      "grad_norm": 6.105201721191406,
      "learning_rate": 4.0120437956204385e-05,
      "loss": 1.1415,
      "step": 27070
    },
    {
      "epoch": 197.66423357664235,
      "grad_norm": 1.3732045888900757,
      "learning_rate": 4.0116788321167886e-05,
      "loss": 1.0736,
      "step": 27080
    },
    {
      "epoch": 197.73722627737226,
      "grad_norm": 12.291654586791992,
      "learning_rate": 4.011313868613139e-05,
      "loss": 1.1413,
      "step": 27090
    },
    {
      "epoch": 197.8102189781022,
      "grad_norm": 10.382074356079102,
      "learning_rate": 4.0109489051094894e-05,
      "loss": 0.9973,
      "step": 27100
    },
    {
      "epoch": 197.8832116788321,
      "grad_norm": 5.684659957885742,
      "learning_rate": 4.0105839416058394e-05,
      "loss": 1.0397,
      "step": 27110
    },
    {
      "epoch": 197.95620437956205,
      "grad_norm": 10.32947063446045,
      "learning_rate": 4.01021897810219e-05,
      "loss": 1.2887,
      "step": 27120
    },
    {
      "epoch": 198.02919708029196,
      "grad_norm": 8.870746612548828,
      "learning_rate": 4.00985401459854e-05,
      "loss": 0.9287,
      "step": 27130
    },
    {
      "epoch": 198.1021897810219,
      "grad_norm": 20.080780029296875,
      "learning_rate": 4.009489051094891e-05,
      "loss": 1.1734,
      "step": 27140
    },
    {
      "epoch": 198.17518248175182,
      "grad_norm": 9.88145923614502,
      "learning_rate": 4.009124087591241e-05,
      "loss": 0.6818,
      "step": 27150
    },
    {
      "epoch": 198.24817518248176,
      "grad_norm": 15.055675506591797,
      "learning_rate": 4.008759124087592e-05,
      "loss": 1.0634,
      "step": 27160
    },
    {
      "epoch": 198.32116788321167,
      "grad_norm": 14.066554069519043,
      "learning_rate": 4.008394160583942e-05,
      "loss": 1.2183,
      "step": 27170
    },
    {
      "epoch": 198.3941605839416,
      "grad_norm": 1.461565613746643,
      "learning_rate": 4.008029197080292e-05,
      "loss": 1.0518,
      "step": 27180
    },
    {
      "epoch": 198.46715328467153,
      "grad_norm": 16.729524612426758,
      "learning_rate": 4.0076642335766425e-05,
      "loss": 1.1525,
      "step": 27190
    },
    {
      "epoch": 198.54014598540147,
      "grad_norm": 7.1564459800720215,
      "learning_rate": 4.0072992700729926e-05,
      "loss": 0.8821,
      "step": 27200
    },
    {
      "epoch": 198.61313868613138,
      "grad_norm": 11.11719036102295,
      "learning_rate": 4.006934306569343e-05,
      "loss": 1.2786,
      "step": 27210
    },
    {
      "epoch": 198.68613138686132,
      "grad_norm": 10.296343803405762,
      "learning_rate": 4.0065693430656933e-05,
      "loss": 1.2285,
      "step": 27220
    },
    {
      "epoch": 198.75912408759123,
      "grad_norm": 6.839786529541016,
      "learning_rate": 4.006204379562044e-05,
      "loss": 0.9713,
      "step": 27230
    },
    {
      "epoch": 198.83211678832117,
      "grad_norm": 1.803925633430481,
      "learning_rate": 4.005839416058395e-05,
      "loss": 0.9334,
      "step": 27240
    },
    {
      "epoch": 198.90510948905109,
      "grad_norm": 7.338203430175781,
      "learning_rate": 4.005474452554745e-05,
      "loss": 1.2815,
      "step": 27250
    },
    {
      "epoch": 198.97810218978103,
      "grad_norm": 11.03641128540039,
      "learning_rate": 4.005109489051095e-05,
      "loss": 1.0712,
      "step": 27260
    },
    {
      "epoch": 199.05109489051094,
      "grad_norm": 8.134154319763184,
      "learning_rate": 4.0047445255474456e-05,
      "loss": 1.1354,
      "step": 27270
    },
    {
      "epoch": 199.12408759124088,
      "grad_norm": 12.009696960449219,
      "learning_rate": 4.004379562043796e-05,
      "loss": 1.1846,
      "step": 27280
    },
    {
      "epoch": 199.1970802919708,
      "grad_norm": 10.155829429626465,
      "learning_rate": 4.0040145985401464e-05,
      "loss": 1.1826,
      "step": 27290
    },
    {
      "epoch": 199.27007299270073,
      "grad_norm": 5.584637641906738,
      "learning_rate": 4.0036496350364964e-05,
      "loss": 1.2767,
      "step": 27300
    },
    {
      "epoch": 199.34306569343065,
      "grad_norm": 3.4683120250701904,
      "learning_rate": 4.003284671532847e-05,
      "loss": 0.9617,
      "step": 27310
    },
    {
      "epoch": 199.4160583941606,
      "grad_norm": 8.856817245483398,
      "learning_rate": 4.002919708029197e-05,
      "loss": 0.7932,
      "step": 27320
    },
    {
      "epoch": 199.4890510948905,
      "grad_norm": 6.9978227615356445,
      "learning_rate": 4.002554744525548e-05,
      "loss": 1.2876,
      "step": 27330
    },
    {
      "epoch": 199.56204379562044,
      "grad_norm": 1.38407564163208,
      "learning_rate": 4.002189781021898e-05,
      "loss": 0.7258,
      "step": 27340
    },
    {
      "epoch": 199.63503649635035,
      "grad_norm": 3.296844482421875,
      "learning_rate": 4.001824817518248e-05,
      "loss": 0.8247,
      "step": 27350
    },
    {
      "epoch": 199.7080291970803,
      "grad_norm": 6.152909755706787,
      "learning_rate": 4.001459854014599e-05,
      "loss": 1.5112,
      "step": 27360
    },
    {
      "epoch": 199.7810218978102,
      "grad_norm": 10.49949836730957,
      "learning_rate": 4.001094890510949e-05,
      "loss": 0.8857,
      "step": 27370
    },
    {
      "epoch": 199.85401459854015,
      "grad_norm": 9.853184700012207,
      "learning_rate": 4.0007299270072996e-05,
      "loss": 1.2722,
      "step": 27380
    },
    {
      "epoch": 199.92700729927006,
      "grad_norm": 10.375765800476074,
      "learning_rate": 4.00036496350365e-05,
      "loss": 1.2081,
      "step": 27390
    },
    {
      "epoch": 200.0,
      "grad_norm": 24.14751434326172,
      "learning_rate": 4e-05,
      "loss": 0.8536,
      "step": 27400
    },
    {
      "epoch": 200.07299270072994,
      "grad_norm": 2.4521121978759766,
      "learning_rate": 3.9996350364963504e-05,
      "loss": 0.9955,
      "step": 27410
    },
    {
      "epoch": 200.14598540145985,
      "grad_norm": 10.168066024780273,
      "learning_rate": 3.9992700729927004e-05,
      "loss": 0.9307,
      "step": 27420
    },
    {
      "epoch": 200.2189781021898,
      "grad_norm": 7.341985702514648,
      "learning_rate": 3.998905109489051e-05,
      "loss": 1.0308,
      "step": 27430
    },
    {
      "epoch": 200.2919708029197,
      "grad_norm": 4.560168266296387,
      "learning_rate": 3.998540145985402e-05,
      "loss": 0.8088,
      "step": 27440
    },
    {
      "epoch": 200.36496350364965,
      "grad_norm": 9.572088241577148,
      "learning_rate": 3.998175182481752e-05,
      "loss": 1.0473,
      "step": 27450
    },
    {
      "epoch": 200.43795620437956,
      "grad_norm": 6.329179763793945,
      "learning_rate": 3.9978102189781027e-05,
      "loss": 1.0888,
      "step": 27460
    },
    {
      "epoch": 200.5109489051095,
      "grad_norm": 10.197334289550781,
      "learning_rate": 3.997445255474453e-05,
      "loss": 0.9947,
      "step": 27470
    },
    {
      "epoch": 200.5839416058394,
      "grad_norm": 7.023116588592529,
      "learning_rate": 3.9970802919708034e-05,
      "loss": 1.0203,
      "step": 27480
    },
    {
      "epoch": 200.65693430656935,
      "grad_norm": 3.309795618057251,
      "learning_rate": 3.9967153284671535e-05,
      "loss": 0.8382,
      "step": 27490
    },
    {
      "epoch": 200.72992700729927,
      "grad_norm": 6.8380560874938965,
      "learning_rate": 3.9963503649635035e-05,
      "loss": 1.0106,
      "step": 27500
    },
    {
      "epoch": 200.8029197080292,
      "grad_norm": 15.443429946899414,
      "learning_rate": 3.995985401459854e-05,
      "loss": 1.4807,
      "step": 27510
    },
    {
      "epoch": 200.87591240875912,
      "grad_norm": 7.508293151855469,
      "learning_rate": 3.995620437956204e-05,
      "loss": 1.1093,
      "step": 27520
    },
    {
      "epoch": 200.94890510948906,
      "grad_norm": 0.5761296153068542,
      "learning_rate": 3.995255474452555e-05,
      "loss": 0.9639,
      "step": 27530
    },
    {
      "epoch": 201.02189781021897,
      "grad_norm": 6.4141926765441895,
      "learning_rate": 3.994890510948906e-05,
      "loss": 1.0822,
      "step": 27540
    },
    {
      "epoch": 201.09489051094891,
      "grad_norm": 10.217467308044434,
      "learning_rate": 3.994525547445256e-05,
      "loss": 1.0786,
      "step": 27550
    },
    {
      "epoch": 201.16788321167883,
      "grad_norm": 7.43280553817749,
      "learning_rate": 3.9941605839416065e-05,
      "loss": 1.1624,
      "step": 27560
    },
    {
      "epoch": 201.24087591240877,
      "grad_norm": 7.384479999542236,
      "learning_rate": 3.993795620437956e-05,
      "loss": 0.6813,
      "step": 27570
    },
    {
      "epoch": 201.31386861313868,
      "grad_norm": 6.14248514175415,
      "learning_rate": 3.9934306569343066e-05,
      "loss": 0.8753,
      "step": 27580
    },
    {
      "epoch": 201.38686131386862,
      "grad_norm": 8.942935943603516,
      "learning_rate": 3.9930656934306574e-05,
      "loss": 0.9101,
      "step": 27590
    },
    {
      "epoch": 201.45985401459853,
      "grad_norm": 9.560371398925781,
      "learning_rate": 3.9927007299270074e-05,
      "loss": 1.0769,
      "step": 27600
    },
    {
      "epoch": 201.53284671532847,
      "grad_norm": 17.62803840637207,
      "learning_rate": 3.992335766423358e-05,
      "loss": 1.0119,
      "step": 27610
    },
    {
      "epoch": 201.6058394160584,
      "grad_norm": 6.982954025268555,
      "learning_rate": 3.991970802919708e-05,
      "loss": 1.2711,
      "step": 27620
    },
    {
      "epoch": 201.67883211678833,
      "grad_norm": 14.750631332397461,
      "learning_rate": 3.991605839416059e-05,
      "loss": 1.0638,
      "step": 27630
    },
    {
      "epoch": 201.75182481751824,
      "grad_norm": 1.7071523666381836,
      "learning_rate": 3.991240875912409e-05,
      "loss": 1.0529,
      "step": 27640
    },
    {
      "epoch": 201.82481751824818,
      "grad_norm": 8.763486862182617,
      "learning_rate": 3.990875912408759e-05,
      "loss": 1.0616,
      "step": 27650
    },
    {
      "epoch": 201.8978102189781,
      "grad_norm": 7.472681045532227,
      "learning_rate": 3.99051094890511e-05,
      "loss": 0.9471,
      "step": 27660
    },
    {
      "epoch": 201.97080291970804,
      "grad_norm": 5.804947376251221,
      "learning_rate": 3.99014598540146e-05,
      "loss": 1.4328,
      "step": 27670
    },
    {
      "epoch": 202.04379562043795,
      "grad_norm": 7.16338586807251,
      "learning_rate": 3.9897810218978105e-05,
      "loss": 1.1144,
      "step": 27680
    },
    {
      "epoch": 202.1167883211679,
      "grad_norm": 6.225701332092285,
      "learning_rate": 3.9894160583941606e-05,
      "loss": 0.9954,
      "step": 27690
    },
    {
      "epoch": 202.1897810218978,
      "grad_norm": 9.746435165405273,
      "learning_rate": 3.989051094890511e-05,
      "loss": 1.1545,
      "step": 27700
    },
    {
      "epoch": 202.26277372262774,
      "grad_norm": 1.2715716361999512,
      "learning_rate": 3.988686131386862e-05,
      "loss": 0.8827,
      "step": 27710
    },
    {
      "epoch": 202.33576642335765,
      "grad_norm": 15.219902992248535,
      "learning_rate": 3.9883211678832114e-05,
      "loss": 1.0713,
      "step": 27720
    },
    {
      "epoch": 202.4087591240876,
      "grad_norm": 9.451903343200684,
      "learning_rate": 3.987956204379562e-05,
      "loss": 1.2732,
      "step": 27730
    },
    {
      "epoch": 202.4817518248175,
      "grad_norm": 6.667080879211426,
      "learning_rate": 3.987591240875913e-05,
      "loss": 1.0871,
      "step": 27740
    },
    {
      "epoch": 202.55474452554745,
      "grad_norm": 6.296162128448486,
      "learning_rate": 3.987226277372263e-05,
      "loss": 1.0376,
      "step": 27750
    },
    {
      "epoch": 202.62773722627736,
      "grad_norm": 1.690003514289856,
      "learning_rate": 3.9868613138686136e-05,
      "loss": 0.8085,
      "step": 27760
    },
    {
      "epoch": 202.7007299270073,
      "grad_norm": 7.248085021972656,
      "learning_rate": 3.986496350364964e-05,
      "loss": 1.4583,
      "step": 27770
    },
    {
      "epoch": 202.77372262773721,
      "grad_norm": 0.5117124319076538,
      "learning_rate": 3.9861313868613144e-05,
      "loss": 0.8252,
      "step": 27780
    },
    {
      "epoch": 202.84671532846716,
      "grad_norm": 11.731241226196289,
      "learning_rate": 3.9857664233576645e-05,
      "loss": 1.0574,
      "step": 27790
    },
    {
      "epoch": 202.91970802919707,
      "grad_norm": 8.02646255493164,
      "learning_rate": 3.9854014598540145e-05,
      "loss": 0.9376,
      "step": 27800
    },
    {
      "epoch": 202.992700729927,
      "grad_norm": 7.639122486114502,
      "learning_rate": 3.985036496350365e-05,
      "loss": 0.8192,
      "step": 27810
    },
    {
      "epoch": 203.06569343065692,
      "grad_norm": 2.945945978164673,
      "learning_rate": 3.984671532846715e-05,
      "loss": 0.647,
      "step": 27820
    },
    {
      "epoch": 203.13868613138686,
      "grad_norm": 13.881284713745117,
      "learning_rate": 3.984306569343066e-05,
      "loss": 0.902,
      "step": 27830
    },
    {
      "epoch": 203.21167883211677,
      "grad_norm": 8.46246337890625,
      "learning_rate": 3.983941605839416e-05,
      "loss": 0.8841,
      "step": 27840
    },
    {
      "epoch": 203.28467153284672,
      "grad_norm": 13.21179485321045,
      "learning_rate": 3.983576642335767e-05,
      "loss": 1.1316,
      "step": 27850
    },
    {
      "epoch": 203.35766423357666,
      "grad_norm": 9.730725288391113,
      "learning_rate": 3.9832116788321175e-05,
      "loss": 0.9158,
      "step": 27860
    },
    {
      "epoch": 203.43065693430657,
      "grad_norm": 6.5028862953186035,
      "learning_rate": 3.982846715328467e-05,
      "loss": 1.4912,
      "step": 27870
    },
    {
      "epoch": 203.5036496350365,
      "grad_norm": 1.888793706893921,
      "learning_rate": 3.9824817518248176e-05,
      "loss": 0.9405,
      "step": 27880
    },
    {
      "epoch": 203.57664233576642,
      "grad_norm": 10.408132553100586,
      "learning_rate": 3.9821167883211677e-05,
      "loss": 1.0897,
      "step": 27890
    },
    {
      "epoch": 203.64963503649636,
      "grad_norm": 0.3668268322944641,
      "learning_rate": 3.9817518248175184e-05,
      "loss": 0.902,
      "step": 27900
    },
    {
      "epoch": 203.72262773722628,
      "grad_norm": 12.342918395996094,
      "learning_rate": 3.981386861313869e-05,
      "loss": 1.0148,
      "step": 27910
    },
    {
      "epoch": 203.79562043795622,
      "grad_norm": 7.390781879425049,
      "learning_rate": 3.981021897810219e-05,
      "loss": 1.1317,
      "step": 27920
    },
    {
      "epoch": 203.86861313868613,
      "grad_norm": 9.902729034423828,
      "learning_rate": 3.98065693430657e-05,
      "loss": 1.3168,
      "step": 27930
    },
    {
      "epoch": 203.94160583941607,
      "grad_norm": 5.88020658493042,
      "learning_rate": 3.98029197080292e-05,
      "loss": 1.2955,
      "step": 27940
    },
    {
      "epoch": 204.01459854014598,
      "grad_norm": 6.337609767913818,
      "learning_rate": 3.97992700729927e-05,
      "loss": 0.807,
      "step": 27950
    },
    {
      "epoch": 204.08759124087592,
      "grad_norm": 7.936643123626709,
      "learning_rate": 3.979562043795621e-05,
      "loss": 1.1773,
      "step": 27960
    },
    {
      "epoch": 204.16058394160584,
      "grad_norm": 9.475824356079102,
      "learning_rate": 3.979197080291971e-05,
      "loss": 0.9368,
      "step": 27970
    },
    {
      "epoch": 204.23357664233578,
      "grad_norm": 10.115717887878418,
      "learning_rate": 3.9788321167883215e-05,
      "loss": 1.0328,
      "step": 27980
    },
    {
      "epoch": 204.3065693430657,
      "grad_norm": 12.005903244018555,
      "learning_rate": 3.9784671532846715e-05,
      "loss": 1.1345,
      "step": 27990
    },
    {
      "epoch": 204.37956204379563,
      "grad_norm": 13.951033592224121,
      "learning_rate": 3.978102189781022e-05,
      "loss": 0.9823,
      "step": 28000
    },
    {
      "epoch": 204.45255474452554,
      "grad_norm": 6.160393714904785,
      "learning_rate": 3.977737226277373e-05,
      "loss": 1.2481,
      "step": 28010
    },
    {
      "epoch": 204.52554744525548,
      "grad_norm": 2.1951091289520264,
      "learning_rate": 3.9773722627737224e-05,
      "loss": 0.5252,
      "step": 28020
    },
    {
      "epoch": 204.5985401459854,
      "grad_norm": 7.113212585449219,
      "learning_rate": 3.977007299270073e-05,
      "loss": 0.9617,
      "step": 28030
    },
    {
      "epoch": 204.67153284671534,
      "grad_norm": 11.382425308227539,
      "learning_rate": 3.976642335766423e-05,
      "loss": 1.2136,
      "step": 28040
    },
    {
      "epoch": 204.74452554744525,
      "grad_norm": 10.142194747924805,
      "learning_rate": 3.976277372262774e-05,
      "loss": 1.0884,
      "step": 28050
    },
    {
      "epoch": 204.8175182481752,
      "grad_norm": 11.227090835571289,
      "learning_rate": 3.9759124087591246e-05,
      "loss": 1.176,
      "step": 28060
    },
    {
      "epoch": 204.8905109489051,
      "grad_norm": 15.773706436157227,
      "learning_rate": 3.9755474452554746e-05,
      "loss": 1.0871,
      "step": 28070
    },
    {
      "epoch": 204.96350364963504,
      "grad_norm": 2.072413921356201,
      "learning_rate": 3.9751824817518254e-05,
      "loss": 0.7069,
      "step": 28080
    },
    {
      "epoch": 205.03649635036496,
      "grad_norm": 1.496436357498169,
      "learning_rate": 3.9748175182481754e-05,
      "loss": 1.2093,
      "step": 28090
    },
    {
      "epoch": 205.1094890510949,
      "grad_norm": 4.383372783660889,
      "learning_rate": 3.9744525547445255e-05,
      "loss": 1.2131,
      "step": 28100
    },
    {
      "epoch": 205.1824817518248,
      "grad_norm": 6.143411636352539,
      "learning_rate": 3.974087591240876e-05,
      "loss": 0.9277,
      "step": 28110
    },
    {
      "epoch": 205.25547445255475,
      "grad_norm": 1.1094975471496582,
      "learning_rate": 3.973722627737226e-05,
      "loss": 1.2434,
      "step": 28120
    },
    {
      "epoch": 205.32846715328466,
      "grad_norm": 6.651000499725342,
      "learning_rate": 3.973357664233577e-05,
      "loss": 0.5817,
      "step": 28130
    },
    {
      "epoch": 205.4014598540146,
      "grad_norm": 0.5804933905601501,
      "learning_rate": 3.972992700729927e-05,
      "loss": 1.0037,
      "step": 28140
    },
    {
      "epoch": 205.47445255474452,
      "grad_norm": 0.7803956270217896,
      "learning_rate": 3.972627737226278e-05,
      "loss": 0.9755,
      "step": 28150
    },
    {
      "epoch": 205.54744525547446,
      "grad_norm": 9.870315551757812,
      "learning_rate": 3.972262773722628e-05,
      "loss": 1.0629,
      "step": 28160
    },
    {
      "epoch": 205.62043795620437,
      "grad_norm": 10.523553848266602,
      "learning_rate": 3.9718978102189785e-05,
      "loss": 1.2605,
      "step": 28170
    },
    {
      "epoch": 205.6934306569343,
      "grad_norm": 11.038818359375,
      "learning_rate": 3.9715328467153286e-05,
      "loss": 1.5459,
      "step": 28180
    },
    {
      "epoch": 205.76642335766422,
      "grad_norm": 12.186514854431152,
      "learning_rate": 3.9711678832116786e-05,
      "loss": 1.2056,
      "step": 28190
    },
    {
      "epoch": 205.83941605839416,
      "grad_norm": 6.785292625427246,
      "learning_rate": 3.9708029197080294e-05,
      "loss": 0.9298,
      "step": 28200
    },
    {
      "epoch": 205.91240875912408,
      "grad_norm": 14.96768856048584,
      "learning_rate": 3.97043795620438e-05,
      "loss": 0.6099,
      "step": 28210
    },
    {
      "epoch": 205.98540145985402,
      "grad_norm": 0.9289437532424927,
      "learning_rate": 3.97007299270073e-05,
      "loss": 1.2324,
      "step": 28220
    },
    {
      "epoch": 206.05839416058393,
      "grad_norm": 9.06054401397705,
      "learning_rate": 3.969708029197081e-05,
      "loss": 1.2271,
      "step": 28230
    },
    {
      "epoch": 206.13138686131387,
      "grad_norm": 10.145367622375488,
      "learning_rate": 3.969343065693431e-05,
      "loss": 0.9568,
      "step": 28240
    },
    {
      "epoch": 206.20437956204378,
      "grad_norm": 4.890730857849121,
      "learning_rate": 3.968978102189781e-05,
      "loss": 1.0543,
      "step": 28250
    },
    {
      "epoch": 206.27737226277372,
      "grad_norm": 9.399983406066895,
      "learning_rate": 3.968613138686132e-05,
      "loss": 0.8492,
      "step": 28260
    },
    {
      "epoch": 206.35036496350364,
      "grad_norm": 8.842301368713379,
      "learning_rate": 3.968248175182482e-05,
      "loss": 1.146,
      "step": 28270
    },
    {
      "epoch": 206.42335766423358,
      "grad_norm": 7.825766086578369,
      "learning_rate": 3.9678832116788325e-05,
      "loss": 1.0771,
      "step": 28280
    },
    {
      "epoch": 206.4963503649635,
      "grad_norm": 6.9861884117126465,
      "learning_rate": 3.9675182481751825e-05,
      "loss": 1.0332,
      "step": 28290
    },
    {
      "epoch": 206.56934306569343,
      "grad_norm": 9.583135604858398,
      "learning_rate": 3.967153284671533e-05,
      "loss": 1.1419,
      "step": 28300
    },
    {
      "epoch": 206.64233576642334,
      "grad_norm": 7.836287975311279,
      "learning_rate": 3.966788321167883e-05,
      "loss": 1.0835,
      "step": 28310
    },
    {
      "epoch": 206.71532846715328,
      "grad_norm": 0.8681437373161316,
      "learning_rate": 3.966423357664234e-05,
      "loss": 0.8209,
      "step": 28320
    },
    {
      "epoch": 206.78832116788323,
      "grad_norm": 7.415846824645996,
      "learning_rate": 3.966058394160584e-05,
      "loss": 1.0652,
      "step": 28330
    },
    {
      "epoch": 206.86131386861314,
      "grad_norm": 5.227663993835449,
      "learning_rate": 3.965693430656934e-05,
      "loss": 0.9541,
      "step": 28340
    },
    {
      "epoch": 206.93430656934308,
      "grad_norm": 10.389774322509766,
      "learning_rate": 3.965328467153285e-05,
      "loss": 0.8938,
      "step": 28350
    },
    {
      "epoch": 207.007299270073,
      "grad_norm": 1.6865004301071167,
      "learning_rate": 3.964963503649635e-05,
      "loss": 0.7946,
      "step": 28360
    },
    {
      "epoch": 207.08029197080293,
      "grad_norm": 8.084211349487305,
      "learning_rate": 3.9645985401459856e-05,
      "loss": 1.0654,
      "step": 28370
    },
    {
      "epoch": 207.15328467153284,
      "grad_norm": 10.535300254821777,
      "learning_rate": 3.9642335766423363e-05,
      "loss": 0.8794,
      "step": 28380
    },
    {
      "epoch": 207.22627737226279,
      "grad_norm": 12.810361862182617,
      "learning_rate": 3.9638686131386864e-05,
      "loss": 1.4831,
      "step": 28390
    },
    {
      "epoch": 207.2992700729927,
      "grad_norm": 7.739424228668213,
      "learning_rate": 3.963503649635037e-05,
      "loss": 0.7402,
      "step": 28400
    },
    {
      "epoch": 207.37226277372264,
      "grad_norm": 12.092601776123047,
      "learning_rate": 3.963138686131387e-05,
      "loss": 1.4999,
      "step": 28410
    },
    {
      "epoch": 207.44525547445255,
      "grad_norm": 5.279628753662109,
      "learning_rate": 3.962773722627737e-05,
      "loss": 1.1268,
      "step": 28420
    },
    {
      "epoch": 207.5182481751825,
      "grad_norm": 12.042491912841797,
      "learning_rate": 3.962408759124088e-05,
      "loss": 0.924,
      "step": 28430
    },
    {
      "epoch": 207.5912408759124,
      "grad_norm": 7.812570095062256,
      "learning_rate": 3.962043795620438e-05,
      "loss": 1.2942,
      "step": 28440
    },
    {
      "epoch": 207.66423357664235,
      "grad_norm": 3.1990625858306885,
      "learning_rate": 3.961678832116789e-05,
      "loss": 0.6731,
      "step": 28450
    },
    {
      "epoch": 207.73722627737226,
      "grad_norm": 14.075053215026855,
      "learning_rate": 3.961313868613139e-05,
      "loss": 1.1419,
      "step": 28460
    },
    {
      "epoch": 207.8102189781022,
      "grad_norm": 14.8776216506958,
      "learning_rate": 3.9609489051094895e-05,
      "loss": 1.2174,
      "step": 28470
    },
    {
      "epoch": 207.8832116788321,
      "grad_norm": 7.775969505310059,
      "learning_rate": 3.9605839416058395e-05,
      "loss": 0.8364,
      "step": 28480
    },
    {
      "epoch": 207.95620437956205,
      "grad_norm": 1.7173097133636475,
      "learning_rate": 3.9602189781021896e-05,
      "loss": 0.8093,
      "step": 28490
    },
    {
      "epoch": 208.02919708029196,
      "grad_norm": 11.672113418579102,
      "learning_rate": 3.95985401459854e-05,
      "loss": 0.8917,
      "step": 28500
    },
    {
      "epoch": 208.1021897810219,
      "grad_norm": 5.992690563201904,
      "learning_rate": 3.9594890510948904e-05,
      "loss": 1.3677,
      "step": 28510
    },
    {
      "epoch": 208.17518248175182,
      "grad_norm": 9.335545539855957,
      "learning_rate": 3.959124087591241e-05,
      "loss": 0.9385,
      "step": 28520
    },
    {
      "epoch": 208.24817518248176,
      "grad_norm": 9.465346336364746,
      "learning_rate": 3.958759124087592e-05,
      "loss": 0.8938,
      "step": 28530
    },
    {
      "epoch": 208.32116788321167,
      "grad_norm": 6.529771327972412,
      "learning_rate": 3.958394160583942e-05,
      "loss": 1.0228,
      "step": 28540
    },
    {
      "epoch": 208.3941605839416,
      "grad_norm": 9.81042766571045,
      "learning_rate": 3.9580291970802926e-05,
      "loss": 0.836,
      "step": 28550
    },
    {
      "epoch": 208.46715328467153,
      "grad_norm": 9.749954223632812,
      "learning_rate": 3.957664233576642e-05,
      "loss": 0.7615,
      "step": 28560
    },
    {
      "epoch": 208.54014598540147,
      "grad_norm": 14.182703971862793,
      "learning_rate": 3.957299270072993e-05,
      "loss": 1.2631,
      "step": 28570
    },
    {
      "epoch": 208.61313868613138,
      "grad_norm": 7.761545181274414,
      "learning_rate": 3.9569343065693434e-05,
      "loss": 1.1291,
      "step": 28580
    },
    {
      "epoch": 208.68613138686132,
      "grad_norm": 12.088006973266602,
      "learning_rate": 3.9565693430656935e-05,
      "loss": 0.6481,
      "step": 28590
    },
    {
      "epoch": 208.75912408759123,
      "grad_norm": 5.553859710693359,
      "learning_rate": 3.956204379562044e-05,
      "loss": 0.806,
      "step": 28600
    },
    {
      "epoch": 208.83211678832117,
      "grad_norm": 9.13768482208252,
      "learning_rate": 3.955839416058394e-05,
      "loss": 1.2242,
      "step": 28610
    },
    {
      "epoch": 208.90510948905109,
      "grad_norm": 12.82821273803711,
      "learning_rate": 3.955474452554745e-05,
      "loss": 1.1217,
      "step": 28620
    },
    {
      "epoch": 208.97810218978103,
      "grad_norm": 13.149258613586426,
      "learning_rate": 3.955109489051095e-05,
      "loss": 1.1278,
      "step": 28630
    },
    {
      "epoch": 209.05109489051094,
      "grad_norm": 7.345319747924805,
      "learning_rate": 3.954744525547445e-05,
      "loss": 1.3468,
      "step": 28640
    },
    {
      "epoch": 209.12408759124088,
      "grad_norm": 15.111007690429688,
      "learning_rate": 3.954379562043796e-05,
      "loss": 1.2649,
      "step": 28650
    },
    {
      "epoch": 209.1970802919708,
      "grad_norm": 4.272373676300049,
      "learning_rate": 3.954014598540146e-05,
      "loss": 1.0682,
      "step": 28660
    },
    {
      "epoch": 209.27007299270073,
      "grad_norm": 11.513618469238281,
      "learning_rate": 3.9536496350364966e-05,
      "loss": 1.1772,
      "step": 28670
    },
    {
      "epoch": 209.34306569343065,
      "grad_norm": 1.2618571519851685,
      "learning_rate": 3.953284671532847e-05,
      "loss": 1.0057,
      "step": 28680
    },
    {
      "epoch": 209.4160583941606,
      "grad_norm": 7.5162129402160645,
      "learning_rate": 3.9529197080291974e-05,
      "loss": 0.7469,
      "step": 28690
    },
    {
      "epoch": 209.4890510948905,
      "grad_norm": 6.470959186553955,
      "learning_rate": 3.952554744525548e-05,
      "loss": 1.4329,
      "step": 28700
    },
    {
      "epoch": 209.56204379562044,
      "grad_norm": 11.853144645690918,
      "learning_rate": 3.9521897810218975e-05,
      "loss": 1.3595,
      "step": 28710
    },
    {
      "epoch": 209.63503649635035,
      "grad_norm": 5.347021579742432,
      "learning_rate": 3.951824817518248e-05,
      "loss": 0.8015,
      "step": 28720
    },
    {
      "epoch": 209.7080291970803,
      "grad_norm": 6.266706943511963,
      "learning_rate": 3.951459854014599e-05,
      "loss": 0.7513,
      "step": 28730
    },
    {
      "epoch": 209.7810218978102,
      "grad_norm": 9.553845405578613,
      "learning_rate": 3.951094890510949e-05,
      "loss": 1.2337,
      "step": 28740
    },
    {
      "epoch": 209.85401459854015,
      "grad_norm": 10.604093551635742,
      "learning_rate": 3.9507299270073e-05,
      "loss": 0.9121,
      "step": 28750
    },
    {
      "epoch": 209.92700729927006,
      "grad_norm": 6.353921413421631,
      "learning_rate": 3.95036496350365e-05,
      "loss": 0.6866,
      "step": 28760
    },
    {
      "epoch": 210.0,
      "grad_norm": 3.4633371829986572,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.804,
      "step": 28770
    },
    {
      "epoch": 210.07299270072994,
      "grad_norm": 2.9396450519561768,
      "learning_rate": 3.9496350364963505e-05,
      "loss": 0.8092,
      "step": 28780
    },
    {
      "epoch": 210.14598540145985,
      "grad_norm": 1.3280922174453735,
      "learning_rate": 3.9492700729927006e-05,
      "loss": 1.1584,
      "step": 28790
    },
    {
      "epoch": 210.2189781021898,
      "grad_norm": 13.163546562194824,
      "learning_rate": 3.948905109489051e-05,
      "loss": 1.0137,
      "step": 28800
    },
    {
      "epoch": 210.2919708029197,
      "grad_norm": 9.363343238830566,
      "learning_rate": 3.9485401459854013e-05,
      "loss": 0.9253,
      "step": 28810
    },
    {
      "epoch": 210.36496350364965,
      "grad_norm": 10.167718887329102,
      "learning_rate": 3.948175182481752e-05,
      "loss": 1.0241,
      "step": 28820
    },
    {
      "epoch": 210.43795620437956,
      "grad_norm": 16.42410659790039,
      "learning_rate": 3.947810218978102e-05,
      "loss": 1.5311,
      "step": 28830
    },
    {
      "epoch": 210.5109489051095,
      "grad_norm": 11.234258651733398,
      "learning_rate": 3.947445255474453e-05,
      "loss": 0.7598,
      "step": 28840
    },
    {
      "epoch": 210.5839416058394,
      "grad_norm": 2.1068811416625977,
      "learning_rate": 3.9470802919708036e-05,
      "loss": 1.2758,
      "step": 28850
    },
    {
      "epoch": 210.65693430656935,
      "grad_norm": 10.309624671936035,
      "learning_rate": 3.9467153284671536e-05,
      "loss": 1.1265,
      "step": 28860
    },
    {
      "epoch": 210.72992700729927,
      "grad_norm": 14.043591499328613,
      "learning_rate": 3.946350364963504e-05,
      "loss": 0.9769,
      "step": 28870
    },
    {
      "epoch": 210.8029197080292,
      "grad_norm": 18.260507583618164,
      "learning_rate": 3.9459854014598544e-05,
      "loss": 1.0993,
      "step": 28880
    },
    {
      "epoch": 210.87591240875912,
      "grad_norm": 6.064921855926514,
      "learning_rate": 3.9456204379562044e-05,
      "loss": 0.5704,
      "step": 28890
    },
    {
      "epoch": 210.94890510948906,
      "grad_norm": 10.427383422851562,
      "learning_rate": 3.945255474452555e-05,
      "loss": 1.0349,
      "step": 28900
    },
    {
      "epoch": 211.02189781021897,
      "grad_norm": 7.128303050994873,
      "learning_rate": 3.944890510948905e-05,
      "loss": 1.3246,
      "step": 28910
    },
    {
      "epoch": 211.09489051094891,
      "grad_norm": 9.671611785888672,
      "learning_rate": 3.944525547445256e-05,
      "loss": 0.6325,
      "step": 28920
    },
    {
      "epoch": 211.16788321167883,
      "grad_norm": 14.574846267700195,
      "learning_rate": 3.944160583941606e-05,
      "loss": 1.1214,
      "step": 28930
    },
    {
      "epoch": 211.24087591240877,
      "grad_norm": 7.53560733795166,
      "learning_rate": 3.943795620437956e-05,
      "loss": 1.3105,
      "step": 28940
    },
    {
      "epoch": 211.31386861313868,
      "grad_norm": 7.57446813583374,
      "learning_rate": 3.943430656934307e-05,
      "loss": 1.1578,
      "step": 28950
    },
    {
      "epoch": 211.38686131386862,
      "grad_norm": 8.35260009765625,
      "learning_rate": 3.943065693430657e-05,
      "loss": 0.5026,
      "step": 28960
    },
    {
      "epoch": 211.45985401459853,
      "grad_norm": 13.469294548034668,
      "learning_rate": 3.9427007299270076e-05,
      "loss": 0.8449,
      "step": 28970
    },
    {
      "epoch": 211.53284671532847,
      "grad_norm": 10.113871574401855,
      "learning_rate": 3.9423357664233576e-05,
      "loss": 1.3018,
      "step": 28980
    },
    {
      "epoch": 211.6058394160584,
      "grad_norm": 1.6960550546646118,
      "learning_rate": 3.941970802919708e-05,
      "loss": 0.8133,
      "step": 28990
    },
    {
      "epoch": 211.67883211678833,
      "grad_norm": 5.533512115478516,
      "learning_rate": 3.941605839416059e-05,
      "loss": 1.0391,
      "step": 29000
    },
    {
      "epoch": 211.75182481751824,
      "grad_norm": 11.194537162780762,
      "learning_rate": 3.941240875912409e-05,
      "loss": 1.0682,
      "step": 29010
    },
    {
      "epoch": 211.82481751824818,
      "grad_norm": 8.754744529724121,
      "learning_rate": 3.940875912408759e-05,
      "loss": 1.3869,
      "step": 29020
    },
    {
      "epoch": 211.8978102189781,
      "grad_norm": 1.4349815845489502,
      "learning_rate": 3.940510948905109e-05,
      "loss": 0.8051,
      "step": 29030
    },
    {
      "epoch": 211.97080291970804,
      "grad_norm": 10.552725791931152,
      "learning_rate": 3.94014598540146e-05,
      "loss": 0.8926,
      "step": 29040
    },
    {
      "epoch": 212.04379562043795,
      "grad_norm": 6.072731971740723,
      "learning_rate": 3.9397810218978107e-05,
      "loss": 1.1289,
      "step": 29050
    },
    {
      "epoch": 212.1167883211679,
      "grad_norm": 1.2333234548568726,
      "learning_rate": 3.939416058394161e-05,
      "loss": 0.9246,
      "step": 29060
    },
    {
      "epoch": 212.1897810218978,
      "grad_norm": 5.341067790985107,
      "learning_rate": 3.9390510948905114e-05,
      "loss": 0.6993,
      "step": 29070
    },
    {
      "epoch": 212.26277372262774,
      "grad_norm": 10.659151077270508,
      "learning_rate": 3.9386861313868615e-05,
      "loss": 1.2766,
      "step": 29080
    },
    {
      "epoch": 212.33576642335765,
      "grad_norm": 7.172031879425049,
      "learning_rate": 3.9383211678832115e-05,
      "loss": 0.8207,
      "step": 29090
    },
    {
      "epoch": 212.4087591240876,
      "grad_norm": 0.589878261089325,
      "learning_rate": 3.937956204379562e-05,
      "loss": 0.7166,
      "step": 29100
    },
    {
      "epoch": 212.4817518248175,
      "grad_norm": 13.322250366210938,
      "learning_rate": 3.937591240875912e-05,
      "loss": 1.1086,
      "step": 29110
    },
    {
      "epoch": 212.55474452554745,
      "grad_norm": 6.39725923538208,
      "learning_rate": 3.937226277372263e-05,
      "loss": 0.9535,
      "step": 29120
    },
    {
      "epoch": 212.62773722627736,
      "grad_norm": 5.575498580932617,
      "learning_rate": 3.936861313868613e-05,
      "loss": 1.3626,
      "step": 29130
    },
    {
      "epoch": 212.7007299270073,
      "grad_norm": 13.918225288391113,
      "learning_rate": 3.936496350364964e-05,
      "loss": 1.2184,
      "step": 29140
    },
    {
      "epoch": 212.77372262773721,
      "grad_norm": 7.043027400970459,
      "learning_rate": 3.9361313868613145e-05,
      "loss": 0.9186,
      "step": 29150
    },
    {
      "epoch": 212.84671532846716,
      "grad_norm": 8.902764320373535,
      "learning_rate": 3.9357664233576646e-05,
      "loss": 1.3557,
      "step": 29160
    },
    {
      "epoch": 212.91970802919707,
      "grad_norm": 6.3859381675720215,
      "learning_rate": 3.9354014598540146e-05,
      "loss": 1.2467,
      "step": 29170
    },
    {
      "epoch": 212.992700729927,
      "grad_norm": 7.873430252075195,
      "learning_rate": 3.935036496350365e-05,
      "loss": 0.99,
      "step": 29180
    },
    {
      "epoch": 213.06569343065692,
      "grad_norm": 11.14430046081543,
      "learning_rate": 3.9346715328467154e-05,
      "loss": 0.9409,
      "step": 29190
    },
    {
      "epoch": 213.13868613138686,
      "grad_norm": 14.420984268188477,
      "learning_rate": 3.934306569343066e-05,
      "loss": 1.3173,
      "step": 29200
    },
    {
      "epoch": 213.21167883211677,
      "grad_norm": 13.788272857666016,
      "learning_rate": 3.933941605839416e-05,
      "loss": 1.3991,
      "step": 29210
    },
    {
      "epoch": 213.28467153284672,
      "grad_norm": 13.917632102966309,
      "learning_rate": 3.933576642335767e-05,
      "loss": 0.9044,
      "step": 29220
    },
    {
      "epoch": 213.35766423357666,
      "grad_norm": 0.8452343940734863,
      "learning_rate": 3.933211678832117e-05,
      "loss": 0.6192,
      "step": 29230
    },
    {
      "epoch": 213.43065693430657,
      "grad_norm": 6.150034427642822,
      "learning_rate": 3.932846715328468e-05,
      "loss": 0.908,
      "step": 29240
    },
    {
      "epoch": 213.5036496350365,
      "grad_norm": 9.604662895202637,
      "learning_rate": 3.932481751824818e-05,
      "loss": 1.0164,
      "step": 29250
    },
    {
      "epoch": 213.57664233576642,
      "grad_norm": 14.929967880249023,
      "learning_rate": 3.932116788321168e-05,
      "loss": 0.8905,
      "step": 29260
    },
    {
      "epoch": 213.64963503649636,
      "grad_norm": 8.762874603271484,
      "learning_rate": 3.9317518248175185e-05,
      "loss": 1.0494,
      "step": 29270
    },
    {
      "epoch": 213.72262773722628,
      "grad_norm": 6.960531711578369,
      "learning_rate": 3.9313868613138686e-05,
      "loss": 0.9802,
      "step": 29280
    },
    {
      "epoch": 213.79562043795622,
      "grad_norm": 0.512199878692627,
      "learning_rate": 3.931021897810219e-05,
      "loss": 1.158,
      "step": 29290
    },
    {
      "epoch": 213.86861313868613,
      "grad_norm": 11.263583183288574,
      "learning_rate": 3.9306569343065693e-05,
      "loss": 1.2364,
      "step": 29300
    },
    {
      "epoch": 213.94160583941607,
      "grad_norm": 8.063952445983887,
      "learning_rate": 3.93029197080292e-05,
      "loss": 0.9965,
      "step": 29310
    },
    {
      "epoch": 214.01459854014598,
      "grad_norm": 0.7211459875106812,
      "learning_rate": 3.92992700729927e-05,
      "loss": 0.8456,
      "step": 29320
    },
    {
      "epoch": 214.08759124087592,
      "grad_norm": 15.753315925598145,
      "learning_rate": 3.92956204379562e-05,
      "loss": 1.5948,
      "step": 29330
    },
    {
      "epoch": 214.16058394160584,
      "grad_norm": 4.8334174156188965,
      "learning_rate": 3.929197080291971e-05,
      "loss": 0.9018,
      "step": 29340
    },
    {
      "epoch": 214.23357664233578,
      "grad_norm": 0.9982694387435913,
      "learning_rate": 3.9288321167883216e-05,
      "loss": 0.8495,
      "step": 29350
    },
    {
      "epoch": 214.3065693430657,
      "grad_norm": 9.890713691711426,
      "learning_rate": 3.928467153284672e-05,
      "loss": 1.0876,
      "step": 29360
    },
    {
      "epoch": 214.37956204379563,
      "grad_norm": 13.479253768920898,
      "learning_rate": 3.9281021897810224e-05,
      "loss": 0.7004,
      "step": 29370
    },
    {
      "epoch": 214.45255474452554,
      "grad_norm": 15.352836608886719,
      "learning_rate": 3.9277372262773725e-05,
      "loss": 0.8216,
      "step": 29380
    },
    {
      "epoch": 214.52554744525548,
      "grad_norm": 11.953685760498047,
      "learning_rate": 3.927372262773723e-05,
      "loss": 1.1618,
      "step": 29390
    },
    {
      "epoch": 214.5985401459854,
      "grad_norm": 12.539539337158203,
      "learning_rate": 3.927007299270073e-05,
      "loss": 0.8087,
      "step": 29400
    },
    {
      "epoch": 214.67153284671534,
      "grad_norm": 4.44620943069458,
      "learning_rate": 3.926642335766423e-05,
      "loss": 0.8747,
      "step": 29410
    },
    {
      "epoch": 214.74452554744525,
      "grad_norm": 12.983745574951172,
      "learning_rate": 3.926277372262774e-05,
      "loss": 1.6963,
      "step": 29420
    },
    {
      "epoch": 214.8175182481752,
      "grad_norm": 7.751906394958496,
      "learning_rate": 3.925912408759124e-05,
      "loss": 1.1887,
      "step": 29430
    },
    {
      "epoch": 214.8905109489051,
      "grad_norm": 13.621399879455566,
      "learning_rate": 3.925547445255475e-05,
      "loss": 1.1437,
      "step": 29440
    },
    {
      "epoch": 214.96350364963504,
      "grad_norm": 11.218783378601074,
      "learning_rate": 3.925182481751825e-05,
      "loss": 1.0787,
      "step": 29450
    },
    {
      "epoch": 215.03649635036496,
      "grad_norm": 12.58516788482666,
      "learning_rate": 3.9248175182481756e-05,
      "loss": 0.9882,
      "step": 29460
    },
    {
      "epoch": 215.1094890510949,
      "grad_norm": 1.129713773727417,
      "learning_rate": 3.924452554744526e-05,
      "loss": 1.1143,
      "step": 29470
    },
    {
      "epoch": 215.1824817518248,
      "grad_norm": 10.170361518859863,
      "learning_rate": 3.9240875912408757e-05,
      "loss": 1.112,
      "step": 29480
    },
    {
      "epoch": 215.25547445255475,
      "grad_norm": 0.7267736196517944,
      "learning_rate": 3.9237226277372264e-05,
      "loss": 0.7391,
      "step": 29490
    },
    {
      "epoch": 215.32846715328466,
      "grad_norm": 18.135520935058594,
      "learning_rate": 3.9233576642335764e-05,
      "loss": 1.0148,
      "step": 29500
    },
    {
      "epoch": 215.4014598540146,
      "grad_norm": 0.8620611429214478,
      "learning_rate": 3.922992700729927e-05,
      "loss": 0.997,
      "step": 29510
    },
    {
      "epoch": 215.47445255474452,
      "grad_norm": 6.666980743408203,
      "learning_rate": 3.922627737226278e-05,
      "loss": 1.2343,
      "step": 29520
    },
    {
      "epoch": 215.54744525547446,
      "grad_norm": 6.204648494720459,
      "learning_rate": 3.922262773722628e-05,
      "loss": 0.8739,
      "step": 29530
    },
    {
      "epoch": 215.62043795620437,
      "grad_norm": 12.830927848815918,
      "learning_rate": 3.921897810218979e-05,
      "loss": 1.0712,
      "step": 29540
    },
    {
      "epoch": 215.6934306569343,
      "grad_norm": 8.987960815429688,
      "learning_rate": 3.921532846715329e-05,
      "loss": 0.9229,
      "step": 29550
    },
    {
      "epoch": 215.76642335766422,
      "grad_norm": 12.843037605285645,
      "learning_rate": 3.921167883211679e-05,
      "loss": 1.1742,
      "step": 29560
    },
    {
      "epoch": 215.83941605839416,
      "grad_norm": 10.236949920654297,
      "learning_rate": 3.9208029197080295e-05,
      "loss": 0.9675,
      "step": 29570
    },
    {
      "epoch": 215.91240875912408,
      "grad_norm": 6.654982566833496,
      "learning_rate": 3.9204379562043795e-05,
      "loss": 1.0949,
      "step": 29580
    },
    {
      "epoch": 215.98540145985402,
      "grad_norm": 5.26638650894165,
      "learning_rate": 3.92007299270073e-05,
      "loss": 0.9499,
      "step": 29590
    },
    {
      "epoch": 216.05839416058393,
      "grad_norm": 6.412834644317627,
      "learning_rate": 3.91970802919708e-05,
      "loss": 1.1253,
      "step": 29600
    },
    {
      "epoch": 216.13138686131387,
      "grad_norm": 8.82624340057373,
      "learning_rate": 3.919343065693431e-05,
      "loss": 1.0855,
      "step": 29610
    },
    {
      "epoch": 216.20437956204378,
      "grad_norm": 0.9961880445480347,
      "learning_rate": 3.918978102189782e-05,
      "loss": 0.7596,
      "step": 29620
    },
    {
      "epoch": 216.27737226277372,
      "grad_norm": 2.0422608852386475,
      "learning_rate": 3.918613138686131e-05,
      "loss": 1.0755,
      "step": 29630
    },
    {
      "epoch": 216.35036496350364,
      "grad_norm": 1.205444574356079,
      "learning_rate": 3.918248175182482e-05,
      "loss": 0.6651,
      "step": 29640
    },
    {
      "epoch": 216.42335766423358,
      "grad_norm": 11.5007963180542,
      "learning_rate": 3.917883211678832e-05,
      "loss": 1.4995,
      "step": 29650
    },
    {
      "epoch": 216.4963503649635,
      "grad_norm": 0.7814586162567139,
      "learning_rate": 3.9175182481751826e-05,
      "loss": 1.3548,
      "step": 29660
    },
    {
      "epoch": 216.56934306569343,
      "grad_norm": 6.320345401763916,
      "learning_rate": 3.9171532846715334e-05,
      "loss": 1.0032,
      "step": 29670
    },
    {
      "epoch": 216.64233576642334,
      "grad_norm": 6.432851791381836,
      "learning_rate": 3.9167883211678834e-05,
      "loss": 0.7117,
      "step": 29680
    },
    {
      "epoch": 216.71532846715328,
      "grad_norm": 9.690475463867188,
      "learning_rate": 3.916423357664234e-05,
      "loss": 0.9498,
      "step": 29690
    },
    {
      "epoch": 216.78832116788323,
      "grad_norm": 12.504847526550293,
      "learning_rate": 3.916058394160584e-05,
      "loss": 0.8758,
      "step": 29700
    },
    {
      "epoch": 216.86131386861314,
      "grad_norm": 6.074024677276611,
      "learning_rate": 3.915693430656934e-05,
      "loss": 0.7763,
      "step": 29710
    },
    {
      "epoch": 216.93430656934308,
      "grad_norm": 8.434093475341797,
      "learning_rate": 3.915328467153285e-05,
      "loss": 1.143,
      "step": 29720
    },
    {
      "epoch": 217.007299270073,
      "grad_norm": 5.705660820007324,
      "learning_rate": 3.914963503649635e-05,
      "loss": 1.1066,
      "step": 29730
    },
    {
      "epoch": 217.08029197080293,
      "grad_norm": 1.301464319229126,
      "learning_rate": 3.914598540145986e-05,
      "loss": 0.8883,
      "step": 29740
    },
    {
      "epoch": 217.15328467153284,
      "grad_norm": 1.153749704360962,
      "learning_rate": 3.914233576642336e-05,
      "loss": 0.8449,
      "step": 29750
    },
    {
      "epoch": 217.22627737226279,
      "grad_norm": 10.477709770202637,
      "learning_rate": 3.9138686131386865e-05,
      "loss": 1.0707,
      "step": 29760
    },
    {
      "epoch": 217.2992700729927,
      "grad_norm": 6.181640625,
      "learning_rate": 3.9135036496350366e-05,
      "loss": 1.0047,
      "step": 29770
    },
    {
      "epoch": 217.37226277372264,
      "grad_norm": 11.491801261901855,
      "learning_rate": 3.9131386861313866e-05,
      "loss": 1.3438,
      "step": 29780
    },
    {
      "epoch": 217.44525547445255,
      "grad_norm": 15.521112442016602,
      "learning_rate": 3.9127737226277374e-05,
      "loss": 0.8648,
      "step": 29790
    },
    {
      "epoch": 217.5182481751825,
      "grad_norm": 14.700401306152344,
      "learning_rate": 3.9124087591240874e-05,
      "loss": 1.2081,
      "step": 29800
    },
    {
      "epoch": 217.5912408759124,
      "grad_norm": 13.399259567260742,
      "learning_rate": 3.912043795620438e-05,
      "loss": 0.7783,
      "step": 29810
    },
    {
      "epoch": 217.66423357664235,
      "grad_norm": 1.033563494682312,
      "learning_rate": 3.911678832116789e-05,
      "loss": 0.9773,
      "step": 29820
    },
    {
      "epoch": 217.73722627737226,
      "grad_norm": 0.9087743163108826,
      "learning_rate": 3.911313868613139e-05,
      "loss": 1.1879,
      "step": 29830
    },
    {
      "epoch": 217.8102189781022,
      "grad_norm": 9.524660110473633,
      "learning_rate": 3.9109489051094896e-05,
      "loss": 0.9989,
      "step": 29840
    },
    {
      "epoch": 217.8832116788321,
      "grad_norm": 8.5736665725708,
      "learning_rate": 3.91058394160584e-05,
      "loss": 0.7584,
      "step": 29850
    },
    {
      "epoch": 217.95620437956205,
      "grad_norm": 6.9252705574035645,
      "learning_rate": 3.91021897810219e-05,
      "loss": 0.7672,
      "step": 29860
    },
    {
      "epoch": 218.02919708029196,
      "grad_norm": 10.862317085266113,
      "learning_rate": 3.9098540145985405e-05,
      "loss": 1.5876,
      "step": 29870
    },
    {
      "epoch": 218.1021897810219,
      "grad_norm": 14.683027267456055,
      "learning_rate": 3.9094890510948905e-05,
      "loss": 1.4722,
      "step": 29880
    },
    {
      "epoch": 218.17518248175182,
      "grad_norm": 6.727493762969971,
      "learning_rate": 3.909124087591241e-05,
      "loss": 1.5306,
      "step": 29890
    },
    {
      "epoch": 218.24817518248176,
      "grad_norm": 7.381871223449707,
      "learning_rate": 3.908759124087591e-05,
      "loss": 0.7751,
      "step": 29900
    },
    {
      "epoch": 218.32116788321167,
      "grad_norm": 7.198759078979492,
      "learning_rate": 3.908394160583942e-05,
      "loss": 1.1078,
      "step": 29910
    },
    {
      "epoch": 218.3941605839416,
      "grad_norm": 7.126803398132324,
      "learning_rate": 3.908029197080292e-05,
      "loss": 0.9299,
      "step": 29920
    },
    {
      "epoch": 218.46715328467153,
      "grad_norm": 11.185190200805664,
      "learning_rate": 3.907664233576643e-05,
      "loss": 0.6267,
      "step": 29930
    },
    {
      "epoch": 218.54014598540147,
      "grad_norm": 13.433547973632812,
      "learning_rate": 3.907299270072993e-05,
      "loss": 1.0519,
      "step": 29940
    },
    {
      "epoch": 218.61313868613138,
      "grad_norm": 7.866268157958984,
      "learning_rate": 3.906934306569343e-05,
      "loss": 0.7239,
      "step": 29950
    },
    {
      "epoch": 218.68613138686132,
      "grad_norm": 7.183019161224365,
      "learning_rate": 3.9065693430656936e-05,
      "loss": 0.7855,
      "step": 29960
    },
    {
      "epoch": 218.75912408759123,
      "grad_norm": 6.2782769203186035,
      "learning_rate": 3.906204379562044e-05,
      "loss": 0.6795,
      "step": 29970
    },
    {
      "epoch": 218.83211678832117,
      "grad_norm": 14.050276756286621,
      "learning_rate": 3.9058394160583944e-05,
      "loss": 1.042,
      "step": 29980
    },
    {
      "epoch": 218.90510948905109,
      "grad_norm": 13.162118911743164,
      "learning_rate": 3.905474452554745e-05,
      "loss": 0.982,
      "step": 29990
    },
    {
      "epoch": 218.97810218978103,
      "grad_norm": 12.629400253295898,
      "learning_rate": 3.905109489051095e-05,
      "loss": 1.3671,
      "step": 30000
    },
    {
      "epoch": 219.05109489051094,
      "grad_norm": 8.301851272583008,
      "learning_rate": 3.904744525547445e-05,
      "loss": 1.2412,
      "step": 30010
    },
    {
      "epoch": 219.12408759124088,
      "grad_norm": 14.21432113647461,
      "learning_rate": 3.904379562043796e-05,
      "loss": 1.4692,
      "step": 30020
    },
    {
      "epoch": 219.1970802919708,
      "grad_norm": 1.1133631467819214,
      "learning_rate": 3.904014598540146e-05,
      "loss": 0.9067,
      "step": 30030
    },
    {
      "epoch": 219.27007299270073,
      "grad_norm": 12.787132263183594,
      "learning_rate": 3.903649635036497e-05,
      "loss": 0.6452,
      "step": 30040
    },
    {
      "epoch": 219.34306569343065,
      "grad_norm": 5.655392169952393,
      "learning_rate": 3.903284671532847e-05,
      "loss": 0.6245,
      "step": 30050
    },
    {
      "epoch": 219.4160583941606,
      "grad_norm": 7.138301849365234,
      "learning_rate": 3.9029197080291975e-05,
      "loss": 1.4614,
      "step": 30060
    },
    {
      "epoch": 219.4890510948905,
      "grad_norm": 6.772522449493408,
      "learning_rate": 3.9025547445255476e-05,
      "loss": 1.0237,
      "step": 30070
    },
    {
      "epoch": 219.56204379562044,
      "grad_norm": 5.691394805908203,
      "learning_rate": 3.902189781021898e-05,
      "loss": 0.9139,
      "step": 30080
    },
    {
      "epoch": 219.63503649635035,
      "grad_norm": 7.95517635345459,
      "learning_rate": 3.901824817518248e-05,
      "loss": 1.2663,
      "step": 30090
    },
    {
      "epoch": 219.7080291970803,
      "grad_norm": 4.259575843811035,
      "learning_rate": 3.9014598540145984e-05,
      "loss": 0.6107,
      "step": 30100
    },
    {
      "epoch": 219.7810218978102,
      "grad_norm": 13.110428810119629,
      "learning_rate": 3.901094890510949e-05,
      "loss": 1.2709,
      "step": 30110
    },
    {
      "epoch": 219.85401459854015,
      "grad_norm": 1.1282424926757812,
      "learning_rate": 3.900729927007299e-05,
      "loss": 0.7438,
      "step": 30120
    },
    {
      "epoch": 219.92700729927006,
      "grad_norm": 1.5881708860397339,
      "learning_rate": 3.90036496350365e-05,
      "loss": 1.0003,
      "step": 30130
    },
    {
      "epoch": 220.0,
      "grad_norm": 0.6788268685340881,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.9852,
      "step": 30140
    },
    {
      "epoch": 220.07299270072994,
      "grad_norm": 8.138225555419922,
      "learning_rate": 3.8996350364963507e-05,
      "loss": 1.1028,
      "step": 30150
    },
    {
      "epoch": 220.14598540145985,
      "grad_norm": 6.773683547973633,
      "learning_rate": 3.899270072992701e-05,
      "loss": 1.2316,
      "step": 30160
    },
    {
      "epoch": 220.2189781021898,
      "grad_norm": 10.75016975402832,
      "learning_rate": 3.898905109489051e-05,
      "loss": 1.0977,
      "step": 30170
    },
    {
      "epoch": 220.2919708029197,
      "grad_norm": 9.876723289489746,
      "learning_rate": 3.8985401459854015e-05,
      "loss": 1.0158,
      "step": 30180
    },
    {
      "epoch": 220.36496350364965,
      "grad_norm": 9.09649658203125,
      "learning_rate": 3.898175182481752e-05,
      "loss": 0.6809,
      "step": 30190
    },
    {
      "epoch": 220.43795620437956,
      "grad_norm": 18.648889541625977,
      "learning_rate": 3.897810218978102e-05,
      "loss": 1.2483,
      "step": 30200
    },
    {
      "epoch": 220.5109489051095,
      "grad_norm": 6.434421539306641,
      "learning_rate": 3.897445255474453e-05,
      "loss": 1.3432,
      "step": 30210
    },
    {
      "epoch": 220.5839416058394,
      "grad_norm": 9.945091247558594,
      "learning_rate": 3.897080291970803e-05,
      "loss": 0.9425,
      "step": 30220
    },
    {
      "epoch": 220.65693430656935,
      "grad_norm": 0.9150399565696716,
      "learning_rate": 3.896715328467154e-05,
      "loss": 0.8071,
      "step": 30230
    },
    {
      "epoch": 220.72992700729927,
      "grad_norm": 12.588156700134277,
      "learning_rate": 3.896350364963504e-05,
      "loss": 1.0437,
      "step": 30240
    },
    {
      "epoch": 220.8029197080292,
      "grad_norm": 7.724477767944336,
      "learning_rate": 3.895985401459854e-05,
      "loss": 0.7838,
      "step": 30250
    },
    {
      "epoch": 220.87591240875912,
      "grad_norm": 0.8961383700370789,
      "learning_rate": 3.8956204379562046e-05,
      "loss": 1.053,
      "step": 30260
    },
    {
      "epoch": 220.94890510948906,
      "grad_norm": 1.793308973312378,
      "learning_rate": 3.8952554744525546e-05,
      "loss": 1.0043,
      "step": 30270
    },
    {
      "epoch": 221.02189781021897,
      "grad_norm": 11.47340202331543,
      "learning_rate": 3.8948905109489054e-05,
      "loss": 0.7358,
      "step": 30280
    },
    {
      "epoch": 221.09489051094891,
      "grad_norm": 14.147034645080566,
      "learning_rate": 3.894525547445256e-05,
      "loss": 1.1699,
      "step": 30290
    },
    {
      "epoch": 221.16788321167883,
      "grad_norm": 16.78630828857422,
      "learning_rate": 3.894160583941606e-05,
      "loss": 1.0483,
      "step": 30300
    },
    {
      "epoch": 221.24087591240877,
      "grad_norm": 10.198079109191895,
      "learning_rate": 3.893795620437957e-05,
      "loss": 0.7639,
      "step": 30310
    },
    {
      "epoch": 221.31386861313868,
      "grad_norm": 0.6565610766410828,
      "learning_rate": 3.893430656934306e-05,
      "loss": 1.0768,
      "step": 30320
    },
    {
      "epoch": 221.38686131386862,
      "grad_norm": 8.235809326171875,
      "learning_rate": 3.893065693430657e-05,
      "loss": 1.1348,
      "step": 30330
    },
    {
      "epoch": 221.45985401459853,
      "grad_norm": 6.486917972564697,
      "learning_rate": 3.892700729927008e-05,
      "loss": 1.0608,
      "step": 30340
    },
    {
      "epoch": 221.53284671532847,
      "grad_norm": 8.7066068649292,
      "learning_rate": 3.892335766423358e-05,
      "loss": 0.9059,
      "step": 30350
    },
    {
      "epoch": 221.6058394160584,
      "grad_norm": 9.625665664672852,
      "learning_rate": 3.8919708029197085e-05,
      "loss": 1.1296,
      "step": 30360
    },
    {
      "epoch": 221.67883211678833,
      "grad_norm": 0.726428747177124,
      "learning_rate": 3.8916058394160585e-05,
      "loss": 0.886,
      "step": 30370
    },
    {
      "epoch": 221.75182481751824,
      "grad_norm": 11.940632820129395,
      "learning_rate": 3.891240875912409e-05,
      "loss": 1.0495,
      "step": 30380
    },
    {
      "epoch": 221.82481751824818,
      "grad_norm": 6.424959659576416,
      "learning_rate": 3.890875912408759e-05,
      "loss": 0.7492,
      "step": 30390
    },
    {
      "epoch": 221.8978102189781,
      "grad_norm": 10.867072105407715,
      "learning_rate": 3.8905109489051093e-05,
      "loss": 1.0583,
      "step": 30400
    },
    {
      "epoch": 221.97080291970804,
      "grad_norm": 9.353291511535645,
      "learning_rate": 3.89014598540146e-05,
      "loss": 1.3667,
      "step": 30410
    },
    {
      "epoch": 222.04379562043795,
      "grad_norm": 15.890813827514648,
      "learning_rate": 3.88978102189781e-05,
      "loss": 0.8959,
      "step": 30420
    },
    {
      "epoch": 222.1167883211679,
      "grad_norm": 6.363070011138916,
      "learning_rate": 3.889416058394161e-05,
      "loss": 0.9438,
      "step": 30430
    },
    {
      "epoch": 222.1897810218978,
      "grad_norm": 0.9744788408279419,
      "learning_rate": 3.889051094890511e-05,
      "loss": 1.0373,
      "step": 30440
    },
    {
      "epoch": 222.26277372262774,
      "grad_norm": 11.69324016571045,
      "learning_rate": 3.8886861313868616e-05,
      "loss": 1.3714,
      "step": 30450
    },
    {
      "epoch": 222.33576642335765,
      "grad_norm": 14.2091703414917,
      "learning_rate": 3.8883211678832124e-05,
      "loss": 1.2079,
      "step": 30460
    },
    {
      "epoch": 222.4087591240876,
      "grad_norm": 7.294344902038574,
      "learning_rate": 3.887956204379562e-05,
      "loss": 0.9894,
      "step": 30470
    },
    {
      "epoch": 222.4817518248175,
      "grad_norm": 11.53700065612793,
      "learning_rate": 3.8875912408759125e-05,
      "loss": 0.5918,
      "step": 30480
    },
    {
      "epoch": 222.55474452554745,
      "grad_norm": 6.939404487609863,
      "learning_rate": 3.887226277372263e-05,
      "loss": 0.6607,
      "step": 30490
    },
    {
      "epoch": 222.62773722627736,
      "grad_norm": 1.1368759870529175,
      "learning_rate": 3.886861313868613e-05,
      "loss": 0.962,
      "step": 30500
    },
    {
      "epoch": 222.7007299270073,
      "grad_norm": 12.396568298339844,
      "learning_rate": 3.886496350364964e-05,
      "loss": 1.0166,
      "step": 30510
    },
    {
      "epoch": 222.77372262773721,
      "grad_norm": 12.614086151123047,
      "learning_rate": 3.886131386861314e-05,
      "loss": 1.4517,
      "step": 30520
    },
    {
      "epoch": 222.84671532846716,
      "grad_norm": 9.756759643554688,
      "learning_rate": 3.885766423357665e-05,
      "loss": 1.1387,
      "step": 30530
    },
    {
      "epoch": 222.91970802919707,
      "grad_norm": 12.588435173034668,
      "learning_rate": 3.885401459854015e-05,
      "loss": 1.0154,
      "step": 30540
    },
    {
      "epoch": 222.992700729927,
      "grad_norm": 7.588494777679443,
      "learning_rate": 3.885036496350365e-05,
      "loss": 1.0669,
      "step": 30550
    },
    {
      "epoch": 223.06569343065692,
      "grad_norm": 8.9548978805542,
      "learning_rate": 3.8846715328467156e-05,
      "loss": 0.7831,
      "step": 30560
    },
    {
      "epoch": 223.13868613138686,
      "grad_norm": 8.754439353942871,
      "learning_rate": 3.8843065693430656e-05,
      "loss": 0.8325,
      "step": 30570
    },
    {
      "epoch": 223.21167883211677,
      "grad_norm": 16.361562728881836,
      "learning_rate": 3.883941605839416e-05,
      "loss": 0.9997,
      "step": 30580
    },
    {
      "epoch": 223.28467153284672,
      "grad_norm": 10.55897045135498,
      "learning_rate": 3.8835766423357664e-05,
      "loss": 1.0393,
      "step": 30590
    },
    {
      "epoch": 223.35766423357666,
      "grad_norm": 8.249321937561035,
      "learning_rate": 3.883211678832117e-05,
      "loss": 0.9612,
      "step": 30600
    },
    {
      "epoch": 223.43065693430657,
      "grad_norm": 14.590934753417969,
      "learning_rate": 3.882846715328468e-05,
      "loss": 1.0924,
      "step": 30610
    },
    {
      "epoch": 223.5036496350365,
      "grad_norm": 5.738839626312256,
      "learning_rate": 3.882481751824817e-05,
      "loss": 0.6289,
      "step": 30620
    },
    {
      "epoch": 223.57664233576642,
      "grad_norm": 10.739585876464844,
      "learning_rate": 3.882116788321168e-05,
      "loss": 0.9196,
      "step": 30630
    },
    {
      "epoch": 223.64963503649636,
      "grad_norm": 8.71619701385498,
      "learning_rate": 3.881751824817518e-05,
      "loss": 1.0843,
      "step": 30640
    },
    {
      "epoch": 223.72262773722628,
      "grad_norm": 10.435855865478516,
      "learning_rate": 3.881386861313869e-05,
      "loss": 1.0993,
      "step": 30650
    },
    {
      "epoch": 223.79562043795622,
      "grad_norm": 8.013938903808594,
      "learning_rate": 3.8810218978102194e-05,
      "loss": 1.2303,
      "step": 30660
    },
    {
      "epoch": 223.86861313868613,
      "grad_norm": 0.5626688599586487,
      "learning_rate": 3.8806569343065695e-05,
      "loss": 0.9675,
      "step": 30670
    },
    {
      "epoch": 223.94160583941607,
      "grad_norm": 15.324679374694824,
      "learning_rate": 3.88029197080292e-05,
      "loss": 1.1307,
      "step": 30680
    },
    {
      "epoch": 224.01459854014598,
      "grad_norm": 12.3859281539917,
      "learning_rate": 3.87992700729927e-05,
      "loss": 0.9794,
      "step": 30690
    },
    {
      "epoch": 224.08759124087592,
      "grad_norm": 10.060595512390137,
      "learning_rate": 3.87956204379562e-05,
      "loss": 1.1431,
      "step": 30700
    },
    {
      "epoch": 224.16058394160584,
      "grad_norm": 8.22258472442627,
      "learning_rate": 3.879197080291971e-05,
      "loss": 0.9994,
      "step": 30710
    },
    {
      "epoch": 224.23357664233578,
      "grad_norm": 10.767486572265625,
      "learning_rate": 3.878832116788321e-05,
      "loss": 1.2958,
      "step": 30720
    },
    {
      "epoch": 224.3065693430657,
      "grad_norm": 8.506851196289062,
      "learning_rate": 3.878467153284672e-05,
      "loss": 0.8879,
      "step": 30730
    },
    {
      "epoch": 224.37956204379563,
      "grad_norm": 10.844874382019043,
      "learning_rate": 3.878102189781022e-05,
      "loss": 0.6189,
      "step": 30740
    },
    {
      "epoch": 224.45255474452554,
      "grad_norm": 11.757006645202637,
      "learning_rate": 3.8777372262773726e-05,
      "loss": 1.1895,
      "step": 30750
    },
    {
      "epoch": 224.52554744525548,
      "grad_norm": 1.0059635639190674,
      "learning_rate": 3.877372262773723e-05,
      "loss": 1.116,
      "step": 30760
    },
    {
      "epoch": 224.5985401459854,
      "grad_norm": 6.198850631713867,
      "learning_rate": 3.8770072992700734e-05,
      "loss": 1.0127,
      "step": 30770
    },
    {
      "epoch": 224.67153284671534,
      "grad_norm": 6.543097019195557,
      "learning_rate": 3.8766423357664234e-05,
      "loss": 0.8201,
      "step": 30780
    },
    {
      "epoch": 224.74452554744525,
      "grad_norm": 0.6694110631942749,
      "learning_rate": 3.8762773722627735e-05,
      "loss": 1.0489,
      "step": 30790
    },
    {
      "epoch": 224.8175182481752,
      "grad_norm": 12.818185806274414,
      "learning_rate": 3.875912408759124e-05,
      "loss": 1.0553,
      "step": 30800
    },
    {
      "epoch": 224.8905109489051,
      "grad_norm": 0.36139753460884094,
      "learning_rate": 3.875547445255475e-05,
      "loss": 1.063,
      "step": 30810
    },
    {
      "epoch": 224.96350364963504,
      "grad_norm": 0.6165782809257507,
      "learning_rate": 3.875182481751825e-05,
      "loss": 0.9271,
      "step": 30820
    },
    {
      "epoch": 225.03649635036496,
      "grad_norm": 8.886940002441406,
      "learning_rate": 3.874817518248176e-05,
      "loss": 1.1702,
      "step": 30830
    },
    {
      "epoch": 225.1094890510949,
      "grad_norm": 10.090763092041016,
      "learning_rate": 3.874452554744526e-05,
      "loss": 1.097,
      "step": 30840
    },
    {
      "epoch": 225.1824817518248,
      "grad_norm": 14.517339706420898,
      "learning_rate": 3.874087591240876e-05,
      "loss": 0.7947,
      "step": 30850
    },
    {
      "epoch": 225.25547445255475,
      "grad_norm": 1.3946586847305298,
      "learning_rate": 3.8737226277372265e-05,
      "loss": 0.7335,
      "step": 30860
    },
    {
      "epoch": 225.32846715328466,
      "grad_norm": 13.711796760559082,
      "learning_rate": 3.8733576642335766e-05,
      "loss": 1.1774,
      "step": 30870
    },
    {
      "epoch": 225.4014598540146,
      "grad_norm": 5.870817184448242,
      "learning_rate": 3.872992700729927e-05,
      "loss": 1.1299,
      "step": 30880
    },
    {
      "epoch": 225.47445255474452,
      "grad_norm": 6.706241607666016,
      "learning_rate": 3.8726277372262774e-05,
      "loss": 1.1517,
      "step": 30890
    },
    {
      "epoch": 225.54744525547446,
      "grad_norm": 12.077492713928223,
      "learning_rate": 3.872262773722628e-05,
      "loss": 0.9967,
      "step": 30900
    },
    {
      "epoch": 225.62043795620437,
      "grad_norm": 19.003917694091797,
      "learning_rate": 3.871897810218978e-05,
      "loss": 0.9489,
      "step": 30910
    },
    {
      "epoch": 225.6934306569343,
      "grad_norm": 9.835110664367676,
      "learning_rate": 3.871532846715329e-05,
      "loss": 1.239,
      "step": 30920
    },
    {
      "epoch": 225.76642335766422,
      "grad_norm": 0.7383311986923218,
      "learning_rate": 3.871167883211679e-05,
      "loss": 0.9255,
      "step": 30930
    },
    {
      "epoch": 225.83941605839416,
      "grad_norm": 7.802011013031006,
      "learning_rate": 3.870802919708029e-05,
      "loss": 1.0039,
      "step": 30940
    },
    {
      "epoch": 225.91240875912408,
      "grad_norm": 15.504222869873047,
      "learning_rate": 3.87043795620438e-05,
      "loss": 0.904,
      "step": 30950
    },
    {
      "epoch": 225.98540145985402,
      "grad_norm": 6.7533793449401855,
      "learning_rate": 3.8700729927007304e-05,
      "loss": 1.1185,
      "step": 30960
    },
    {
      "epoch": 226.05839416058393,
      "grad_norm": 9.506683349609375,
      "learning_rate": 3.8697080291970805e-05,
      "loss": 0.8844,
      "step": 30970
    },
    {
      "epoch": 226.13138686131387,
      "grad_norm": 7.073442459106445,
      "learning_rate": 3.869343065693431e-05,
      "loss": 1.2283,
      "step": 30980
    },
    {
      "epoch": 226.20437956204378,
      "grad_norm": 6.437262535095215,
      "learning_rate": 3.868978102189781e-05,
      "loss": 0.8159,
      "step": 30990
    },
    {
      "epoch": 226.27737226277372,
      "grad_norm": 7.100558280944824,
      "learning_rate": 3.868613138686132e-05,
      "loss": 0.8149,
      "step": 31000
    },
    {
      "epoch": 226.35036496350364,
      "grad_norm": 1.2115103006362915,
      "learning_rate": 3.868248175182482e-05,
      "loss": 1.0959,
      "step": 31010
    },
    {
      "epoch": 226.42335766423358,
      "grad_norm": 11.948487281799316,
      "learning_rate": 3.867883211678832e-05,
      "loss": 1.0983,
      "step": 31020
    },
    {
      "epoch": 226.4963503649635,
      "grad_norm": 7.050270080566406,
      "learning_rate": 3.867518248175183e-05,
      "loss": 0.8861,
      "step": 31030
    },
    {
      "epoch": 226.56934306569343,
      "grad_norm": 16.38467788696289,
      "learning_rate": 3.867153284671533e-05,
      "loss": 1.0658,
      "step": 31040
    },
    {
      "epoch": 226.64233576642334,
      "grad_norm": 13.359196662902832,
      "learning_rate": 3.8667883211678836e-05,
      "loss": 1.3709,
      "step": 31050
    },
    {
      "epoch": 226.71532846715328,
      "grad_norm": 6.078658103942871,
      "learning_rate": 3.8664233576642336e-05,
      "loss": 0.8778,
      "step": 31060
    },
    {
      "epoch": 226.78832116788323,
      "grad_norm": 12.912680625915527,
      "learning_rate": 3.8660583941605843e-05,
      "loss": 1.2051,
      "step": 31070
    },
    {
      "epoch": 226.86131386861314,
      "grad_norm": 8.990757942199707,
      "learning_rate": 3.8656934306569344e-05,
      "loss": 0.7905,
      "step": 31080
    },
    {
      "epoch": 226.93430656934308,
      "grad_norm": 6.590953826904297,
      "learning_rate": 3.8653284671532844e-05,
      "loss": 0.7344,
      "step": 31090
    },
    {
      "epoch": 227.007299270073,
      "grad_norm": 5.849486351013184,
      "learning_rate": 3.864963503649635e-05,
      "loss": 0.9227,
      "step": 31100
    },
    {
      "epoch": 227.08029197080293,
      "grad_norm": 10.473390579223633,
      "learning_rate": 3.864598540145985e-05,
      "loss": 0.7735,
      "step": 31110
    },
    {
      "epoch": 227.15328467153284,
      "grad_norm": 0.766271710395813,
      "learning_rate": 3.864233576642336e-05,
      "loss": 0.8929,
      "step": 31120
    },
    {
      "epoch": 227.22627737226279,
      "grad_norm": 12.142693519592285,
      "learning_rate": 3.863868613138687e-05,
      "loss": 0.7619,
      "step": 31130
    },
    {
      "epoch": 227.2992700729927,
      "grad_norm": 5.820830345153809,
      "learning_rate": 3.863503649635037e-05,
      "loss": 1.0508,
      "step": 31140
    },
    {
      "epoch": 227.37226277372264,
      "grad_norm": 11.449009895324707,
      "learning_rate": 3.8631386861313874e-05,
      "loss": 1.2121,
      "step": 31150
    },
    {
      "epoch": 227.44525547445255,
      "grad_norm": 10.022554397583008,
      "learning_rate": 3.8627737226277375e-05,
      "loss": 1.2574,
      "step": 31160
    },
    {
      "epoch": 227.5182481751825,
      "grad_norm": 7.617771148681641,
      "learning_rate": 3.8624087591240875e-05,
      "loss": 0.7781,
      "step": 31170
    },
    {
      "epoch": 227.5912408759124,
      "grad_norm": 11.151719093322754,
      "learning_rate": 3.862043795620438e-05,
      "loss": 1.1569,
      "step": 31180
    },
    {
      "epoch": 227.66423357664235,
      "grad_norm": 9.963022232055664,
      "learning_rate": 3.861678832116788e-05,
      "loss": 0.7762,
      "step": 31190
    },
    {
      "epoch": 227.73722627737226,
      "grad_norm": 18.01909828186035,
      "learning_rate": 3.861313868613139e-05,
      "loss": 1.1561,
      "step": 31200
    },
    {
      "epoch": 227.8102189781022,
      "grad_norm": 7.315528869628906,
      "learning_rate": 3.860948905109489e-05,
      "loss": 0.9167,
      "step": 31210
    },
    {
      "epoch": 227.8832116788321,
      "grad_norm": 6.305544376373291,
      "learning_rate": 3.86058394160584e-05,
      "loss": 1.4706,
      "step": 31220
    },
    {
      "epoch": 227.95620437956205,
      "grad_norm": 0.764846682548523,
      "learning_rate": 3.8602189781021906e-05,
      "loss": 1.0402,
      "step": 31230
    },
    {
      "epoch": 228.02919708029196,
      "grad_norm": 6.229407787322998,
      "learning_rate": 3.85985401459854e-05,
      "loss": 0.7732,
      "step": 31240
    },
    {
      "epoch": 228.1021897810219,
      "grad_norm": 15.054549217224121,
      "learning_rate": 3.8594890510948907e-05,
      "loss": 1.2857,
      "step": 31250
    },
    {
      "epoch": 228.17518248175182,
      "grad_norm": 1.4255225658416748,
      "learning_rate": 3.859124087591241e-05,
      "loss": 0.8871,
      "step": 31260
    },
    {
      "epoch": 228.24817518248176,
      "grad_norm": 12.677343368530273,
      "learning_rate": 3.8587591240875914e-05,
      "loss": 0.9688,
      "step": 31270
    },
    {
      "epoch": 228.32116788321167,
      "grad_norm": 7.529817581176758,
      "learning_rate": 3.858394160583942e-05,
      "loss": 0.9753,
      "step": 31280
    },
    {
      "epoch": 228.3941605839416,
      "grad_norm": 10.504258155822754,
      "learning_rate": 3.858029197080292e-05,
      "loss": 1.1614,
      "step": 31290
    },
    {
      "epoch": 228.46715328467153,
      "grad_norm": 11.273666381835938,
      "learning_rate": 3.857664233576643e-05,
      "loss": 1.0378,
      "step": 31300
    },
    {
      "epoch": 228.54014598540147,
      "grad_norm": 7.876564979553223,
      "learning_rate": 3.857299270072992e-05,
      "loss": 0.8914,
      "step": 31310
    },
    {
      "epoch": 228.61313868613138,
      "grad_norm": 15.682731628417969,
      "learning_rate": 3.856934306569343e-05,
      "loss": 1.0592,
      "step": 31320
    },
    {
      "epoch": 228.68613138686132,
      "grad_norm": 2.1728973388671875,
      "learning_rate": 3.856569343065694e-05,
      "loss": 0.8228,
      "step": 31330
    },
    {
      "epoch": 228.75912408759123,
      "grad_norm": 6.938289642333984,
      "learning_rate": 3.856204379562044e-05,
      "loss": 0.7585,
      "step": 31340
    },
    {
      "epoch": 228.83211678832117,
      "grad_norm": 12.863070487976074,
      "learning_rate": 3.8558394160583945e-05,
      "loss": 0.9644,
      "step": 31350
    },
    {
      "epoch": 228.90510948905109,
      "grad_norm": 7.700769424438477,
      "learning_rate": 3.8554744525547446e-05,
      "loss": 1.042,
      "step": 31360
    },
    {
      "epoch": 228.97810218978103,
      "grad_norm": 5.254496097564697,
      "learning_rate": 3.855109489051095e-05,
      "loss": 1.0012,
      "step": 31370
    },
    {
      "epoch": 229.05109489051094,
      "grad_norm": 12.298680305480957,
      "learning_rate": 3.8547445255474454e-05,
      "loss": 1.1791,
      "step": 31380
    },
    {
      "epoch": 229.12408759124088,
      "grad_norm": 16.27877426147461,
      "learning_rate": 3.8543795620437954e-05,
      "loss": 1.2121,
      "step": 31390
    },
    {
      "epoch": 229.1970802919708,
      "grad_norm": 12.463446617126465,
      "learning_rate": 3.854014598540146e-05,
      "loss": 0.8754,
      "step": 31400
    },
    {
      "epoch": 229.27007299270073,
      "grad_norm": 0.5311179757118225,
      "learning_rate": 3.853649635036496e-05,
      "loss": 0.8342,
      "step": 31410
    },
    {
      "epoch": 229.34306569343065,
      "grad_norm": 9.801490783691406,
      "learning_rate": 3.853284671532847e-05,
      "loss": 0.8949,
      "step": 31420
    },
    {
      "epoch": 229.4160583941606,
      "grad_norm": 7.066499710083008,
      "learning_rate": 3.8529197080291976e-05,
      "loss": 0.9524,
      "step": 31430
    },
    {
      "epoch": 229.4890510948905,
      "grad_norm": 8.818488121032715,
      "learning_rate": 3.852554744525548e-05,
      "loss": 0.8973,
      "step": 31440
    },
    {
      "epoch": 229.56204379562044,
      "grad_norm": 0.6259199976921082,
      "learning_rate": 3.8521897810218984e-05,
      "loss": 1.2539,
      "step": 31450
    },
    {
      "epoch": 229.63503649635035,
      "grad_norm": 11.219849586486816,
      "learning_rate": 3.851824817518248e-05,
      "loss": 1.2405,
      "step": 31460
    },
    {
      "epoch": 229.7080291970803,
      "grad_norm": 7.7548627853393555,
      "learning_rate": 3.8514598540145985e-05,
      "loss": 0.7509,
      "step": 31470
    },
    {
      "epoch": 229.7810218978102,
      "grad_norm": 11.686845779418945,
      "learning_rate": 3.851094890510949e-05,
      "loss": 0.7964,
      "step": 31480
    },
    {
      "epoch": 229.85401459854015,
      "grad_norm": 13.550610542297363,
      "learning_rate": 3.850729927007299e-05,
      "loss": 1.0287,
      "step": 31490
    },
    {
      "epoch": 229.92700729927006,
      "grad_norm": 8.511439323425293,
      "learning_rate": 3.85036496350365e-05,
      "loss": 1.0985,
      "step": 31500
    },
    {
      "epoch": 230.0,
      "grad_norm": 8.587488174438477,
      "learning_rate": 3.85e-05,
      "loss": 1.0922,
      "step": 31510
    },
    {
      "epoch": 230.07299270072994,
      "grad_norm": 9.183687210083008,
      "learning_rate": 3.849635036496351e-05,
      "loss": 1.0455,
      "step": 31520
    },
    {
      "epoch": 230.14598540145985,
      "grad_norm": 0.7364776134490967,
      "learning_rate": 3.849270072992701e-05,
      "loss": 0.9971,
      "step": 31530
    },
    {
      "epoch": 230.2189781021898,
      "grad_norm": 0.6622846126556396,
      "learning_rate": 3.848905109489051e-05,
      "loss": 0.7092,
      "step": 31540
    },
    {
      "epoch": 230.2919708029197,
      "grad_norm": 3.9999759197235107,
      "learning_rate": 3.8485401459854016e-05,
      "loss": 0.9402,
      "step": 31550
    },
    {
      "epoch": 230.36496350364965,
      "grad_norm": 5.404412269592285,
      "learning_rate": 3.848175182481752e-05,
      "loss": 0.9592,
      "step": 31560
    },
    {
      "epoch": 230.43795620437956,
      "grad_norm": 10.709064483642578,
      "learning_rate": 3.8478102189781024e-05,
      "loss": 0.9473,
      "step": 31570
    },
    {
      "epoch": 230.5109489051095,
      "grad_norm": 5.876015663146973,
      "learning_rate": 3.8474452554744524e-05,
      "loss": 0.887,
      "step": 31580
    },
    {
      "epoch": 230.5839416058394,
      "grad_norm": 13.73124885559082,
      "learning_rate": 3.847080291970803e-05,
      "loss": 1.2187,
      "step": 31590
    },
    {
      "epoch": 230.65693430656935,
      "grad_norm": 16.10669708251953,
      "learning_rate": 3.846715328467154e-05,
      "loss": 1.2154,
      "step": 31600
    },
    {
      "epoch": 230.72992700729927,
      "grad_norm": 9.061525344848633,
      "learning_rate": 3.846350364963504e-05,
      "loss": 1.2423,
      "step": 31610
    },
    {
      "epoch": 230.8029197080292,
      "grad_norm": 8.523874282836914,
      "learning_rate": 3.845985401459854e-05,
      "loss": 0.8376,
      "step": 31620
    },
    {
      "epoch": 230.87591240875912,
      "grad_norm": 5.20676326751709,
      "learning_rate": 3.845620437956205e-05,
      "loss": 1.1418,
      "step": 31630
    },
    {
      "epoch": 230.94890510948906,
      "grad_norm": 11.338469505310059,
      "learning_rate": 3.845255474452555e-05,
      "loss": 1.0653,
      "step": 31640
    },
    {
      "epoch": 231.02189781021897,
      "grad_norm": 7.814271926879883,
      "learning_rate": 3.8448905109489055e-05,
      "loss": 0.8785,
      "step": 31650
    },
    {
      "epoch": 231.09489051094891,
      "grad_norm": 12.132800102233887,
      "learning_rate": 3.8445255474452556e-05,
      "loss": 1.1061,
      "step": 31660
    },
    {
      "epoch": 231.16788321167883,
      "grad_norm": 7.739879608154297,
      "learning_rate": 3.844160583941606e-05,
      "loss": 1.1127,
      "step": 31670
    },
    {
      "epoch": 231.24087591240877,
      "grad_norm": 10.197370529174805,
      "learning_rate": 3.843795620437956e-05,
      "loss": 0.7579,
      "step": 31680
    },
    {
      "epoch": 231.31386861313868,
      "grad_norm": 0.7774041891098022,
      "learning_rate": 3.8434306569343064e-05,
      "loss": 0.7416,
      "step": 31690
    },
    {
      "epoch": 231.38686131386862,
      "grad_norm": 5.579699516296387,
      "learning_rate": 3.843065693430657e-05,
      "loss": 0.82,
      "step": 31700
    },
    {
      "epoch": 231.45985401459853,
      "grad_norm": 13.083735466003418,
      "learning_rate": 3.842700729927007e-05,
      "loss": 0.8805,
      "step": 31710
    },
    {
      "epoch": 231.53284671532847,
      "grad_norm": 10.757732391357422,
      "learning_rate": 3.842335766423358e-05,
      "loss": 1.0352,
      "step": 31720
    },
    {
      "epoch": 231.6058394160584,
      "grad_norm": 10.14863109588623,
      "learning_rate": 3.841970802919708e-05,
      "loss": 0.8157,
      "step": 31730
    },
    {
      "epoch": 231.67883211678833,
      "grad_norm": 0.3859027624130249,
      "learning_rate": 3.8416058394160587e-05,
      "loss": 1.2453,
      "step": 31740
    },
    {
      "epoch": 231.75182481751824,
      "grad_norm": 0.6640548706054688,
      "learning_rate": 3.8412408759124094e-05,
      "loss": 0.9801,
      "step": 31750
    },
    {
      "epoch": 231.82481751824818,
      "grad_norm": 1.003476619720459,
      "learning_rate": 3.8408759124087594e-05,
      "loss": 1.171,
      "step": 31760
    },
    {
      "epoch": 231.8978102189781,
      "grad_norm": 8.465008735656738,
      "learning_rate": 3.8405109489051095e-05,
      "loss": 0.8256,
      "step": 31770
    },
    {
      "epoch": 231.97080291970804,
      "grad_norm": 0.669685423374176,
      "learning_rate": 3.8401459854014595e-05,
      "loss": 0.9932,
      "step": 31780
    },
    {
      "epoch": 232.04379562043795,
      "grad_norm": 6.472485542297363,
      "learning_rate": 3.83978102189781e-05,
      "loss": 1.1736,
      "step": 31790
    },
    {
      "epoch": 232.1167883211679,
      "grad_norm": 17.90809440612793,
      "learning_rate": 3.839416058394161e-05,
      "loss": 1.2764,
      "step": 31800
    },
    {
      "epoch": 232.1897810218978,
      "grad_norm": 13.402056694030762,
      "learning_rate": 3.839051094890511e-05,
      "loss": 1.0893,
      "step": 31810
    },
    {
      "epoch": 232.26277372262774,
      "grad_norm": 3.770463228225708,
      "learning_rate": 3.838686131386862e-05,
      "loss": 0.6906,
      "step": 31820
    },
    {
      "epoch": 232.33576642335765,
      "grad_norm": 0.7015887498855591,
      "learning_rate": 3.838321167883212e-05,
      "loss": 1.2528,
      "step": 31830
    },
    {
      "epoch": 232.4087591240876,
      "grad_norm": 7.784176349639893,
      "learning_rate": 3.8379562043795625e-05,
      "loss": 1.2542,
      "step": 31840
    },
    {
      "epoch": 232.4817518248175,
      "grad_norm": 7.907881259918213,
      "learning_rate": 3.8375912408759126e-05,
      "loss": 1.1315,
      "step": 31850
    },
    {
      "epoch": 232.55474452554745,
      "grad_norm": 8.924213409423828,
      "learning_rate": 3.8372262773722626e-05,
      "loss": 0.8564,
      "step": 31860
    },
    {
      "epoch": 232.62773722627736,
      "grad_norm": 10.21410846710205,
      "learning_rate": 3.8368613138686134e-05,
      "loss": 1.2646,
      "step": 31870
    },
    {
      "epoch": 232.7007299270073,
      "grad_norm": 0.5259483456611633,
      "learning_rate": 3.8364963503649634e-05,
      "loss": 0.6782,
      "step": 31880
    },
    {
      "epoch": 232.77372262773721,
      "grad_norm": 2.5132734775543213,
      "learning_rate": 3.836131386861314e-05,
      "loss": 0.7078,
      "step": 31890
    },
    {
      "epoch": 232.84671532846716,
      "grad_norm": 13.139579772949219,
      "learning_rate": 3.835766423357665e-05,
      "loss": 1.1332,
      "step": 31900
    },
    {
      "epoch": 232.91970802919707,
      "grad_norm": 7.828014850616455,
      "learning_rate": 3.835401459854015e-05,
      "loss": 1.147,
      "step": 31910
    },
    {
      "epoch": 232.992700729927,
      "grad_norm": 10.731871604919434,
      "learning_rate": 3.835036496350365e-05,
      "loss": 0.8562,
      "step": 31920
    },
    {
      "epoch": 233.06569343065692,
      "grad_norm": 6.640815258026123,
      "learning_rate": 3.834671532846715e-05,
      "loss": 1.0187,
      "step": 31930
    },
    {
      "epoch": 233.13868613138686,
      "grad_norm": 7.320017337799072,
      "learning_rate": 3.834306569343066e-05,
      "loss": 0.9664,
      "step": 31940
    },
    {
      "epoch": 233.21167883211677,
      "grad_norm": 9.337206840515137,
      "learning_rate": 3.8339416058394165e-05,
      "loss": 1.3629,
      "step": 31950
    },
    {
      "epoch": 233.28467153284672,
      "grad_norm": 9.13456916809082,
      "learning_rate": 3.8335766423357665e-05,
      "loss": 1.0951,
      "step": 31960
    },
    {
      "epoch": 233.35766423357666,
      "grad_norm": 12.41690444946289,
      "learning_rate": 3.833211678832117e-05,
      "loss": 0.9428,
      "step": 31970
    },
    {
      "epoch": 233.43065693430657,
      "grad_norm": 7.070225238800049,
      "learning_rate": 3.832846715328467e-05,
      "loss": 0.6575,
      "step": 31980
    },
    {
      "epoch": 233.5036496350365,
      "grad_norm": 9.474210739135742,
      "learning_rate": 3.832481751824818e-05,
      "loss": 0.9447,
      "step": 31990
    },
    {
      "epoch": 233.57664233576642,
      "grad_norm": 10.93753433227539,
      "learning_rate": 3.832116788321168e-05,
      "loss": 0.941,
      "step": 32000
    },
    {
      "epoch": 233.64963503649636,
      "grad_norm": 24.322111129760742,
      "learning_rate": 3.831751824817518e-05,
      "loss": 1.1475,
      "step": 32010
    },
    {
      "epoch": 233.72262773722628,
      "grad_norm": 1.8206971883773804,
      "learning_rate": 3.831386861313869e-05,
      "loss": 0.806,
      "step": 32020
    },
    {
      "epoch": 233.79562043795622,
      "grad_norm": 11.922144889831543,
      "learning_rate": 3.831021897810219e-05,
      "loss": 0.9467,
      "step": 32030
    },
    {
      "epoch": 233.86861313868613,
      "grad_norm": 7.168044567108154,
      "learning_rate": 3.8306569343065696e-05,
      "loss": 0.7067,
      "step": 32040
    },
    {
      "epoch": 233.94160583941607,
      "grad_norm": 7.103567123413086,
      "learning_rate": 3.83029197080292e-05,
      "loss": 1.0387,
      "step": 32050
    },
    {
      "epoch": 234.01459854014598,
      "grad_norm": 6.779224872589111,
      "learning_rate": 3.8299270072992704e-05,
      "loss": 1.0983,
      "step": 32060
    },
    {
      "epoch": 234.08759124087592,
      "grad_norm": 11.958906173706055,
      "learning_rate": 3.829562043795621e-05,
      "loss": 1.1295,
      "step": 32070
    },
    {
      "epoch": 234.16058394160584,
      "grad_norm": 8.016715049743652,
      "learning_rate": 3.8291970802919705e-05,
      "loss": 1.2527,
      "step": 32080
    },
    {
      "epoch": 234.23357664233578,
      "grad_norm": 11.842495918273926,
      "learning_rate": 3.828832116788321e-05,
      "loss": 1.087,
      "step": 32090
    },
    {
      "epoch": 234.3065693430657,
      "grad_norm": 8.716636657714844,
      "learning_rate": 3.828467153284672e-05,
      "loss": 1.0416,
      "step": 32100
    },
    {
      "epoch": 234.37956204379563,
      "grad_norm": 9.67544174194336,
      "learning_rate": 3.828102189781022e-05,
      "loss": 0.7442,
      "step": 32110
    },
    {
      "epoch": 234.45255474452554,
      "grad_norm": 11.322113037109375,
      "learning_rate": 3.827737226277373e-05,
      "loss": 0.7617,
      "step": 32120
    },
    {
      "epoch": 234.52554744525548,
      "grad_norm": 10.203608512878418,
      "learning_rate": 3.827372262773723e-05,
      "loss": 0.9756,
      "step": 32130
    },
    {
      "epoch": 234.5985401459854,
      "grad_norm": 0.5746563673019409,
      "learning_rate": 3.8270072992700735e-05,
      "loss": 0.7829,
      "step": 32140
    },
    {
      "epoch": 234.67153284671534,
      "grad_norm": 0.46668437123298645,
      "learning_rate": 3.8266423357664236e-05,
      "loss": 1.3435,
      "step": 32150
    },
    {
      "epoch": 234.74452554744525,
      "grad_norm": 0.7845814824104309,
      "learning_rate": 3.8262773722627736e-05,
      "loss": 0.9126,
      "step": 32160
    },
    {
      "epoch": 234.8175182481752,
      "grad_norm": 17.98900032043457,
      "learning_rate": 3.825912408759124e-05,
      "loss": 1.2638,
      "step": 32170
    },
    {
      "epoch": 234.8905109489051,
      "grad_norm": 0.5911403894424438,
      "learning_rate": 3.8255474452554744e-05,
      "loss": 1.1695,
      "step": 32180
    },
    {
      "epoch": 234.96350364963504,
      "grad_norm": 11.394774436950684,
      "learning_rate": 3.825182481751825e-05,
      "loss": 0.7725,
      "step": 32190
    },
    {
      "epoch": 235.03649635036496,
      "grad_norm": 14.353915214538574,
      "learning_rate": 3.824817518248175e-05,
      "loss": 0.846,
      "step": 32200
    },
    {
      "epoch": 235.1094890510949,
      "grad_norm": 9.013334274291992,
      "learning_rate": 3.824452554744526e-05,
      "loss": 0.8385,
      "step": 32210
    },
    {
      "epoch": 235.1824817518248,
      "grad_norm": 7.856682300567627,
      "learning_rate": 3.8240875912408766e-05,
      "loss": 1.1388,
      "step": 32220
    },
    {
      "epoch": 235.25547445255475,
      "grad_norm": 0.6587315797805786,
      "learning_rate": 3.823722627737226e-05,
      "loss": 1.055,
      "step": 32230
    },
    {
      "epoch": 235.32846715328466,
      "grad_norm": 0.7455125451087952,
      "learning_rate": 3.823357664233577e-05,
      "loss": 0.897,
      "step": 32240
    },
    {
      "epoch": 235.4014598540146,
      "grad_norm": 12.46745491027832,
      "learning_rate": 3.822992700729927e-05,
      "loss": 0.9671,
      "step": 32250
    },
    {
      "epoch": 235.47445255474452,
      "grad_norm": 8.926090240478516,
      "learning_rate": 3.8226277372262775e-05,
      "loss": 0.7606,
      "step": 32260
    },
    {
      "epoch": 235.54744525547446,
      "grad_norm": 9.290812492370605,
      "learning_rate": 3.822262773722628e-05,
      "loss": 1.0099,
      "step": 32270
    },
    {
      "epoch": 235.62043795620437,
      "grad_norm": 12.63752555847168,
      "learning_rate": 3.821897810218978e-05,
      "loss": 0.9025,
      "step": 32280
    },
    {
      "epoch": 235.6934306569343,
      "grad_norm": 11.245576858520508,
      "learning_rate": 3.821532846715329e-05,
      "loss": 1.1343,
      "step": 32290
    },
    {
      "epoch": 235.76642335766422,
      "grad_norm": 0.8209776878356934,
      "learning_rate": 3.821167883211679e-05,
      "loss": 1.0381,
      "step": 32300
    },
    {
      "epoch": 235.83941605839416,
      "grad_norm": 9.511421203613281,
      "learning_rate": 3.820802919708029e-05,
      "loss": 1.4329,
      "step": 32310
    },
    {
      "epoch": 235.91240875912408,
      "grad_norm": 4.5190815925598145,
      "learning_rate": 3.82043795620438e-05,
      "loss": 0.8858,
      "step": 32320
    },
    {
      "epoch": 235.98540145985402,
      "grad_norm": 12.746252059936523,
      "learning_rate": 3.82007299270073e-05,
      "loss": 1.0557,
      "step": 32330
    },
    {
      "epoch": 236.05839416058393,
      "grad_norm": 15.3572359085083,
      "learning_rate": 3.8197080291970806e-05,
      "loss": 1.2027,
      "step": 32340
    },
    {
      "epoch": 236.13138686131387,
      "grad_norm": 15.062521934509277,
      "learning_rate": 3.8193430656934306e-05,
      "loss": 0.5802,
      "step": 32350
    },
    {
      "epoch": 236.20437956204378,
      "grad_norm": 12.186612129211426,
      "learning_rate": 3.8189781021897814e-05,
      "loss": 1.4346,
      "step": 32360
    },
    {
      "epoch": 236.27737226277372,
      "grad_norm": 4.674811363220215,
      "learning_rate": 3.818613138686132e-05,
      "loss": 0.7975,
      "step": 32370
    },
    {
      "epoch": 236.35036496350364,
      "grad_norm": 0.6828569769859314,
      "learning_rate": 3.8182481751824815e-05,
      "loss": 1.2221,
      "step": 32380
    },
    {
      "epoch": 236.42335766423358,
      "grad_norm": 10.923643112182617,
      "learning_rate": 3.817883211678832e-05,
      "loss": 1.1041,
      "step": 32390
    },
    {
      "epoch": 236.4963503649635,
      "grad_norm": 8.85355281829834,
      "learning_rate": 3.817518248175182e-05,
      "loss": 0.8655,
      "step": 32400
    },
    {
      "epoch": 236.56934306569343,
      "grad_norm": 8.407931327819824,
      "learning_rate": 3.817153284671533e-05,
      "loss": 1.0717,
      "step": 32410
    },
    {
      "epoch": 236.64233576642334,
      "grad_norm": 6.988124847412109,
      "learning_rate": 3.816788321167884e-05,
      "loss": 0.9982,
      "step": 32420
    },
    {
      "epoch": 236.71532846715328,
      "grad_norm": 7.7594685554504395,
      "learning_rate": 3.816423357664234e-05,
      "loss": 0.5913,
      "step": 32430
    },
    {
      "epoch": 236.78832116788323,
      "grad_norm": 6.952567100524902,
      "learning_rate": 3.8160583941605845e-05,
      "loss": 1.0565,
      "step": 32440
    },
    {
      "epoch": 236.86131386861314,
      "grad_norm": 11.015656471252441,
      "learning_rate": 3.8156934306569345e-05,
      "loss": 0.6374,
      "step": 32450
    },
    {
      "epoch": 236.93430656934308,
      "grad_norm": 4.612483024597168,
      "learning_rate": 3.8153284671532846e-05,
      "loss": 1.0695,
      "step": 32460
    },
    {
      "epoch": 237.007299270073,
      "grad_norm": 10.933575630187988,
      "learning_rate": 3.814963503649635e-05,
      "loss": 1.1682,
      "step": 32470
    },
    {
      "epoch": 237.08029197080293,
      "grad_norm": 0.6113466620445251,
      "learning_rate": 3.8145985401459854e-05,
      "loss": 0.7696,
      "step": 32480
    },
    {
      "epoch": 237.15328467153284,
      "grad_norm": 11.640645980834961,
      "learning_rate": 3.814233576642336e-05,
      "loss": 0.9586,
      "step": 32490
    },
    {
      "epoch": 237.22627737226279,
      "grad_norm": 12.88986587524414,
      "learning_rate": 3.813868613138686e-05,
      "loss": 1.058,
      "step": 32500
    },
    {
      "epoch": 237.2992700729927,
      "grad_norm": 0.8024841547012329,
      "learning_rate": 3.813503649635037e-05,
      "loss": 0.87,
      "step": 32510
    },
    {
      "epoch": 237.37226277372264,
      "grad_norm": 7.538798809051514,
      "learning_rate": 3.8131386861313876e-05,
      "loss": 1.1836,
      "step": 32520
    },
    {
      "epoch": 237.44525547445255,
      "grad_norm": 9.661772727966309,
      "learning_rate": 3.812773722627737e-05,
      "loss": 0.8944,
      "step": 32530
    },
    {
      "epoch": 237.5182481751825,
      "grad_norm": 9.039657592773438,
      "learning_rate": 3.812408759124088e-05,
      "loss": 0.687,
      "step": 32540
    },
    {
      "epoch": 237.5912408759124,
      "grad_norm": 0.5690784454345703,
      "learning_rate": 3.812043795620438e-05,
      "loss": 0.8745,
      "step": 32550
    },
    {
      "epoch": 237.66423357664235,
      "grad_norm": 11.020123481750488,
      "learning_rate": 3.8116788321167885e-05,
      "loss": 1.17,
      "step": 32560
    },
    {
      "epoch": 237.73722627737226,
      "grad_norm": 17.683208465576172,
      "learning_rate": 3.811313868613139e-05,
      "loss": 1.4068,
      "step": 32570
    },
    {
      "epoch": 237.8102189781022,
      "grad_norm": 0.9141077399253845,
      "learning_rate": 3.810948905109489e-05,
      "loss": 1.0672,
      "step": 32580
    },
    {
      "epoch": 237.8832116788321,
      "grad_norm": 7.885007858276367,
      "learning_rate": 3.81058394160584e-05,
      "loss": 1.2408,
      "step": 32590
    },
    {
      "epoch": 237.95620437956205,
      "grad_norm": 6.5352606773376465,
      "learning_rate": 3.81021897810219e-05,
      "loss": 0.8329,
      "step": 32600
    },
    {
      "epoch": 238.02919708029196,
      "grad_norm": 12.311739921569824,
      "learning_rate": 3.80985401459854e-05,
      "loss": 0.7235,
      "step": 32610
    },
    {
      "epoch": 238.1021897810219,
      "grad_norm": 8.21984577178955,
      "learning_rate": 3.809489051094891e-05,
      "loss": 1.206,
      "step": 32620
    },
    {
      "epoch": 238.17518248175182,
      "grad_norm": 7.064599990844727,
      "learning_rate": 3.809124087591241e-05,
      "loss": 0.8399,
      "step": 32630
    },
    {
      "epoch": 238.24817518248176,
      "grad_norm": 6.505527019500732,
      "learning_rate": 3.8087591240875916e-05,
      "loss": 0.9248,
      "step": 32640
    },
    {
      "epoch": 238.32116788321167,
      "grad_norm": 6.620207786560059,
      "learning_rate": 3.8083941605839416e-05,
      "loss": 0.9201,
      "step": 32650
    },
    {
      "epoch": 238.3941605839416,
      "grad_norm": 9.971648216247559,
      "learning_rate": 3.8080291970802923e-05,
      "loss": 1.3272,
      "step": 32660
    },
    {
      "epoch": 238.46715328467153,
      "grad_norm": 11.683136940002441,
      "learning_rate": 3.8076642335766424e-05,
      "loss": 1.1014,
      "step": 32670
    },
    {
      "epoch": 238.54014598540147,
      "grad_norm": 6.750007152557373,
      "learning_rate": 3.807299270072993e-05,
      "loss": 0.9416,
      "step": 32680
    },
    {
      "epoch": 238.61313868613138,
      "grad_norm": 10.098831176757812,
      "learning_rate": 3.806934306569343e-05,
      "loss": 1.1735,
      "step": 32690
    },
    {
      "epoch": 238.68613138686132,
      "grad_norm": 5.674203395843506,
      "learning_rate": 3.806569343065693e-05,
      "loss": 1.0557,
      "step": 32700
    },
    {
      "epoch": 238.75912408759123,
      "grad_norm": 6.398702621459961,
      "learning_rate": 3.806204379562044e-05,
      "loss": 0.5041,
      "step": 32710
    },
    {
      "epoch": 238.83211678832117,
      "grad_norm": 4.099429607391357,
      "learning_rate": 3.805839416058394e-05,
      "loss": 1.2224,
      "step": 32720
    },
    {
      "epoch": 238.90510948905109,
      "grad_norm": 0.559461236000061,
      "learning_rate": 3.805474452554745e-05,
      "loss": 0.9601,
      "step": 32730
    },
    {
      "epoch": 238.97810218978103,
      "grad_norm": 11.367867469787598,
      "learning_rate": 3.8051094890510955e-05,
      "loss": 0.8915,
      "step": 32740
    },
    {
      "epoch": 239.05109489051094,
      "grad_norm": 7.9644646644592285,
      "learning_rate": 3.8047445255474455e-05,
      "loss": 0.5644,
      "step": 32750
    },
    {
      "epoch": 239.12408759124088,
      "grad_norm": 8.585735321044922,
      "learning_rate": 3.8043795620437956e-05,
      "loss": 0.7672,
      "step": 32760
    },
    {
      "epoch": 239.1970802919708,
      "grad_norm": 12.082544326782227,
      "learning_rate": 3.804014598540146e-05,
      "loss": 1.0437,
      "step": 32770
    },
    {
      "epoch": 239.27007299270073,
      "grad_norm": 8.185888290405273,
      "learning_rate": 3.803649635036496e-05,
      "loss": 0.8576,
      "step": 32780
    },
    {
      "epoch": 239.34306569343065,
      "grad_norm": 12.62039566040039,
      "learning_rate": 3.803284671532847e-05,
      "loss": 1.3015,
      "step": 32790
    },
    {
      "epoch": 239.4160583941606,
      "grad_norm": 0.799206018447876,
      "learning_rate": 3.802919708029197e-05,
      "loss": 0.7237,
      "step": 32800
    },
    {
      "epoch": 239.4890510948905,
      "grad_norm": 2.9523913860321045,
      "learning_rate": 3.802554744525548e-05,
      "loss": 0.6322,
      "step": 32810
    },
    {
      "epoch": 239.56204379562044,
      "grad_norm": 14.062577247619629,
      "learning_rate": 3.802189781021898e-05,
      "loss": 1.3241,
      "step": 32820
    },
    {
      "epoch": 239.63503649635035,
      "grad_norm": 12.07003116607666,
      "learning_rate": 3.8018248175182486e-05,
      "loss": 0.9503,
      "step": 32830
    },
    {
      "epoch": 239.7080291970803,
      "grad_norm": 7.651564121246338,
      "learning_rate": 3.8014598540145987e-05,
      "loss": 0.8551,
      "step": 32840
    },
    {
      "epoch": 239.7810218978102,
      "grad_norm": 8.926024436950684,
      "learning_rate": 3.801094890510949e-05,
      "loss": 1.2401,
      "step": 32850
    },
    {
      "epoch": 239.85401459854015,
      "grad_norm": 5.179491996765137,
      "learning_rate": 3.8007299270072994e-05,
      "loss": 1.1991,
      "step": 32860
    },
    {
      "epoch": 239.92700729927006,
      "grad_norm": 8.002636909484863,
      "learning_rate": 3.8003649635036495e-05,
      "loss": 1.363,
      "step": 32870
    },
    {
      "epoch": 240.0,
      "grad_norm": 0.7246866226196289,
      "learning_rate": 3.8e-05,
      "loss": 1.0493,
      "step": 32880
    },
    {
      "epoch": 240.07299270072994,
      "grad_norm": 4.477957248687744,
      "learning_rate": 3.799635036496351e-05,
      "loss": 0.8718,
      "step": 32890
    },
    {
      "epoch": 240.14598540145985,
      "grad_norm": 15.43423843383789,
      "learning_rate": 3.799270072992701e-05,
      "loss": 1.1975,
      "step": 32900
    },
    {
      "epoch": 240.2189781021898,
      "grad_norm": 7.190573692321777,
      "learning_rate": 3.798905109489052e-05,
      "loss": 1.1051,
      "step": 32910
    },
    {
      "epoch": 240.2919708029197,
      "grad_norm": 12.59792709350586,
      "learning_rate": 3.798540145985401e-05,
      "loss": 0.842,
      "step": 32920
    },
    {
      "epoch": 240.36496350364965,
      "grad_norm": 14.516373634338379,
      "learning_rate": 3.798175182481752e-05,
      "loss": 1.1919,
      "step": 32930
    },
    {
      "epoch": 240.43795620437956,
      "grad_norm": 0.5523526668548584,
      "learning_rate": 3.7978102189781025e-05,
      "loss": 0.8371,
      "step": 32940
    },
    {
      "epoch": 240.5109489051095,
      "grad_norm": 7.0483222007751465,
      "learning_rate": 3.7974452554744526e-05,
      "loss": 0.8376,
      "step": 32950
    },
    {
      "epoch": 240.5839416058394,
      "grad_norm": 7.899038314819336,
      "learning_rate": 3.797080291970803e-05,
      "loss": 1.1957,
      "step": 32960
    },
    {
      "epoch": 240.65693430656935,
      "grad_norm": 5.492786407470703,
      "learning_rate": 3.7967153284671534e-05,
      "loss": 1.1672,
      "step": 32970
    },
    {
      "epoch": 240.72992700729927,
      "grad_norm": 1.0066651105880737,
      "learning_rate": 3.796350364963504e-05,
      "loss": 0.8389,
      "step": 32980
    },
    {
      "epoch": 240.8029197080292,
      "grad_norm": 10.764443397521973,
      "learning_rate": 3.795985401459854e-05,
      "loss": 0.7,
      "step": 32990
    },
    {
      "epoch": 240.87591240875912,
      "grad_norm": 7.514235019683838,
      "learning_rate": 3.795620437956204e-05,
      "loss": 0.7851,
      "step": 33000
    },
    {
      "epoch": 240.94890510948906,
      "grad_norm": 7.473750591278076,
      "learning_rate": 3.795255474452555e-05,
      "loss": 0.9826,
      "step": 33010
    },
    {
      "epoch": 241.02189781021897,
      "grad_norm": 0.5893487334251404,
      "learning_rate": 3.794890510948905e-05,
      "loss": 1.178,
      "step": 33020
    },
    {
      "epoch": 241.09489051094891,
      "grad_norm": 7.837960243225098,
      "learning_rate": 3.794525547445256e-05,
      "loss": 1.1004,
      "step": 33030
    },
    {
      "epoch": 241.16788321167883,
      "grad_norm": 6.187265396118164,
      "learning_rate": 3.7941605839416064e-05,
      "loss": 1.1134,
      "step": 33040
    },
    {
      "epoch": 241.24087591240877,
      "grad_norm": 6.6546783447265625,
      "learning_rate": 3.7937956204379565e-05,
      "loss": 1.0015,
      "step": 33050
    },
    {
      "epoch": 241.31386861313868,
      "grad_norm": 5.35516357421875,
      "learning_rate": 3.793430656934307e-05,
      "loss": 0.8113,
      "step": 33060
    },
    {
      "epoch": 241.38686131386862,
      "grad_norm": 11.89522933959961,
      "learning_rate": 3.7930656934306566e-05,
      "loss": 1.2775,
      "step": 33070
    },
    {
      "epoch": 241.45985401459853,
      "grad_norm": 0.6939970850944519,
      "learning_rate": 3.792700729927007e-05,
      "loss": 0.9093,
      "step": 33080
    },
    {
      "epoch": 241.53284671532847,
      "grad_norm": 5.083444118499756,
      "learning_rate": 3.792335766423358e-05,
      "loss": 1.228,
      "step": 33090
    },
    {
      "epoch": 241.6058394160584,
      "grad_norm": 6.466309547424316,
      "learning_rate": 3.791970802919708e-05,
      "loss": 0.8093,
      "step": 33100
    },
    {
      "epoch": 241.67883211678833,
      "grad_norm": 6.11033821105957,
      "learning_rate": 3.791605839416059e-05,
      "loss": 0.8038,
      "step": 33110
    },
    {
      "epoch": 241.75182481751824,
      "grad_norm": 11.62597370147705,
      "learning_rate": 3.791240875912409e-05,
      "loss": 0.7803,
      "step": 33120
    },
    {
      "epoch": 241.82481751824818,
      "grad_norm": 10.713980674743652,
      "learning_rate": 3.7908759124087596e-05,
      "loss": 1.2791,
      "step": 33130
    },
    {
      "epoch": 241.8978102189781,
      "grad_norm": 9.559910774230957,
      "learning_rate": 3.7905109489051096e-05,
      "loss": 1.2421,
      "step": 33140
    },
    {
      "epoch": 241.97080291970804,
      "grad_norm": 7.053889274597168,
      "learning_rate": 3.79014598540146e-05,
      "loss": 1.0781,
      "step": 33150
    },
    {
      "epoch": 242.04379562043795,
      "grad_norm": 10.251178741455078,
      "learning_rate": 3.7897810218978104e-05,
      "loss": 0.9887,
      "step": 33160
    },
    {
      "epoch": 242.1167883211679,
      "grad_norm": 0.324372798204422,
      "learning_rate": 3.7894160583941605e-05,
      "loss": 0.8606,
      "step": 33170
    },
    {
      "epoch": 242.1897810218978,
      "grad_norm": 14.945734977722168,
      "learning_rate": 3.789051094890511e-05,
      "loss": 1.0909,
      "step": 33180
    },
    {
      "epoch": 242.26277372262774,
      "grad_norm": 10.086723327636719,
      "learning_rate": 3.788686131386862e-05,
      "loss": 0.8191,
      "step": 33190
    },
    {
      "epoch": 242.33576642335765,
      "grad_norm": 6.332509994506836,
      "learning_rate": 3.788321167883212e-05,
      "loss": 1.0842,
      "step": 33200
    },
    {
      "epoch": 242.4087591240876,
      "grad_norm": 10.034584999084473,
      "learning_rate": 3.787956204379563e-05,
      "loss": 0.8275,
      "step": 33210
    },
    {
      "epoch": 242.4817518248175,
      "grad_norm": 9.160573959350586,
      "learning_rate": 3.787591240875912e-05,
      "loss": 1.0877,
      "step": 33220
    },
    {
      "epoch": 242.55474452554745,
      "grad_norm": 0.6220024824142456,
      "learning_rate": 3.787226277372263e-05,
      "loss": 1.176,
      "step": 33230
    },
    {
      "epoch": 242.62773722627736,
      "grad_norm": 17.463193893432617,
      "learning_rate": 3.7868613138686135e-05,
      "loss": 1.1288,
      "step": 33240
    },
    {
      "epoch": 242.7007299270073,
      "grad_norm": 12.552191734313965,
      "learning_rate": 3.7864963503649636e-05,
      "loss": 1.6544,
      "step": 33250
    },
    {
      "epoch": 242.77372262773721,
      "grad_norm": 0.3313007056713104,
      "learning_rate": 3.786131386861314e-05,
      "loss": 0.8267,
      "step": 33260
    },
    {
      "epoch": 242.84671532846716,
      "grad_norm": 8.267088890075684,
      "learning_rate": 3.785766423357664e-05,
      "loss": 0.622,
      "step": 33270
    },
    {
      "epoch": 242.91970802919707,
      "grad_norm": 17.086355209350586,
      "learning_rate": 3.785401459854015e-05,
      "loss": 1.1162,
      "step": 33280
    },
    {
      "epoch": 242.992700729927,
      "grad_norm": 0.6663373708724976,
      "learning_rate": 3.785036496350365e-05,
      "loss": 0.77,
      "step": 33290
    },
    {
      "epoch": 243.06569343065692,
      "grad_norm": 11.120993614196777,
      "learning_rate": 3.784671532846715e-05,
      "loss": 1.0501,
      "step": 33300
    },
    {
      "epoch": 243.13868613138686,
      "grad_norm": 0.3359836935997009,
      "learning_rate": 3.784306569343066e-05,
      "loss": 0.9743,
      "step": 33310
    },
    {
      "epoch": 243.21167883211677,
      "grad_norm": 6.241805076599121,
      "learning_rate": 3.783941605839416e-05,
      "loss": 0.8181,
      "step": 33320
    },
    {
      "epoch": 243.28467153284672,
      "grad_norm": 7.585179805755615,
      "learning_rate": 3.783576642335767e-05,
      "loss": 1.1054,
      "step": 33330
    },
    {
      "epoch": 243.35766423357666,
      "grad_norm": 10.065328598022461,
      "learning_rate": 3.783211678832117e-05,
      "loss": 0.944,
      "step": 33340
    },
    {
      "epoch": 243.43065693430657,
      "grad_norm": 7.082613945007324,
      "learning_rate": 3.7828467153284674e-05,
      "loss": 0.9668,
      "step": 33350
    },
    {
      "epoch": 243.5036496350365,
      "grad_norm": 7.597323417663574,
      "learning_rate": 3.782481751824818e-05,
      "loss": 0.9768,
      "step": 33360
    },
    {
      "epoch": 243.57664233576642,
      "grad_norm": 7.38414192199707,
      "learning_rate": 3.782116788321168e-05,
      "loss": 0.9032,
      "step": 33370
    },
    {
      "epoch": 243.64963503649636,
      "grad_norm": 10.116873741149902,
      "learning_rate": 3.781751824817518e-05,
      "loss": 1.1536,
      "step": 33380
    },
    {
      "epoch": 243.72262773722628,
      "grad_norm": 8.001922607421875,
      "learning_rate": 3.781386861313869e-05,
      "loss": 0.9666,
      "step": 33390
    },
    {
      "epoch": 243.79562043795622,
      "grad_norm": 10.128993034362793,
      "learning_rate": 3.781021897810219e-05,
      "loss": 0.6744,
      "step": 33400
    },
    {
      "epoch": 243.86861313868613,
      "grad_norm": 9.550692558288574,
      "learning_rate": 3.78065693430657e-05,
      "loss": 1.049,
      "step": 33410
    },
    {
      "epoch": 243.94160583941607,
      "grad_norm": 12.810745239257812,
      "learning_rate": 3.78029197080292e-05,
      "loss": 1.2043,
      "step": 33420
    },
    {
      "epoch": 244.01459854014598,
      "grad_norm": 17.048791885375977,
      "learning_rate": 3.7799270072992705e-05,
      "loss": 1.1991,
      "step": 33430
    },
    {
      "epoch": 244.08759124087592,
      "grad_norm": 0.32655104994773865,
      "learning_rate": 3.7795620437956206e-05,
      "loss": 1.1644,
      "step": 33440
    },
    {
      "epoch": 244.16058394160584,
      "grad_norm": 14.705206871032715,
      "learning_rate": 3.7791970802919706e-05,
      "loss": 0.9333,
      "step": 33450
    },
    {
      "epoch": 244.23357664233578,
      "grad_norm": 13.708380699157715,
      "learning_rate": 3.7788321167883214e-05,
      "loss": 1.2706,
      "step": 33460
    },
    {
      "epoch": 244.3065693430657,
      "grad_norm": 5.732089519500732,
      "learning_rate": 3.7784671532846714e-05,
      "loss": 1.1153,
      "step": 33470
    },
    {
      "epoch": 244.37956204379563,
      "grad_norm": 13.593663215637207,
      "learning_rate": 3.778102189781022e-05,
      "loss": 1.0735,
      "step": 33480
    },
    {
      "epoch": 244.45255474452554,
      "grad_norm": 9.238545417785645,
      "learning_rate": 3.777737226277372e-05,
      "loss": 0.9671,
      "step": 33490
    },
    {
      "epoch": 244.52554744525548,
      "grad_norm": 0.6736421585083008,
      "learning_rate": 3.777372262773723e-05,
      "loss": 1.0149,
      "step": 33500
    },
    {
      "epoch": 244.5985401459854,
      "grad_norm": 8.247512817382812,
      "learning_rate": 3.7770072992700737e-05,
      "loss": 0.8298,
      "step": 33510
    },
    {
      "epoch": 244.67153284671534,
      "grad_norm": 15.861438751220703,
      "learning_rate": 3.776642335766424e-05,
      "loss": 1.2242,
      "step": 33520
    },
    {
      "epoch": 244.74452554744525,
      "grad_norm": 14.565801620483398,
      "learning_rate": 3.776277372262774e-05,
      "loss": 0.9012,
      "step": 33530
    },
    {
      "epoch": 244.8175182481752,
      "grad_norm": 5.59210729598999,
      "learning_rate": 3.775912408759124e-05,
      "loss": 1.0196,
      "step": 33540
    },
    {
      "epoch": 244.8905109489051,
      "grad_norm": 0.2791522443294525,
      "learning_rate": 3.7755474452554745e-05,
      "loss": 0.6099,
      "step": 33550
    },
    {
      "epoch": 244.96350364963504,
      "grad_norm": 0.5835039615631104,
      "learning_rate": 3.775182481751825e-05,
      "loss": 0.5831,
      "step": 33560
    },
    {
      "epoch": 245.03649635036496,
      "grad_norm": 7.636497974395752,
      "learning_rate": 3.774817518248175e-05,
      "loss": 0.6573,
      "step": 33570
    },
    {
      "epoch": 245.1094890510949,
      "grad_norm": 6.953592777252197,
      "learning_rate": 3.774452554744526e-05,
      "loss": 1.1175,
      "step": 33580
    },
    {
      "epoch": 245.1824817518248,
      "grad_norm": 10.687210083007812,
      "learning_rate": 3.774087591240876e-05,
      "loss": 0.8103,
      "step": 33590
    },
    {
      "epoch": 245.25547445255475,
      "grad_norm": 10.125954627990723,
      "learning_rate": 3.773722627737227e-05,
      "loss": 1.2007,
      "step": 33600
    },
    {
      "epoch": 245.32846715328466,
      "grad_norm": 2.1494204998016357,
      "learning_rate": 3.773357664233577e-05,
      "loss": 1.2793,
      "step": 33610
    },
    {
      "epoch": 245.4014598540146,
      "grad_norm": 9.030644416809082,
      "learning_rate": 3.772992700729927e-05,
      "loss": 1.1081,
      "step": 33620
    },
    {
      "epoch": 245.47445255474452,
      "grad_norm": 17.39065933227539,
      "learning_rate": 3.7726277372262776e-05,
      "loss": 1.4238,
      "step": 33630
    },
    {
      "epoch": 245.54744525547446,
      "grad_norm": 12.909612655639648,
      "learning_rate": 3.772262773722628e-05,
      "loss": 1.1287,
      "step": 33640
    },
    {
      "epoch": 245.62043795620437,
      "grad_norm": 4.587426662445068,
      "learning_rate": 3.7718978102189784e-05,
      "loss": 0.9009,
      "step": 33650
    },
    {
      "epoch": 245.6934306569343,
      "grad_norm": 0.9434600472450256,
      "learning_rate": 3.771532846715329e-05,
      "loss": 1.1477,
      "step": 33660
    },
    {
      "epoch": 245.76642335766422,
      "grad_norm": 8.510566711425781,
      "learning_rate": 3.771167883211679e-05,
      "loss": 0.7806,
      "step": 33670
    },
    {
      "epoch": 245.83941605839416,
      "grad_norm": 9.292496681213379,
      "learning_rate": 3.770802919708029e-05,
      "loss": 0.8942,
      "step": 33680
    },
    {
      "epoch": 245.91240875912408,
      "grad_norm": 7.3416314125061035,
      "learning_rate": 3.770437956204379e-05,
      "loss": 0.9551,
      "step": 33690
    },
    {
      "epoch": 245.98540145985402,
      "grad_norm": 13.214588165283203,
      "learning_rate": 3.77007299270073e-05,
      "loss": 0.8334,
      "step": 33700
    },
    {
      "epoch": 246.05839416058393,
      "grad_norm": 18.546192169189453,
      "learning_rate": 3.769708029197081e-05,
      "loss": 1.0065,
      "step": 33710
    },
    {
      "epoch": 246.13138686131387,
      "grad_norm": 8.416979789733887,
      "learning_rate": 3.769343065693431e-05,
      "loss": 0.9224,
      "step": 33720
    },
    {
      "epoch": 246.20437956204378,
      "grad_norm": 10.843522071838379,
      "learning_rate": 3.7689781021897815e-05,
      "loss": 1.1397,
      "step": 33730
    },
    {
      "epoch": 246.27737226277372,
      "grad_norm": 12.052538871765137,
      "learning_rate": 3.7686131386861316e-05,
      "loss": 0.9774,
      "step": 33740
    },
    {
      "epoch": 246.35036496350364,
      "grad_norm": 7.7489399909973145,
      "learning_rate": 3.768248175182482e-05,
      "loss": 1.0982,
      "step": 33750
    },
    {
      "epoch": 246.42335766423358,
      "grad_norm": 11.017237663269043,
      "learning_rate": 3.7678832116788323e-05,
      "loss": 0.936,
      "step": 33760
    },
    {
      "epoch": 246.4963503649635,
      "grad_norm": 7.553371429443359,
      "learning_rate": 3.7675182481751824e-05,
      "loss": 0.852,
      "step": 33770
    },
    {
      "epoch": 246.56934306569343,
      "grad_norm": 7.175889492034912,
      "learning_rate": 3.767153284671533e-05,
      "loss": 0.8758,
      "step": 33780
    },
    {
      "epoch": 246.64233576642334,
      "grad_norm": 10.59473991394043,
      "learning_rate": 3.766788321167883e-05,
      "loss": 1.13,
      "step": 33790
    },
    {
      "epoch": 246.71532846715328,
      "grad_norm": 0.4269905686378479,
      "learning_rate": 3.766423357664234e-05,
      "loss": 0.6533,
      "step": 33800
    },
    {
      "epoch": 246.78832116788323,
      "grad_norm": 12.211609840393066,
      "learning_rate": 3.766058394160584e-05,
      "loss": 0.5776,
      "step": 33810
    },
    {
      "epoch": 246.86131386861314,
      "grad_norm": 11.773446083068848,
      "learning_rate": 3.765693430656935e-05,
      "loss": 1.345,
      "step": 33820
    },
    {
      "epoch": 246.93430656934308,
      "grad_norm": 0.1614827662706375,
      "learning_rate": 3.765328467153285e-05,
      "loss": 0.7965,
      "step": 33830
    },
    {
      "epoch": 247.007299270073,
      "grad_norm": 10.69664478302002,
      "learning_rate": 3.764963503649635e-05,
      "loss": 0.6997,
      "step": 33840
    },
    {
      "epoch": 247.08029197080293,
      "grad_norm": 12.757048606872559,
      "learning_rate": 3.7645985401459855e-05,
      "loss": 1.0783,
      "step": 33850
    },
    {
      "epoch": 247.15328467153284,
      "grad_norm": 10.128470420837402,
      "learning_rate": 3.764233576642336e-05,
      "loss": 0.7711,
      "step": 33860
    },
    {
      "epoch": 247.22627737226279,
      "grad_norm": 9.705911636352539,
      "learning_rate": 3.763868613138686e-05,
      "loss": 1.1174,
      "step": 33870
    },
    {
      "epoch": 247.2992700729927,
      "grad_norm": 12.82787799835205,
      "learning_rate": 3.763503649635037e-05,
      "loss": 0.8574,
      "step": 33880
    },
    {
      "epoch": 247.37226277372264,
      "grad_norm": 6.274191856384277,
      "learning_rate": 3.763138686131387e-05,
      "loss": 1.0695,
      "step": 33890
    },
    {
      "epoch": 247.44525547445255,
      "grad_norm": 7.696327209472656,
      "learning_rate": 3.762773722627738e-05,
      "loss": 1.3197,
      "step": 33900
    },
    {
      "epoch": 247.5182481751825,
      "grad_norm": 13.310741424560547,
      "learning_rate": 3.762408759124088e-05,
      "loss": 0.8996,
      "step": 33910
    },
    {
      "epoch": 247.5912408759124,
      "grad_norm": 9.946107864379883,
      "learning_rate": 3.762043795620438e-05,
      "loss": 1.0297,
      "step": 33920
    },
    {
      "epoch": 247.66423357664235,
      "grad_norm": 0.5840349197387695,
      "learning_rate": 3.7616788321167886e-05,
      "loss": 0.7282,
      "step": 33930
    },
    {
      "epoch": 247.73722627737226,
      "grad_norm": 8.199921607971191,
      "learning_rate": 3.7613138686131387e-05,
      "loss": 0.9461,
      "step": 33940
    },
    {
      "epoch": 247.8102189781022,
      "grad_norm": 9.143484115600586,
      "learning_rate": 3.7609489051094894e-05,
      "loss": 1.2806,
      "step": 33950
    },
    {
      "epoch": 247.8832116788321,
      "grad_norm": 0.32712528109550476,
      "learning_rate": 3.7605839416058394e-05,
      "loss": 1.2139,
      "step": 33960
    },
    {
      "epoch": 247.95620437956205,
      "grad_norm": 7.537038803100586,
      "learning_rate": 3.76021897810219e-05,
      "loss": 0.8403,
      "step": 33970
    },
    {
      "epoch": 248.02919708029196,
      "grad_norm": 11.995244026184082,
      "learning_rate": 3.759854014598541e-05,
      "loss": 0.9605,
      "step": 33980
    },
    {
      "epoch": 248.1021897810219,
      "grad_norm": 9.108434677124023,
      "learning_rate": 3.75948905109489e-05,
      "loss": 1.0584,
      "step": 33990
    },
    {
      "epoch": 248.17518248175182,
      "grad_norm": 0.30195707082748413,
      "learning_rate": 3.759124087591241e-05,
      "loss": 0.9419,
      "step": 34000
    },
    {
      "epoch": 248.24817518248176,
      "grad_norm": 0.8761876821517944,
      "learning_rate": 3.758759124087591e-05,
      "loss": 0.8101,
      "step": 34010
    },
    {
      "epoch": 248.32116788321167,
      "grad_norm": 7.929121017456055,
      "learning_rate": 3.758394160583942e-05,
      "loss": 1.1091,
      "step": 34020
    },
    {
      "epoch": 248.3941605839416,
      "grad_norm": 0.5393502712249756,
      "learning_rate": 3.7580291970802925e-05,
      "loss": 0.842,
      "step": 34030
    },
    {
      "epoch": 248.46715328467153,
      "grad_norm": 0.7773473262786865,
      "learning_rate": 3.7576642335766425e-05,
      "loss": 0.7442,
      "step": 34040
    },
    {
      "epoch": 248.54014598540147,
      "grad_norm": 0.15899768471717834,
      "learning_rate": 3.757299270072993e-05,
      "loss": 0.8485,
      "step": 34050
    },
    {
      "epoch": 248.61313868613138,
      "grad_norm": 11.966382026672363,
      "learning_rate": 3.756934306569343e-05,
      "loss": 1.2051,
      "step": 34060
    },
    {
      "epoch": 248.68613138686132,
      "grad_norm": 7.432438850402832,
      "learning_rate": 3.7565693430656934e-05,
      "loss": 1.0394,
      "step": 34070
    },
    {
      "epoch": 248.75912408759123,
      "grad_norm": 15.32882022857666,
      "learning_rate": 3.756204379562044e-05,
      "loss": 1.059,
      "step": 34080
    },
    {
      "epoch": 248.83211678832117,
      "grad_norm": 12.62569808959961,
      "learning_rate": 3.755839416058394e-05,
      "loss": 1.0875,
      "step": 34090
    },
    {
      "epoch": 248.90510948905109,
      "grad_norm": 14.33763599395752,
      "learning_rate": 3.755474452554745e-05,
      "loss": 1.1524,
      "step": 34100
    },
    {
      "epoch": 248.97810218978103,
      "grad_norm": 13.283294677734375,
      "learning_rate": 3.755109489051095e-05,
      "loss": 1.0864,
      "step": 34110
    },
    {
      "epoch": 249.05109489051094,
      "grad_norm": 11.741842269897461,
      "learning_rate": 3.7547445255474456e-05,
      "loss": 0.6854,
      "step": 34120
    },
    {
      "epoch": 249.12408759124088,
      "grad_norm": 17.88593101501465,
      "learning_rate": 3.7543795620437964e-05,
      "loss": 1.0158,
      "step": 34130
    },
    {
      "epoch": 249.1970802919708,
      "grad_norm": 0.41438961029052734,
      "learning_rate": 3.754014598540146e-05,
      "loss": 1.0573,
      "step": 34140
    },
    {
      "epoch": 249.27007299270073,
      "grad_norm": 10.92777156829834,
      "learning_rate": 3.7536496350364965e-05,
      "loss": 0.9586,
      "step": 34150
    },
    {
      "epoch": 249.34306569343065,
      "grad_norm": 1.4506977796554565,
      "learning_rate": 3.7532846715328465e-05,
      "loss": 0.8474,
      "step": 34160
    },
    {
      "epoch": 249.4160583941606,
      "grad_norm": 7.24778413772583,
      "learning_rate": 3.752919708029197e-05,
      "loss": 1.1818,
      "step": 34170
    },
    {
      "epoch": 249.4890510948905,
      "grad_norm": 11.489816665649414,
      "learning_rate": 3.752554744525548e-05,
      "loss": 1.3554,
      "step": 34180
    },
    {
      "epoch": 249.56204379562044,
      "grad_norm": 8.34181022644043,
      "learning_rate": 3.752189781021898e-05,
      "loss": 1.0269,
      "step": 34190
    },
    {
      "epoch": 249.63503649635035,
      "grad_norm": 0.6002253293991089,
      "learning_rate": 3.751824817518249e-05,
      "loss": 1.0069,
      "step": 34200
    },
    {
      "epoch": 249.7080291970803,
      "grad_norm": 3.1419167518615723,
      "learning_rate": 3.751459854014599e-05,
      "loss": 0.5681,
      "step": 34210
    },
    {
      "epoch": 249.7810218978102,
      "grad_norm": 13.735396385192871,
      "learning_rate": 3.751094890510949e-05,
      "loss": 1.1111,
      "step": 34220
    },
    {
      "epoch": 249.85401459854015,
      "grad_norm": 11.202268600463867,
      "learning_rate": 3.7507299270072996e-05,
      "loss": 0.811,
      "step": 34230
    },
    {
      "epoch": 249.92700729927006,
      "grad_norm": 15.672788619995117,
      "learning_rate": 3.7503649635036496e-05,
      "loss": 1.2062,
      "step": 34240
    },
    {
      "epoch": 250.0,
      "grad_norm": 28.816957473754883,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.922,
      "step": 34250
    },
    {
      "epoch": 250.07299270072994,
      "grad_norm": 10.3966646194458,
      "learning_rate": 3.7496350364963504e-05,
      "loss": 0.8713,
      "step": 34260
    },
    {
      "epoch": 250.14598540145985,
      "grad_norm": 5.94020938873291,
      "learning_rate": 3.749270072992701e-05,
      "loss": 0.6689,
      "step": 34270
    },
    {
      "epoch": 250.2189781021898,
      "grad_norm": 20.214584350585938,
      "learning_rate": 3.748905109489051e-05,
      "loss": 1.1447,
      "step": 34280
    },
    {
      "epoch": 250.2919708029197,
      "grad_norm": 12.986705780029297,
      "learning_rate": 3.748540145985401e-05,
      "loss": 1.3443,
      "step": 34290
    },
    {
      "epoch": 250.36496350364965,
      "grad_norm": 1.9064937829971313,
      "learning_rate": 3.748175182481752e-05,
      "loss": 0.642,
      "step": 34300
    },
    {
      "epoch": 250.43795620437956,
      "grad_norm": 0.326505184173584,
      "learning_rate": 3.747810218978102e-05,
      "loss": 0.9726,
      "step": 34310
    },
    {
      "epoch": 250.5109489051095,
      "grad_norm": 4.372950553894043,
      "learning_rate": 3.747445255474453e-05,
      "loss": 0.8658,
      "step": 34320
    },
    {
      "epoch": 250.5839416058394,
      "grad_norm": 9.48953628540039,
      "learning_rate": 3.7470802919708035e-05,
      "loss": 0.7659,
      "step": 34330
    },
    {
      "epoch": 250.65693430656935,
      "grad_norm": 7.790646553039551,
      "learning_rate": 3.7467153284671535e-05,
      "loss": 1.2586,
      "step": 34340
    },
    {
      "epoch": 250.72992700729927,
      "grad_norm": 8.13029670715332,
      "learning_rate": 3.746350364963504e-05,
      "loss": 1.0886,
      "step": 34350
    },
    {
      "epoch": 250.8029197080292,
      "grad_norm": 9.956730842590332,
      "learning_rate": 3.745985401459854e-05,
      "loss": 1.1978,
      "step": 34360
    },
    {
      "epoch": 250.87591240875912,
      "grad_norm": 7.1684417724609375,
      "learning_rate": 3.745620437956204e-05,
      "loss": 1.1287,
      "step": 34370
    },
    {
      "epoch": 250.94890510948906,
      "grad_norm": 8.099974632263184,
      "learning_rate": 3.745255474452555e-05,
      "loss": 0.7365,
      "step": 34380
    },
    {
      "epoch": 251.02189781021897,
      "grad_norm": 8.234679222106934,
      "learning_rate": 3.744890510948905e-05,
      "loss": 1.0106,
      "step": 34390
    },
    {
      "epoch": 251.09489051094891,
      "grad_norm": 15.012931823730469,
      "learning_rate": 3.744525547445256e-05,
      "loss": 0.5429,
      "step": 34400
    },
    {
      "epoch": 251.16788321167883,
      "grad_norm": 10.549967765808105,
      "learning_rate": 3.744160583941606e-05,
      "loss": 0.926,
      "step": 34410
    },
    {
      "epoch": 251.24087591240877,
      "grad_norm": 15.109740257263184,
      "learning_rate": 3.7437956204379566e-05,
      "loss": 1.1982,
      "step": 34420
    },
    {
      "epoch": 251.31386861313868,
      "grad_norm": 6.160704612731934,
      "learning_rate": 3.7434306569343067e-05,
      "loss": 0.774,
      "step": 34430
    },
    {
      "epoch": 251.38686131386862,
      "grad_norm": 0.26532599329948425,
      "learning_rate": 3.7430656934306574e-05,
      "loss": 0.9094,
      "step": 34440
    },
    {
      "epoch": 251.45985401459853,
      "grad_norm": 0.49755412340164185,
      "learning_rate": 3.7427007299270074e-05,
      "loss": 1.1678,
      "step": 34450
    },
    {
      "epoch": 251.53284671532847,
      "grad_norm": 6.813625335693359,
      "learning_rate": 3.7423357664233575e-05,
      "loss": 1.2161,
      "step": 34460
    },
    {
      "epoch": 251.6058394160584,
      "grad_norm": 7.192750453948975,
      "learning_rate": 3.741970802919708e-05,
      "loss": 0.7312,
      "step": 34470
    },
    {
      "epoch": 251.67883211678833,
      "grad_norm": 6.62999963760376,
      "learning_rate": 3.741605839416058e-05,
      "loss": 1.2895,
      "step": 34480
    },
    {
      "epoch": 251.75182481751824,
      "grad_norm": 9.953713417053223,
      "learning_rate": 3.741240875912409e-05,
      "loss": 1.4019,
      "step": 34490
    },
    {
      "epoch": 251.82481751824818,
      "grad_norm": 10.512203216552734,
      "learning_rate": 3.74087591240876e-05,
      "loss": 0.9624,
      "step": 34500
    },
    {
      "epoch": 251.8978102189781,
      "grad_norm": 5.09022331237793,
      "learning_rate": 3.74051094890511e-05,
      "loss": 1.0858,
      "step": 34510
    },
    {
      "epoch": 251.97080291970804,
      "grad_norm": 0.4320356249809265,
      "learning_rate": 3.74014598540146e-05,
      "loss": 0.5878,
      "step": 34520
    },
    {
      "epoch": 252.04379562043795,
      "grad_norm": 10.386667251586914,
      "learning_rate": 3.7397810218978105e-05,
      "loss": 1.1793,
      "step": 34530
    },
    {
      "epoch": 252.1167883211679,
      "grad_norm": 1.0486019849777222,
      "learning_rate": 3.7394160583941606e-05,
      "loss": 0.7595,
      "step": 34540
    },
    {
      "epoch": 252.1897810218978,
      "grad_norm": 0.6698530912399292,
      "learning_rate": 3.739051094890511e-05,
      "loss": 0.759,
      "step": 34550
    },
    {
      "epoch": 252.26277372262774,
      "grad_norm": 6.361825466156006,
      "learning_rate": 3.7386861313868614e-05,
      "loss": 0.7771,
      "step": 34560
    },
    {
      "epoch": 252.33576642335765,
      "grad_norm": 7.4479594230651855,
      "learning_rate": 3.738321167883212e-05,
      "loss": 0.9124,
      "step": 34570
    },
    {
      "epoch": 252.4087591240876,
      "grad_norm": 0.6638984680175781,
      "learning_rate": 3.737956204379562e-05,
      "loss": 0.8975,
      "step": 34580
    },
    {
      "epoch": 252.4817518248175,
      "grad_norm": 13.113160133361816,
      "learning_rate": 3.737591240875913e-05,
      "loss": 0.8441,
      "step": 34590
    },
    {
      "epoch": 252.55474452554745,
      "grad_norm": 9.409289360046387,
      "learning_rate": 3.737226277372263e-05,
      "loss": 1.3286,
      "step": 34600
    },
    {
      "epoch": 252.62773722627736,
      "grad_norm": 4.458404064178467,
      "learning_rate": 3.736861313868613e-05,
      "loss": 1.0246,
      "step": 34610
    },
    {
      "epoch": 252.7007299270073,
      "grad_norm": 7.454993724822998,
      "learning_rate": 3.736496350364964e-05,
      "loss": 0.9472,
      "step": 34620
    },
    {
      "epoch": 252.77372262773721,
      "grad_norm": 7.363403797149658,
      "learning_rate": 3.736131386861314e-05,
      "loss": 1.2213,
      "step": 34630
    },
    {
      "epoch": 252.84671532846716,
      "grad_norm": 9.013558387756348,
      "learning_rate": 3.7357664233576645e-05,
      "loss": 1.0466,
      "step": 34640
    },
    {
      "epoch": 252.91970802919707,
      "grad_norm": 5.8525776863098145,
      "learning_rate": 3.735401459854015e-05,
      "loss": 0.8588,
      "step": 34650
    },
    {
      "epoch": 252.992700729927,
      "grad_norm": 13.953075408935547,
      "learning_rate": 3.735036496350365e-05,
      "loss": 1.1111,
      "step": 34660
    },
    {
      "epoch": 253.06569343065692,
      "grad_norm": 2.0770294666290283,
      "learning_rate": 3.734671532846716e-05,
      "loss": 0.898,
      "step": 34670
    },
    {
      "epoch": 253.13868613138686,
      "grad_norm": 9.774271011352539,
      "learning_rate": 3.7343065693430653e-05,
      "loss": 0.906,
      "step": 34680
    },
    {
      "epoch": 253.21167883211677,
      "grad_norm": 0.47269806265830994,
      "learning_rate": 3.733941605839416e-05,
      "loss": 0.7452,
      "step": 34690
    },
    {
      "epoch": 253.28467153284672,
      "grad_norm": 6.537405967712402,
      "learning_rate": 3.733576642335767e-05,
      "loss": 0.9919,
      "step": 34700
    },
    {
      "epoch": 253.35766423357666,
      "grad_norm": 0.749965250492096,
      "learning_rate": 3.733211678832117e-05,
      "loss": 1.1489,
      "step": 34710
    },
    {
      "epoch": 253.43065693430657,
      "grad_norm": 14.116777420043945,
      "learning_rate": 3.7328467153284676e-05,
      "loss": 1.4393,
      "step": 34720
    },
    {
      "epoch": 253.5036496350365,
      "grad_norm": 13.355603218078613,
      "learning_rate": 3.7324817518248176e-05,
      "loss": 0.7853,
      "step": 34730
    },
    {
      "epoch": 253.57664233576642,
      "grad_norm": 13.43594741821289,
      "learning_rate": 3.7321167883211684e-05,
      "loss": 1.346,
      "step": 34740
    },
    {
      "epoch": 253.64963503649636,
      "grad_norm": 12.55128288269043,
      "learning_rate": 3.7317518248175184e-05,
      "loss": 1.0922,
      "step": 34750
    },
    {
      "epoch": 253.72262773722628,
      "grad_norm": 9.551806449890137,
      "learning_rate": 3.7313868613138685e-05,
      "loss": 0.76,
      "step": 34760
    },
    {
      "epoch": 253.79562043795622,
      "grad_norm": 14.304665565490723,
      "learning_rate": 3.731021897810219e-05,
      "loss": 0.896,
      "step": 34770
    },
    {
      "epoch": 253.86861313868613,
      "grad_norm": 8.248283386230469,
      "learning_rate": 3.730656934306569e-05,
      "loss": 1.0178,
      "step": 34780
    },
    {
      "epoch": 253.94160583941607,
      "grad_norm": 22.14232635498047,
      "learning_rate": 3.73029197080292e-05,
      "loss": 0.7713,
      "step": 34790
    },
    {
      "epoch": 254.01459854014598,
      "grad_norm": 11.376960754394531,
      "learning_rate": 3.729927007299271e-05,
      "loss": 1.1209,
      "step": 34800
    },
    {
      "epoch": 254.08759124087592,
      "grad_norm": 7.577733516693115,
      "learning_rate": 3.729562043795621e-05,
      "loss": 0.7861,
      "step": 34810
    },
    {
      "epoch": 254.16058394160584,
      "grad_norm": 8.926843643188477,
      "learning_rate": 3.7291970802919715e-05,
      "loss": 1.0556,
      "step": 34820
    },
    {
      "epoch": 254.23357664233578,
      "grad_norm": 11.190763473510742,
      "learning_rate": 3.728832116788321e-05,
      "loss": 0.8116,
      "step": 34830
    },
    {
      "epoch": 254.3065693430657,
      "grad_norm": 12.5968017578125,
      "learning_rate": 3.7284671532846716e-05,
      "loss": 1.0857,
      "step": 34840
    },
    {
      "epoch": 254.37956204379563,
      "grad_norm": 8.75607967376709,
      "learning_rate": 3.728102189781022e-05,
      "loss": 1.2738,
      "step": 34850
    },
    {
      "epoch": 254.45255474452554,
      "grad_norm": 9.477681159973145,
      "learning_rate": 3.727737226277372e-05,
      "loss": 0.7895,
      "step": 34860
    },
    {
      "epoch": 254.52554744525548,
      "grad_norm": 11.024638175964355,
      "learning_rate": 3.727372262773723e-05,
      "loss": 1.2946,
      "step": 34870
    },
    {
      "epoch": 254.5985401459854,
      "grad_norm": 8.972151756286621,
      "learning_rate": 3.727007299270073e-05,
      "loss": 1.2394,
      "step": 34880
    },
    {
      "epoch": 254.67153284671534,
      "grad_norm": 7.2028703689575195,
      "learning_rate": 3.726642335766424e-05,
      "loss": 0.666,
      "step": 34890
    },
    {
      "epoch": 254.74452554744525,
      "grad_norm": 12.144003868103027,
      "learning_rate": 3.726277372262774e-05,
      "loss": 1.3904,
      "step": 34900
    },
    {
      "epoch": 254.8175182481752,
      "grad_norm": 0.6541233062744141,
      "learning_rate": 3.725912408759124e-05,
      "loss": 0.7755,
      "step": 34910
    },
    {
      "epoch": 254.8905109489051,
      "grad_norm": 6.943643093109131,
      "learning_rate": 3.725547445255475e-05,
      "loss": 0.518,
      "step": 34920
    },
    {
      "epoch": 254.96350364963504,
      "grad_norm": 0.5397148132324219,
      "learning_rate": 3.725182481751825e-05,
      "loss": 1.1238,
      "step": 34930
    },
    {
      "epoch": 255.03649635036496,
      "grad_norm": 0.2529485821723938,
      "learning_rate": 3.7248175182481754e-05,
      "loss": 0.6691,
      "step": 34940
    },
    {
      "epoch": 255.1094890510949,
      "grad_norm": 9.967259407043457,
      "learning_rate": 3.7244525547445255e-05,
      "loss": 0.6341,
      "step": 34950
    },
    {
      "epoch": 255.1824817518248,
      "grad_norm": 0.6307079792022705,
      "learning_rate": 3.724087591240876e-05,
      "loss": 0.8684,
      "step": 34960
    },
    {
      "epoch": 255.25547445255475,
      "grad_norm": 0.3501683473587036,
      "learning_rate": 3.723722627737227e-05,
      "loss": 1.0777,
      "step": 34970
    },
    {
      "epoch": 255.32846715328466,
      "grad_norm": 0.4880191385746002,
      "learning_rate": 3.723357664233576e-05,
      "loss": 0.6101,
      "step": 34980
    },
    {
      "epoch": 255.4014598540146,
      "grad_norm": 10.525609970092773,
      "learning_rate": 3.722992700729927e-05,
      "loss": 1.0712,
      "step": 34990
    },
    {
      "epoch": 255.47445255474452,
      "grad_norm": 8.960329055786133,
      "learning_rate": 3.722627737226278e-05,
      "loss": 1.4784,
      "step": 35000
    },
    {
      "epoch": 255.54744525547446,
      "grad_norm": 7.853755474090576,
      "learning_rate": 3.722262773722628e-05,
      "loss": 1.0976,
      "step": 35010
    },
    {
      "epoch": 255.62043795620437,
      "grad_norm": 11.466877937316895,
      "learning_rate": 3.7218978102189785e-05,
      "loss": 1.0389,
      "step": 35020
    },
    {
      "epoch": 255.6934306569343,
      "grad_norm": 2.2450413703918457,
      "learning_rate": 3.7215328467153286e-05,
      "loss": 0.9051,
      "step": 35030
    },
    {
      "epoch": 255.76642335766422,
      "grad_norm": 10.599787712097168,
      "learning_rate": 3.721167883211679e-05,
      "loss": 1.028,
      "step": 35040
    },
    {
      "epoch": 255.83941605839416,
      "grad_norm": 0.6119278073310852,
      "learning_rate": 3.7208029197080294e-05,
      "loss": 1.0551,
      "step": 35050
    },
    {
      "epoch": 255.91240875912408,
      "grad_norm": 8.780781745910645,
      "learning_rate": 3.7204379562043794e-05,
      "loss": 0.8947,
      "step": 35060
    },
    {
      "epoch": 255.98540145985402,
      "grad_norm": 6.298953056335449,
      "learning_rate": 3.72007299270073e-05,
      "loss": 1.0437,
      "step": 35070
    },
    {
      "epoch": 256.05839416058393,
      "grad_norm": 6.765431880950928,
      "learning_rate": 3.71970802919708e-05,
      "loss": 0.8384,
      "step": 35080
    },
    {
      "epoch": 256.13138686131384,
      "grad_norm": 9.46426010131836,
      "learning_rate": 3.719343065693431e-05,
      "loss": 0.6741,
      "step": 35090
    },
    {
      "epoch": 256.2043795620438,
      "grad_norm": 0.5507437586784363,
      "learning_rate": 3.718978102189781e-05,
      "loss": 0.6865,
      "step": 35100
    },
    {
      "epoch": 256.2773722627737,
      "grad_norm": 15.831633567810059,
      "learning_rate": 3.718613138686132e-05,
      "loss": 0.8214,
      "step": 35110
    },
    {
      "epoch": 256.35036496350364,
      "grad_norm": 8.922972679138184,
      "learning_rate": 3.7182481751824824e-05,
      "loss": 1.0358,
      "step": 35120
    },
    {
      "epoch": 256.42335766423355,
      "grad_norm": 14.223913192749023,
      "learning_rate": 3.717883211678832e-05,
      "loss": 1.0154,
      "step": 35130
    },
    {
      "epoch": 256.4963503649635,
      "grad_norm": 6.765570640563965,
      "learning_rate": 3.7175182481751825e-05,
      "loss": 1.072,
      "step": 35140
    },
    {
      "epoch": 256.56934306569343,
      "grad_norm": 6.886953353881836,
      "learning_rate": 3.7171532846715326e-05,
      "loss": 1.1247,
      "step": 35150
    },
    {
      "epoch": 256.64233576642334,
      "grad_norm": 11.239632606506348,
      "learning_rate": 3.716788321167883e-05,
      "loss": 1.1032,
      "step": 35160
    },
    {
      "epoch": 256.7153284671533,
      "grad_norm": 8.145627975463867,
      "learning_rate": 3.716423357664234e-05,
      "loss": 1.196,
      "step": 35170
    },
    {
      "epoch": 256.7883211678832,
      "grad_norm": 8.673015594482422,
      "learning_rate": 3.716058394160584e-05,
      "loss": 1.1079,
      "step": 35180
    },
    {
      "epoch": 256.86131386861314,
      "grad_norm": 9.786032676696777,
      "learning_rate": 3.715693430656935e-05,
      "loss": 0.8037,
      "step": 35190
    },
    {
      "epoch": 256.93430656934305,
      "grad_norm": 14.845252990722656,
      "learning_rate": 3.715328467153285e-05,
      "loss": 0.6768,
      "step": 35200
    },
    {
      "epoch": 257.007299270073,
      "grad_norm": 10.89473819732666,
      "learning_rate": 3.714963503649635e-05,
      "loss": 1.4402,
      "step": 35210
    },
    {
      "epoch": 257.08029197080293,
      "grad_norm": 12.299745559692383,
      "learning_rate": 3.7145985401459856e-05,
      "loss": 0.9699,
      "step": 35220
    },
    {
      "epoch": 257.15328467153284,
      "grad_norm": 11.096778869628906,
      "learning_rate": 3.714233576642336e-05,
      "loss": 1.2282,
      "step": 35230
    },
    {
      "epoch": 257.22627737226276,
      "grad_norm": 12.09681224822998,
      "learning_rate": 3.7138686131386864e-05,
      "loss": 0.648,
      "step": 35240
    },
    {
      "epoch": 257.2992700729927,
      "grad_norm": 8.978696823120117,
      "learning_rate": 3.7135036496350365e-05,
      "loss": 1.2151,
      "step": 35250
    },
    {
      "epoch": 257.37226277372264,
      "grad_norm": 11.89455795288086,
      "learning_rate": 3.713138686131387e-05,
      "loss": 0.8583,
      "step": 35260
    },
    {
      "epoch": 257.44525547445255,
      "grad_norm": 0.675910234451294,
      "learning_rate": 3.712773722627738e-05,
      "loss": 0.8246,
      "step": 35270
    },
    {
      "epoch": 257.51824817518246,
      "grad_norm": 12.338778495788574,
      "learning_rate": 3.712408759124088e-05,
      "loss": 0.5991,
      "step": 35280
    },
    {
      "epoch": 257.59124087591243,
      "grad_norm": 1.821115493774414,
      "learning_rate": 3.712043795620438e-05,
      "loss": 0.9613,
      "step": 35290
    },
    {
      "epoch": 257.66423357664235,
      "grad_norm": 10.806930541992188,
      "learning_rate": 3.711678832116788e-05,
      "loss": 0.9585,
      "step": 35300
    },
    {
      "epoch": 257.73722627737226,
      "grad_norm": 0.6648596525192261,
      "learning_rate": 3.711313868613139e-05,
      "loss": 1.2326,
      "step": 35310
    },
    {
      "epoch": 257.81021897810217,
      "grad_norm": 0.44476476311683655,
      "learning_rate": 3.7109489051094895e-05,
      "loss": 1.1665,
      "step": 35320
    },
    {
      "epoch": 257.88321167883214,
      "grad_norm": 0.46605193614959717,
      "learning_rate": 3.7105839416058396e-05,
      "loss": 0.3812,
      "step": 35330
    },
    {
      "epoch": 257.95620437956205,
      "grad_norm": 11.456670761108398,
      "learning_rate": 3.71021897810219e-05,
      "loss": 1.2378,
      "step": 35340
    },
    {
      "epoch": 258.02919708029196,
      "grad_norm": 16.23848533630371,
      "learning_rate": 3.7098540145985403e-05,
      "loss": 1.154,
      "step": 35350
    },
    {
      "epoch": 258.1021897810219,
      "grad_norm": 0.7437942624092102,
      "learning_rate": 3.7094890510948904e-05,
      "loss": 1.0282,
      "step": 35360
    },
    {
      "epoch": 258.17518248175185,
      "grad_norm": 15.464971542358398,
      "learning_rate": 3.709124087591241e-05,
      "loss": 0.9519,
      "step": 35370
    },
    {
      "epoch": 258.24817518248176,
      "grad_norm": 8.398432731628418,
      "learning_rate": 3.708759124087591e-05,
      "loss": 1.0002,
      "step": 35380
    },
    {
      "epoch": 258.3211678832117,
      "grad_norm": 6.770168781280518,
      "learning_rate": 3.708394160583942e-05,
      "loss": 0.8709,
      "step": 35390
    },
    {
      "epoch": 258.3941605839416,
      "grad_norm": 13.283463478088379,
      "learning_rate": 3.708029197080292e-05,
      "loss": 0.9101,
      "step": 35400
    },
    {
      "epoch": 258.46715328467155,
      "grad_norm": 10.127320289611816,
      "learning_rate": 3.707664233576643e-05,
      "loss": 0.978,
      "step": 35410
    },
    {
      "epoch": 258.54014598540147,
      "grad_norm": 12.44628620147705,
      "learning_rate": 3.707299270072993e-05,
      "loss": 1.1276,
      "step": 35420
    },
    {
      "epoch": 258.6131386861314,
      "grad_norm": 2.53517746925354,
      "learning_rate": 3.7069343065693435e-05,
      "loss": 0.6765,
      "step": 35430
    },
    {
      "epoch": 258.6861313868613,
      "grad_norm": 7.354858875274658,
      "learning_rate": 3.7065693430656935e-05,
      "loss": 0.9236,
      "step": 35440
    },
    {
      "epoch": 258.75912408759126,
      "grad_norm": 16.42146873474121,
      "learning_rate": 3.7062043795620436e-05,
      "loss": 1.3288,
      "step": 35450
    },
    {
      "epoch": 258.8321167883212,
      "grad_norm": 9.941300392150879,
      "learning_rate": 3.705839416058394e-05,
      "loss": 1.0192,
      "step": 35460
    },
    {
      "epoch": 258.9051094890511,
      "grad_norm": 9.27319049835205,
      "learning_rate": 3.705474452554745e-05,
      "loss": 0.8065,
      "step": 35470
    },
    {
      "epoch": 258.978102189781,
      "grad_norm": 10.172263145446777,
      "learning_rate": 3.705109489051095e-05,
      "loss": 1.0432,
      "step": 35480
    },
    {
      "epoch": 259.05109489051097,
      "grad_norm": 9.96863079071045,
      "learning_rate": 3.704744525547446e-05,
      "loss": 1.3775,
      "step": 35490
    },
    {
      "epoch": 259.1240875912409,
      "grad_norm": 0.5263703465461731,
      "learning_rate": 3.704379562043796e-05,
      "loss": 1.1047,
      "step": 35500
    },
    {
      "epoch": 259.1970802919708,
      "grad_norm": 6.576995372772217,
      "learning_rate": 3.7040145985401466e-05,
      "loss": 1.056,
      "step": 35510
    },
    {
      "epoch": 259.2700729927007,
      "grad_norm": 7.954487323760986,
      "learning_rate": 3.7036496350364966e-05,
      "loss": 1.1791,
      "step": 35520
    },
    {
      "epoch": 259.3430656934307,
      "grad_norm": 15.065089225769043,
      "learning_rate": 3.7032846715328467e-05,
      "loss": 0.5989,
      "step": 35530
    },
    {
      "epoch": 259.4160583941606,
      "grad_norm": 6.394049644470215,
      "learning_rate": 3.7029197080291974e-05,
      "loss": 0.7771,
      "step": 35540
    },
    {
      "epoch": 259.4890510948905,
      "grad_norm": 11.186970710754395,
      "learning_rate": 3.7025547445255474e-05,
      "loss": 0.867,
      "step": 35550
    },
    {
      "epoch": 259.5620437956204,
      "grad_norm": 0.2610945701599121,
      "learning_rate": 3.702189781021898e-05,
      "loss": 1.0208,
      "step": 35560
    },
    {
      "epoch": 259.6350364963504,
      "grad_norm": 0.597703218460083,
      "learning_rate": 3.701824817518248e-05,
      "loss": 1.088,
      "step": 35570
    },
    {
      "epoch": 259.7080291970803,
      "grad_norm": 5.2255048751831055,
      "learning_rate": 3.701459854014599e-05,
      "loss": 1.1099,
      "step": 35580
    },
    {
      "epoch": 259.7810218978102,
      "grad_norm": 2.201021194458008,
      "learning_rate": 3.701094890510949e-05,
      "loss": 1.007,
      "step": 35590
    },
    {
      "epoch": 259.8540145985401,
      "grad_norm": 11.136722564697266,
      "learning_rate": 3.700729927007299e-05,
      "loss": 1.1713,
      "step": 35600
    },
    {
      "epoch": 259.9270072992701,
      "grad_norm": 13.352477073669434,
      "learning_rate": 3.70036496350365e-05,
      "loss": 0.6977,
      "step": 35610
    },
    {
      "epoch": 260.0,
      "grad_norm": 0.34297648072242737,
      "learning_rate": 3.7e-05,
      "loss": 0.9481,
      "step": 35620
    },
    {
      "epoch": 260.0729927007299,
      "grad_norm": 9.323821067810059,
      "learning_rate": 3.6996350364963505e-05,
      "loss": 1.4884,
      "step": 35630
    },
    {
      "epoch": 260.1459854014599,
      "grad_norm": 11.744949340820312,
      "learning_rate": 3.699270072992701e-05,
      "loss": 0.8351,
      "step": 35640
    },
    {
      "epoch": 260.2189781021898,
      "grad_norm": 10.530327796936035,
      "learning_rate": 3.698905109489051e-05,
      "loss": 1.3525,
      "step": 35650
    },
    {
      "epoch": 260.2919708029197,
      "grad_norm": 11.315234184265137,
      "learning_rate": 3.698540145985402e-05,
      "loss": 0.962,
      "step": 35660
    },
    {
      "epoch": 260.3649635036496,
      "grad_norm": 5.229686737060547,
      "learning_rate": 3.698175182481752e-05,
      "loss": 1.2108,
      "step": 35670
    },
    {
      "epoch": 260.4379562043796,
      "grad_norm": 9.390719413757324,
      "learning_rate": 3.697810218978102e-05,
      "loss": 0.7418,
      "step": 35680
    },
    {
      "epoch": 260.5109489051095,
      "grad_norm": 8.04962158203125,
      "learning_rate": 3.697445255474453e-05,
      "loss": 0.9143,
      "step": 35690
    },
    {
      "epoch": 260.5839416058394,
      "grad_norm": 8.186067581176758,
      "learning_rate": 3.697080291970803e-05,
      "loss": 0.8478,
      "step": 35700
    },
    {
      "epoch": 260.6569343065693,
      "grad_norm": 0.4821707308292389,
      "learning_rate": 3.6967153284671536e-05,
      "loss": 0.808,
      "step": 35710
    },
    {
      "epoch": 260.7299270072993,
      "grad_norm": 11.121946334838867,
      "learning_rate": 3.696350364963504e-05,
      "loss": 1.1298,
      "step": 35720
    },
    {
      "epoch": 260.8029197080292,
      "grad_norm": 7.530833721160889,
      "learning_rate": 3.6959854014598544e-05,
      "loss": 0.8208,
      "step": 35730
    },
    {
      "epoch": 260.8759124087591,
      "grad_norm": 8.035229682922363,
      "learning_rate": 3.695620437956205e-05,
      "loss": 0.9364,
      "step": 35740
    },
    {
      "epoch": 260.94890510948903,
      "grad_norm": 16.30998420715332,
      "learning_rate": 3.6952554744525545e-05,
      "loss": 0.8364,
      "step": 35750
    },
    {
      "epoch": 261.021897810219,
      "grad_norm": 0.5464495420455933,
      "learning_rate": 3.694890510948905e-05,
      "loss": 0.6485,
      "step": 35760
    },
    {
      "epoch": 261.0948905109489,
      "grad_norm": 2.0912935733795166,
      "learning_rate": 3.694525547445255e-05,
      "loss": 0.7165,
      "step": 35770
    },
    {
      "epoch": 261.1678832116788,
      "grad_norm": 9.654708862304688,
      "learning_rate": 3.694160583941606e-05,
      "loss": 1.1395,
      "step": 35780
    },
    {
      "epoch": 261.24087591240874,
      "grad_norm": 8.667320251464844,
      "learning_rate": 3.693795620437957e-05,
      "loss": 0.8879,
      "step": 35790
    },
    {
      "epoch": 261.3138686131387,
      "grad_norm": 17.594921112060547,
      "learning_rate": 3.693430656934307e-05,
      "loss": 1.5327,
      "step": 35800
    },
    {
      "epoch": 261.3868613138686,
      "grad_norm": 20.3114070892334,
      "learning_rate": 3.6930656934306575e-05,
      "loss": 0.9851,
      "step": 35810
    },
    {
      "epoch": 261.45985401459853,
      "grad_norm": 11.879814147949219,
      "learning_rate": 3.692700729927007e-05,
      "loss": 0.919,
      "step": 35820
    },
    {
      "epoch": 261.53284671532845,
      "grad_norm": 9.280314445495605,
      "learning_rate": 3.6923357664233576e-05,
      "loss": 0.9958,
      "step": 35830
    },
    {
      "epoch": 261.6058394160584,
      "grad_norm": 15.019655227661133,
      "learning_rate": 3.6919708029197084e-05,
      "loss": 1.0496,
      "step": 35840
    },
    {
      "epoch": 261.6788321167883,
      "grad_norm": 7.873799800872803,
      "learning_rate": 3.6916058394160584e-05,
      "loss": 0.865,
      "step": 35850
    },
    {
      "epoch": 261.75182481751824,
      "grad_norm": 2.353956699371338,
      "learning_rate": 3.691240875912409e-05,
      "loss": 0.9056,
      "step": 35860
    },
    {
      "epoch": 261.82481751824815,
      "grad_norm": 12.224845886230469,
      "learning_rate": 3.690875912408759e-05,
      "loss": 1.0769,
      "step": 35870
    },
    {
      "epoch": 261.8978102189781,
      "grad_norm": 0.3778988718986511,
      "learning_rate": 3.69051094890511e-05,
      "loss": 0.4786,
      "step": 35880
    },
    {
      "epoch": 261.97080291970804,
      "grad_norm": 12.869728088378906,
      "learning_rate": 3.69014598540146e-05,
      "loss": 0.9824,
      "step": 35890
    },
    {
      "epoch": 262.04379562043795,
      "grad_norm": 7.931936264038086,
      "learning_rate": 3.68978102189781e-05,
      "loss": 1.2909,
      "step": 35900
    },
    {
      "epoch": 262.11678832116786,
      "grad_norm": 8.316325187683105,
      "learning_rate": 3.689416058394161e-05,
      "loss": 0.8349,
      "step": 35910
    },
    {
      "epoch": 262.18978102189783,
      "grad_norm": 11.714238166809082,
      "learning_rate": 3.689051094890511e-05,
      "loss": 0.9621,
      "step": 35920
    },
    {
      "epoch": 262.26277372262774,
      "grad_norm": 3.797781229019165,
      "learning_rate": 3.6886861313868615e-05,
      "loss": 0.9067,
      "step": 35930
    },
    {
      "epoch": 262.33576642335765,
      "grad_norm": 6.92357873916626,
      "learning_rate": 3.688321167883212e-05,
      "loss": 0.7551,
      "step": 35940
    },
    {
      "epoch": 262.40875912408757,
      "grad_norm": 13.813665390014648,
      "learning_rate": 3.687956204379562e-05,
      "loss": 1.0054,
      "step": 35950
    },
    {
      "epoch": 262.48175182481754,
      "grad_norm": 0.45049166679382324,
      "learning_rate": 3.687591240875913e-05,
      "loss": 1.0479,
      "step": 35960
    },
    {
      "epoch": 262.55474452554745,
      "grad_norm": 8.164660453796387,
      "learning_rate": 3.687226277372263e-05,
      "loss": 0.9704,
      "step": 35970
    },
    {
      "epoch": 262.62773722627736,
      "grad_norm": 14.593470573425293,
      "learning_rate": 3.686861313868613e-05,
      "loss": 1.0469,
      "step": 35980
    },
    {
      "epoch": 262.7007299270073,
      "grad_norm": 14.249715805053711,
      "learning_rate": 3.686496350364964e-05,
      "loss": 1.093,
      "step": 35990
    },
    {
      "epoch": 262.77372262773724,
      "grad_norm": 9.374923706054688,
      "learning_rate": 3.686131386861314e-05,
      "loss": 1.0523,
      "step": 36000
    },
    {
      "epoch": 262.84671532846716,
      "grad_norm": 8.043811798095703,
      "learning_rate": 3.6857664233576646e-05,
      "loss": 1.2498,
      "step": 36010
    },
    {
      "epoch": 262.91970802919707,
      "grad_norm": 6.237432956695557,
      "learning_rate": 3.685401459854015e-05,
      "loss": 0.6162,
      "step": 36020
    },
    {
      "epoch": 262.992700729927,
      "grad_norm": 1.0345113277435303,
      "learning_rate": 3.6850364963503654e-05,
      "loss": 1.364,
      "step": 36030
    },
    {
      "epoch": 263.06569343065695,
      "grad_norm": 9.751803398132324,
      "learning_rate": 3.6846715328467154e-05,
      "loss": 0.9298,
      "step": 36040
    },
    {
      "epoch": 263.13868613138686,
      "grad_norm": 7.5584893226623535,
      "learning_rate": 3.6843065693430655e-05,
      "loss": 1.0139,
      "step": 36050
    },
    {
      "epoch": 263.2116788321168,
      "grad_norm": 7.017646789550781,
      "learning_rate": 3.683941605839416e-05,
      "loss": 0.9433,
      "step": 36060
    },
    {
      "epoch": 263.2846715328467,
      "grad_norm": 8.396591186523438,
      "learning_rate": 3.683576642335766e-05,
      "loss": 0.9031,
      "step": 36070
    },
    {
      "epoch": 263.35766423357666,
      "grad_norm": 0.7129418849945068,
      "learning_rate": 3.683211678832117e-05,
      "loss": 0.9582,
      "step": 36080
    },
    {
      "epoch": 263.43065693430657,
      "grad_norm": 5.717674732208252,
      "learning_rate": 3.682846715328467e-05,
      "loss": 0.5702,
      "step": 36090
    },
    {
      "epoch": 263.5036496350365,
      "grad_norm": 7.638621807098389,
      "learning_rate": 3.682481751824818e-05,
      "loss": 1.3245,
      "step": 36100
    },
    {
      "epoch": 263.57664233576645,
      "grad_norm": 14.875577926635742,
      "learning_rate": 3.6821167883211685e-05,
      "loss": 0.9591,
      "step": 36110
    },
    {
      "epoch": 263.64963503649636,
      "grad_norm": 0.9931161403656006,
      "learning_rate": 3.6817518248175185e-05,
      "loss": 0.6786,
      "step": 36120
    },
    {
      "epoch": 263.7226277372263,
      "grad_norm": 15.688556671142578,
      "learning_rate": 3.6813868613138686e-05,
      "loss": 1.2971,
      "step": 36130
    },
    {
      "epoch": 263.7956204379562,
      "grad_norm": 10.064343452453613,
      "learning_rate": 3.681021897810219e-05,
      "loss": 0.83,
      "step": 36140
    },
    {
      "epoch": 263.86861313868616,
      "grad_norm": 10.524456977844238,
      "learning_rate": 3.6806569343065694e-05,
      "loss": 0.7861,
      "step": 36150
    },
    {
      "epoch": 263.94160583941607,
      "grad_norm": 7.7830891609191895,
      "learning_rate": 3.68029197080292e-05,
      "loss": 1.2972,
      "step": 36160
    },
    {
      "epoch": 264.014598540146,
      "grad_norm": 5.713804721832275,
      "learning_rate": 3.67992700729927e-05,
      "loss": 1.3745,
      "step": 36170
    },
    {
      "epoch": 264.0875912408759,
      "grad_norm": 10.239108085632324,
      "learning_rate": 3.679562043795621e-05,
      "loss": 0.9491,
      "step": 36180
    },
    {
      "epoch": 264.16058394160586,
      "grad_norm": 6.980926513671875,
      "learning_rate": 3.679197080291971e-05,
      "loss": 1.2098,
      "step": 36190
    },
    {
      "epoch": 264.2335766423358,
      "grad_norm": 0.3221891522407532,
      "learning_rate": 3.678832116788321e-05,
      "loss": 0.8636,
      "step": 36200
    },
    {
      "epoch": 264.3065693430657,
      "grad_norm": 8.03815746307373,
      "learning_rate": 3.678467153284672e-05,
      "loss": 1.2235,
      "step": 36210
    },
    {
      "epoch": 264.3795620437956,
      "grad_norm": 8.008033752441406,
      "learning_rate": 3.678102189781022e-05,
      "loss": 1.0125,
      "step": 36220
    },
    {
      "epoch": 264.45255474452557,
      "grad_norm": 0.3390915095806122,
      "learning_rate": 3.6777372262773725e-05,
      "loss": 0.962,
      "step": 36230
    },
    {
      "epoch": 264.5255474452555,
      "grad_norm": 12.411471366882324,
      "learning_rate": 3.6773722627737225e-05,
      "loss": 1.6306,
      "step": 36240
    },
    {
      "epoch": 264.5985401459854,
      "grad_norm": 7.958985328674316,
      "learning_rate": 3.677007299270073e-05,
      "loss": 1.1857,
      "step": 36250
    },
    {
      "epoch": 264.6715328467153,
      "grad_norm": 7.911508560180664,
      "learning_rate": 3.676642335766424e-05,
      "loss": 1.0619,
      "step": 36260
    },
    {
      "epoch": 264.7445255474453,
      "grad_norm": 0.36341845989227295,
      "learning_rate": 3.676277372262774e-05,
      "loss": 0.6183,
      "step": 36270
    },
    {
      "epoch": 264.8175182481752,
      "grad_norm": 10.454692840576172,
      "learning_rate": 3.675912408759124e-05,
      "loss": 0.8189,
      "step": 36280
    },
    {
      "epoch": 264.8905109489051,
      "grad_norm": 9.970671653747559,
      "learning_rate": 3.675547445255474e-05,
      "loss": 0.8905,
      "step": 36290
    },
    {
      "epoch": 264.963503649635,
      "grad_norm": 10.136560440063477,
      "learning_rate": 3.675182481751825e-05,
      "loss": 0.9053,
      "step": 36300
    },
    {
      "epoch": 265.036496350365,
      "grad_norm": 11.83752727508545,
      "learning_rate": 3.6748175182481756e-05,
      "loss": 0.736,
      "step": 36310
    },
    {
      "epoch": 265.1094890510949,
      "grad_norm": 14.94119930267334,
      "learning_rate": 3.6744525547445256e-05,
      "loss": 0.8437,
      "step": 36320
    },
    {
      "epoch": 265.1824817518248,
      "grad_norm": 11.451570510864258,
      "learning_rate": 3.6740875912408764e-05,
      "loss": 1.1827,
      "step": 36330
    },
    {
      "epoch": 265.2554744525547,
      "grad_norm": 7.585148334503174,
      "learning_rate": 3.6737226277372264e-05,
      "loss": 0.9153,
      "step": 36340
    },
    {
      "epoch": 265.3284671532847,
      "grad_norm": 5.814395904541016,
      "learning_rate": 3.673357664233577e-05,
      "loss": 1.0927,
      "step": 36350
    },
    {
      "epoch": 265.4014598540146,
      "grad_norm": 9.637008666992188,
      "learning_rate": 3.672992700729927e-05,
      "loss": 0.7271,
      "step": 36360
    },
    {
      "epoch": 265.4744525547445,
      "grad_norm": 17.407787322998047,
      "learning_rate": 3.672627737226277e-05,
      "loss": 1.1729,
      "step": 36370
    },
    {
      "epoch": 265.54744525547443,
      "grad_norm": 8.394237518310547,
      "learning_rate": 3.672262773722628e-05,
      "loss": 0.8446,
      "step": 36380
    },
    {
      "epoch": 265.6204379562044,
      "grad_norm": 12.99605941772461,
      "learning_rate": 3.671897810218978e-05,
      "loss": 0.5627,
      "step": 36390
    },
    {
      "epoch": 265.6934306569343,
      "grad_norm": 8.765167236328125,
      "learning_rate": 3.671532846715329e-05,
      "loss": 0.8081,
      "step": 36400
    },
    {
      "epoch": 265.7664233576642,
      "grad_norm": 7.9598870277404785,
      "learning_rate": 3.6711678832116795e-05,
      "loss": 1.2658,
      "step": 36410
    },
    {
      "epoch": 265.83941605839414,
      "grad_norm": 6.291006565093994,
      "learning_rate": 3.6708029197080295e-05,
      "loss": 1.0537,
      "step": 36420
    },
    {
      "epoch": 265.9124087591241,
      "grad_norm": 0.4934854209423065,
      "learning_rate": 3.6704379562043796e-05,
      "loss": 0.9122,
      "step": 36430
    },
    {
      "epoch": 265.985401459854,
      "grad_norm": 14.014728546142578,
      "learning_rate": 3.6700729927007296e-05,
      "loss": 1.2568,
      "step": 36440
    },
    {
      "epoch": 266.05839416058393,
      "grad_norm": 7.06048059463501,
      "learning_rate": 3.6697080291970803e-05,
      "loss": 0.7793,
      "step": 36450
    },
    {
      "epoch": 266.13138686131384,
      "grad_norm": 7.792038440704346,
      "learning_rate": 3.669343065693431e-05,
      "loss": 0.7431,
      "step": 36460
    },
    {
      "epoch": 266.2043795620438,
      "grad_norm": 1.362471580505371,
      "learning_rate": 3.668978102189781e-05,
      "loss": 0.7009,
      "step": 36470
    },
    {
      "epoch": 266.2773722627737,
      "grad_norm": 8.42055892944336,
      "learning_rate": 3.668613138686132e-05,
      "loss": 0.8427,
      "step": 36480
    },
    {
      "epoch": 266.35036496350364,
      "grad_norm": 10.025335311889648,
      "learning_rate": 3.668248175182482e-05,
      "loss": 0.9398,
      "step": 36490
    },
    {
      "epoch": 266.42335766423355,
      "grad_norm": 0.3376784026622772,
      "learning_rate": 3.6678832116788326e-05,
      "loss": 0.8329,
      "step": 36500
    },
    {
      "epoch": 266.4963503649635,
      "grad_norm": 7.083630084991455,
      "learning_rate": 3.667518248175183e-05,
      "loss": 1.3698,
      "step": 36510
    },
    {
      "epoch": 266.56934306569343,
      "grad_norm": 6.995541095733643,
      "learning_rate": 3.667153284671533e-05,
      "loss": 1.0294,
      "step": 36520
    },
    {
      "epoch": 266.64233576642334,
      "grad_norm": 8.104028701782227,
      "learning_rate": 3.6667883211678834e-05,
      "loss": 1.0114,
      "step": 36530
    },
    {
      "epoch": 266.7153284671533,
      "grad_norm": 10.420220375061035,
      "learning_rate": 3.6664233576642335e-05,
      "loss": 1.1452,
      "step": 36540
    },
    {
      "epoch": 266.7883211678832,
      "grad_norm": 0.4283059537410736,
      "learning_rate": 3.666058394160584e-05,
      "loss": 0.8904,
      "step": 36550
    },
    {
      "epoch": 266.86131386861314,
      "grad_norm": 14.291475296020508,
      "learning_rate": 3.665693430656934e-05,
      "loss": 1.0531,
      "step": 36560
    },
    {
      "epoch": 266.93430656934305,
      "grad_norm": 0.32873478531837463,
      "learning_rate": 3.665328467153285e-05,
      "loss": 1.2053,
      "step": 36570
    },
    {
      "epoch": 267.007299270073,
      "grad_norm": 20.09617042541504,
      "learning_rate": 3.664963503649636e-05,
      "loss": 1.1181,
      "step": 36580
    },
    {
      "epoch": 267.08029197080293,
      "grad_norm": 0.36813458800315857,
      "learning_rate": 3.664598540145985e-05,
      "loss": 0.6606,
      "step": 36590
    },
    {
      "epoch": 267.15328467153284,
      "grad_norm": 8.209311485290527,
      "learning_rate": 3.664233576642336e-05,
      "loss": 0.7985,
      "step": 36600
    },
    {
      "epoch": 267.22627737226276,
      "grad_norm": 10.0217866897583,
      "learning_rate": 3.6638686131386866e-05,
      "loss": 1.1718,
      "step": 36610
    },
    {
      "epoch": 267.2992700729927,
      "grad_norm": 11.617095947265625,
      "learning_rate": 3.6635036496350366e-05,
      "loss": 1.0963,
      "step": 36620
    },
    {
      "epoch": 267.37226277372264,
      "grad_norm": 8.233964920043945,
      "learning_rate": 3.663138686131387e-05,
      "loss": 0.8449,
      "step": 36630
    },
    {
      "epoch": 267.44525547445255,
      "grad_norm": 8.254171371459961,
      "learning_rate": 3.6627737226277374e-05,
      "loss": 0.6491,
      "step": 36640
    },
    {
      "epoch": 267.51824817518246,
      "grad_norm": 11.383434295654297,
      "learning_rate": 3.662408759124088e-05,
      "loss": 0.8276,
      "step": 36650
    },
    {
      "epoch": 267.59124087591243,
      "grad_norm": 9.250067710876465,
      "learning_rate": 3.662043795620438e-05,
      "loss": 1.326,
      "step": 36660
    },
    {
      "epoch": 267.66423357664235,
      "grad_norm": 10.201397895812988,
      "learning_rate": 3.661678832116788e-05,
      "loss": 1.1221,
      "step": 36670
    },
    {
      "epoch": 267.73722627737226,
      "grad_norm": 2.6550614833831787,
      "learning_rate": 3.661313868613139e-05,
      "loss": 1.0192,
      "step": 36680
    },
    {
      "epoch": 267.81021897810217,
      "grad_norm": 7.784510612487793,
      "learning_rate": 3.660948905109489e-05,
      "loss": 0.5603,
      "step": 36690
    },
    {
      "epoch": 267.88321167883214,
      "grad_norm": 6.46811056137085,
      "learning_rate": 3.66058394160584e-05,
      "loss": 0.8275,
      "step": 36700
    },
    {
      "epoch": 267.95620437956205,
      "grad_norm": 0.11904392391443253,
      "learning_rate": 3.66021897810219e-05,
      "loss": 1.3855,
      "step": 36710
    },
    {
      "epoch": 268.02919708029196,
      "grad_norm": 17.264381408691406,
      "learning_rate": 3.6598540145985405e-05,
      "loss": 0.9295,
      "step": 36720
    },
    {
      "epoch": 268.1021897810219,
      "grad_norm": 16.730979919433594,
      "learning_rate": 3.659489051094891e-05,
      "loss": 1.3191,
      "step": 36730
    },
    {
      "epoch": 268.17518248175185,
      "grad_norm": 0.4210568964481354,
      "learning_rate": 3.6591240875912406e-05,
      "loss": 0.8209,
      "step": 36740
    },
    {
      "epoch": 268.24817518248176,
      "grad_norm": 16.569507598876953,
      "learning_rate": 3.658759124087591e-05,
      "loss": 0.9679,
      "step": 36750
    },
    {
      "epoch": 268.3211678832117,
      "grad_norm": 8.977432250976562,
      "learning_rate": 3.6583941605839414e-05,
      "loss": 1.0103,
      "step": 36760
    },
    {
      "epoch": 268.3941605839416,
      "grad_norm": 17.775697708129883,
      "learning_rate": 3.658029197080292e-05,
      "loss": 1.0754,
      "step": 36770
    },
    {
      "epoch": 268.46715328467155,
      "grad_norm": 13.845149993896484,
      "learning_rate": 3.657664233576643e-05,
      "loss": 1.1128,
      "step": 36780
    },
    {
      "epoch": 268.54014598540147,
      "grad_norm": 0.27765047550201416,
      "learning_rate": 3.657299270072993e-05,
      "loss": 0.8115,
      "step": 36790
    },
    {
      "epoch": 268.6131386861314,
      "grad_norm": 8.300509452819824,
      "learning_rate": 3.6569343065693436e-05,
      "loss": 1.0462,
      "step": 36800
    },
    {
      "epoch": 268.6861313868613,
      "grad_norm": 4.5425262451171875,
      "learning_rate": 3.6565693430656936e-05,
      "loss": 0.8553,
      "step": 36810
    },
    {
      "epoch": 268.75912408759126,
      "grad_norm": 8.73849868774414,
      "learning_rate": 3.656204379562044e-05,
      "loss": 1.0415,
      "step": 36820
    },
    {
      "epoch": 268.8321167883212,
      "grad_norm": 5.175739765167236,
      "learning_rate": 3.6558394160583944e-05,
      "loss": 0.7434,
      "step": 36830
    },
    {
      "epoch": 268.9051094890511,
      "grad_norm": 10.507973670959473,
      "learning_rate": 3.6554744525547445e-05,
      "loss": 0.6168,
      "step": 36840
    },
    {
      "epoch": 268.978102189781,
      "grad_norm": 9.278984069824219,
      "learning_rate": 3.655109489051095e-05,
      "loss": 1.029,
      "step": 36850
    },
    {
      "epoch": 269.05109489051097,
      "grad_norm": 7.0692667961120605,
      "learning_rate": 3.654744525547445e-05,
      "loss": 0.9138,
      "step": 36860
    },
    {
      "epoch": 269.1240875912409,
      "grad_norm": 0.7754501700401306,
      "learning_rate": 3.654379562043796e-05,
      "loss": 0.7692,
      "step": 36870
    },
    {
      "epoch": 269.1970802919708,
      "grad_norm": 10.26037311553955,
      "learning_rate": 3.654014598540147e-05,
      "loss": 0.9388,
      "step": 36880
    },
    {
      "epoch": 269.2700729927007,
      "grad_norm": 13.985448837280273,
      "learning_rate": 3.653649635036496e-05,
      "loss": 1.0124,
      "step": 36890
    },
    {
      "epoch": 269.3430656934307,
      "grad_norm": 14.850703239440918,
      "learning_rate": 3.653284671532847e-05,
      "loss": 0.9294,
      "step": 36900
    },
    {
      "epoch": 269.4160583941606,
      "grad_norm": 11.329399108886719,
      "learning_rate": 3.652919708029197e-05,
      "loss": 1.2727,
      "step": 36910
    },
    {
      "epoch": 269.4890510948905,
      "grad_norm": 7.637462139129639,
      "learning_rate": 3.6525547445255476e-05,
      "loss": 0.8032,
      "step": 36920
    },
    {
      "epoch": 269.5620437956204,
      "grad_norm": 10.840795516967773,
      "learning_rate": 3.652189781021898e-05,
      "loss": 1.1137,
      "step": 36930
    },
    {
      "epoch": 269.6350364963504,
      "grad_norm": 10.614272117614746,
      "learning_rate": 3.6518248175182483e-05,
      "loss": 1.1146,
      "step": 36940
    },
    {
      "epoch": 269.7080291970803,
      "grad_norm": 15.583791732788086,
      "learning_rate": 3.651459854014599e-05,
      "loss": 0.7295,
      "step": 36950
    },
    {
      "epoch": 269.7810218978102,
      "grad_norm": 7.144659042358398,
      "learning_rate": 3.651094890510949e-05,
      "loss": 1.2277,
      "step": 36960
    },
    {
      "epoch": 269.8540145985401,
      "grad_norm": 0.3914180099964142,
      "learning_rate": 3.650729927007299e-05,
      "loss": 1.0952,
      "step": 36970
    },
    {
      "epoch": 269.9270072992701,
      "grad_norm": 15.248856544494629,
      "learning_rate": 3.65036496350365e-05,
      "loss": 0.8872,
      "step": 36980
    },
    {
      "epoch": 270.0,
      "grad_norm": 0.632809042930603,
      "learning_rate": 3.65e-05,
      "loss": 0.5694,
      "step": 36990
    },
    {
      "epoch": 270.0729927007299,
      "grad_norm": 9.641105651855469,
      "learning_rate": 3.649635036496351e-05,
      "loss": 1.1576,
      "step": 37000
    },
    {
      "epoch": 270.1459854014599,
      "grad_norm": 0.424073189496994,
      "learning_rate": 3.649270072992701e-05,
      "loss": 0.9793,
      "step": 37010
    },
    {
      "epoch": 270.2189781021898,
      "grad_norm": 7.893100261688232,
      "learning_rate": 3.6489051094890515e-05,
      "loss": 0.8772,
      "step": 37020
    },
    {
      "epoch": 270.2919708029197,
      "grad_norm": 11.573596000671387,
      "learning_rate": 3.6485401459854015e-05,
      "loss": 0.9176,
      "step": 37030
    },
    {
      "epoch": 270.3649635036496,
      "grad_norm": 19.990501403808594,
      "learning_rate": 3.648175182481752e-05,
      "loss": 1.3759,
      "step": 37040
    },
    {
      "epoch": 270.4379562043796,
      "grad_norm": 11.066097259521484,
      "learning_rate": 3.647810218978102e-05,
      "loss": 1.0958,
      "step": 37050
    },
    {
      "epoch": 270.5109489051095,
      "grad_norm": 12.10835075378418,
      "learning_rate": 3.647445255474452e-05,
      "loss": 1.0849,
      "step": 37060
    },
    {
      "epoch": 270.5839416058394,
      "grad_norm": 0.3753141760826111,
      "learning_rate": 3.647080291970803e-05,
      "loss": 0.6636,
      "step": 37070
    },
    {
      "epoch": 270.6569343065693,
      "grad_norm": 0.1280682384967804,
      "learning_rate": 3.646715328467154e-05,
      "loss": 0.8675,
      "step": 37080
    },
    {
      "epoch": 270.7299270072993,
      "grad_norm": 8.044718742370605,
      "learning_rate": 3.646350364963504e-05,
      "loss": 0.8079,
      "step": 37090
    },
    {
      "epoch": 270.8029197080292,
      "grad_norm": 9.603745460510254,
      "learning_rate": 3.6459854014598546e-05,
      "loss": 1.2523,
      "step": 37100
    },
    {
      "epoch": 270.8759124087591,
      "grad_norm": 0.45874282717704773,
      "learning_rate": 3.6456204379562046e-05,
      "loss": 0.9674,
      "step": 37110
    },
    {
      "epoch": 270.94890510948903,
      "grad_norm": 18.707006454467773,
      "learning_rate": 3.6452554744525547e-05,
      "loss": 0.9694,
      "step": 37120
    },
    {
      "epoch": 271.021897810219,
      "grad_norm": 14.284481048583984,
      "learning_rate": 3.6448905109489054e-05,
      "loss": 0.7834,
      "step": 37130
    },
    {
      "epoch": 271.0948905109489,
      "grad_norm": 0.399882048368454,
      "learning_rate": 3.6445255474452554e-05,
      "loss": 0.6796,
      "step": 37140
    },
    {
      "epoch": 271.1678832116788,
      "grad_norm": 8.359456062316895,
      "learning_rate": 3.644160583941606e-05,
      "loss": 1.2241,
      "step": 37150
    },
    {
      "epoch": 271.24087591240874,
      "grad_norm": 0.19901560246944427,
      "learning_rate": 3.643795620437956e-05,
      "loss": 0.8679,
      "step": 37160
    },
    {
      "epoch": 271.3138686131387,
      "grad_norm": 4.857087135314941,
      "learning_rate": 3.643430656934307e-05,
      "loss": 0.8048,
      "step": 37170
    },
    {
      "epoch": 271.3868613138686,
      "grad_norm": 0.23163676261901855,
      "learning_rate": 3.643065693430657e-05,
      "loss": 0.8475,
      "step": 37180
    },
    {
      "epoch": 271.45985401459853,
      "grad_norm": 0.3835836350917816,
      "learning_rate": 3.642700729927008e-05,
      "loss": 1.274,
      "step": 37190
    },
    {
      "epoch": 271.53284671532845,
      "grad_norm": 7.689734935760498,
      "learning_rate": 3.642335766423358e-05,
      "loss": 1.3141,
      "step": 37200
    },
    {
      "epoch": 271.6058394160584,
      "grad_norm": 7.891137599945068,
      "learning_rate": 3.641970802919708e-05,
      "loss": 1.2062,
      "step": 37210
    },
    {
      "epoch": 271.6788321167883,
      "grad_norm": 0.9553526043891907,
      "learning_rate": 3.6416058394160585e-05,
      "loss": 1.0027,
      "step": 37220
    },
    {
      "epoch": 271.75182481751824,
      "grad_norm": 10.172269821166992,
      "learning_rate": 3.6412408759124086e-05,
      "loss": 0.9533,
      "step": 37230
    },
    {
      "epoch": 271.82481751824815,
      "grad_norm": 6.407896041870117,
      "learning_rate": 3.640875912408759e-05,
      "loss": 0.9694,
      "step": 37240
    },
    {
      "epoch": 271.8978102189781,
      "grad_norm": 0.3528554141521454,
      "learning_rate": 3.64051094890511e-05,
      "loss": 0.9264,
      "step": 37250
    },
    {
      "epoch": 271.97080291970804,
      "grad_norm": 8.25921630859375,
      "learning_rate": 3.64014598540146e-05,
      "loss": 0.6494,
      "step": 37260
    },
    {
      "epoch": 272.04379562043795,
      "grad_norm": 5.390810012817383,
      "learning_rate": 3.63978102189781e-05,
      "loss": 0.5011,
      "step": 37270
    },
    {
      "epoch": 272.11678832116786,
      "grad_norm": 0.6432549357414246,
      "learning_rate": 3.639416058394161e-05,
      "loss": 0.9936,
      "step": 37280
    },
    {
      "epoch": 272.18978102189783,
      "grad_norm": 19.164920806884766,
      "learning_rate": 3.639051094890511e-05,
      "loss": 0.841,
      "step": 37290
    },
    {
      "epoch": 272.26277372262774,
      "grad_norm": 0.27953794598579407,
      "learning_rate": 3.6386861313868616e-05,
      "loss": 0.8071,
      "step": 37300
    },
    {
      "epoch": 272.33576642335765,
      "grad_norm": 10.36352252960205,
      "learning_rate": 3.638321167883212e-05,
      "loss": 0.8935,
      "step": 37310
    },
    {
      "epoch": 272.40875912408757,
      "grad_norm": 9.461353302001953,
      "learning_rate": 3.6379562043795624e-05,
      "loss": 0.9139,
      "step": 37320
    },
    {
      "epoch": 272.48175182481754,
      "grad_norm": 8.80402660369873,
      "learning_rate": 3.6375912408759125e-05,
      "loss": 0.7662,
      "step": 37330
    },
    {
      "epoch": 272.55474452554745,
      "grad_norm": 7.880624294281006,
      "learning_rate": 3.637226277372263e-05,
      "loss": 1.0548,
      "step": 37340
    },
    {
      "epoch": 272.62773722627736,
      "grad_norm": 12.680246353149414,
      "learning_rate": 3.636861313868613e-05,
      "loss": 1.0975,
      "step": 37350
    },
    {
      "epoch": 272.7007299270073,
      "grad_norm": 11.087593078613281,
      "learning_rate": 3.636496350364963e-05,
      "loss": 1.0634,
      "step": 37360
    },
    {
      "epoch": 272.77372262773724,
      "grad_norm": 11.029417991638184,
      "learning_rate": 3.636131386861314e-05,
      "loss": 1.1648,
      "step": 37370
    },
    {
      "epoch": 272.84671532846716,
      "grad_norm": 0.12388096004724503,
      "learning_rate": 3.635766423357664e-05,
      "loss": 1.1125,
      "step": 37380
    },
    {
      "epoch": 272.91970802919707,
      "grad_norm": 7.6904616355896,
      "learning_rate": 3.635401459854015e-05,
      "loss": 0.9045,
      "step": 37390
    },
    {
      "epoch": 272.992700729927,
      "grad_norm": 7.1310715675354,
      "learning_rate": 3.6350364963503655e-05,
      "loss": 0.9676,
      "step": 37400
    },
    {
      "epoch": 273.06569343065695,
      "grad_norm": 7.091824531555176,
      "learning_rate": 3.6346715328467156e-05,
      "loss": 1.3261,
      "step": 37410
    },
    {
      "epoch": 273.13868613138686,
      "grad_norm": 5.849269390106201,
      "learning_rate": 3.634306569343066e-05,
      "loss": 1.2099,
      "step": 37420
    },
    {
      "epoch": 273.2116788321168,
      "grad_norm": 0.43791285157203674,
      "learning_rate": 3.633941605839416e-05,
      "loss": 0.6864,
      "step": 37430
    },
    {
      "epoch": 273.2846715328467,
      "grad_norm": 0.43391919136047363,
      "learning_rate": 3.6335766423357664e-05,
      "loss": 1.0939,
      "step": 37440
    },
    {
      "epoch": 273.35766423357666,
      "grad_norm": 6.38131046295166,
      "learning_rate": 3.633211678832117e-05,
      "loss": 0.6794,
      "step": 37450
    },
    {
      "epoch": 273.43065693430657,
      "grad_norm": 9.540862083435059,
      "learning_rate": 3.632846715328467e-05,
      "loss": 1.419,
      "step": 37460
    },
    {
      "epoch": 273.5036496350365,
      "grad_norm": 14.984391212463379,
      "learning_rate": 3.632481751824818e-05,
      "loss": 1.0909,
      "step": 37470
    },
    {
      "epoch": 273.57664233576645,
      "grad_norm": 14.777237892150879,
      "learning_rate": 3.632116788321168e-05,
      "loss": 0.6228,
      "step": 37480
    },
    {
      "epoch": 273.64963503649636,
      "grad_norm": 10.103075981140137,
      "learning_rate": 3.631751824817519e-05,
      "loss": 1.1824,
      "step": 37490
    },
    {
      "epoch": 273.7226277372263,
      "grad_norm": 11.17077350616455,
      "learning_rate": 3.631386861313869e-05,
      "loss": 0.7879,
      "step": 37500
    },
    {
      "epoch": 273.7956204379562,
      "grad_norm": 7.692721366882324,
      "learning_rate": 3.631021897810219e-05,
      "loss": 0.9174,
      "step": 37510
    },
    {
      "epoch": 273.86861313868616,
      "grad_norm": 10.618331909179688,
      "learning_rate": 3.6306569343065695e-05,
      "loss": 0.7776,
      "step": 37520
    },
    {
      "epoch": 273.94160583941607,
      "grad_norm": 11.759725570678711,
      "learning_rate": 3.6302919708029196e-05,
      "loss": 1.0942,
      "step": 37530
    },
    {
      "epoch": 274.014598540146,
      "grad_norm": 14.208351135253906,
      "learning_rate": 3.62992700729927e-05,
      "loss": 0.8413,
      "step": 37540
    },
    {
      "epoch": 274.0875912408759,
      "grad_norm": 7.977705478668213,
      "learning_rate": 3.629562043795621e-05,
      "loss": 0.5765,
      "step": 37550
    },
    {
      "epoch": 274.16058394160586,
      "grad_norm": 9.470653533935547,
      "learning_rate": 3.629197080291971e-05,
      "loss": 1.2512,
      "step": 37560
    },
    {
      "epoch": 274.2335766423358,
      "grad_norm": 0.4434223473072052,
      "learning_rate": 3.628832116788322e-05,
      "loss": 0.8466,
      "step": 37570
    },
    {
      "epoch": 274.3065693430657,
      "grad_norm": 1.4719072580337524,
      "learning_rate": 3.628467153284671e-05,
      "loss": 0.5505,
      "step": 37580
    },
    {
      "epoch": 274.3795620437956,
      "grad_norm": 15.978586196899414,
      "learning_rate": 3.628102189781022e-05,
      "loss": 1.0234,
      "step": 37590
    },
    {
      "epoch": 274.45255474452557,
      "grad_norm": 2.372209310531616,
      "learning_rate": 3.6277372262773726e-05,
      "loss": 0.9748,
      "step": 37600
    },
    {
      "epoch": 274.5255474452555,
      "grad_norm": 1.1798254251480103,
      "learning_rate": 3.627372262773723e-05,
      "loss": 1.325,
      "step": 37610
    },
    {
      "epoch": 274.5985401459854,
      "grad_norm": 12.35338020324707,
      "learning_rate": 3.6270072992700734e-05,
      "loss": 1.4667,
      "step": 37620
    },
    {
      "epoch": 274.6715328467153,
      "grad_norm": 7.208354949951172,
      "learning_rate": 3.6266423357664234e-05,
      "loss": 0.9576,
      "step": 37630
    },
    {
      "epoch": 274.7445255474453,
      "grad_norm": 9.649591445922852,
      "learning_rate": 3.626277372262774e-05,
      "loss": 0.8787,
      "step": 37640
    },
    {
      "epoch": 274.8175182481752,
      "grad_norm": 11.540204048156738,
      "learning_rate": 3.625912408759124e-05,
      "loss": 1.1487,
      "step": 37650
    },
    {
      "epoch": 274.8905109489051,
      "grad_norm": 0.35003170371055603,
      "learning_rate": 3.625547445255474e-05,
      "loss": 0.805,
      "step": 37660
    },
    {
      "epoch": 274.963503649635,
      "grad_norm": 10.96728229522705,
      "learning_rate": 3.625182481751825e-05,
      "loss": 0.8775,
      "step": 37670
    },
    {
      "epoch": 275.036496350365,
      "grad_norm": 12.467053413391113,
      "learning_rate": 3.624817518248175e-05,
      "loss": 0.8206,
      "step": 37680
    },
    {
      "epoch": 275.1094890510949,
      "grad_norm": 5.013726234436035,
      "learning_rate": 3.624452554744526e-05,
      "loss": 0.9542,
      "step": 37690
    },
    {
      "epoch": 275.1824817518248,
      "grad_norm": 7.153072834014893,
      "learning_rate": 3.624087591240876e-05,
      "loss": 0.8154,
      "step": 37700
    },
    {
      "epoch": 275.2554744525547,
      "grad_norm": 13.434599876403809,
      "learning_rate": 3.6237226277372265e-05,
      "loss": 0.6189,
      "step": 37710
    },
    {
      "epoch": 275.3284671532847,
      "grad_norm": 11.586174011230469,
      "learning_rate": 3.623357664233577e-05,
      "loss": 0.9837,
      "step": 37720
    },
    {
      "epoch": 275.4014598540146,
      "grad_norm": 0.3575267195701599,
      "learning_rate": 3.6229927007299266e-05,
      "loss": 1.2628,
      "step": 37730
    },
    {
      "epoch": 275.4744525547445,
      "grad_norm": 7.32661247253418,
      "learning_rate": 3.6226277372262774e-05,
      "loss": 0.923,
      "step": 37740
    },
    {
      "epoch": 275.54744525547443,
      "grad_norm": 5.229490280151367,
      "learning_rate": 3.622262773722628e-05,
      "loss": 1.2788,
      "step": 37750
    },
    {
      "epoch": 275.6204379562044,
      "grad_norm": 9.693232536315918,
      "learning_rate": 3.621897810218978e-05,
      "loss": 1.0439,
      "step": 37760
    },
    {
      "epoch": 275.6934306569343,
      "grad_norm": 13.84471607208252,
      "learning_rate": 3.621532846715329e-05,
      "loss": 0.9633,
      "step": 37770
    },
    {
      "epoch": 275.7664233576642,
      "grad_norm": 6.246534824371338,
      "learning_rate": 3.621167883211679e-05,
      "loss": 0.9437,
      "step": 37780
    },
    {
      "epoch": 275.83941605839414,
      "grad_norm": 9.97623062133789,
      "learning_rate": 3.6208029197080297e-05,
      "loss": 0.7958,
      "step": 37790
    },
    {
      "epoch": 275.9124087591241,
      "grad_norm": 7.6555352210998535,
      "learning_rate": 3.62043795620438e-05,
      "loss": 0.9486,
      "step": 37800
    },
    {
      "epoch": 275.985401459854,
      "grad_norm": 8.837455749511719,
      "learning_rate": 3.62007299270073e-05,
      "loss": 0.8173,
      "step": 37810
    },
    {
      "epoch": 276.05839416058393,
      "grad_norm": 7.237739562988281,
      "learning_rate": 3.6197080291970805e-05,
      "loss": 0.9679,
      "step": 37820
    },
    {
      "epoch": 276.13138686131384,
      "grad_norm": 13.97852897644043,
      "learning_rate": 3.6193430656934305e-05,
      "loss": 1.1868,
      "step": 37830
    },
    {
      "epoch": 276.2043795620438,
      "grad_norm": 11.23373794555664,
      "learning_rate": 3.618978102189781e-05,
      "loss": 0.9874,
      "step": 37840
    },
    {
      "epoch": 276.2773722627737,
      "grad_norm": 15.333867073059082,
      "learning_rate": 3.618613138686131e-05,
      "loss": 1.1901,
      "step": 37850
    },
    {
      "epoch": 276.35036496350364,
      "grad_norm": 15.042081832885742,
      "learning_rate": 3.618248175182482e-05,
      "loss": 0.8023,
      "step": 37860
    },
    {
      "epoch": 276.42335766423355,
      "grad_norm": 0.29277557134628296,
      "learning_rate": 3.617883211678833e-05,
      "loss": 0.8885,
      "step": 37870
    },
    {
      "epoch": 276.4963503649635,
      "grad_norm": 7.800875663757324,
      "learning_rate": 3.617518248175183e-05,
      "loss": 1.4919,
      "step": 37880
    },
    {
      "epoch": 276.56934306569343,
      "grad_norm": 0.34956198930740356,
      "learning_rate": 3.617153284671533e-05,
      "loss": 0.7627,
      "step": 37890
    },
    {
      "epoch": 276.64233576642334,
      "grad_norm": 12.342411041259766,
      "learning_rate": 3.616788321167883e-05,
      "loss": 0.8301,
      "step": 37900
    },
    {
      "epoch": 276.7153284671533,
      "grad_norm": 10.445293426513672,
      "learning_rate": 3.6164233576642336e-05,
      "loss": 1.0261,
      "step": 37910
    },
    {
      "epoch": 276.7883211678832,
      "grad_norm": 0.5991395115852356,
      "learning_rate": 3.6160583941605844e-05,
      "loss": 0.6724,
      "step": 37920
    },
    {
      "epoch": 276.86131386861314,
      "grad_norm": 7.095548629760742,
      "learning_rate": 3.6156934306569344e-05,
      "loss": 0.9211,
      "step": 37930
    },
    {
      "epoch": 276.93430656934305,
      "grad_norm": 5.710428237915039,
      "learning_rate": 3.615328467153285e-05,
      "loss": 0.7725,
      "step": 37940
    },
    {
      "epoch": 277.007299270073,
      "grad_norm": 0.6431015729904175,
      "learning_rate": 3.614963503649635e-05,
      "loss": 0.6459,
      "step": 37950
    },
    {
      "epoch": 277.08029197080293,
      "grad_norm": 12.133193969726562,
      "learning_rate": 3.614598540145985e-05,
      "loss": 0.7755,
      "step": 37960
    },
    {
      "epoch": 277.15328467153284,
      "grad_norm": 17.07303810119629,
      "learning_rate": 3.614233576642336e-05,
      "loss": 1.0195,
      "step": 37970
    },
    {
      "epoch": 277.22627737226276,
      "grad_norm": 15.288426399230957,
      "learning_rate": 3.613868613138686e-05,
      "loss": 0.9604,
      "step": 37980
    },
    {
      "epoch": 277.2992700729927,
      "grad_norm": 5.776284217834473,
      "learning_rate": 3.613503649635037e-05,
      "loss": 1.0921,
      "step": 37990
    },
    {
      "epoch": 277.37226277372264,
      "grad_norm": 1.2617597579956055,
      "learning_rate": 3.613138686131387e-05,
      "loss": 0.7838,
      "step": 38000
    },
    {
      "epoch": 277.44525547445255,
      "grad_norm": 7.763383865356445,
      "learning_rate": 3.6127737226277375e-05,
      "loss": 0.7995,
      "step": 38010
    },
    {
      "epoch": 277.51824817518246,
      "grad_norm": 11.70012092590332,
      "learning_rate": 3.612408759124088e-05,
      "loss": 0.9988,
      "step": 38020
    },
    {
      "epoch": 277.59124087591243,
      "grad_norm": 12.546904563903809,
      "learning_rate": 3.612043795620438e-05,
      "loss": 0.9519,
      "step": 38030
    },
    {
      "epoch": 277.66423357664235,
      "grad_norm": 7.816218852996826,
      "learning_rate": 3.6116788321167883e-05,
      "loss": 0.6705,
      "step": 38040
    },
    {
      "epoch": 277.73722627737226,
      "grad_norm": 7.611772060394287,
      "learning_rate": 3.6113138686131384e-05,
      "loss": 1.5242,
      "step": 38050
    },
    {
      "epoch": 277.81021897810217,
      "grad_norm": 0.4766080975532532,
      "learning_rate": 3.610948905109489e-05,
      "loss": 1.1275,
      "step": 38060
    },
    {
      "epoch": 277.88321167883214,
      "grad_norm": 8.554286003112793,
      "learning_rate": 3.61058394160584e-05,
      "loss": 0.7616,
      "step": 38070
    },
    {
      "epoch": 277.95620437956205,
      "grad_norm": 14.959745407104492,
      "learning_rate": 3.61021897810219e-05,
      "loss": 1.0166,
      "step": 38080
    },
    {
      "epoch": 278.02919708029196,
      "grad_norm": 0.36542680859565735,
      "learning_rate": 3.6098540145985406e-05,
      "loss": 0.4771,
      "step": 38090
    },
    {
      "epoch": 278.1021897810219,
      "grad_norm": 6.159599304199219,
      "learning_rate": 3.609489051094891e-05,
      "loss": 0.5146,
      "step": 38100
    },
    {
      "epoch": 278.17518248175185,
      "grad_norm": 18.862327575683594,
      "learning_rate": 3.6091240875912414e-05,
      "loss": 1.1972,
      "step": 38110
    },
    {
      "epoch": 278.24817518248176,
      "grad_norm": 14.56135082244873,
      "learning_rate": 3.6087591240875915e-05,
      "loss": 0.8966,
      "step": 38120
    },
    {
      "epoch": 278.3211678832117,
      "grad_norm": 8.271857261657715,
      "learning_rate": 3.6083941605839415e-05,
      "loss": 1.4123,
      "step": 38130
    },
    {
      "epoch": 278.3941605839416,
      "grad_norm": 5.126132965087891,
      "learning_rate": 3.608029197080292e-05,
      "loss": 1.0211,
      "step": 38140
    },
    {
      "epoch": 278.46715328467155,
      "grad_norm": 12.188854217529297,
      "learning_rate": 3.607664233576642e-05,
      "loss": 0.823,
      "step": 38150
    },
    {
      "epoch": 278.54014598540147,
      "grad_norm": 9.84946060180664,
      "learning_rate": 3.607299270072993e-05,
      "loss": 0.8709,
      "step": 38160
    },
    {
      "epoch": 278.6131386861314,
      "grad_norm": 11.747218132019043,
      "learning_rate": 3.606934306569343e-05,
      "loss": 0.9441,
      "step": 38170
    },
    {
      "epoch": 278.6861313868613,
      "grad_norm": 9.034661293029785,
      "learning_rate": 3.606569343065694e-05,
      "loss": 0.7245,
      "step": 38180
    },
    {
      "epoch": 278.75912408759126,
      "grad_norm": 17.566770553588867,
      "learning_rate": 3.606204379562044e-05,
      "loss": 1.2983,
      "step": 38190
    },
    {
      "epoch": 278.8321167883212,
      "grad_norm": 6.496399879455566,
      "learning_rate": 3.605839416058394e-05,
      "loss": 1.1587,
      "step": 38200
    },
    {
      "epoch": 278.9051094890511,
      "grad_norm": 10.202799797058105,
      "learning_rate": 3.6054744525547446e-05,
      "loss": 0.8844,
      "step": 38210
    },
    {
      "epoch": 278.978102189781,
      "grad_norm": 12.412261962890625,
      "learning_rate": 3.605109489051095e-05,
      "loss": 1.036,
      "step": 38220
    },
    {
      "epoch": 279.05109489051097,
      "grad_norm": 0.22938498854637146,
      "learning_rate": 3.6047445255474454e-05,
      "loss": 1.0045,
      "step": 38230
    },
    {
      "epoch": 279.1240875912409,
      "grad_norm": 5.894747257232666,
      "learning_rate": 3.604379562043796e-05,
      "loss": 0.7892,
      "step": 38240
    },
    {
      "epoch": 279.1970802919708,
      "grad_norm": 1.0109294652938843,
      "learning_rate": 3.604014598540146e-05,
      "loss": 0.7591,
      "step": 38250
    },
    {
      "epoch": 279.2700729927007,
      "grad_norm": 7.341871738433838,
      "learning_rate": 3.603649635036497e-05,
      "loss": 0.9114,
      "step": 38260
    },
    {
      "epoch": 279.3430656934307,
      "grad_norm": 19.961881637573242,
      "learning_rate": 3.603284671532847e-05,
      "loss": 1.5019,
      "step": 38270
    },
    {
      "epoch": 279.4160583941606,
      "grad_norm": 9.907402992248535,
      "learning_rate": 3.602919708029197e-05,
      "loss": 0.9716,
      "step": 38280
    },
    {
      "epoch": 279.4890510948905,
      "grad_norm": 7.018636226654053,
      "learning_rate": 3.602554744525548e-05,
      "loss": 0.7876,
      "step": 38290
    },
    {
      "epoch": 279.5620437956204,
      "grad_norm": 3.8333182334899902,
      "learning_rate": 3.602189781021898e-05,
      "loss": 0.9483,
      "step": 38300
    },
    {
      "epoch": 279.6350364963504,
      "grad_norm": 8.090664863586426,
      "learning_rate": 3.6018248175182485e-05,
      "loss": 1.2192,
      "step": 38310
    },
    {
      "epoch": 279.7080291970803,
      "grad_norm": 8.46039867401123,
      "learning_rate": 3.6014598540145985e-05,
      "loss": 1.1542,
      "step": 38320
    },
    {
      "epoch": 279.7810218978102,
      "grad_norm": 0.4286738634109497,
      "learning_rate": 3.601094890510949e-05,
      "loss": 1.1554,
      "step": 38330
    },
    {
      "epoch": 279.8540145985401,
      "grad_norm": 7.112813949584961,
      "learning_rate": 3.6007299270073e-05,
      "loss": 1.0422,
      "step": 38340
    },
    {
      "epoch": 279.9270072992701,
      "grad_norm": 14.72226619720459,
      "learning_rate": 3.6003649635036494e-05,
      "loss": 0.7623,
      "step": 38350
    },
    {
      "epoch": 280.0,
      "grad_norm": 3.4009430408477783,
      "learning_rate": 3.6e-05,
      "loss": 0.8042,
      "step": 38360
    },
    {
      "epoch": 280.0729927007299,
      "grad_norm": 6.8519978523254395,
      "learning_rate": 3.59963503649635e-05,
      "loss": 0.621,
      "step": 38370
    },
    {
      "epoch": 280.1459854014599,
      "grad_norm": 9.295707702636719,
      "learning_rate": 3.599270072992701e-05,
      "loss": 1.0465,
      "step": 38380
    },
    {
      "epoch": 280.2189781021898,
      "grad_norm": 12.737053871154785,
      "learning_rate": 3.5989051094890516e-05,
      "loss": 0.6646,
      "step": 38390
    },
    {
      "epoch": 280.2919708029197,
      "grad_norm": 6.879408836364746,
      "learning_rate": 3.5985401459854016e-05,
      "loss": 1.0154,
      "step": 38400
    },
    {
      "epoch": 280.3649635036496,
      "grad_norm": 11.158991813659668,
      "learning_rate": 3.5981751824817524e-05,
      "loss": 0.9801,
      "step": 38410
    },
    {
      "epoch": 280.4379562043796,
      "grad_norm": 11.253195762634277,
      "learning_rate": 3.5978102189781024e-05,
      "loss": 0.8654,
      "step": 38420
    },
    {
      "epoch": 280.5109489051095,
      "grad_norm": 7.830173969268799,
      "learning_rate": 3.5974452554744525e-05,
      "loss": 0.7795,
      "step": 38430
    },
    {
      "epoch": 280.5839416058394,
      "grad_norm": 10.998154640197754,
      "learning_rate": 3.597080291970803e-05,
      "loss": 0.9501,
      "step": 38440
    },
    {
      "epoch": 280.6569343065693,
      "grad_norm": 6.881110191345215,
      "learning_rate": 3.596715328467153e-05,
      "loss": 1.0029,
      "step": 38450
    },
    {
      "epoch": 280.7299270072993,
      "grad_norm": 11.770785331726074,
      "learning_rate": 3.596350364963504e-05,
      "loss": 1.0349,
      "step": 38460
    },
    {
      "epoch": 280.8029197080292,
      "grad_norm": 9.652350425720215,
      "learning_rate": 3.595985401459854e-05,
      "loss": 1.5604,
      "step": 38470
    },
    {
      "epoch": 280.8759124087591,
      "grad_norm": 2.3085882663726807,
      "learning_rate": 3.595620437956205e-05,
      "loss": 1.0211,
      "step": 38480
    },
    {
      "epoch": 280.94890510948903,
      "grad_norm": 0.43261921405792236,
      "learning_rate": 3.5952554744525555e-05,
      "loss": 0.9194,
      "step": 38490
    },
    {
      "epoch": 281.021897810219,
      "grad_norm": 7.508110046386719,
      "learning_rate": 3.594890510948905e-05,
      "loss": 0.898,
      "step": 38500
    },
    {
      "epoch": 281.0948905109489,
      "grad_norm": 7.641971111297607,
      "learning_rate": 3.5945255474452556e-05,
      "loss": 0.8029,
      "step": 38510
    },
    {
      "epoch": 281.1678832116788,
      "grad_norm": 2.559340238571167,
      "learning_rate": 3.5941605839416056e-05,
      "loss": 0.8198,
      "step": 38520
    },
    {
      "epoch": 281.24087591240874,
      "grad_norm": 17.448335647583008,
      "learning_rate": 3.5937956204379564e-05,
      "loss": 1.215,
      "step": 38530
    },
    {
      "epoch": 281.3138686131387,
      "grad_norm": 0.905695915222168,
      "learning_rate": 3.593430656934307e-05,
      "loss": 0.9173,
      "step": 38540
    },
    {
      "epoch": 281.3868613138686,
      "grad_norm": 7.533150672912598,
      "learning_rate": 3.593065693430657e-05,
      "loss": 1.3394,
      "step": 38550
    },
    {
      "epoch": 281.45985401459853,
      "grad_norm": 9.201786041259766,
      "learning_rate": 3.592700729927008e-05,
      "loss": 0.983,
      "step": 38560
    },
    {
      "epoch": 281.53284671532845,
      "grad_norm": 11.031424522399902,
      "learning_rate": 3.592335766423357e-05,
      "loss": 0.9139,
      "step": 38570
    },
    {
      "epoch": 281.6058394160584,
      "grad_norm": 4.684665679931641,
      "learning_rate": 3.591970802919708e-05,
      "loss": 0.621,
      "step": 38580
    },
    {
      "epoch": 281.6788321167883,
      "grad_norm": 19.202890396118164,
      "learning_rate": 3.591605839416059e-05,
      "loss": 0.8927,
      "step": 38590
    },
    {
      "epoch": 281.75182481751824,
      "grad_norm": 11.370891571044922,
      "learning_rate": 3.591240875912409e-05,
      "loss": 1.1293,
      "step": 38600
    },
    {
      "epoch": 281.82481751824815,
      "grad_norm": 19.43605613708496,
      "learning_rate": 3.5908759124087595e-05,
      "loss": 1.0723,
      "step": 38610
    },
    {
      "epoch": 281.8978102189781,
      "grad_norm": 5.147244453430176,
      "learning_rate": 3.5905109489051095e-05,
      "loss": 0.7134,
      "step": 38620
    },
    {
      "epoch": 281.97080291970804,
      "grad_norm": 9.30567455291748,
      "learning_rate": 3.59014598540146e-05,
      "loss": 1.3724,
      "step": 38630
    },
    {
      "epoch": 282.04379562043795,
      "grad_norm": 9.44318675994873,
      "learning_rate": 3.589781021897811e-05,
      "loss": 0.7104,
      "step": 38640
    },
    {
      "epoch": 282.11678832116786,
      "grad_norm": 9.386695861816406,
      "learning_rate": 3.58941605839416e-05,
      "loss": 0.7041,
      "step": 38650
    },
    {
      "epoch": 282.18978102189783,
      "grad_norm": 1.6202058792114258,
      "learning_rate": 3.589051094890511e-05,
      "loss": 0.5383,
      "step": 38660
    },
    {
      "epoch": 282.26277372262774,
      "grad_norm": 5.9986090660095215,
      "learning_rate": 3.588686131386861e-05,
      "loss": 1.0654,
      "step": 38670
    },
    {
      "epoch": 282.33576642335765,
      "grad_norm": 8.603659629821777,
      "learning_rate": 3.588321167883212e-05,
      "loss": 1.1125,
      "step": 38680
    },
    {
      "epoch": 282.40875912408757,
      "grad_norm": 13.587160110473633,
      "learning_rate": 3.5879562043795626e-05,
      "loss": 1.1216,
      "step": 38690
    },
    {
      "epoch": 282.48175182481754,
      "grad_norm": 17.028778076171875,
      "learning_rate": 3.5875912408759126e-05,
      "loss": 1.4081,
      "step": 38700
    },
    {
      "epoch": 282.55474452554745,
      "grad_norm": 0.4363359808921814,
      "learning_rate": 3.5872262773722633e-05,
      "loss": 0.8478,
      "step": 38710
    },
    {
      "epoch": 282.62773722627736,
      "grad_norm": 9.324627876281738,
      "learning_rate": 3.5868613138686134e-05,
      "loss": 0.8015,
      "step": 38720
    },
    {
      "epoch": 282.7007299270073,
      "grad_norm": 9.73927116394043,
      "learning_rate": 3.5864963503649634e-05,
      "loss": 1.1596,
      "step": 38730
    },
    {
      "epoch": 282.77372262773724,
      "grad_norm": 5.259645938873291,
      "learning_rate": 3.586131386861314e-05,
      "loss": 1.0625,
      "step": 38740
    },
    {
      "epoch": 282.84671532846716,
      "grad_norm": 0.2520504593849182,
      "learning_rate": 3.585766423357664e-05,
      "loss": 0.5892,
      "step": 38750
    },
    {
      "epoch": 282.91970802919707,
      "grad_norm": 9.30898380279541,
      "learning_rate": 3.585401459854015e-05,
      "loss": 1.025,
      "step": 38760
    },
    {
      "epoch": 282.992700729927,
      "grad_norm": 4.309568405151367,
      "learning_rate": 3.585036496350365e-05,
      "loss": 0.8191,
      "step": 38770
    },
    {
      "epoch": 283.06569343065695,
      "grad_norm": 7.132554054260254,
      "learning_rate": 3.584671532846716e-05,
      "loss": 0.9876,
      "step": 38780
    },
    {
      "epoch": 283.13868613138686,
      "grad_norm": 12.318084716796875,
      "learning_rate": 3.584306569343066e-05,
      "loss": 0.8273,
      "step": 38790
    },
    {
      "epoch": 283.2116788321168,
      "grad_norm": 0.4202438294887543,
      "learning_rate": 3.583941605839416e-05,
      "loss": 1.0335,
      "step": 38800
    },
    {
      "epoch": 283.2846715328467,
      "grad_norm": 11.09555721282959,
      "learning_rate": 3.5835766423357665e-05,
      "loss": 1.215,
      "step": 38810
    },
    {
      "epoch": 283.35766423357666,
      "grad_norm": 6.822847843170166,
      "learning_rate": 3.5832116788321166e-05,
      "loss": 0.7208,
      "step": 38820
    },
    {
      "epoch": 283.43065693430657,
      "grad_norm": 7.375411033630371,
      "learning_rate": 3.582846715328467e-05,
      "loss": 0.8586,
      "step": 38830
    },
    {
      "epoch": 283.5036496350365,
      "grad_norm": 0.42745518684387207,
      "learning_rate": 3.582481751824818e-05,
      "loss": 0.7486,
      "step": 38840
    },
    {
      "epoch": 283.57664233576645,
      "grad_norm": 6.237174987792969,
      "learning_rate": 3.582116788321168e-05,
      "loss": 0.7858,
      "step": 38850
    },
    {
      "epoch": 283.64963503649636,
      "grad_norm": 12.237049102783203,
      "learning_rate": 3.581751824817519e-05,
      "loss": 1.0568,
      "step": 38860
    },
    {
      "epoch": 283.7226277372263,
      "grad_norm": 0.32297956943511963,
      "learning_rate": 3.581386861313869e-05,
      "loss": 1.1572,
      "step": 38870
    },
    {
      "epoch": 283.7956204379562,
      "grad_norm": 20.66997718811035,
      "learning_rate": 3.581021897810219e-05,
      "loss": 1.2045,
      "step": 38880
    },
    {
      "epoch": 283.86861313868616,
      "grad_norm": 11.71750545501709,
      "learning_rate": 3.5806569343065697e-05,
      "loss": 1.1855,
      "step": 38890
    },
    {
      "epoch": 283.94160583941607,
      "grad_norm": 0.7240808010101318,
      "learning_rate": 3.58029197080292e-05,
      "loss": 0.6675,
      "step": 38900
    },
    {
      "epoch": 284.014598540146,
      "grad_norm": 4.356426239013672,
      "learning_rate": 3.5799270072992704e-05,
      "loss": 1.1273,
      "step": 38910
    },
    {
      "epoch": 284.0875912408759,
      "grad_norm": 0.198935866355896,
      "learning_rate": 3.5795620437956205e-05,
      "loss": 0.7208,
      "step": 38920
    },
    {
      "epoch": 284.16058394160586,
      "grad_norm": 6.682859897613525,
      "learning_rate": 3.579197080291971e-05,
      "loss": 0.8297,
      "step": 38930
    },
    {
      "epoch": 284.2335766423358,
      "grad_norm": 0.2887526750564575,
      "learning_rate": 3.578832116788321e-05,
      "loss": 1.0253,
      "step": 38940
    },
    {
      "epoch": 284.3065693430657,
      "grad_norm": 12.609635353088379,
      "learning_rate": 3.578467153284672e-05,
      "loss": 1.0759,
      "step": 38950
    },
    {
      "epoch": 284.3795620437956,
      "grad_norm": 7.3709893226623535,
      "learning_rate": 3.578102189781022e-05,
      "loss": 0.8413,
      "step": 38960
    },
    {
      "epoch": 284.45255474452557,
      "grad_norm": 10.640641212463379,
      "learning_rate": 3.577737226277372e-05,
      "loss": 1.1208,
      "step": 38970
    },
    {
      "epoch": 284.5255474452555,
      "grad_norm": 5.5542449951171875,
      "learning_rate": 3.577372262773723e-05,
      "loss": 0.8311,
      "step": 38980
    },
    {
      "epoch": 284.5985401459854,
      "grad_norm": 11.487289428710938,
      "learning_rate": 3.577007299270073e-05,
      "loss": 1.2249,
      "step": 38990
    },
    {
      "epoch": 284.6715328467153,
      "grad_norm": 11.639473915100098,
      "learning_rate": 3.5766423357664236e-05,
      "loss": 0.9087,
      "step": 39000
    },
    {
      "epoch": 284.7445255474453,
      "grad_norm": 0.8680042028427124,
      "learning_rate": 3.576277372262774e-05,
      "loss": 1.1158,
      "step": 39010
    },
    {
      "epoch": 284.8175182481752,
      "grad_norm": 14.1647367477417,
      "learning_rate": 3.5759124087591244e-05,
      "loss": 1.279,
      "step": 39020
    },
    {
      "epoch": 284.8905109489051,
      "grad_norm": 0.3828187584877014,
      "learning_rate": 3.5755474452554744e-05,
      "loss": 0.7486,
      "step": 39030
    },
    {
      "epoch": 284.963503649635,
      "grad_norm": 4.770852088928223,
      "learning_rate": 3.5751824817518245e-05,
      "loss": 0.9411,
      "step": 39040
    },
    {
      "epoch": 285.036496350365,
      "grad_norm": 11.116242408752441,
      "learning_rate": 3.574817518248175e-05,
      "loss": 0.745,
      "step": 39050
    },
    {
      "epoch": 285.1094890510949,
      "grad_norm": 0.16759556531906128,
      "learning_rate": 3.574452554744526e-05,
      "loss": 0.9535,
      "step": 39060
    },
    {
      "epoch": 285.1824817518248,
      "grad_norm": 19.044252395629883,
      "learning_rate": 3.574087591240876e-05,
      "loss": 0.7739,
      "step": 39070
    },
    {
      "epoch": 285.2554744525547,
      "grad_norm": 13.703289031982422,
      "learning_rate": 3.573722627737227e-05,
      "loss": 0.8636,
      "step": 39080
    },
    {
      "epoch": 285.3284671532847,
      "grad_norm": 18.341354370117188,
      "learning_rate": 3.573357664233577e-05,
      "loss": 1.5189,
      "step": 39090
    },
    {
      "epoch": 285.4014598540146,
      "grad_norm": 6.95332145690918,
      "learning_rate": 3.5729927007299275e-05,
      "loss": 0.9365,
      "step": 39100
    },
    {
      "epoch": 285.4744525547445,
      "grad_norm": 6.233381748199463,
      "learning_rate": 3.5726277372262775e-05,
      "loss": 0.6487,
      "step": 39110
    },
    {
      "epoch": 285.54744525547443,
      "grad_norm": 8.845134735107422,
      "learning_rate": 3.5722627737226276e-05,
      "loss": 0.9513,
      "step": 39120
    },
    {
      "epoch": 285.6204379562044,
      "grad_norm": 7.304450511932373,
      "learning_rate": 3.571897810218978e-05,
      "loss": 1.0269,
      "step": 39130
    },
    {
      "epoch": 285.6934306569343,
      "grad_norm": 12.690587043762207,
      "learning_rate": 3.5715328467153283e-05,
      "loss": 1.0793,
      "step": 39140
    },
    {
      "epoch": 285.7664233576642,
      "grad_norm": 16.955556869506836,
      "learning_rate": 3.571167883211679e-05,
      "loss": 1.0959,
      "step": 39150
    },
    {
      "epoch": 285.83941605839414,
      "grad_norm": 0.37605687975883484,
      "learning_rate": 3.57080291970803e-05,
      "loss": 0.6288,
      "step": 39160
    },
    {
      "epoch": 285.9124087591241,
      "grad_norm": 11.725196838378906,
      "learning_rate": 3.57043795620438e-05,
      "loss": 0.9765,
      "step": 39170
    },
    {
      "epoch": 285.985401459854,
      "grad_norm": 18.81650161743164,
      "learning_rate": 3.5700729927007306e-05,
      "loss": 0.845,
      "step": 39180
    },
    {
      "epoch": 286.05839416058393,
      "grad_norm": 7.644461631774902,
      "learning_rate": 3.56970802919708e-05,
      "loss": 1.2002,
      "step": 39190
    },
    {
      "epoch": 286.13138686131384,
      "grad_norm": 9.787680625915527,
      "learning_rate": 3.569343065693431e-05,
      "loss": 1.2341,
      "step": 39200
    },
    {
      "epoch": 286.2043795620438,
      "grad_norm": 14.758851051330566,
      "learning_rate": 3.5689781021897814e-05,
      "loss": 0.983,
      "step": 39210
    },
    {
      "epoch": 286.2773722627737,
      "grad_norm": 5.364892959594727,
      "learning_rate": 3.5686131386861314e-05,
      "loss": 1.0989,
      "step": 39220
    },
    {
      "epoch": 286.35036496350364,
      "grad_norm": 7.808872222900391,
      "learning_rate": 3.568248175182482e-05,
      "loss": 0.877,
      "step": 39230
    },
    {
      "epoch": 286.42335766423355,
      "grad_norm": 8.439496040344238,
      "learning_rate": 3.567883211678832e-05,
      "loss": 0.8682,
      "step": 39240
    },
    {
      "epoch": 286.4963503649635,
      "grad_norm": 0.5960268974304199,
      "learning_rate": 3.567518248175183e-05,
      "loss": 1.188,
      "step": 39250
    },
    {
      "epoch": 286.56934306569343,
      "grad_norm": 8.148419380187988,
      "learning_rate": 3.567153284671533e-05,
      "loss": 0.6227,
      "step": 39260
    },
    {
      "epoch": 286.64233576642334,
      "grad_norm": 0.24203330278396606,
      "learning_rate": 3.566788321167883e-05,
      "loss": 0.4434,
      "step": 39270
    },
    {
      "epoch": 286.7153284671533,
      "grad_norm": 7.827065467834473,
      "learning_rate": 3.566423357664234e-05,
      "loss": 0.9717,
      "step": 39280
    },
    {
      "epoch": 286.7883211678832,
      "grad_norm": 8.427547454833984,
      "learning_rate": 3.566058394160584e-05,
      "loss": 0.9039,
      "step": 39290
    },
    {
      "epoch": 286.86131386861314,
      "grad_norm": 14.283346176147461,
      "learning_rate": 3.5656934306569346e-05,
      "loss": 0.9887,
      "step": 39300
    },
    {
      "epoch": 286.93430656934305,
      "grad_norm": 0.3089713454246521,
      "learning_rate": 3.565328467153285e-05,
      "loss": 0.9282,
      "step": 39310
    },
    {
      "epoch": 287.007299270073,
      "grad_norm": 12.765236854553223,
      "learning_rate": 3.564963503649635e-05,
      "loss": 1.3355,
      "step": 39320
    },
    {
      "epoch": 287.08029197080293,
      "grad_norm": 7.036113262176514,
      "learning_rate": 3.564598540145986e-05,
      "loss": 0.8702,
      "step": 39330
    },
    {
      "epoch": 287.15328467153284,
      "grad_norm": 6.359994411468506,
      "learning_rate": 3.5642335766423354e-05,
      "loss": 0.7238,
      "step": 39340
    },
    {
      "epoch": 287.22627737226276,
      "grad_norm": 0.14702627062797546,
      "learning_rate": 3.563868613138686e-05,
      "loss": 0.9215,
      "step": 39350
    },
    {
      "epoch": 287.2992700729927,
      "grad_norm": 12.04109001159668,
      "learning_rate": 3.563503649635037e-05,
      "loss": 1.1066,
      "step": 39360
    },
    {
      "epoch": 287.37226277372264,
      "grad_norm": 0.6315999031066895,
      "learning_rate": 3.563138686131387e-05,
      "loss": 0.7263,
      "step": 39370
    },
    {
      "epoch": 287.44525547445255,
      "grad_norm": 14.687203407287598,
      "learning_rate": 3.5627737226277377e-05,
      "loss": 0.6519,
      "step": 39380
    },
    {
      "epoch": 287.51824817518246,
      "grad_norm": 0.398163378238678,
      "learning_rate": 3.562408759124088e-05,
      "loss": 1.2462,
      "step": 39390
    },
    {
      "epoch": 287.59124087591243,
      "grad_norm": 16.372833251953125,
      "learning_rate": 3.5620437956204384e-05,
      "loss": 1.1975,
      "step": 39400
    },
    {
      "epoch": 287.66423357664235,
      "grad_norm": 14.94190788269043,
      "learning_rate": 3.5616788321167885e-05,
      "loss": 0.9773,
      "step": 39410
    },
    {
      "epoch": 287.73722627737226,
      "grad_norm": 10.806426048278809,
      "learning_rate": 3.5613138686131385e-05,
      "loss": 0.896,
      "step": 39420
    },
    {
      "epoch": 287.81021897810217,
      "grad_norm": 12.581637382507324,
      "learning_rate": 3.560948905109489e-05,
      "loss": 1.6488,
      "step": 39430
    },
    {
      "epoch": 287.88321167883214,
      "grad_norm": 9.754459381103516,
      "learning_rate": 3.560583941605839e-05,
      "loss": 0.7091,
      "step": 39440
    },
    {
      "epoch": 287.95620437956205,
      "grad_norm": 8.096588134765625,
      "learning_rate": 3.56021897810219e-05,
      "loss": 0.96,
      "step": 39450
    },
    {
      "epoch": 288.02919708029196,
      "grad_norm": 0.28632378578186035,
      "learning_rate": 3.55985401459854e-05,
      "loss": 0.7068,
      "step": 39460
    },
    {
      "epoch": 288.1021897810219,
      "grad_norm": 10.768577575683594,
      "learning_rate": 3.559489051094891e-05,
      "loss": 1.1041,
      "step": 39470
    },
    {
      "epoch": 288.17518248175185,
      "grad_norm": 11.408696174621582,
      "learning_rate": 3.5591240875912415e-05,
      "loss": 0.8436,
      "step": 39480
    },
    {
      "epoch": 288.24817518248176,
      "grad_norm": 3.0381109714508057,
      "learning_rate": 3.558759124087591e-05,
      "loss": 0.749,
      "step": 39490
    },
    {
      "epoch": 288.3211678832117,
      "grad_norm": 7.900692939758301,
      "learning_rate": 3.5583941605839416e-05,
      "loss": 0.9149,
      "step": 39500
    },
    {
      "epoch": 288.3941605839416,
      "grad_norm": 0.3925570249557495,
      "learning_rate": 3.5580291970802924e-05,
      "loss": 1.1147,
      "step": 39510
    },
    {
      "epoch": 288.46715328467155,
      "grad_norm": 16.30549430847168,
      "learning_rate": 3.5576642335766424e-05,
      "loss": 1.1402,
      "step": 39520
    },
    {
      "epoch": 288.54014598540147,
      "grad_norm": 7.517287731170654,
      "learning_rate": 3.557299270072993e-05,
      "loss": 0.8715,
      "step": 39530
    },
    {
      "epoch": 288.6131386861314,
      "grad_norm": 8.068476676940918,
      "learning_rate": 3.556934306569343e-05,
      "loss": 1.0161,
      "step": 39540
    },
    {
      "epoch": 288.6861313868613,
      "grad_norm": 7.991148471832275,
      "learning_rate": 3.556569343065694e-05,
      "loss": 0.9778,
      "step": 39550
    },
    {
      "epoch": 288.75912408759126,
      "grad_norm": 18.819049835205078,
      "learning_rate": 3.556204379562044e-05,
      "loss": 0.624,
      "step": 39560
    },
    {
      "epoch": 288.8321167883212,
      "grad_norm": 17.766773223876953,
      "learning_rate": 3.555839416058394e-05,
      "loss": 0.9443,
      "step": 39570
    },
    {
      "epoch": 288.9051094890511,
      "grad_norm": 1.4502151012420654,
      "learning_rate": 3.555474452554745e-05,
      "loss": 0.7604,
      "step": 39580
    },
    {
      "epoch": 288.978102189781,
      "grad_norm": 0.16278785467147827,
      "learning_rate": 3.555109489051095e-05,
      "loss": 1.0459,
      "step": 39590
    },
    {
      "epoch": 289.05109489051097,
      "grad_norm": 0.8579954504966736,
      "learning_rate": 3.5547445255474455e-05,
      "loss": 0.9184,
      "step": 39600
    },
    {
      "epoch": 289.1240875912409,
      "grad_norm": 15.395220756530762,
      "learning_rate": 3.5543795620437956e-05,
      "loss": 0.9788,
      "step": 39610
    },
    {
      "epoch": 289.1970802919708,
      "grad_norm": 9.683396339416504,
      "learning_rate": 3.554014598540146e-05,
      "loss": 0.7752,
      "step": 39620
    },
    {
      "epoch": 289.2700729927007,
      "grad_norm": 7.170835018157959,
      "learning_rate": 3.553649635036497e-05,
      "loss": 0.9587,
      "step": 39630
    },
    {
      "epoch": 289.3430656934307,
      "grad_norm": 14.110149383544922,
      "learning_rate": 3.5532846715328464e-05,
      "loss": 1.0515,
      "step": 39640
    },
    {
      "epoch": 289.4160583941606,
      "grad_norm": 9.269691467285156,
      "learning_rate": 3.552919708029197e-05,
      "loss": 0.905,
      "step": 39650
    },
    {
      "epoch": 289.4890510948905,
      "grad_norm": 0.3595752716064453,
      "learning_rate": 3.552554744525547e-05,
      "loss": 0.589,
      "step": 39660
    },
    {
      "epoch": 289.5620437956204,
      "grad_norm": 0.337480753660202,
      "learning_rate": 3.552189781021898e-05,
      "loss": 0.7471,
      "step": 39670
    },
    {
      "epoch": 289.6350364963504,
      "grad_norm": 7.709750175476074,
      "learning_rate": 3.5518248175182486e-05,
      "loss": 1.314,
      "step": 39680
    },
    {
      "epoch": 289.7080291970803,
      "grad_norm": 6.9375319480896,
      "learning_rate": 3.551459854014599e-05,
      "loss": 0.6241,
      "step": 39690
    },
    {
      "epoch": 289.7810218978102,
      "grad_norm": 8.905115127563477,
      "learning_rate": 3.5510948905109494e-05,
      "loss": 1.2936,
      "step": 39700
    },
    {
      "epoch": 289.8540145985401,
      "grad_norm": 4.049141883850098,
      "learning_rate": 3.5507299270072995e-05,
      "loss": 1.2915,
      "step": 39710
    },
    {
      "epoch": 289.9270072992701,
      "grad_norm": 7.2270636558532715,
      "learning_rate": 3.5503649635036495e-05,
      "loss": 0.6271,
      "step": 39720
    },
    {
      "epoch": 290.0,
      "grad_norm": 0.6056579351425171,
      "learning_rate": 3.55e-05,
      "loss": 0.8765,
      "step": 39730
    },
    {
      "epoch": 290.0729927007299,
      "grad_norm": 14.15949821472168,
      "learning_rate": 3.54963503649635e-05,
      "loss": 0.7794,
      "step": 39740
    },
    {
      "epoch": 290.1459854014599,
      "grad_norm": 0.5146638751029968,
      "learning_rate": 3.549270072992701e-05,
      "loss": 0.8346,
      "step": 39750
    },
    {
      "epoch": 290.2189781021898,
      "grad_norm": 10.30103588104248,
      "learning_rate": 3.548905109489051e-05,
      "loss": 1.0721,
      "step": 39760
    },
    {
      "epoch": 290.2919708029197,
      "grad_norm": 8.77528190612793,
      "learning_rate": 3.548540145985402e-05,
      "loss": 0.8103,
      "step": 39770
    },
    {
      "epoch": 290.3649635036496,
      "grad_norm": 7.783024311065674,
      "learning_rate": 3.5481751824817525e-05,
      "loss": 1.128,
      "step": 39780
    },
    {
      "epoch": 290.4379562043796,
      "grad_norm": 0.5191977024078369,
      "learning_rate": 3.5478102189781026e-05,
      "loss": 0.9678,
      "step": 39790
    },
    {
      "epoch": 290.5109489051095,
      "grad_norm": 9.442784309387207,
      "learning_rate": 3.5474452554744526e-05,
      "loss": 1.2323,
      "step": 39800
    },
    {
      "epoch": 290.5839416058394,
      "grad_norm": 12.12469482421875,
      "learning_rate": 3.5470802919708027e-05,
      "loss": 1.0724,
      "step": 39810
    },
    {
      "epoch": 290.6569343065693,
      "grad_norm": 8.296554565429688,
      "learning_rate": 3.5467153284671534e-05,
      "loss": 0.802,
      "step": 39820
    },
    {
      "epoch": 290.7299270072993,
      "grad_norm": 10.39548110961914,
      "learning_rate": 3.546350364963504e-05,
      "loss": 1.3185,
      "step": 39830
    },
    {
      "epoch": 290.8029197080292,
      "grad_norm": 9.506258010864258,
      "learning_rate": 3.545985401459854e-05,
      "loss": 0.8019,
      "step": 39840
    },
    {
      "epoch": 290.8759124087591,
      "grad_norm": 0.3785247206687927,
      "learning_rate": 3.545620437956205e-05,
      "loss": 0.8267,
      "step": 39850
    },
    {
      "epoch": 290.94890510948903,
      "grad_norm": 0.2733266353607178,
      "learning_rate": 3.545255474452555e-05,
      "loss": 0.826,
      "step": 39860
    },
    {
      "epoch": 291.021897810219,
      "grad_norm": 8.160276412963867,
      "learning_rate": 3.544890510948905e-05,
      "loss": 0.8226,
      "step": 39870
    },
    {
      "epoch": 291.0948905109489,
      "grad_norm": 7.172329425811768,
      "learning_rate": 3.544525547445256e-05,
      "loss": 0.8952,
      "step": 39880
    },
    {
      "epoch": 291.1678832116788,
      "grad_norm": 13.226259231567383,
      "learning_rate": 3.544160583941606e-05,
      "loss": 0.672,
      "step": 39890
    },
    {
      "epoch": 291.24087591240874,
      "grad_norm": 12.402935981750488,
      "learning_rate": 3.5437956204379565e-05,
      "loss": 0.9332,
      "step": 39900
    },
    {
      "epoch": 291.3138686131387,
      "grad_norm": 0.28840208053588867,
      "learning_rate": 3.5434306569343065e-05,
      "loss": 0.9832,
      "step": 39910
    },
    {
      "epoch": 291.3868613138686,
      "grad_norm": 11.133665084838867,
      "learning_rate": 3.543065693430657e-05,
      "loss": 1.0357,
      "step": 39920
    },
    {
      "epoch": 291.45985401459853,
      "grad_norm": 8.037772178649902,
      "learning_rate": 3.542700729927007e-05,
      "loss": 0.8819,
      "step": 39930
    },
    {
      "epoch": 291.53284671532845,
      "grad_norm": 9.86722469329834,
      "learning_rate": 3.542335766423358e-05,
      "loss": 0.6617,
      "step": 39940
    },
    {
      "epoch": 291.6058394160584,
      "grad_norm": 17.035253524780273,
      "learning_rate": 3.541970802919708e-05,
      "loss": 0.8807,
      "step": 39950
    },
    {
      "epoch": 291.6788321167883,
      "grad_norm": 11.424586296081543,
      "learning_rate": 3.541605839416058e-05,
      "loss": 1.0495,
      "step": 39960
    },
    {
      "epoch": 291.75182481751824,
      "grad_norm": 11.587383270263672,
      "learning_rate": 3.541240875912409e-05,
      "loss": 1.1668,
      "step": 39970
    },
    {
      "epoch": 291.82481751824815,
      "grad_norm": 8.014158248901367,
      "learning_rate": 3.5408759124087596e-05,
      "loss": 1.1545,
      "step": 39980
    },
    {
      "epoch": 291.8978102189781,
      "grad_norm": 14.786253929138184,
      "learning_rate": 3.5405109489051096e-05,
      "loss": 1.277,
      "step": 39990
    },
    {
      "epoch": 291.97080291970804,
      "grad_norm": 5.592662334442139,
      "learning_rate": 3.5401459854014604e-05,
      "loss": 0.7727,
      "step": 40000
    },
    {
      "epoch": 292.04379562043795,
      "grad_norm": 7.097254276275635,
      "learning_rate": 3.5397810218978104e-05,
      "loss": 1.1925,
      "step": 40010
    },
    {
      "epoch": 292.11678832116786,
      "grad_norm": 10.97447395324707,
      "learning_rate": 3.539416058394161e-05,
      "loss": 1.5184,
      "step": 40020
    },
    {
      "epoch": 292.18978102189783,
      "grad_norm": 7.135696887969971,
      "learning_rate": 3.539051094890511e-05,
      "loss": 0.8296,
      "step": 40030
    },
    {
      "epoch": 292.26277372262774,
      "grad_norm": 11.843688011169434,
      "learning_rate": 3.538686131386861e-05,
      "loss": 0.6524,
      "step": 40040
    },
    {
      "epoch": 292.33576642335765,
      "grad_norm": 10.787898063659668,
      "learning_rate": 3.538321167883212e-05,
      "loss": 0.9937,
      "step": 40050
    },
    {
      "epoch": 292.40875912408757,
      "grad_norm": 9.71081256866455,
      "learning_rate": 3.537956204379562e-05,
      "loss": 1.2564,
      "step": 40060
    },
    {
      "epoch": 292.48175182481754,
      "grad_norm": 0.39727452397346497,
      "learning_rate": 3.537591240875913e-05,
      "loss": 0.9912,
      "step": 40070
    },
    {
      "epoch": 292.55474452554745,
      "grad_norm": 9.147289276123047,
      "learning_rate": 3.537226277372263e-05,
      "loss": 1.0294,
      "step": 40080
    },
    {
      "epoch": 292.62773722627736,
      "grad_norm": 12.773950576782227,
      "learning_rate": 3.5368613138686135e-05,
      "loss": 0.6959,
      "step": 40090
    },
    {
      "epoch": 292.7007299270073,
      "grad_norm": 10.391386985778809,
      "learning_rate": 3.5364963503649636e-05,
      "loss": 1.2689,
      "step": 40100
    },
    {
      "epoch": 292.77372262773724,
      "grad_norm": 0.26792407035827637,
      "learning_rate": 3.5361313868613136e-05,
      "loss": 0.8104,
      "step": 40110
    },
    {
      "epoch": 292.84671532846716,
      "grad_norm": 10.088665962219238,
      "learning_rate": 3.5357664233576644e-05,
      "loss": 0.9219,
      "step": 40120
    },
    {
      "epoch": 292.91970802919707,
      "grad_norm": 0.35468265414237976,
      "learning_rate": 3.5354014598540144e-05,
      "loss": 0.8933,
      "step": 40130
    },
    {
      "epoch": 292.992700729927,
      "grad_norm": 7.984591960906982,
      "learning_rate": 3.535036496350365e-05,
      "loss": 1.0449,
      "step": 40140
    },
    {
      "epoch": 293.06569343065695,
      "grad_norm": 12.676151275634766,
      "learning_rate": 3.534671532846716e-05,
      "loss": 0.8582,
      "step": 40150
    },
    {
      "epoch": 293.13868613138686,
      "grad_norm": 0.28181591629981995,
      "learning_rate": 3.534306569343066e-05,
      "loss": 0.669,
      "step": 40160
    },
    {
      "epoch": 293.2116788321168,
      "grad_norm": 0.22017589211463928,
      "learning_rate": 3.5339416058394166e-05,
      "loss": 0.8892,
      "step": 40170
    },
    {
      "epoch": 293.2846715328467,
      "grad_norm": 7.864794731140137,
      "learning_rate": 3.533576642335767e-05,
      "loss": 0.6599,
      "step": 40180
    },
    {
      "epoch": 293.35766423357666,
      "grad_norm": 9.613788604736328,
      "learning_rate": 3.533211678832117e-05,
      "loss": 0.9075,
      "step": 40190
    },
    {
      "epoch": 293.43065693430657,
      "grad_norm": 16.04698371887207,
      "learning_rate": 3.5328467153284675e-05,
      "loss": 1.1199,
      "step": 40200
    },
    {
      "epoch": 293.5036496350365,
      "grad_norm": 13.973163604736328,
      "learning_rate": 3.5324817518248175e-05,
      "loss": 1.0114,
      "step": 40210
    },
    {
      "epoch": 293.57664233576645,
      "grad_norm": 8.653788566589355,
      "learning_rate": 3.532116788321168e-05,
      "loss": 1.0277,
      "step": 40220
    },
    {
      "epoch": 293.64963503649636,
      "grad_norm": 10.932631492614746,
      "learning_rate": 3.531751824817518e-05,
      "loss": 1.0578,
      "step": 40230
    },
    {
      "epoch": 293.7226277372263,
      "grad_norm": 9.888434410095215,
      "learning_rate": 3.531386861313869e-05,
      "loss": 1.1413,
      "step": 40240
    },
    {
      "epoch": 293.7956204379562,
      "grad_norm": 0.233990877866745,
      "learning_rate": 3.53102189781022e-05,
      "loss": 0.9828,
      "step": 40250
    },
    {
      "epoch": 293.86861313868616,
      "grad_norm": 10.944392204284668,
      "learning_rate": 3.530656934306569e-05,
      "loss": 1.021,
      "step": 40260
    },
    {
      "epoch": 293.94160583941607,
      "grad_norm": 14.431487083435059,
      "learning_rate": 3.53029197080292e-05,
      "loss": 0.9358,
      "step": 40270
    },
    {
      "epoch": 294.014598540146,
      "grad_norm": 0.22812378406524658,
      "learning_rate": 3.52992700729927e-05,
      "loss": 0.9816,
      "step": 40280
    },
    {
      "epoch": 294.0875912408759,
      "grad_norm": 0.6096228957176208,
      "learning_rate": 3.5295620437956206e-05,
      "loss": 0.9106,
      "step": 40290
    },
    {
      "epoch": 294.16058394160586,
      "grad_norm": 9.71998119354248,
      "learning_rate": 3.5291970802919713e-05,
      "loss": 1.0248,
      "step": 40300
    },
    {
      "epoch": 294.2335766423358,
      "grad_norm": 12.445755958557129,
      "learning_rate": 3.5288321167883214e-05,
      "loss": 1.1928,
      "step": 40310
    },
    {
      "epoch": 294.3065693430657,
      "grad_norm": 5.407766819000244,
      "learning_rate": 3.528467153284672e-05,
      "loss": 0.9604,
      "step": 40320
    },
    {
      "epoch": 294.3795620437956,
      "grad_norm": 7.250444412231445,
      "learning_rate": 3.5281021897810215e-05,
      "loss": 1.1464,
      "step": 40330
    },
    {
      "epoch": 294.45255474452557,
      "grad_norm": 8.295169830322266,
      "learning_rate": 3.527737226277372e-05,
      "loss": 1.0472,
      "step": 40340
    },
    {
      "epoch": 294.5255474452555,
      "grad_norm": 7.01361608505249,
      "learning_rate": 3.527372262773723e-05,
      "loss": 0.7192,
      "step": 40350
    },
    {
      "epoch": 294.5985401459854,
      "grad_norm": 0.35826727747917175,
      "learning_rate": 3.527007299270073e-05,
      "loss": 0.5197,
      "step": 40360
    },
    {
      "epoch": 294.6715328467153,
      "grad_norm": 7.332484722137451,
      "learning_rate": 3.526642335766424e-05,
      "loss": 0.5017,
      "step": 40370
    },
    {
      "epoch": 294.7445255474453,
      "grad_norm": 9.475950241088867,
      "learning_rate": 3.526277372262774e-05,
      "loss": 1.2331,
      "step": 40380
    },
    {
      "epoch": 294.8175182481752,
      "grad_norm": 5.3401079177856445,
      "learning_rate": 3.5259124087591245e-05,
      "loss": 0.8364,
      "step": 40390
    },
    {
      "epoch": 294.8905109489051,
      "grad_norm": 10.13105297088623,
      "learning_rate": 3.5255474452554745e-05,
      "loss": 1.0393,
      "step": 40400
    },
    {
      "epoch": 294.963503649635,
      "grad_norm": 0.2194090187549591,
      "learning_rate": 3.5251824817518246e-05,
      "loss": 1.0531,
      "step": 40410
    },
    {
      "epoch": 295.036496350365,
      "grad_norm": 9.60697078704834,
      "learning_rate": 3.524817518248175e-05,
      "loss": 1.4801,
      "step": 40420
    },
    {
      "epoch": 295.1094890510949,
      "grad_norm": 5.11164665222168,
      "learning_rate": 3.5244525547445254e-05,
      "loss": 0.7621,
      "step": 40430
    },
    {
      "epoch": 295.1824817518248,
      "grad_norm": 10.054932594299316,
      "learning_rate": 3.524087591240876e-05,
      "loss": 0.4857,
      "step": 40440
    },
    {
      "epoch": 295.2554744525547,
      "grad_norm": 6.048291206359863,
      "learning_rate": 3.523722627737227e-05,
      "loss": 1.5036,
      "step": 40450
    },
    {
      "epoch": 295.3284671532847,
      "grad_norm": 8.128507614135742,
      "learning_rate": 3.523357664233577e-05,
      "loss": 0.7344,
      "step": 40460
    },
    {
      "epoch": 295.4014598540146,
      "grad_norm": 10.51230239868164,
      "learning_rate": 3.5229927007299276e-05,
      "loss": 0.5029,
      "step": 40470
    },
    {
      "epoch": 295.4744525547445,
      "grad_norm": 6.811116695404053,
      "learning_rate": 3.5226277372262777e-05,
      "loss": 0.9985,
      "step": 40480
    },
    {
      "epoch": 295.54744525547443,
      "grad_norm": 8.934565544128418,
      "learning_rate": 3.522262773722628e-05,
      "loss": 1.0974,
      "step": 40490
    },
    {
      "epoch": 295.6204379562044,
      "grad_norm": 6.063356876373291,
      "learning_rate": 3.5218978102189784e-05,
      "loss": 0.495,
      "step": 40500
    },
    {
      "epoch": 295.6934306569343,
      "grad_norm": 16.565256118774414,
      "learning_rate": 3.5215328467153285e-05,
      "loss": 0.9136,
      "step": 40510
    },
    {
      "epoch": 295.7664233576642,
      "grad_norm": 9.4490385055542,
      "learning_rate": 3.521167883211679e-05,
      "loss": 1.0155,
      "step": 40520
    },
    {
      "epoch": 295.83941605839414,
      "grad_norm": 7.823617935180664,
      "learning_rate": 3.520802919708029e-05,
      "loss": 1.0701,
      "step": 40530
    },
    {
      "epoch": 295.9124087591241,
      "grad_norm": 8.049654960632324,
      "learning_rate": 3.52043795620438e-05,
      "loss": 1.1498,
      "step": 40540
    },
    {
      "epoch": 295.985401459854,
      "grad_norm": 8.276248931884766,
      "learning_rate": 3.52007299270073e-05,
      "loss": 1.2762,
      "step": 40550
    },
    {
      "epoch": 296.05839416058393,
      "grad_norm": 8.584901809692383,
      "learning_rate": 3.51970802919708e-05,
      "loss": 0.8166,
      "step": 40560
    },
    {
      "epoch": 296.13138686131384,
      "grad_norm": 0.28033891320228577,
      "learning_rate": 3.519343065693431e-05,
      "loss": 0.8708,
      "step": 40570
    },
    {
      "epoch": 296.2043795620438,
      "grad_norm": 13.844053268432617,
      "learning_rate": 3.518978102189781e-05,
      "loss": 0.9461,
      "step": 40580
    },
    {
      "epoch": 296.2773722627737,
      "grad_norm": 0.2052689492702484,
      "learning_rate": 3.5186131386861316e-05,
      "loss": 0.4297,
      "step": 40590
    },
    {
      "epoch": 296.35036496350364,
      "grad_norm": 0.2246699184179306,
      "learning_rate": 3.5182481751824816e-05,
      "loss": 1.1384,
      "step": 40600
    },
    {
      "epoch": 296.42335766423355,
      "grad_norm": 7.011178970336914,
      "learning_rate": 3.5178832116788324e-05,
      "loss": 0.5946,
      "step": 40610
    },
    {
      "epoch": 296.4963503649635,
      "grad_norm": 7.574851989746094,
      "learning_rate": 3.517518248175183e-05,
      "loss": 1.4425,
      "step": 40620
    },
    {
      "epoch": 296.56934306569343,
      "grad_norm": 5.207489490509033,
      "learning_rate": 3.517153284671533e-05,
      "loss": 0.9025,
      "step": 40630
    },
    {
      "epoch": 296.64233576642334,
      "grad_norm": 8.830692291259766,
      "learning_rate": 3.516788321167883e-05,
      "loss": 0.9762,
      "step": 40640
    },
    {
      "epoch": 296.7153284671533,
      "grad_norm": 0.26880943775177,
      "learning_rate": 3.516423357664234e-05,
      "loss": 0.822,
      "step": 40650
    },
    {
      "epoch": 296.7883211678832,
      "grad_norm": 14.451557159423828,
      "learning_rate": 3.516058394160584e-05,
      "loss": 1.1796,
      "step": 40660
    },
    {
      "epoch": 296.86131386861314,
      "grad_norm": 8.910361289978027,
      "learning_rate": 3.515693430656935e-05,
      "loss": 1.2156,
      "step": 40670
    },
    {
      "epoch": 296.93430656934305,
      "grad_norm": 9.036703109741211,
      "learning_rate": 3.515328467153285e-05,
      "loss": 0.9724,
      "step": 40680
    },
    {
      "epoch": 297.007299270073,
      "grad_norm": 0.12055708467960358,
      "learning_rate": 3.5149635036496355e-05,
      "loss": 0.6686,
      "step": 40690
    },
    {
      "epoch": 297.08029197080293,
      "grad_norm": 7.874307632446289,
      "learning_rate": 3.5145985401459855e-05,
      "loss": 0.6691,
      "step": 40700
    },
    {
      "epoch": 297.15328467153284,
      "grad_norm": 9.59371566772461,
      "learning_rate": 3.514233576642336e-05,
      "loss": 0.8232,
      "step": 40710
    },
    {
      "epoch": 297.22627737226276,
      "grad_norm": 7.58829927444458,
      "learning_rate": 3.513868613138686e-05,
      "loss": 0.9088,
      "step": 40720
    },
    {
      "epoch": 297.2992700729927,
      "grad_norm": 8.423219680786133,
      "learning_rate": 3.5135036496350363e-05,
      "loss": 0.9967,
      "step": 40730
    },
    {
      "epoch": 297.37226277372264,
      "grad_norm": 6.162844657897949,
      "learning_rate": 3.513138686131387e-05,
      "loss": 0.9407,
      "step": 40740
    },
    {
      "epoch": 297.44525547445255,
      "grad_norm": 9.268722534179688,
      "learning_rate": 3.512773722627737e-05,
      "loss": 1.3241,
      "step": 40750
    },
    {
      "epoch": 297.51824817518246,
      "grad_norm": 7.048867702484131,
      "learning_rate": 3.512408759124088e-05,
      "loss": 1.1403,
      "step": 40760
    },
    {
      "epoch": 297.59124087591243,
      "grad_norm": 7.638245582580566,
      "learning_rate": 3.5120437956204386e-05,
      "loss": 0.7592,
      "step": 40770
    },
    {
      "epoch": 297.66423357664235,
      "grad_norm": 10.124223709106445,
      "learning_rate": 3.5116788321167886e-05,
      "loss": 1.19,
      "step": 40780
    },
    {
      "epoch": 297.73722627737226,
      "grad_norm": 7.38831090927124,
      "learning_rate": 3.511313868613139e-05,
      "loss": 0.7211,
      "step": 40790
    },
    {
      "epoch": 297.81021897810217,
      "grad_norm": 8.195154190063477,
      "learning_rate": 3.510948905109489e-05,
      "loss": 1.1704,
      "step": 40800
    },
    {
      "epoch": 297.88321167883214,
      "grad_norm": 3.2329511642456055,
      "learning_rate": 3.5105839416058395e-05,
      "loss": 0.7313,
      "step": 40810
    },
    {
      "epoch": 297.95620437956205,
      "grad_norm": 11.197083473205566,
      "learning_rate": 3.51021897810219e-05,
      "loss": 1.3602,
      "step": 40820
    },
    {
      "epoch": 298.02919708029196,
      "grad_norm": 15.672460556030273,
      "learning_rate": 3.50985401459854e-05,
      "loss": 1.2564,
      "step": 40830
    },
    {
      "epoch": 298.1021897810219,
      "grad_norm": 14.141012191772461,
      "learning_rate": 3.509489051094891e-05,
      "loss": 0.837,
      "step": 40840
    },
    {
      "epoch": 298.17518248175185,
      "grad_norm": 2.0052661895751953,
      "learning_rate": 3.509124087591241e-05,
      "loss": 0.9386,
      "step": 40850
    },
    {
      "epoch": 298.24817518248176,
      "grad_norm": 5.719991683959961,
      "learning_rate": 3.508759124087592e-05,
      "loss": 1.0309,
      "step": 40860
    },
    {
      "epoch": 298.3211678832117,
      "grad_norm": 10.76933765411377,
      "learning_rate": 3.508394160583942e-05,
      "loss": 1.3408,
      "step": 40870
    },
    {
      "epoch": 298.3941605839416,
      "grad_norm": 10.82071590423584,
      "learning_rate": 3.508029197080292e-05,
      "loss": 0.9986,
      "step": 40880
    },
    {
      "epoch": 298.46715328467155,
      "grad_norm": 16.90443229675293,
      "learning_rate": 3.5076642335766426e-05,
      "loss": 0.8518,
      "step": 40890
    },
    {
      "epoch": 298.54014598540147,
      "grad_norm": 10.948728561401367,
      "learning_rate": 3.5072992700729926e-05,
      "loss": 0.8035,
      "step": 40900
    },
    {
      "epoch": 298.6131386861314,
      "grad_norm": 14.994029998779297,
      "learning_rate": 3.506934306569343e-05,
      "loss": 0.9646,
      "step": 40910
    },
    {
      "epoch": 298.6861313868613,
      "grad_norm": 0.43147072196006775,
      "learning_rate": 3.506569343065694e-05,
      "loss": 0.8443,
      "step": 40920
    },
    {
      "epoch": 298.75912408759126,
      "grad_norm": 7.557641506195068,
      "learning_rate": 3.506204379562044e-05,
      "loss": 0.9062,
      "step": 40930
    },
    {
      "epoch": 298.8321167883212,
      "grad_norm": 0.10422489047050476,
      "learning_rate": 3.505839416058394e-05,
      "loss": 0.6661,
      "step": 40940
    },
    {
      "epoch": 298.9051094890511,
      "grad_norm": 17.721431732177734,
      "learning_rate": 3.505474452554744e-05,
      "loss": 1.0797,
      "step": 40950
    },
    {
      "epoch": 298.978102189781,
      "grad_norm": 14.093273162841797,
      "learning_rate": 3.505109489051095e-05,
      "loss": 0.991,
      "step": 40960
    },
    {
      "epoch": 299.05109489051097,
      "grad_norm": 17.680004119873047,
      "learning_rate": 3.504744525547446e-05,
      "loss": 1.2639,
      "step": 40970
    },
    {
      "epoch": 299.1240875912409,
      "grad_norm": 7.182007789611816,
      "learning_rate": 3.504379562043796e-05,
      "loss": 0.9126,
      "step": 40980
    },
    {
      "epoch": 299.1970802919708,
      "grad_norm": 3.361830711364746,
      "learning_rate": 3.5040145985401464e-05,
      "loss": 0.8272,
      "step": 40990
    },
    {
      "epoch": 299.2700729927007,
      "grad_norm": 10.613895416259766,
      "learning_rate": 3.5036496350364965e-05,
      "loss": 1.0956,
      "step": 41000
    },
    {
      "epoch": 299.3430656934307,
      "grad_norm": 11.685458183288574,
      "learning_rate": 3.503284671532847e-05,
      "loss": 1.2713,
      "step": 41010
    },
    {
      "epoch": 299.4160583941606,
      "grad_norm": 11.319518089294434,
      "learning_rate": 3.502919708029197e-05,
      "loss": 0.8855,
      "step": 41020
    },
    {
      "epoch": 299.4890510948905,
      "grad_norm": 10.612685203552246,
      "learning_rate": 3.502554744525547e-05,
      "loss": 1.0644,
      "step": 41030
    },
    {
      "epoch": 299.5620437956204,
      "grad_norm": 16.977924346923828,
      "learning_rate": 3.502189781021898e-05,
      "loss": 1.0264,
      "step": 41040
    },
    {
      "epoch": 299.6350364963504,
      "grad_norm": 19.487958908081055,
      "learning_rate": 3.501824817518248e-05,
      "loss": 1.0268,
      "step": 41050
    },
    {
      "epoch": 299.7080291970803,
      "grad_norm": 0.06169859692454338,
      "learning_rate": 3.501459854014599e-05,
      "loss": 0.778,
      "step": 41060
    },
    {
      "epoch": 299.7810218978102,
      "grad_norm": 9.208978652954102,
      "learning_rate": 3.501094890510949e-05,
      "loss": 0.7513,
      "step": 41070
    },
    {
      "epoch": 299.8540145985401,
      "grad_norm": 0.34522393345832825,
      "learning_rate": 3.5007299270072996e-05,
      "loss": 0.7049,
      "step": 41080
    },
    {
      "epoch": 299.9270072992701,
      "grad_norm": 10.73845386505127,
      "learning_rate": 3.50036496350365e-05,
      "loss": 1.2135,
      "step": 41090
    },
    {
      "epoch": 300.0,
      "grad_norm": 0.14906266331672668,
      "learning_rate": 3.5e-05,
      "loss": 0.7774,
      "step": 41100
    },
    {
      "epoch": 300.0729927007299,
      "grad_norm": 8.538241386413574,
      "learning_rate": 3.4996350364963504e-05,
      "loss": 1.2045,
      "step": 41110
    },
    {
      "epoch": 300.1459854014599,
      "grad_norm": 2.8074846267700195,
      "learning_rate": 3.499270072992701e-05,
      "loss": 0.8987,
      "step": 41120
    },
    {
      "epoch": 300.2189781021898,
      "grad_norm": 16.613174438476562,
      "learning_rate": 3.498905109489051e-05,
      "loss": 0.9353,
      "step": 41130
    },
    {
      "epoch": 300.2919708029197,
      "grad_norm": 10.536100387573242,
      "learning_rate": 3.498540145985402e-05,
      "loss": 1.2078,
      "step": 41140
    },
    {
      "epoch": 300.3649635036496,
      "grad_norm": 5.758100509643555,
      "learning_rate": 3.498175182481752e-05,
      "loss": 0.6585,
      "step": 41150
    },
    {
      "epoch": 300.4379562043796,
      "grad_norm": 0.33325478434562683,
      "learning_rate": 3.497810218978103e-05,
      "loss": 0.7585,
      "step": 41160
    },
    {
      "epoch": 300.5109489051095,
      "grad_norm": 0.40935638546943665,
      "learning_rate": 3.497445255474453e-05,
      "loss": 1.0734,
      "step": 41170
    },
    {
      "epoch": 300.5839416058394,
      "grad_norm": 16.850765228271484,
      "learning_rate": 3.497080291970803e-05,
      "loss": 0.9998,
      "step": 41180
    },
    {
      "epoch": 300.6569343065693,
      "grad_norm": 9.08624267578125,
      "learning_rate": 3.4967153284671535e-05,
      "loss": 0.9091,
      "step": 41190
    },
    {
      "epoch": 300.7299270072993,
      "grad_norm": 0.1840226799249649,
      "learning_rate": 3.4963503649635036e-05,
      "loss": 1.087,
      "step": 41200
    },
    {
      "epoch": 300.8029197080292,
      "grad_norm": 11.616572380065918,
      "learning_rate": 3.495985401459854e-05,
      "loss": 0.8819,
      "step": 41210
    },
    {
      "epoch": 300.8759124087591,
      "grad_norm": 9.650344848632812,
      "learning_rate": 3.4956204379562044e-05,
      "loss": 0.7799,
      "step": 41220
    },
    {
      "epoch": 300.94890510948903,
      "grad_norm": 6.930993556976318,
      "learning_rate": 3.495255474452555e-05,
      "loss": 0.8979,
      "step": 41230
    },
    {
      "epoch": 301.021897810219,
      "grad_norm": 9.263710975646973,
      "learning_rate": 3.494890510948906e-05,
      "loss": 1.24,
      "step": 41240
    },
    {
      "epoch": 301.0948905109489,
      "grad_norm": 7.339749813079834,
      "learning_rate": 3.494525547445255e-05,
      "loss": 0.8161,
      "step": 41250
    },
    {
      "epoch": 301.1678832116788,
      "grad_norm": 15.505559921264648,
      "learning_rate": 3.494160583941606e-05,
      "loss": 0.7927,
      "step": 41260
    },
    {
      "epoch": 301.24087591240874,
      "grad_norm": 5.831384658813477,
      "learning_rate": 3.493795620437956e-05,
      "loss": 1.0611,
      "step": 41270
    },
    {
      "epoch": 301.3138686131387,
      "grad_norm": 7.690574645996094,
      "learning_rate": 3.493430656934307e-05,
      "loss": 0.609,
      "step": 41280
    },
    {
      "epoch": 301.3868613138686,
      "grad_norm": 9.896401405334473,
      "learning_rate": 3.4930656934306574e-05,
      "loss": 0.8792,
      "step": 41290
    },
    {
      "epoch": 301.45985401459853,
      "grad_norm": 0.20370912551879883,
      "learning_rate": 3.4927007299270075e-05,
      "loss": 1.2983,
      "step": 41300
    },
    {
      "epoch": 301.53284671532845,
      "grad_norm": 8.070996284484863,
      "learning_rate": 3.492335766423358e-05,
      "loss": 0.8391,
      "step": 41310
    },
    {
      "epoch": 301.6058394160584,
      "grad_norm": 7.327545166015625,
      "learning_rate": 3.491970802919708e-05,
      "loss": 0.8811,
      "step": 41320
    },
    {
      "epoch": 301.6788321167883,
      "grad_norm": 10.707118034362793,
      "learning_rate": 3.491605839416058e-05,
      "loss": 0.9863,
      "step": 41330
    },
    {
      "epoch": 301.75182481751824,
      "grad_norm": 8.048399925231934,
      "learning_rate": 3.491240875912409e-05,
      "loss": 0.7349,
      "step": 41340
    },
    {
      "epoch": 301.82481751824815,
      "grad_norm": 9.338507652282715,
      "learning_rate": 3.490875912408759e-05,
      "loss": 1.2561,
      "step": 41350
    },
    {
      "epoch": 301.8978102189781,
      "grad_norm": 14.605006217956543,
      "learning_rate": 3.49051094890511e-05,
      "loss": 1.1532,
      "step": 41360
    },
    {
      "epoch": 301.97080291970804,
      "grad_norm": 11.670498847961426,
      "learning_rate": 3.49014598540146e-05,
      "loss": 0.8204,
      "step": 41370
    },
    {
      "epoch": 302.04379562043795,
      "grad_norm": 12.752223014831543,
      "learning_rate": 3.4897810218978106e-05,
      "loss": 0.9407,
      "step": 41380
    },
    {
      "epoch": 302.11678832116786,
      "grad_norm": 7.193747043609619,
      "learning_rate": 3.489416058394161e-05,
      "loss": 0.9515,
      "step": 41390
    },
    {
      "epoch": 302.18978102189783,
      "grad_norm": 0.16584740579128265,
      "learning_rate": 3.489051094890511e-05,
      "loss": 1.2755,
      "step": 41400
    },
    {
      "epoch": 302.26277372262774,
      "grad_norm": 7.584878921508789,
      "learning_rate": 3.4886861313868614e-05,
      "loss": 0.8208,
      "step": 41410
    },
    {
      "epoch": 302.33576642335765,
      "grad_norm": 0.19253413379192352,
      "learning_rate": 3.4883211678832114e-05,
      "loss": 0.5397,
      "step": 41420
    },
    {
      "epoch": 302.40875912408757,
      "grad_norm": 14.627964973449707,
      "learning_rate": 3.487956204379562e-05,
      "loss": 1.1554,
      "step": 41430
    },
    {
      "epoch": 302.48175182481754,
      "grad_norm": 6.014034271240234,
      "learning_rate": 3.487591240875913e-05,
      "loss": 0.6743,
      "step": 41440
    },
    {
      "epoch": 302.55474452554745,
      "grad_norm": 5.416425704956055,
      "learning_rate": 3.487226277372263e-05,
      "loss": 1.0998,
      "step": 41450
    },
    {
      "epoch": 302.62773722627736,
      "grad_norm": 12.66031265258789,
      "learning_rate": 3.486861313868614e-05,
      "loss": 0.748,
      "step": 41460
    },
    {
      "epoch": 302.7007299270073,
      "grad_norm": 8.642300605773926,
      "learning_rate": 3.486496350364964e-05,
      "loss": 0.7375,
      "step": 41470
    },
    {
      "epoch": 302.77372262773724,
      "grad_norm": 17.8309383392334,
      "learning_rate": 3.486131386861314e-05,
      "loss": 0.9638,
      "step": 41480
    },
    {
      "epoch": 302.84671532846716,
      "grad_norm": 13.264225006103516,
      "learning_rate": 3.4857664233576645e-05,
      "loss": 1.5716,
      "step": 41490
    },
    {
      "epoch": 302.91970802919707,
      "grad_norm": 0.3672686815261841,
      "learning_rate": 3.4854014598540145e-05,
      "loss": 0.5624,
      "step": 41500
    },
    {
      "epoch": 302.992700729927,
      "grad_norm": 16.113359451293945,
      "learning_rate": 3.485036496350365e-05,
      "loss": 0.8214,
      "step": 41510
    },
    {
      "epoch": 303.06569343065695,
      "grad_norm": 0.7461183667182922,
      "learning_rate": 3.484671532846715e-05,
      "loss": 0.8184,
      "step": 41520
    },
    {
      "epoch": 303.13868613138686,
      "grad_norm": 5.805561065673828,
      "learning_rate": 3.484306569343066e-05,
      "loss": 1.1426,
      "step": 41530
    },
    {
      "epoch": 303.2116788321168,
      "grad_norm": 7.474915504455566,
      "learning_rate": 3.483941605839416e-05,
      "loss": 0.9845,
      "step": 41540
    },
    {
      "epoch": 303.2846715328467,
      "grad_norm": 12.279958724975586,
      "learning_rate": 3.483576642335767e-05,
      "loss": 0.7593,
      "step": 41550
    },
    {
      "epoch": 303.35766423357666,
      "grad_norm": 10.440286636352539,
      "learning_rate": 3.483211678832117e-05,
      "loss": 1.1146,
      "step": 41560
    },
    {
      "epoch": 303.43065693430657,
      "grad_norm": 0.35925808548927307,
      "learning_rate": 3.482846715328467e-05,
      "loss": 0.5128,
      "step": 41570
    },
    {
      "epoch": 303.5036496350365,
      "grad_norm": 0.16726507246494293,
      "learning_rate": 3.4824817518248177e-05,
      "loss": 0.4999,
      "step": 41580
    },
    {
      "epoch": 303.57664233576645,
      "grad_norm": 11.278273582458496,
      "learning_rate": 3.4821167883211684e-05,
      "loss": 0.9791,
      "step": 41590
    },
    {
      "epoch": 303.64963503649636,
      "grad_norm": 16.100984573364258,
      "learning_rate": 3.4817518248175184e-05,
      "loss": 1.3443,
      "step": 41600
    },
    {
      "epoch": 303.7226277372263,
      "grad_norm": 7.643356800079346,
      "learning_rate": 3.481386861313869e-05,
      "loss": 1.0472,
      "step": 41610
    },
    {
      "epoch": 303.7956204379562,
      "grad_norm": 10.394146919250488,
      "learning_rate": 3.481021897810219e-05,
      "loss": 0.9724,
      "step": 41620
    },
    {
      "epoch": 303.86861313868616,
      "grad_norm": 7.793087959289551,
      "learning_rate": 3.480656934306569e-05,
      "loss": 1.1258,
      "step": 41630
    },
    {
      "epoch": 303.94160583941607,
      "grad_norm": 19.066360473632812,
      "learning_rate": 3.48029197080292e-05,
      "loss": 1.0403,
      "step": 41640
    },
    {
      "epoch": 304.014598540146,
      "grad_norm": 13.343240737915039,
      "learning_rate": 3.47992700729927e-05,
      "loss": 1.0635,
      "step": 41650
    },
    {
      "epoch": 304.0875912408759,
      "grad_norm": 9.54952335357666,
      "learning_rate": 3.479562043795621e-05,
      "loss": 0.7854,
      "step": 41660
    },
    {
      "epoch": 304.16058394160586,
      "grad_norm": 7.468142509460449,
      "learning_rate": 3.479197080291971e-05,
      "loss": 0.656,
      "step": 41670
    },
    {
      "epoch": 304.2335766423358,
      "grad_norm": 0.17904460430145264,
      "learning_rate": 3.4788321167883215e-05,
      "loss": 0.7775,
      "step": 41680
    },
    {
      "epoch": 304.3065693430657,
      "grad_norm": 14.45541763305664,
      "learning_rate": 3.4784671532846716e-05,
      "loss": 0.9073,
      "step": 41690
    },
    {
      "epoch": 304.3795620437956,
      "grad_norm": 13.070542335510254,
      "learning_rate": 3.478102189781022e-05,
      "loss": 0.7544,
      "step": 41700
    },
    {
      "epoch": 304.45255474452557,
      "grad_norm": 12.353251457214355,
      "learning_rate": 3.4777372262773724e-05,
      "loss": 0.9357,
      "step": 41710
    },
    {
      "epoch": 304.5255474452555,
      "grad_norm": 8.434762001037598,
      "learning_rate": 3.4773722627737224e-05,
      "loss": 1.1345,
      "step": 41720
    },
    {
      "epoch": 304.5985401459854,
      "grad_norm": 12.247983932495117,
      "learning_rate": 3.477007299270073e-05,
      "loss": 0.9752,
      "step": 41730
    },
    {
      "epoch": 304.6715328467153,
      "grad_norm": 5.7331061363220215,
      "learning_rate": 3.476642335766423e-05,
      "loss": 0.6545,
      "step": 41740
    },
    {
      "epoch": 304.7445255474453,
      "grad_norm": 16.260570526123047,
      "learning_rate": 3.476277372262774e-05,
      "loss": 1.1121,
      "step": 41750
    },
    {
      "epoch": 304.8175182481752,
      "grad_norm": 12.186725616455078,
      "learning_rate": 3.4759124087591246e-05,
      "loss": 1.0595,
      "step": 41760
    },
    {
      "epoch": 304.8905109489051,
      "grad_norm": 5.664741516113281,
      "learning_rate": 3.475547445255475e-05,
      "loss": 1.0817,
      "step": 41770
    },
    {
      "epoch": 304.963503649635,
      "grad_norm": 5.248229503631592,
      "learning_rate": 3.4751824817518254e-05,
      "loss": 1.0936,
      "step": 41780
    },
    {
      "epoch": 305.036496350365,
      "grad_norm": 15.35875129699707,
      "learning_rate": 3.4748175182481755e-05,
      "loss": 1.2235,
      "step": 41790
    },
    {
      "epoch": 305.1094890510949,
      "grad_norm": 12.097756385803223,
      "learning_rate": 3.4744525547445255e-05,
      "loss": 1.1076,
      "step": 41800
    },
    {
      "epoch": 305.1824817518248,
      "grad_norm": 5.144562721252441,
      "learning_rate": 3.474087591240876e-05,
      "loss": 0.8393,
      "step": 41810
    },
    {
      "epoch": 305.2554744525547,
      "grad_norm": 20.278766632080078,
      "learning_rate": 3.473722627737226e-05,
      "loss": 1.3131,
      "step": 41820
    },
    {
      "epoch": 305.3284671532847,
      "grad_norm": 6.423654079437256,
      "learning_rate": 3.473357664233577e-05,
      "loss": 0.783,
      "step": 41830
    },
    {
      "epoch": 305.4014598540146,
      "grad_norm": 14.206681251525879,
      "learning_rate": 3.472992700729927e-05,
      "loss": 1.0665,
      "step": 41840
    },
    {
      "epoch": 305.4744525547445,
      "grad_norm": 7.336202144622803,
      "learning_rate": 3.472627737226278e-05,
      "loss": 0.9132,
      "step": 41850
    },
    {
      "epoch": 305.54744525547443,
      "grad_norm": 15.08346176147461,
      "learning_rate": 3.472262773722628e-05,
      "loss": 0.8607,
      "step": 41860
    },
    {
      "epoch": 305.6204379562044,
      "grad_norm": 18.946741104125977,
      "learning_rate": 3.471897810218978e-05,
      "loss": 0.95,
      "step": 41870
    },
    {
      "epoch": 305.6934306569343,
      "grad_norm": 7.663620471954346,
      "learning_rate": 3.4715328467153286e-05,
      "loss": 0.8267,
      "step": 41880
    },
    {
      "epoch": 305.7664233576642,
      "grad_norm": 13.948918342590332,
      "learning_rate": 3.471167883211679e-05,
      "loss": 1.0582,
      "step": 41890
    },
    {
      "epoch": 305.83941605839414,
      "grad_norm": 11.995963096618652,
      "learning_rate": 3.4708029197080294e-05,
      "loss": 1.029,
      "step": 41900
    },
    {
      "epoch": 305.9124087591241,
      "grad_norm": 6.765169620513916,
      "learning_rate": 3.47043795620438e-05,
      "loss": 0.8026,
      "step": 41910
    },
    {
      "epoch": 305.985401459854,
      "grad_norm": 10.653693199157715,
      "learning_rate": 3.47007299270073e-05,
      "loss": 0.954,
      "step": 41920
    },
    {
      "epoch": 306.05839416058393,
      "grad_norm": 16.40959358215332,
      "learning_rate": 3.469708029197081e-05,
      "loss": 1.2348,
      "step": 41930
    },
    {
      "epoch": 306.13138686131384,
      "grad_norm": 15.387977600097656,
      "learning_rate": 3.46934306569343e-05,
      "loss": 0.9247,
      "step": 41940
    },
    {
      "epoch": 306.2043795620438,
      "grad_norm": 10.354545593261719,
      "learning_rate": 3.468978102189781e-05,
      "loss": 0.9983,
      "step": 41950
    },
    {
      "epoch": 306.2773722627737,
      "grad_norm": 18.805341720581055,
      "learning_rate": 3.468613138686132e-05,
      "loss": 0.8481,
      "step": 41960
    },
    {
      "epoch": 306.35036496350364,
      "grad_norm": 7.261643409729004,
      "learning_rate": 3.468248175182482e-05,
      "loss": 0.9055,
      "step": 41970
    },
    {
      "epoch": 306.42335766423355,
      "grad_norm": 0.17238463461399078,
      "learning_rate": 3.4678832116788325e-05,
      "loss": 0.9458,
      "step": 41980
    },
    {
      "epoch": 306.4963503649635,
      "grad_norm": 7.806451320648193,
      "learning_rate": 3.4675182481751826e-05,
      "loss": 1.0547,
      "step": 41990
    },
    {
      "epoch": 306.56934306569343,
      "grad_norm": 6.799272060394287,
      "learning_rate": 3.467153284671533e-05,
      "loss": 0.9445,
      "step": 42000
    },
    {
      "epoch": 306.64233576642334,
      "grad_norm": 7.884363174438477,
      "learning_rate": 3.466788321167883e-05,
      "loss": 1.0197,
      "step": 42010
    },
    {
      "epoch": 306.7153284671533,
      "grad_norm": 9.761833190917969,
      "learning_rate": 3.4664233576642334e-05,
      "loss": 1.0242,
      "step": 42020
    },
    {
      "epoch": 306.7883211678832,
      "grad_norm": 0.23832623660564423,
      "learning_rate": 3.466058394160584e-05,
      "loss": 1.0299,
      "step": 42030
    },
    {
      "epoch": 306.86131386861314,
      "grad_norm": 0.18526938557624817,
      "learning_rate": 3.465693430656934e-05,
      "loss": 0.8232,
      "step": 42040
    },
    {
      "epoch": 306.93430656934305,
      "grad_norm": 9.4511137008667,
      "learning_rate": 3.465328467153285e-05,
      "loss": 1.0476,
      "step": 42050
    },
    {
      "epoch": 307.007299270073,
      "grad_norm": 8.630495071411133,
      "learning_rate": 3.4649635036496356e-05,
      "loss": 0.932,
      "step": 42060
    },
    {
      "epoch": 307.08029197080293,
      "grad_norm": 0.9053415060043335,
      "learning_rate": 3.4645985401459857e-05,
      "loss": 0.8288,
      "step": 42070
    },
    {
      "epoch": 307.15328467153284,
      "grad_norm": 11.696438789367676,
      "learning_rate": 3.4642335766423364e-05,
      "loss": 0.9625,
      "step": 42080
    },
    {
      "epoch": 307.22627737226276,
      "grad_norm": 6.969164848327637,
      "learning_rate": 3.463868613138686e-05,
      "loss": 0.8236,
      "step": 42090
    },
    {
      "epoch": 307.2992700729927,
      "grad_norm": 7.7977294921875,
      "learning_rate": 3.4635036496350365e-05,
      "loss": 0.9067,
      "step": 42100
    },
    {
      "epoch": 307.37226277372264,
      "grad_norm": 6.448862075805664,
      "learning_rate": 3.463138686131387e-05,
      "loss": 0.6989,
      "step": 42110
    },
    {
      "epoch": 307.44525547445255,
      "grad_norm": 15.244275093078613,
      "learning_rate": 3.462773722627737e-05,
      "loss": 1.3346,
      "step": 42120
    },
    {
      "epoch": 307.51824817518246,
      "grad_norm": 13.522743225097656,
      "learning_rate": 3.462408759124088e-05,
      "loss": 1.0373,
      "step": 42130
    },
    {
      "epoch": 307.59124087591243,
      "grad_norm": 7.8076605796813965,
      "learning_rate": 3.462043795620438e-05,
      "loss": 1.4837,
      "step": 42140
    },
    {
      "epoch": 307.66423357664235,
      "grad_norm": 0.6317799687385559,
      "learning_rate": 3.461678832116789e-05,
      "loss": 0.985,
      "step": 42150
    },
    {
      "epoch": 307.73722627737226,
      "grad_norm": 9.232341766357422,
      "learning_rate": 3.461313868613139e-05,
      "loss": 1.2121,
      "step": 42160
    },
    {
      "epoch": 307.81021897810217,
      "grad_norm": 6.297534942626953,
      "learning_rate": 3.460948905109489e-05,
      "loss": 0.7736,
      "step": 42170
    },
    {
      "epoch": 307.88321167883214,
      "grad_norm": 14.522034645080566,
      "learning_rate": 3.4605839416058396e-05,
      "loss": 0.8437,
      "step": 42180
    },
    {
      "epoch": 307.95620437956205,
      "grad_norm": 10.68983268737793,
      "learning_rate": 3.4602189781021896e-05,
      "loss": 0.7776,
      "step": 42190
    },
    {
      "epoch": 308.02919708029196,
      "grad_norm": 10.977657318115234,
      "learning_rate": 3.4598540145985404e-05,
      "loss": 0.7336,
      "step": 42200
    },
    {
      "epoch": 308.1021897810219,
      "grad_norm": 21.816600799560547,
      "learning_rate": 3.4594890510948904e-05,
      "loss": 0.8881,
      "step": 42210
    },
    {
      "epoch": 308.17518248175185,
      "grad_norm": 6.9574103355407715,
      "learning_rate": 3.459124087591241e-05,
      "loss": 1.1727,
      "step": 42220
    },
    {
      "epoch": 308.24817518248176,
      "grad_norm": 7.0328369140625,
      "learning_rate": 3.458759124087592e-05,
      "loss": 1.0346,
      "step": 42230
    },
    {
      "epoch": 308.3211678832117,
      "grad_norm": 8.543371200561523,
      "learning_rate": 3.458394160583941e-05,
      "loss": 1.0168,
      "step": 42240
    },
    {
      "epoch": 308.3941605839416,
      "grad_norm": 5.466625213623047,
      "learning_rate": 3.458029197080292e-05,
      "loss": 0.5969,
      "step": 42250
    },
    {
      "epoch": 308.46715328467155,
      "grad_norm": 0.213145911693573,
      "learning_rate": 3.457664233576643e-05,
      "loss": 1.0182,
      "step": 42260
    },
    {
      "epoch": 308.54014598540147,
      "grad_norm": 17.86804962158203,
      "learning_rate": 3.457299270072993e-05,
      "loss": 0.8637,
      "step": 42270
    },
    {
      "epoch": 308.6131386861314,
      "grad_norm": 8.59257698059082,
      "learning_rate": 3.4569343065693435e-05,
      "loss": 0.7802,
      "step": 42280
    },
    {
      "epoch": 308.6861313868613,
      "grad_norm": 9.761734962463379,
      "learning_rate": 3.4565693430656935e-05,
      "loss": 0.7551,
      "step": 42290
    },
    {
      "epoch": 308.75912408759126,
      "grad_norm": 1.264656662940979,
      "learning_rate": 3.456204379562044e-05,
      "loss": 0.9601,
      "step": 42300
    },
    {
      "epoch": 308.8321167883212,
      "grad_norm": 9.258929252624512,
      "learning_rate": 3.455839416058394e-05,
      "loss": 1.1507,
      "step": 42310
    },
    {
      "epoch": 308.9051094890511,
      "grad_norm": 10.125690460205078,
      "learning_rate": 3.4554744525547443e-05,
      "loss": 0.945,
      "step": 42320
    },
    {
      "epoch": 308.978102189781,
      "grad_norm": 11.167696952819824,
      "learning_rate": 3.455109489051095e-05,
      "loss": 1.1224,
      "step": 42330
    },
    {
      "epoch": 309.05109489051097,
      "grad_norm": 12.112932205200195,
      "learning_rate": 3.454744525547445e-05,
      "loss": 1.0159,
      "step": 42340
    },
    {
      "epoch": 309.1240875912409,
      "grad_norm": 7.177712917327881,
      "learning_rate": 3.454379562043796e-05,
      "loss": 0.774,
      "step": 42350
    },
    {
      "epoch": 309.1970802919708,
      "grad_norm": 13.867748260498047,
      "learning_rate": 3.454014598540146e-05,
      "loss": 0.7457,
      "step": 42360
    },
    {
      "epoch": 309.2700729927007,
      "grad_norm": 7.827702045440674,
      "learning_rate": 3.4536496350364966e-05,
      "loss": 1.0771,
      "step": 42370
    },
    {
      "epoch": 309.3430656934307,
      "grad_norm": 13.523242950439453,
      "learning_rate": 3.4532846715328474e-05,
      "loss": 0.999,
      "step": 42380
    },
    {
      "epoch": 309.4160583941606,
      "grad_norm": 0.40305137634277344,
      "learning_rate": 3.4529197080291974e-05,
      "loss": 0.8837,
      "step": 42390
    },
    {
      "epoch": 309.4890510948905,
      "grad_norm": 6.3604631423950195,
      "learning_rate": 3.4525547445255475e-05,
      "loss": 0.8239,
      "step": 42400
    },
    {
      "epoch": 309.5620437956204,
      "grad_norm": 0.14615711569786072,
      "learning_rate": 3.4521897810218975e-05,
      "loss": 0.9343,
      "step": 42410
    },
    {
      "epoch": 309.6350364963504,
      "grad_norm": 12.76909065246582,
      "learning_rate": 3.451824817518248e-05,
      "loss": 0.9806,
      "step": 42420
    },
    {
      "epoch": 309.7080291970803,
      "grad_norm": 8.459830284118652,
      "learning_rate": 3.451459854014599e-05,
      "loss": 1.1849,
      "step": 42430
    },
    {
      "epoch": 309.7810218978102,
      "grad_norm": 19.553247451782227,
      "learning_rate": 3.451094890510949e-05,
      "loss": 1.0931,
      "step": 42440
    },
    {
      "epoch": 309.8540145985401,
      "grad_norm": 8.224798202514648,
      "learning_rate": 3.4507299270073e-05,
      "loss": 0.7654,
      "step": 42450
    },
    {
      "epoch": 309.9270072992701,
      "grad_norm": 0.20341436564922333,
      "learning_rate": 3.45036496350365e-05,
      "loss": 1.0509,
      "step": 42460
    },
    {
      "epoch": 310.0,
      "grad_norm": 18.392597198486328,
      "learning_rate": 3.45e-05,
      "loss": 0.8511,
      "step": 42470
    },
    {
      "epoch": 310.0729927007299,
      "grad_norm": 7.535202503204346,
      "learning_rate": 3.4496350364963506e-05,
      "loss": 0.6639,
      "step": 42480
    },
    {
      "epoch": 310.1459854014599,
      "grad_norm": 13.80628490447998,
      "learning_rate": 3.4492700729927006e-05,
      "loss": 0.8774,
      "step": 42490
    },
    {
      "epoch": 310.2189781021898,
      "grad_norm": 7.7572855949401855,
      "learning_rate": 3.448905109489051e-05,
      "loss": 1.0684,
      "step": 42500
    },
    {
      "epoch": 310.2919708029197,
      "grad_norm": 6.5684404373168945,
      "learning_rate": 3.4485401459854014e-05,
      "loss": 0.5552,
      "step": 42510
    },
    {
      "epoch": 310.3649635036496,
      "grad_norm": 19.113637924194336,
      "learning_rate": 3.448175182481752e-05,
      "loss": 1.3098,
      "step": 42520
    },
    {
      "epoch": 310.4379562043796,
      "grad_norm": 8.827339172363281,
      "learning_rate": 3.447810218978103e-05,
      "loss": 1.0745,
      "step": 42530
    },
    {
      "epoch": 310.5109489051095,
      "grad_norm": 10.690680503845215,
      "learning_rate": 3.447445255474453e-05,
      "loss": 1.0899,
      "step": 42540
    },
    {
      "epoch": 310.5839416058394,
      "grad_norm": 11.003408432006836,
      "learning_rate": 3.447080291970803e-05,
      "loss": 1.3606,
      "step": 42550
    },
    {
      "epoch": 310.6569343065693,
      "grad_norm": 7.580161094665527,
      "learning_rate": 3.446715328467153e-05,
      "loss": 0.9421,
      "step": 42560
    },
    {
      "epoch": 310.7299270072993,
      "grad_norm": 8.359723091125488,
      "learning_rate": 3.446350364963504e-05,
      "loss": 0.8092,
      "step": 42570
    },
    {
      "epoch": 310.8029197080292,
      "grad_norm": 9.820160865783691,
      "learning_rate": 3.4459854014598544e-05,
      "loss": 0.9188,
      "step": 42580
    },
    {
      "epoch": 310.8759124087591,
      "grad_norm": 9.47976303100586,
      "learning_rate": 3.4456204379562045e-05,
      "loss": 1.0782,
      "step": 42590
    },
    {
      "epoch": 310.94890510948903,
      "grad_norm": 10.711433410644531,
      "learning_rate": 3.445255474452555e-05,
      "loss": 0.7855,
      "step": 42600
    },
    {
      "epoch": 311.021897810219,
      "grad_norm": 6.642518997192383,
      "learning_rate": 3.444890510948905e-05,
      "loss": 0.5839,
      "step": 42610
    },
    {
      "epoch": 311.0948905109489,
      "grad_norm": 11.662537574768066,
      "learning_rate": 3.444525547445256e-05,
      "loss": 1.4035,
      "step": 42620
    },
    {
      "epoch": 311.1678832116788,
      "grad_norm": 7.013674259185791,
      "learning_rate": 3.444160583941606e-05,
      "loss": 0.5498,
      "step": 42630
    },
    {
      "epoch": 311.24087591240874,
      "grad_norm": 0.29976674914360046,
      "learning_rate": 3.443795620437956e-05,
      "loss": 0.6824,
      "step": 42640
    },
    {
      "epoch": 311.3138686131387,
      "grad_norm": 9.376080513000488,
      "learning_rate": 3.443430656934307e-05,
      "loss": 0.8205,
      "step": 42650
    },
    {
      "epoch": 311.3868613138686,
      "grad_norm": 6.116659641265869,
      "learning_rate": 3.443065693430657e-05,
      "loss": 0.754,
      "step": 42660
    },
    {
      "epoch": 311.45985401459853,
      "grad_norm": 16.9603271484375,
      "learning_rate": 3.4427007299270076e-05,
      "loss": 0.9869,
      "step": 42670
    },
    {
      "epoch": 311.53284671532845,
      "grad_norm": 7.102238655090332,
      "learning_rate": 3.4423357664233576e-05,
      "loss": 0.7236,
      "step": 42680
    },
    {
      "epoch": 311.6058394160584,
      "grad_norm": 14.649271965026855,
      "learning_rate": 3.4419708029197084e-05,
      "loss": 1.3333,
      "step": 42690
    },
    {
      "epoch": 311.6788321167883,
      "grad_norm": 16.076553344726562,
      "learning_rate": 3.4416058394160584e-05,
      "loss": 1.0737,
      "step": 42700
    },
    {
      "epoch": 311.75182481751824,
      "grad_norm": 16.578414916992188,
      "learning_rate": 3.4412408759124085e-05,
      "loss": 0.952,
      "step": 42710
    },
    {
      "epoch": 311.82481751824815,
      "grad_norm": 11.0111722946167,
      "learning_rate": 3.440875912408759e-05,
      "loss": 0.8752,
      "step": 42720
    },
    {
      "epoch": 311.8978102189781,
      "grad_norm": 2.5200037956237793,
      "learning_rate": 3.44051094890511e-05,
      "loss": 0.8453,
      "step": 42730
    },
    {
      "epoch": 311.97080291970804,
      "grad_norm": 12.60219669342041,
      "learning_rate": 3.44014598540146e-05,
      "loss": 1.2918,
      "step": 42740
    },
    {
      "epoch": 312.04379562043795,
      "grad_norm": 9.033831596374512,
      "learning_rate": 3.439781021897811e-05,
      "loss": 0.8695,
      "step": 42750
    },
    {
      "epoch": 312.11678832116786,
      "grad_norm": 11.803659439086914,
      "learning_rate": 3.439416058394161e-05,
      "loss": 1.0806,
      "step": 42760
    },
    {
      "epoch": 312.18978102189783,
      "grad_norm": 10.609283447265625,
      "learning_rate": 3.4390510948905115e-05,
      "loss": 1.3315,
      "step": 42770
    },
    {
      "epoch": 312.26277372262774,
      "grad_norm": 0.5385826230049133,
      "learning_rate": 3.4386861313868615e-05,
      "loss": 0.9943,
      "step": 42780
    },
    {
      "epoch": 312.33576642335765,
      "grad_norm": 13.379570960998535,
      "learning_rate": 3.4383211678832116e-05,
      "loss": 0.9078,
      "step": 42790
    },
    {
      "epoch": 312.40875912408757,
      "grad_norm": 5.677105903625488,
      "learning_rate": 3.437956204379562e-05,
      "loss": 0.6787,
      "step": 42800
    },
    {
      "epoch": 312.48175182481754,
      "grad_norm": 10.167819023132324,
      "learning_rate": 3.4375912408759124e-05,
      "loss": 1.0143,
      "step": 42810
    },
    {
      "epoch": 312.55474452554745,
      "grad_norm": 4.965444564819336,
      "learning_rate": 3.437226277372263e-05,
      "loss": 0.6596,
      "step": 42820
    },
    {
      "epoch": 312.62773722627736,
      "grad_norm": 6.654158115386963,
      "learning_rate": 3.436861313868613e-05,
      "loss": 0.9216,
      "step": 42830
    },
    {
      "epoch": 312.7007299270073,
      "grad_norm": 7.5901360511779785,
      "learning_rate": 3.436496350364964e-05,
      "loss": 0.5821,
      "step": 42840
    },
    {
      "epoch": 312.77372262773724,
      "grad_norm": 7.491286277770996,
      "learning_rate": 3.4361313868613146e-05,
      "loss": 0.7855,
      "step": 42850
    },
    {
      "epoch": 312.84671532846716,
      "grad_norm": 14.71790885925293,
      "learning_rate": 3.435766423357664e-05,
      "loss": 1.3058,
      "step": 42860
    },
    {
      "epoch": 312.91970802919707,
      "grad_norm": 19.2293643951416,
      "learning_rate": 3.435401459854015e-05,
      "loss": 1.0463,
      "step": 42870
    },
    {
      "epoch": 312.992700729927,
      "grad_norm": 0.501124382019043,
      "learning_rate": 3.435036496350365e-05,
      "loss": 1.0019,
      "step": 42880
    },
    {
      "epoch": 313.06569343065695,
      "grad_norm": 0.2848774790763855,
      "learning_rate": 3.4346715328467155e-05,
      "loss": 0.5444,
      "step": 42890
    },
    {
      "epoch": 313.13868613138686,
      "grad_norm": 7.0995049476623535,
      "learning_rate": 3.434306569343066e-05,
      "loss": 1.0555,
      "step": 42900
    },
    {
      "epoch": 313.2116788321168,
      "grad_norm": 0.231413334608078,
      "learning_rate": 3.433941605839416e-05,
      "loss": 0.9454,
      "step": 42910
    },
    {
      "epoch": 313.2846715328467,
      "grad_norm": 7.568794250488281,
      "learning_rate": 3.433576642335767e-05,
      "loss": 0.9233,
      "step": 42920
    },
    {
      "epoch": 313.35766423357666,
      "grad_norm": 10.890917778015137,
      "learning_rate": 3.433211678832117e-05,
      "loss": 1.162,
      "step": 42930
    },
    {
      "epoch": 313.43065693430657,
      "grad_norm": 14.13752269744873,
      "learning_rate": 3.432846715328467e-05,
      "loss": 0.9556,
      "step": 42940
    },
    {
      "epoch": 313.5036496350365,
      "grad_norm": 11.027472496032715,
      "learning_rate": 3.432481751824818e-05,
      "loss": 0.691,
      "step": 42950
    },
    {
      "epoch": 313.57664233576645,
      "grad_norm": 11.147865295410156,
      "learning_rate": 3.432116788321168e-05,
      "loss": 0.99,
      "step": 42960
    },
    {
      "epoch": 313.64963503649636,
      "grad_norm": 12.418254852294922,
      "learning_rate": 3.4317518248175186e-05,
      "loss": 0.9873,
      "step": 42970
    },
    {
      "epoch": 313.7226277372263,
      "grad_norm": 8.157169342041016,
      "learning_rate": 3.4313868613138686e-05,
      "loss": 0.9604,
      "step": 42980
    },
    {
      "epoch": 313.7956204379562,
      "grad_norm": 20.355348587036133,
      "learning_rate": 3.4310218978102193e-05,
      "loss": 1.2956,
      "step": 42990
    },
    {
      "epoch": 313.86861313868616,
      "grad_norm": 10.30878734588623,
      "learning_rate": 3.43065693430657e-05,
      "loss": 0.9217,
      "step": 43000
    },
    {
      "epoch": 313.94160583941607,
      "grad_norm": 9.451862335205078,
      "learning_rate": 3.4302919708029194e-05,
      "loss": 0.8994,
      "step": 43010
    },
    {
      "epoch": 314.014598540146,
      "grad_norm": 7.5701680183410645,
      "learning_rate": 3.42992700729927e-05,
      "loss": 0.7947,
      "step": 43020
    },
    {
      "epoch": 314.0875912408759,
      "grad_norm": 11.681788444519043,
      "learning_rate": 3.42956204379562e-05,
      "loss": 0.9744,
      "step": 43030
    },
    {
      "epoch": 314.16058394160586,
      "grad_norm": 17.02181625366211,
      "learning_rate": 3.429197080291971e-05,
      "loss": 1.1004,
      "step": 43040
    },
    {
      "epoch": 314.2335766423358,
      "grad_norm": 7.971543312072754,
      "learning_rate": 3.428832116788322e-05,
      "loss": 0.9311,
      "step": 43050
    },
    {
      "epoch": 314.3065693430657,
      "grad_norm": 7.20418119430542,
      "learning_rate": 3.428467153284672e-05,
      "loss": 1.0083,
      "step": 43060
    },
    {
      "epoch": 314.3795620437956,
      "grad_norm": 10.494109153747559,
      "learning_rate": 3.4281021897810225e-05,
      "loss": 1.2549,
      "step": 43070
    },
    {
      "epoch": 314.45255474452557,
      "grad_norm": 0.2470812350511551,
      "learning_rate": 3.4277372262773725e-05,
      "loss": 0.5271,
      "step": 43080
    },
    {
      "epoch": 314.5255474452555,
      "grad_norm": 4.617438793182373,
      "learning_rate": 3.4273722627737225e-05,
      "loss": 0.8464,
      "step": 43090
    },
    {
      "epoch": 314.5985401459854,
      "grad_norm": 7.617283344268799,
      "learning_rate": 3.427007299270073e-05,
      "loss": 1.2781,
      "step": 43100
    },
    {
      "epoch": 314.6715328467153,
      "grad_norm": 10.698844909667969,
      "learning_rate": 3.426642335766423e-05,
      "loss": 0.8777,
      "step": 43110
    },
    {
      "epoch": 314.7445255474453,
      "grad_norm": 0.2610671818256378,
      "learning_rate": 3.426277372262774e-05,
      "loss": 1.1146,
      "step": 43120
    },
    {
      "epoch": 314.8175182481752,
      "grad_norm": 6.2289509773254395,
      "learning_rate": 3.425912408759124e-05,
      "loss": 0.8659,
      "step": 43130
    },
    {
      "epoch": 314.8905109489051,
      "grad_norm": 0.23455217480659485,
      "learning_rate": 3.425547445255475e-05,
      "loss": 0.827,
      "step": 43140
    },
    {
      "epoch": 314.963503649635,
      "grad_norm": 16.37396812438965,
      "learning_rate": 3.425182481751825e-05,
      "loss": 0.5892,
      "step": 43150
    },
    {
      "epoch": 315.036496350365,
      "grad_norm": 11.297066688537598,
      "learning_rate": 3.424817518248175e-05,
      "loss": 0.771,
      "step": 43160
    },
    {
      "epoch": 315.1094890510949,
      "grad_norm": 14.981727600097656,
      "learning_rate": 3.4244525547445257e-05,
      "loss": 1.0516,
      "step": 43170
    },
    {
      "epoch": 315.1824817518248,
      "grad_norm": 16.696035385131836,
      "learning_rate": 3.424087591240876e-05,
      "loss": 0.7543,
      "step": 43180
    },
    {
      "epoch": 315.2554744525547,
      "grad_norm": 6.524272918701172,
      "learning_rate": 3.4237226277372264e-05,
      "loss": 0.9612,
      "step": 43190
    },
    {
      "epoch": 315.3284671532847,
      "grad_norm": 5.971419334411621,
      "learning_rate": 3.423357664233577e-05,
      "loss": 0.9333,
      "step": 43200
    },
    {
      "epoch": 315.4014598540146,
      "grad_norm": 15.94252872467041,
      "learning_rate": 3.422992700729927e-05,
      "loss": 0.803,
      "step": 43210
    },
    {
      "epoch": 315.4744525547445,
      "grad_norm": 8.222007751464844,
      "learning_rate": 3.422627737226278e-05,
      "loss": 0.9602,
      "step": 43220
    },
    {
      "epoch": 315.54744525547443,
      "grad_norm": 6.762219429016113,
      "learning_rate": 3.422262773722628e-05,
      "loss": 1.0773,
      "step": 43230
    },
    {
      "epoch": 315.6204379562044,
      "grad_norm": 9.326507568359375,
      "learning_rate": 3.421897810218978e-05,
      "loss": 0.9603,
      "step": 43240
    },
    {
      "epoch": 315.6934306569343,
      "grad_norm": 5.525665760040283,
      "learning_rate": 3.421532846715329e-05,
      "loss": 0.8092,
      "step": 43250
    },
    {
      "epoch": 315.7664233576642,
      "grad_norm": 14.706693649291992,
      "learning_rate": 3.421167883211679e-05,
      "loss": 0.9258,
      "step": 43260
    },
    {
      "epoch": 315.83941605839414,
      "grad_norm": 8.089984893798828,
      "learning_rate": 3.4208029197080295e-05,
      "loss": 1.3617,
      "step": 43270
    },
    {
      "epoch": 315.9124087591241,
      "grad_norm": 11.270366668701172,
      "learning_rate": 3.4204379562043796e-05,
      "loss": 0.9337,
      "step": 43280
    },
    {
      "epoch": 315.985401459854,
      "grad_norm": 17.076854705810547,
      "learning_rate": 3.42007299270073e-05,
      "loss": 1.0926,
      "step": 43290
    },
    {
      "epoch": 316.05839416058393,
      "grad_norm": 7.322043418884277,
      "learning_rate": 3.4197080291970804e-05,
      "loss": 0.967,
      "step": 43300
    },
    {
      "epoch": 316.13138686131384,
      "grad_norm": 16.71549415588379,
      "learning_rate": 3.4193430656934304e-05,
      "loss": 0.7236,
      "step": 43310
    },
    {
      "epoch": 316.2043795620438,
      "grad_norm": 11.771830558776855,
      "learning_rate": 3.418978102189781e-05,
      "loss": 1.0208,
      "step": 43320
    },
    {
      "epoch": 316.2773722627737,
      "grad_norm": 12.045013427734375,
      "learning_rate": 3.418613138686131e-05,
      "loss": 0.7608,
      "step": 43330
    },
    {
      "epoch": 316.35036496350364,
      "grad_norm": 12.406460762023926,
      "learning_rate": 3.418248175182482e-05,
      "loss": 0.7957,
      "step": 43340
    },
    {
      "epoch": 316.42335766423355,
      "grad_norm": 9.824810028076172,
      "learning_rate": 3.417883211678832e-05,
      "loss": 0.6567,
      "step": 43350
    },
    {
      "epoch": 316.4963503649635,
      "grad_norm": 24.064529418945312,
      "learning_rate": 3.417518248175183e-05,
      "loss": 1.2878,
      "step": 43360
    },
    {
      "epoch": 316.56934306569343,
      "grad_norm": 0.503043532371521,
      "learning_rate": 3.4171532846715334e-05,
      "loss": 1.2943,
      "step": 43370
    },
    {
      "epoch": 316.64233576642334,
      "grad_norm": 14.4955472946167,
      "learning_rate": 3.4167883211678835e-05,
      "loss": 0.918,
      "step": 43380
    },
    {
      "epoch": 316.7153284671533,
      "grad_norm": 12.635396957397461,
      "learning_rate": 3.4164233576642335e-05,
      "loss": 0.7882,
      "step": 43390
    },
    {
      "epoch": 316.7883211678832,
      "grad_norm": 7.264153003692627,
      "learning_rate": 3.416058394160584e-05,
      "loss": 0.9465,
      "step": 43400
    },
    {
      "epoch": 316.86131386861314,
      "grad_norm": 6.877377510070801,
      "learning_rate": 3.415693430656934e-05,
      "loss": 0.8509,
      "step": 43410
    },
    {
      "epoch": 316.93430656934305,
      "grad_norm": 10.026384353637695,
      "learning_rate": 3.415328467153285e-05,
      "loss": 1.1118,
      "step": 43420
    },
    {
      "epoch": 317.007299270073,
      "grad_norm": 11.125836372375488,
      "learning_rate": 3.414963503649635e-05,
      "loss": 1.3367,
      "step": 43430
    },
    {
      "epoch": 317.08029197080293,
      "grad_norm": 19.195219039916992,
      "learning_rate": 3.414598540145986e-05,
      "loss": 1.3228,
      "step": 43440
    },
    {
      "epoch": 317.15328467153284,
      "grad_norm": 0.44015103578567505,
      "learning_rate": 3.414233576642336e-05,
      "loss": 1.0111,
      "step": 43450
    },
    {
      "epoch": 317.22627737226276,
      "grad_norm": 7.497946739196777,
      "learning_rate": 3.4138686131386866e-05,
      "loss": 0.5409,
      "step": 43460
    },
    {
      "epoch": 317.2992700729927,
      "grad_norm": 5.988285541534424,
      "learning_rate": 3.4135036496350366e-05,
      "loss": 0.8761,
      "step": 43470
    },
    {
      "epoch": 317.37226277372264,
      "grad_norm": 8.490315437316895,
      "learning_rate": 3.413138686131387e-05,
      "loss": 1.431,
      "step": 43480
    },
    {
      "epoch": 317.44525547445255,
      "grad_norm": 13.362793922424316,
      "learning_rate": 3.4127737226277374e-05,
      "loss": 0.9581,
      "step": 43490
    },
    {
      "epoch": 317.51824817518246,
      "grad_norm": 11.977730751037598,
      "learning_rate": 3.4124087591240875e-05,
      "loss": 0.6018,
      "step": 43500
    },
    {
      "epoch": 317.59124087591243,
      "grad_norm": 13.69334602355957,
      "learning_rate": 3.412043795620438e-05,
      "loss": 0.9016,
      "step": 43510
    },
    {
      "epoch": 317.66423357664235,
      "grad_norm": 8.800721168518066,
      "learning_rate": 3.411678832116789e-05,
      "loss": 0.7282,
      "step": 43520
    },
    {
      "epoch": 317.73722627737226,
      "grad_norm": 7.190532207489014,
      "learning_rate": 3.411313868613139e-05,
      "loss": 1.0652,
      "step": 43530
    },
    {
      "epoch": 317.81021897810217,
      "grad_norm": 5.6434245109558105,
      "learning_rate": 3.410948905109489e-05,
      "loss": 0.8583,
      "step": 43540
    },
    {
      "epoch": 317.88321167883214,
      "grad_norm": 18.5895938873291,
      "learning_rate": 3.410583941605839e-05,
      "loss": 0.7448,
      "step": 43550
    },
    {
      "epoch": 317.95620437956205,
      "grad_norm": 14.372445106506348,
      "learning_rate": 3.41021897810219e-05,
      "loss": 1.3154,
      "step": 43560
    },
    {
      "epoch": 318.02919708029196,
      "grad_norm": 8.147908210754395,
      "learning_rate": 3.4098540145985405e-05,
      "loss": 0.9849,
      "step": 43570
    },
    {
      "epoch": 318.1021897810219,
      "grad_norm": 14.24605655670166,
      "learning_rate": 3.4094890510948906e-05,
      "loss": 0.7825,
      "step": 43580
    },
    {
      "epoch": 318.17518248175185,
      "grad_norm": 10.844121932983398,
      "learning_rate": 3.409124087591241e-05,
      "loss": 0.8791,
      "step": 43590
    },
    {
      "epoch": 318.24817518248176,
      "grad_norm": 13.809586524963379,
      "learning_rate": 3.408759124087591e-05,
      "loss": 1.0594,
      "step": 43600
    },
    {
      "epoch": 318.3211678832117,
      "grad_norm": 12.742670059204102,
      "learning_rate": 3.408394160583942e-05,
      "loss": 1.226,
      "step": 43610
    },
    {
      "epoch": 318.3941605839416,
      "grad_norm": 5.401890277862549,
      "learning_rate": 3.408029197080292e-05,
      "loss": 0.9202,
      "step": 43620
    },
    {
      "epoch": 318.46715328467155,
      "grad_norm": 0.40647464990615845,
      "learning_rate": 3.407664233576642e-05,
      "loss": 0.8322,
      "step": 43630
    },
    {
      "epoch": 318.54014598540147,
      "grad_norm": 11.628870010375977,
      "learning_rate": 3.407299270072993e-05,
      "loss": 0.8577,
      "step": 43640
    },
    {
      "epoch": 318.6131386861314,
      "grad_norm": 6.901120662689209,
      "learning_rate": 3.406934306569343e-05,
      "loss": 1.0299,
      "step": 43650
    },
    {
      "epoch": 318.6861313868613,
      "grad_norm": 13.418166160583496,
      "learning_rate": 3.406569343065694e-05,
      "loss": 1.0734,
      "step": 43660
    },
    {
      "epoch": 318.75912408759126,
      "grad_norm": 8.168560028076172,
      "learning_rate": 3.4062043795620444e-05,
      "loss": 1.0447,
      "step": 43670
    },
    {
      "epoch": 318.8321167883212,
      "grad_norm": 0.12213994562625885,
      "learning_rate": 3.4058394160583944e-05,
      "loss": 1.0745,
      "step": 43680
    },
    {
      "epoch": 318.9051094890511,
      "grad_norm": 0.5085792541503906,
      "learning_rate": 3.405474452554745e-05,
      "loss": 0.7185,
      "step": 43690
    },
    {
      "epoch": 318.978102189781,
      "grad_norm": 0.32244202494621277,
      "learning_rate": 3.4051094890510945e-05,
      "loss": 0.5886,
      "step": 43700
    },
    {
      "epoch": 319.05109489051097,
      "grad_norm": 0.19819533824920654,
      "learning_rate": 3.404744525547445e-05,
      "loss": 0.9274,
      "step": 43710
    },
    {
      "epoch": 319.1240875912409,
      "grad_norm": 11.438617706298828,
      "learning_rate": 3.404379562043796e-05,
      "loss": 0.974,
      "step": 43720
    },
    {
      "epoch": 319.1970802919708,
      "grad_norm": 7.947981357574463,
      "learning_rate": 3.404014598540146e-05,
      "loss": 1.1,
      "step": 43730
    },
    {
      "epoch": 319.2700729927007,
      "grad_norm": 16.515228271484375,
      "learning_rate": 3.403649635036497e-05,
      "loss": 1.0252,
      "step": 43740
    },
    {
      "epoch": 319.3430656934307,
      "grad_norm": 13.418375015258789,
      "learning_rate": 3.403284671532847e-05,
      "loss": 0.8943,
      "step": 43750
    },
    {
      "epoch": 319.4160583941606,
      "grad_norm": 0.375934362411499,
      "learning_rate": 3.4029197080291975e-05,
      "loss": 0.9073,
      "step": 43760
    },
    {
      "epoch": 319.4890510948905,
      "grad_norm": 0.18337400257587433,
      "learning_rate": 3.4025547445255476e-05,
      "loss": 0.7692,
      "step": 43770
    },
    {
      "epoch": 319.5620437956204,
      "grad_norm": 7.563427925109863,
      "learning_rate": 3.4021897810218976e-05,
      "loss": 1.1441,
      "step": 43780
    },
    {
      "epoch": 319.6350364963504,
      "grad_norm": 9.381840705871582,
      "learning_rate": 3.4018248175182484e-05,
      "loss": 1.1028,
      "step": 43790
    },
    {
      "epoch": 319.7080291970803,
      "grad_norm": 7.258256435394287,
      "learning_rate": 3.4014598540145984e-05,
      "loss": 1.0886,
      "step": 43800
    },
    {
      "epoch": 319.7810218978102,
      "grad_norm": 0.19832175970077515,
      "learning_rate": 3.401094890510949e-05,
      "loss": 0.8221,
      "step": 43810
    },
    {
      "epoch": 319.8540145985401,
      "grad_norm": 8.504522323608398,
      "learning_rate": 3.400729927007299e-05,
      "loss": 0.6445,
      "step": 43820
    },
    {
      "epoch": 319.9270072992701,
      "grad_norm": 13.706254959106445,
      "learning_rate": 3.40036496350365e-05,
      "loss": 0.9491,
      "step": 43830
    },
    {
      "epoch": 320.0,
      "grad_norm": 25.214168548583984,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.1971,
      "step": 43840
    },
    {
      "epoch": 320.0729927007299,
      "grad_norm": 11.8696928024292,
      "learning_rate": 3.39963503649635e-05,
      "loss": 0.6832,
      "step": 43850
    },
    {
      "epoch": 320.1459854014599,
      "grad_norm": 17.952171325683594,
      "learning_rate": 3.399270072992701e-05,
      "loss": 1.2348,
      "step": 43860
    },
    {
      "epoch": 320.2189781021898,
      "grad_norm": 12.299324035644531,
      "learning_rate": 3.3989051094890515e-05,
      "loss": 0.8753,
      "step": 43870
    },
    {
      "epoch": 320.2919708029197,
      "grad_norm": 16.084657669067383,
      "learning_rate": 3.3985401459854015e-05,
      "loss": 1.1734,
      "step": 43880
    },
    {
      "epoch": 320.3649635036496,
      "grad_norm": 8.87919807434082,
      "learning_rate": 3.398175182481752e-05,
      "loss": 0.9105,
      "step": 43890
    },
    {
      "epoch": 320.4379562043796,
      "grad_norm": 6.605688095092773,
      "learning_rate": 3.397810218978102e-05,
      "loss": 0.4741,
      "step": 43900
    },
    {
      "epoch": 320.5109489051095,
      "grad_norm": 9.213983535766602,
      "learning_rate": 3.397445255474453e-05,
      "loss": 0.8827,
      "step": 43910
    },
    {
      "epoch": 320.5839416058394,
      "grad_norm": 5.5389885902404785,
      "learning_rate": 3.397080291970803e-05,
      "loss": 0.7796,
      "step": 43920
    },
    {
      "epoch": 320.6569343065693,
      "grad_norm": 9.699045181274414,
      "learning_rate": 3.396715328467153e-05,
      "loss": 1.2604,
      "step": 43930
    },
    {
      "epoch": 320.7299270072993,
      "grad_norm": 14.008416175842285,
      "learning_rate": 3.396350364963504e-05,
      "loss": 0.6228,
      "step": 43940
    },
    {
      "epoch": 320.8029197080292,
      "grad_norm": 18.514392852783203,
      "learning_rate": 3.395985401459854e-05,
      "loss": 1.1307,
      "step": 43950
    },
    {
      "epoch": 320.8759124087591,
      "grad_norm": 8.910789489746094,
      "learning_rate": 3.3956204379562046e-05,
      "loss": 0.9386,
      "step": 43960
    },
    {
      "epoch": 320.94890510948903,
      "grad_norm": 9.246861457824707,
      "learning_rate": 3.395255474452555e-05,
      "loss": 1.0566,
      "step": 43970
    },
    {
      "epoch": 321.021897810219,
      "grad_norm": 0.5018170475959778,
      "learning_rate": 3.3948905109489054e-05,
      "loss": 1.4586,
      "step": 43980
    },
    {
      "epoch": 321.0948905109489,
      "grad_norm": 5.8936967849731445,
      "learning_rate": 3.394525547445256e-05,
      "loss": 0.8647,
      "step": 43990
    },
    {
      "epoch": 321.1678832116788,
      "grad_norm": 0.6775885820388794,
      "learning_rate": 3.3941605839416055e-05,
      "loss": 0.8661,
      "step": 44000
    },
    {
      "epoch": 321.24087591240874,
      "grad_norm": 5.984816074371338,
      "learning_rate": 3.393795620437956e-05,
      "loss": 0.9355,
      "step": 44010
    },
    {
      "epoch": 321.3138686131387,
      "grad_norm": 7.668908596038818,
      "learning_rate": 3.393430656934306e-05,
      "loss": 1.1488,
      "step": 44020
    },
    {
      "epoch": 321.3868613138686,
      "grad_norm": 10.805521011352539,
      "learning_rate": 3.393065693430657e-05,
      "loss": 1.2717,
      "step": 44030
    },
    {
      "epoch": 321.45985401459853,
      "grad_norm": 10.84749698638916,
      "learning_rate": 3.392700729927008e-05,
      "loss": 0.6619,
      "step": 44040
    },
    {
      "epoch": 321.53284671532845,
      "grad_norm": 0.20321562886238098,
      "learning_rate": 3.392335766423358e-05,
      "loss": 1.0094,
      "step": 44050
    },
    {
      "epoch": 321.6058394160584,
      "grad_norm": 9.984548568725586,
      "learning_rate": 3.3919708029197085e-05,
      "loss": 1.353,
      "step": 44060
    },
    {
      "epoch": 321.6788321167883,
      "grad_norm": 12.028644561767578,
      "learning_rate": 3.3916058394160586e-05,
      "loss": 0.7998,
      "step": 44070
    },
    {
      "epoch": 321.75182481751824,
      "grad_norm": 9.897789001464844,
      "learning_rate": 3.3912408759124086e-05,
      "loss": 0.9165,
      "step": 44080
    },
    {
      "epoch": 321.82481751824815,
      "grad_norm": 6.8159589767456055,
      "learning_rate": 3.3908759124087593e-05,
      "loss": 0.8746,
      "step": 44090
    },
    {
      "epoch": 321.8978102189781,
      "grad_norm": 13.326300621032715,
      "learning_rate": 3.3905109489051094e-05,
      "loss": 0.7741,
      "step": 44100
    },
    {
      "epoch": 321.97080291970804,
      "grad_norm": 6.660660743713379,
      "learning_rate": 3.39014598540146e-05,
      "loss": 0.7371,
      "step": 44110
    },
    {
      "epoch": 322.04379562043795,
      "grad_norm": 10.416001319885254,
      "learning_rate": 3.38978102189781e-05,
      "loss": 1.1331,
      "step": 44120
    },
    {
      "epoch": 322.11678832116786,
      "grad_norm": 12.608978271484375,
      "learning_rate": 3.389416058394161e-05,
      "loss": 1.1086,
      "step": 44130
    },
    {
      "epoch": 322.18978102189783,
      "grad_norm": 6.037632942199707,
      "learning_rate": 3.3890510948905116e-05,
      "loss": 1.0126,
      "step": 44140
    },
    {
      "epoch": 322.26277372262774,
      "grad_norm": 7.082650184631348,
      "learning_rate": 3.388686131386862e-05,
      "loss": 0.5773,
      "step": 44150
    },
    {
      "epoch": 322.33576642335765,
      "grad_norm": 19.8859920501709,
      "learning_rate": 3.388321167883212e-05,
      "loss": 0.7251,
      "step": 44160
    },
    {
      "epoch": 322.40875912408757,
      "grad_norm": 6.412675380706787,
      "learning_rate": 3.387956204379562e-05,
      "loss": 1.066,
      "step": 44170
    },
    {
      "epoch": 322.48175182481754,
      "grad_norm": 13.93326473236084,
      "learning_rate": 3.3875912408759125e-05,
      "loss": 1.0179,
      "step": 44180
    },
    {
      "epoch": 322.55474452554745,
      "grad_norm": 11.132904052734375,
      "learning_rate": 3.387226277372263e-05,
      "loss": 0.8401,
      "step": 44190
    },
    {
      "epoch": 322.62773722627736,
      "grad_norm": 0.3148499131202698,
      "learning_rate": 3.386861313868613e-05,
      "loss": 1.0306,
      "step": 44200
    },
    {
      "epoch": 322.7007299270073,
      "grad_norm": 0.1613350361585617,
      "learning_rate": 3.386496350364964e-05,
      "loss": 1.4481,
      "step": 44210
    },
    {
      "epoch": 322.77372262773724,
      "grad_norm": 7.7421064376831055,
      "learning_rate": 3.386131386861314e-05,
      "loss": 0.5678,
      "step": 44220
    },
    {
      "epoch": 322.84671532846716,
      "grad_norm": 11.50672435760498,
      "learning_rate": 3.385766423357664e-05,
      "loss": 0.9818,
      "step": 44230
    },
    {
      "epoch": 322.91970802919707,
      "grad_norm": 6.321380615234375,
      "learning_rate": 3.385401459854015e-05,
      "loss": 0.9092,
      "step": 44240
    },
    {
      "epoch": 322.992700729927,
      "grad_norm": 0.17535701394081116,
      "learning_rate": 3.385036496350365e-05,
      "loss": 1.0343,
      "step": 44250
    },
    {
      "epoch": 323.06569343065695,
      "grad_norm": 8.287580490112305,
      "learning_rate": 3.3846715328467156e-05,
      "loss": 1.1006,
      "step": 44260
    },
    {
      "epoch": 323.13868613138686,
      "grad_norm": 15.456939697265625,
      "learning_rate": 3.3843065693430657e-05,
      "loss": 1.1187,
      "step": 44270
    },
    {
      "epoch": 323.2116788321168,
      "grad_norm": 0.414298951625824,
      "learning_rate": 3.3839416058394164e-05,
      "loss": 0.6049,
      "step": 44280
    },
    {
      "epoch": 323.2846715328467,
      "grad_norm": 7.1995744705200195,
      "learning_rate": 3.383576642335767e-05,
      "loss": 1.1488,
      "step": 44290
    },
    {
      "epoch": 323.35766423357666,
      "grad_norm": 8.854864120483398,
      "learning_rate": 3.383211678832117e-05,
      "loss": 1.0895,
      "step": 44300
    },
    {
      "epoch": 323.43065693430657,
      "grad_norm": 9.680157661437988,
      "learning_rate": 3.382846715328467e-05,
      "loss": 0.5274,
      "step": 44310
    },
    {
      "epoch": 323.5036496350365,
      "grad_norm": 16.775508880615234,
      "learning_rate": 3.382481751824817e-05,
      "loss": 1.3059,
      "step": 44320
    },
    {
      "epoch": 323.57664233576645,
      "grad_norm": 10.287434577941895,
      "learning_rate": 3.382116788321168e-05,
      "loss": 0.9955,
      "step": 44330
    },
    {
      "epoch": 323.64963503649636,
      "grad_norm": 0.4183445870876312,
      "learning_rate": 3.381751824817519e-05,
      "loss": 1.0524,
      "step": 44340
    },
    {
      "epoch": 323.7226277372263,
      "grad_norm": 12.082353591918945,
      "learning_rate": 3.381386861313869e-05,
      "loss": 1.0192,
      "step": 44350
    },
    {
      "epoch": 323.7956204379562,
      "grad_norm": 8.854918479919434,
      "learning_rate": 3.3810218978102195e-05,
      "loss": 0.7789,
      "step": 44360
    },
    {
      "epoch": 323.86861313868616,
      "grad_norm": 0.07506858557462692,
      "learning_rate": 3.3806569343065695e-05,
      "loss": 0.888,
      "step": 44370
    },
    {
      "epoch": 323.94160583941607,
      "grad_norm": 13.39179515838623,
      "learning_rate": 3.3802919708029196e-05,
      "loss": 0.6271,
      "step": 44380
    },
    {
      "epoch": 324.014598540146,
      "grad_norm": 4.784960746765137,
      "learning_rate": 3.37992700729927e-05,
      "loss": 1.0285,
      "step": 44390
    },
    {
      "epoch": 324.0875912408759,
      "grad_norm": 7.229740142822266,
      "learning_rate": 3.3795620437956204e-05,
      "loss": 0.5075,
      "step": 44400
    },
    {
      "epoch": 324.16058394160586,
      "grad_norm": 13.89629077911377,
      "learning_rate": 3.379197080291971e-05,
      "loss": 1.1704,
      "step": 44410
    },
    {
      "epoch": 324.2335766423358,
      "grad_norm": 13.430214881896973,
      "learning_rate": 3.378832116788321e-05,
      "loss": 0.8187,
      "step": 44420
    },
    {
      "epoch": 324.3065693430657,
      "grad_norm": 0.3107052743434906,
      "learning_rate": 3.378467153284672e-05,
      "loss": 0.6103,
      "step": 44430
    },
    {
      "epoch": 324.3795620437956,
      "grad_norm": 8.835206985473633,
      "learning_rate": 3.378102189781022e-05,
      "loss": 0.833,
      "step": 44440
    },
    {
      "epoch": 324.45255474452557,
      "grad_norm": 12.306488037109375,
      "learning_rate": 3.3777372262773726e-05,
      "loss": 1.2597,
      "step": 44450
    },
    {
      "epoch": 324.5255474452555,
      "grad_norm": 20.503650665283203,
      "learning_rate": 3.377372262773723e-05,
      "loss": 1.1458,
      "step": 44460
    },
    {
      "epoch": 324.5985401459854,
      "grad_norm": 11.551026344299316,
      "learning_rate": 3.377007299270073e-05,
      "loss": 1.3308,
      "step": 44470
    },
    {
      "epoch": 324.6715328467153,
      "grad_norm": 8.340841293334961,
      "learning_rate": 3.3766423357664235e-05,
      "loss": 0.8693,
      "step": 44480
    },
    {
      "epoch": 324.7445255474453,
      "grad_norm": 7.091568946838379,
      "learning_rate": 3.3762773722627735e-05,
      "loss": 0.972,
      "step": 44490
    },
    {
      "epoch": 324.8175182481752,
      "grad_norm": 8.558844566345215,
      "learning_rate": 3.375912408759124e-05,
      "loss": 1.1636,
      "step": 44500
    },
    {
      "epoch": 324.8905109489051,
      "grad_norm": 8.534940719604492,
      "learning_rate": 3.375547445255475e-05,
      "loss": 1.1975,
      "step": 44510
    },
    {
      "epoch": 324.963503649635,
      "grad_norm": 11.033308982849121,
      "learning_rate": 3.375182481751825e-05,
      "loss": 0.6663,
      "step": 44520
    },
    {
      "epoch": 325.036496350365,
      "grad_norm": 8.256962776184082,
      "learning_rate": 3.374817518248176e-05,
      "loss": 1.1332,
      "step": 44530
    },
    {
      "epoch": 325.1094890510949,
      "grad_norm": 0.20885516703128815,
      "learning_rate": 3.374452554744526e-05,
      "loss": 0.7326,
      "step": 44540
    },
    {
      "epoch": 325.1824817518248,
      "grad_norm": 9.587380409240723,
      "learning_rate": 3.374087591240876e-05,
      "loss": 0.7982,
      "step": 44550
    },
    {
      "epoch": 325.2554744525547,
      "grad_norm": 15.848955154418945,
      "learning_rate": 3.3737226277372266e-05,
      "loss": 1.1049,
      "step": 44560
    },
    {
      "epoch": 325.3284671532847,
      "grad_norm": 12.546116828918457,
      "learning_rate": 3.3733576642335766e-05,
      "loss": 1.0837,
      "step": 44570
    },
    {
      "epoch": 325.4014598540146,
      "grad_norm": 0.7740408778190613,
      "learning_rate": 3.3729927007299273e-05,
      "loss": 1.0285,
      "step": 44580
    },
    {
      "epoch": 325.4744525547445,
      "grad_norm": 6.297626972198486,
      "learning_rate": 3.3726277372262774e-05,
      "loss": 1.3003,
      "step": 44590
    },
    {
      "epoch": 325.54744525547443,
      "grad_norm": 0.25093746185302734,
      "learning_rate": 3.372262773722628e-05,
      "loss": 0.9851,
      "step": 44600
    },
    {
      "epoch": 325.6204379562044,
      "grad_norm": 0.1192513108253479,
      "learning_rate": 3.371897810218978e-05,
      "loss": 0.6671,
      "step": 44610
    },
    {
      "epoch": 325.6934306569343,
      "grad_norm": 6.663886547088623,
      "learning_rate": 3.371532846715328e-05,
      "loss": 0.9566,
      "step": 44620
    },
    {
      "epoch": 325.7664233576642,
      "grad_norm": 11.3283052444458,
      "learning_rate": 3.371167883211679e-05,
      "loss": 1.0019,
      "step": 44630
    },
    {
      "epoch": 325.83941605839414,
      "grad_norm": 11.576618194580078,
      "learning_rate": 3.370802919708029e-05,
      "loss": 0.893,
      "step": 44640
    },
    {
      "epoch": 325.9124087591241,
      "grad_norm": 8.100898742675781,
      "learning_rate": 3.37043795620438e-05,
      "loss": 1.0734,
      "step": 44650
    },
    {
      "epoch": 325.985401459854,
      "grad_norm": 8.110098838806152,
      "learning_rate": 3.3700729927007305e-05,
      "loss": 0.7939,
      "step": 44660
    },
    {
      "epoch": 326.05839416058393,
      "grad_norm": 13.718510627746582,
      "learning_rate": 3.3697080291970805e-05,
      "loss": 1.1529,
      "step": 44670
    },
    {
      "epoch": 326.13138686131384,
      "grad_norm": 6.74020528793335,
      "learning_rate": 3.369343065693431e-05,
      "loss": 1.0708,
      "step": 44680
    },
    {
      "epoch": 326.2043795620438,
      "grad_norm": 12.450032234191895,
      "learning_rate": 3.3689781021897806e-05,
      "loss": 0.9573,
      "step": 44690
    },
    {
      "epoch": 326.2773722627737,
      "grad_norm": 11.657584190368652,
      "learning_rate": 3.368613138686131e-05,
      "loss": 0.9399,
      "step": 44700
    },
    {
      "epoch": 326.35036496350364,
      "grad_norm": 7.019603252410889,
      "learning_rate": 3.368248175182482e-05,
      "loss": 0.6791,
      "step": 44710
    },
    {
      "epoch": 326.42335766423355,
      "grad_norm": 10.549782752990723,
      "learning_rate": 3.367883211678832e-05,
      "loss": 0.9795,
      "step": 44720
    },
    {
      "epoch": 326.4963503649635,
      "grad_norm": 0.9237727522850037,
      "learning_rate": 3.367518248175183e-05,
      "loss": 1.0368,
      "step": 44730
    },
    {
      "epoch": 326.56934306569343,
      "grad_norm": 9.25658893585205,
      "learning_rate": 3.367153284671533e-05,
      "loss": 0.8357,
      "step": 44740
    },
    {
      "epoch": 326.64233576642334,
      "grad_norm": 16.3521728515625,
      "learning_rate": 3.3667883211678836e-05,
      "loss": 0.9484,
      "step": 44750
    },
    {
      "epoch": 326.7153284671533,
      "grad_norm": 13.294328689575195,
      "learning_rate": 3.366423357664234e-05,
      "loss": 0.7807,
      "step": 44760
    },
    {
      "epoch": 326.7883211678832,
      "grad_norm": 4.853762626647949,
      "learning_rate": 3.366058394160584e-05,
      "loss": 0.9619,
      "step": 44770
    },
    {
      "epoch": 326.86131386861314,
      "grad_norm": 0.13733616471290588,
      "learning_rate": 3.3656934306569344e-05,
      "loss": 0.8762,
      "step": 44780
    },
    {
      "epoch": 326.93430656934305,
      "grad_norm": 5.848400592803955,
      "learning_rate": 3.3653284671532845e-05,
      "loss": 0.936,
      "step": 44790
    },
    {
      "epoch": 327.007299270073,
      "grad_norm": 13.160099029541016,
      "learning_rate": 3.364963503649635e-05,
      "loss": 1.1557,
      "step": 44800
    },
    {
      "epoch": 327.08029197080293,
      "grad_norm": 14.258469581604004,
      "learning_rate": 3.364598540145986e-05,
      "loss": 1.0414,
      "step": 44810
    },
    {
      "epoch": 327.15328467153284,
      "grad_norm": 10.541624069213867,
      "learning_rate": 3.364233576642336e-05,
      "loss": 1.0524,
      "step": 44820
    },
    {
      "epoch": 327.22627737226276,
      "grad_norm": 10.509953498840332,
      "learning_rate": 3.363868613138687e-05,
      "loss": 0.9757,
      "step": 44830
    },
    {
      "epoch": 327.2992700729927,
      "grad_norm": 5.42280912399292,
      "learning_rate": 3.363503649635036e-05,
      "loss": 0.931,
      "step": 44840
    },
    {
      "epoch": 327.37226277372264,
      "grad_norm": 8.133262634277344,
      "learning_rate": 3.363138686131387e-05,
      "loss": 1.0914,
      "step": 44850
    },
    {
      "epoch": 327.44525547445255,
      "grad_norm": 8.950088500976562,
      "learning_rate": 3.3627737226277375e-05,
      "loss": 0.9211,
      "step": 44860
    },
    {
      "epoch": 327.51824817518246,
      "grad_norm": 8.705745697021484,
      "learning_rate": 3.3624087591240876e-05,
      "loss": 0.6069,
      "step": 44870
    },
    {
      "epoch": 327.59124087591243,
      "grad_norm": 0.6608811616897583,
      "learning_rate": 3.362043795620438e-05,
      "loss": 0.9604,
      "step": 44880
    },
    {
      "epoch": 327.66423357664235,
      "grad_norm": 11.178715705871582,
      "learning_rate": 3.3616788321167884e-05,
      "loss": 0.6906,
      "step": 44890
    },
    {
      "epoch": 327.73722627737226,
      "grad_norm": 7.483331203460693,
      "learning_rate": 3.361313868613139e-05,
      "loss": 0.7648,
      "step": 44900
    },
    {
      "epoch": 327.81021897810217,
      "grad_norm": 0.23988042771816254,
      "learning_rate": 3.360948905109489e-05,
      "loss": 0.779,
      "step": 44910
    },
    {
      "epoch": 327.88321167883214,
      "grad_norm": 15.928779602050781,
      "learning_rate": 3.360583941605839e-05,
      "loss": 1.3145,
      "step": 44920
    },
    {
      "epoch": 327.95620437956205,
      "grad_norm": 12.898677825927734,
      "learning_rate": 3.36021897810219e-05,
      "loss": 0.9745,
      "step": 44930
    },
    {
      "epoch": 328.02919708029196,
      "grad_norm": 15.268195152282715,
      "learning_rate": 3.35985401459854e-05,
      "loss": 1.222,
      "step": 44940
    },
    {
      "epoch": 328.1021897810219,
      "grad_norm": 17.818012237548828,
      "learning_rate": 3.359489051094891e-05,
      "loss": 1.1736,
      "step": 44950
    },
    {
      "epoch": 328.17518248175185,
      "grad_norm": 8.158463478088379,
      "learning_rate": 3.3591240875912414e-05,
      "loss": 0.9697,
      "step": 44960
    },
    {
      "epoch": 328.24817518248176,
      "grad_norm": 5.242183208465576,
      "learning_rate": 3.3587591240875915e-05,
      "loss": 1.203,
      "step": 44970
    },
    {
      "epoch": 328.3211678832117,
      "grad_norm": 0.18575212359428406,
      "learning_rate": 3.358394160583942e-05,
      "loss": 0.6929,
      "step": 44980
    },
    {
      "epoch": 328.3941605839416,
      "grad_norm": 9.839814186096191,
      "learning_rate": 3.358029197080292e-05,
      "loss": 0.7167,
      "step": 44990
    },
    {
      "epoch": 328.46715328467155,
      "grad_norm": 6.992271900177002,
      "learning_rate": 3.357664233576642e-05,
      "loss": 1.2116,
      "step": 45000
    },
    {
      "epoch": 328.54014598540147,
      "grad_norm": 4.330663681030273,
      "learning_rate": 3.357299270072993e-05,
      "loss": 0.5747,
      "step": 45010
    },
    {
      "epoch": 328.6131386861314,
      "grad_norm": 0.2972465455532074,
      "learning_rate": 3.356934306569343e-05,
      "loss": 0.7815,
      "step": 45020
    },
    {
      "epoch": 328.6861313868613,
      "grad_norm": 7.176417827606201,
      "learning_rate": 3.356569343065694e-05,
      "loss": 0.786,
      "step": 45030
    },
    {
      "epoch": 328.75912408759126,
      "grad_norm": 13.938139915466309,
      "learning_rate": 3.356204379562044e-05,
      "loss": 0.7792,
      "step": 45040
    },
    {
      "epoch": 328.8321167883212,
      "grad_norm": 7.734934329986572,
      "learning_rate": 3.3558394160583946e-05,
      "loss": 0.6997,
      "step": 45050
    },
    {
      "epoch": 328.9051094890511,
      "grad_norm": 9.925389289855957,
      "learning_rate": 3.3554744525547446e-05,
      "loss": 1.2225,
      "step": 45060
    },
    {
      "epoch": 328.978102189781,
      "grad_norm": 9.019664764404297,
      "learning_rate": 3.355109489051095e-05,
      "loss": 0.8048,
      "step": 45070
    },
    {
      "epoch": 329.05109489051097,
      "grad_norm": 14.84140396118164,
      "learning_rate": 3.3547445255474454e-05,
      "loss": 0.9574,
      "step": 45080
    },
    {
      "epoch": 329.1240875912409,
      "grad_norm": 19.866294860839844,
      "learning_rate": 3.3543795620437955e-05,
      "loss": 0.6626,
      "step": 45090
    },
    {
      "epoch": 329.1970802919708,
      "grad_norm": 10.32150650024414,
      "learning_rate": 3.354014598540146e-05,
      "loss": 1.0065,
      "step": 45100
    },
    {
      "epoch": 329.2700729927007,
      "grad_norm": 1.0405665636062622,
      "learning_rate": 3.353649635036496e-05,
      "loss": 0.884,
      "step": 45110
    },
    {
      "epoch": 329.3430656934307,
      "grad_norm": 0.16498982906341553,
      "learning_rate": 3.353284671532847e-05,
      "loss": 0.9336,
      "step": 45120
    },
    {
      "epoch": 329.4160583941606,
      "grad_norm": 12.966887474060059,
      "learning_rate": 3.352919708029198e-05,
      "loss": 0.6839,
      "step": 45130
    },
    {
      "epoch": 329.4890510948905,
      "grad_norm": 13.422982215881348,
      "learning_rate": 3.352554744525548e-05,
      "loss": 1.4535,
      "step": 45140
    },
    {
      "epoch": 329.5620437956204,
      "grad_norm": 9.714966773986816,
      "learning_rate": 3.352189781021898e-05,
      "loss": 1.0111,
      "step": 45150
    },
    {
      "epoch": 329.6350364963504,
      "grad_norm": 11.247237205505371,
      "learning_rate": 3.3518248175182485e-05,
      "loss": 0.8349,
      "step": 45160
    },
    {
      "epoch": 329.7080291970803,
      "grad_norm": 12.059983253479004,
      "learning_rate": 3.3514598540145986e-05,
      "loss": 1.3566,
      "step": 45170
    },
    {
      "epoch": 329.7810218978102,
      "grad_norm": 6.85431432723999,
      "learning_rate": 3.351094890510949e-05,
      "loss": 0.7927,
      "step": 45180
    },
    {
      "epoch": 329.8540145985401,
      "grad_norm": 7.547705173492432,
      "learning_rate": 3.350729927007299e-05,
      "loss": 0.8882,
      "step": 45190
    },
    {
      "epoch": 329.9270072992701,
      "grad_norm": 9.53361701965332,
      "learning_rate": 3.35036496350365e-05,
      "loss": 0.7106,
      "step": 45200
    },
    {
      "epoch": 330.0,
      "grad_norm": 0.1748015433549881,
      "learning_rate": 3.35e-05,
      "loss": 1.0852,
      "step": 45210
    },
    {
      "epoch": 330.0729927007299,
      "grad_norm": 9.193456649780273,
      "learning_rate": 3.349635036496351e-05,
      "loss": 0.7112,
      "step": 45220
    },
    {
      "epoch": 330.1459854014599,
      "grad_norm": 12.601433753967285,
      "learning_rate": 3.349270072992701e-05,
      "loss": 1.0913,
      "step": 45230
    },
    {
      "epoch": 330.2189781021898,
      "grad_norm": 8.584576606750488,
      "learning_rate": 3.348905109489051e-05,
      "loss": 1.0338,
      "step": 45240
    },
    {
      "epoch": 330.2919708029197,
      "grad_norm": 0.21723051369190216,
      "learning_rate": 3.348540145985402e-05,
      "loss": 1.1103,
      "step": 45250
    },
    {
      "epoch": 330.3649635036496,
      "grad_norm": 2.372154474258423,
      "learning_rate": 3.348175182481752e-05,
      "loss": 0.893,
      "step": 45260
    },
    {
      "epoch": 330.4379562043796,
      "grad_norm": 11.605198860168457,
      "learning_rate": 3.3478102189781024e-05,
      "loss": 0.8094,
      "step": 45270
    },
    {
      "epoch": 330.5109489051095,
      "grad_norm": 10.43997573852539,
      "learning_rate": 3.347445255474453e-05,
      "loss": 0.9075,
      "step": 45280
    },
    {
      "epoch": 330.5839416058394,
      "grad_norm": 18.107126235961914,
      "learning_rate": 3.347080291970803e-05,
      "loss": 0.9349,
      "step": 45290
    },
    {
      "epoch": 330.6569343065693,
      "grad_norm": 0.22241094708442688,
      "learning_rate": 3.346715328467153e-05,
      "loss": 1.0856,
      "step": 45300
    },
    {
      "epoch": 330.7299270072993,
      "grad_norm": 0.12239377200603485,
      "learning_rate": 3.346350364963503e-05,
      "loss": 0.6438,
      "step": 45310
    },
    {
      "epoch": 330.8029197080292,
      "grad_norm": 12.709197044372559,
      "learning_rate": 3.345985401459854e-05,
      "loss": 1.1707,
      "step": 45320
    },
    {
      "epoch": 330.8759124087591,
      "grad_norm": 11.878270149230957,
      "learning_rate": 3.345620437956205e-05,
      "loss": 1.2106,
      "step": 45330
    },
    {
      "epoch": 330.94890510948903,
      "grad_norm": 11.537312507629395,
      "learning_rate": 3.345255474452555e-05,
      "loss": 0.8801,
      "step": 45340
    },
    {
      "epoch": 331.021897810219,
      "grad_norm": 0.12160065025091171,
      "learning_rate": 3.3448905109489055e-05,
      "loss": 1.2299,
      "step": 45350
    },
    {
      "epoch": 331.0948905109489,
      "grad_norm": 9.653149604797363,
      "learning_rate": 3.3445255474452556e-05,
      "loss": 0.9851,
      "step": 45360
    },
    {
      "epoch": 331.1678832116788,
      "grad_norm": 0.4657864570617676,
      "learning_rate": 3.344160583941606e-05,
      "loss": 0.8755,
      "step": 45370
    },
    {
      "epoch": 331.24087591240874,
      "grad_norm": 10.754805564880371,
      "learning_rate": 3.3437956204379564e-05,
      "loss": 1.1429,
      "step": 45380
    },
    {
      "epoch": 331.3138686131387,
      "grad_norm": 8.533539772033691,
      "learning_rate": 3.3434306569343064e-05,
      "loss": 0.7299,
      "step": 45390
    },
    {
      "epoch": 331.3868613138686,
      "grad_norm": 7.695760726928711,
      "learning_rate": 3.343065693430657e-05,
      "loss": 0.9843,
      "step": 45400
    },
    {
      "epoch": 331.45985401459853,
      "grad_norm": 12.779290199279785,
      "learning_rate": 3.342700729927007e-05,
      "loss": 0.7057,
      "step": 45410
    },
    {
      "epoch": 331.53284671532845,
      "grad_norm": 16.737184524536133,
      "learning_rate": 3.342335766423358e-05,
      "loss": 1.2369,
      "step": 45420
    },
    {
      "epoch": 331.6058394160584,
      "grad_norm": 7.2955827713012695,
      "learning_rate": 3.3419708029197087e-05,
      "loss": 0.6357,
      "step": 45430
    },
    {
      "epoch": 331.6788321167883,
      "grad_norm": 8.173628807067871,
      "learning_rate": 3.341605839416059e-05,
      "loss": 0.7266,
      "step": 45440
    },
    {
      "epoch": 331.75182481751824,
      "grad_norm": 12.94245433807373,
      "learning_rate": 3.3412408759124094e-05,
      "loss": 1.0097,
      "step": 45450
    },
    {
      "epoch": 331.82481751824815,
      "grad_norm": 20.26634407043457,
      "learning_rate": 3.340875912408759e-05,
      "loss": 0.6892,
      "step": 45460
    },
    {
      "epoch": 331.8978102189781,
      "grad_norm": 5.608373641967773,
      "learning_rate": 3.3405109489051095e-05,
      "loss": 1.3473,
      "step": 45470
    },
    {
      "epoch": 331.97080291970804,
      "grad_norm": 16.03092384338379,
      "learning_rate": 3.34014598540146e-05,
      "loss": 1.1066,
      "step": 45480
    },
    {
      "epoch": 332.04379562043795,
      "grad_norm": 7.342341423034668,
      "learning_rate": 3.33978102189781e-05,
      "loss": 0.7842,
      "step": 45490
    },
    {
      "epoch": 332.11678832116786,
      "grad_norm": 0.10783057659864426,
      "learning_rate": 3.339416058394161e-05,
      "loss": 0.677,
      "step": 45500
    },
    {
      "epoch": 332.18978102189783,
      "grad_norm": 12.453083992004395,
      "learning_rate": 3.339051094890511e-05,
      "loss": 1.1684,
      "step": 45510
    },
    {
      "epoch": 332.26277372262774,
      "grad_norm": 17.791290283203125,
      "learning_rate": 3.338686131386862e-05,
      "loss": 0.7694,
      "step": 45520
    },
    {
      "epoch": 332.33576642335765,
      "grad_norm": 8.081838607788086,
      "learning_rate": 3.338321167883212e-05,
      "loss": 1.1442,
      "step": 45530
    },
    {
      "epoch": 332.40875912408757,
      "grad_norm": 14.565987586975098,
      "learning_rate": 3.337956204379562e-05,
      "loss": 1.2767,
      "step": 45540
    },
    {
      "epoch": 332.48175182481754,
      "grad_norm": 17.629512786865234,
      "learning_rate": 3.3375912408759126e-05,
      "loss": 1.2771,
      "step": 45550
    },
    {
      "epoch": 332.55474452554745,
      "grad_norm": 13.590950012207031,
      "learning_rate": 3.337226277372263e-05,
      "loss": 0.733,
      "step": 45560
    },
    {
      "epoch": 332.62773722627736,
      "grad_norm": 12.179167747497559,
      "learning_rate": 3.3368613138686134e-05,
      "loss": 0.7934,
      "step": 45570
    },
    {
      "epoch": 332.7007299270073,
      "grad_norm": 6.575984001159668,
      "learning_rate": 3.3364963503649635e-05,
      "loss": 0.6567,
      "step": 45580
    },
    {
      "epoch": 332.77372262773724,
      "grad_norm": 0.24559634923934937,
      "learning_rate": 3.336131386861314e-05,
      "loss": 0.7208,
      "step": 45590
    },
    {
      "epoch": 332.84671532846716,
      "grad_norm": 0.1845950186252594,
      "learning_rate": 3.335766423357665e-05,
      "loss": 1.0566,
      "step": 45600
    },
    {
      "epoch": 332.91970802919707,
      "grad_norm": 0.16864469647407532,
      "learning_rate": 3.335401459854014e-05,
      "loss": 0.9758,
      "step": 45610
    },
    {
      "epoch": 332.992700729927,
      "grad_norm": 7.292957782745361,
      "learning_rate": 3.335036496350365e-05,
      "loss": 1.4038,
      "step": 45620
    },
    {
      "epoch": 333.06569343065695,
      "grad_norm": 10.711722373962402,
      "learning_rate": 3.334671532846716e-05,
      "loss": 0.88,
      "step": 45630
    },
    {
      "epoch": 333.13868613138686,
      "grad_norm": 10.533363342285156,
      "learning_rate": 3.334306569343066e-05,
      "loss": 0.9636,
      "step": 45640
    },
    {
      "epoch": 333.2116788321168,
      "grad_norm": 0.341039776802063,
      "learning_rate": 3.3339416058394165e-05,
      "loss": 0.8357,
      "step": 45650
    },
    {
      "epoch": 333.2846715328467,
      "grad_norm": 7.203106880187988,
      "learning_rate": 3.3335766423357666e-05,
      "loss": 1.0304,
      "step": 45660
    },
    {
      "epoch": 333.35766423357666,
      "grad_norm": 10.696854591369629,
      "learning_rate": 3.333211678832117e-05,
      "loss": 1.046,
      "step": 45670
    },
    {
      "epoch": 333.43065693430657,
      "grad_norm": 7.747863292694092,
      "learning_rate": 3.3328467153284673e-05,
      "loss": 1.2958,
      "step": 45680
    },
    {
      "epoch": 333.5036496350365,
      "grad_norm": 8.847254753112793,
      "learning_rate": 3.3324817518248174e-05,
      "loss": 0.6412,
      "step": 45690
    },
    {
      "epoch": 333.57664233576645,
      "grad_norm": 20.45975112915039,
      "learning_rate": 3.332116788321168e-05,
      "loss": 0.743,
      "step": 45700
    },
    {
      "epoch": 333.64963503649636,
      "grad_norm": 0.42889493703842163,
      "learning_rate": 3.331751824817518e-05,
      "loss": 0.9479,
      "step": 45710
    },
    {
      "epoch": 333.7226277372263,
      "grad_norm": 0.27599793672561646,
      "learning_rate": 3.331386861313869e-05,
      "loss": 0.6992,
      "step": 45720
    },
    {
      "epoch": 333.7956204379562,
      "grad_norm": 0.18135987222194672,
      "learning_rate": 3.331021897810219e-05,
      "loss": 0.7186,
      "step": 45730
    },
    {
      "epoch": 333.86861313868616,
      "grad_norm": 10.995993614196777,
      "learning_rate": 3.33065693430657e-05,
      "loss": 0.949,
      "step": 45740
    },
    {
      "epoch": 333.94160583941607,
      "grad_norm": 7.943323135375977,
      "learning_rate": 3.3302919708029204e-05,
      "loss": 1.064,
      "step": 45750
    },
    {
      "epoch": 334.014598540146,
      "grad_norm": 5.27506160736084,
      "learning_rate": 3.32992700729927e-05,
      "loss": 1.1215,
      "step": 45760
    },
    {
      "epoch": 334.0875912408759,
      "grad_norm": 10.577157020568848,
      "learning_rate": 3.3295620437956205e-05,
      "loss": 1.0165,
      "step": 45770
    },
    {
      "epoch": 334.16058394160586,
      "grad_norm": 14.298248291015625,
      "learning_rate": 3.3291970802919705e-05,
      "loss": 1.2168,
      "step": 45780
    },
    {
      "epoch": 334.2335766423358,
      "grad_norm": 6.5960307121276855,
      "learning_rate": 3.328832116788321e-05,
      "loss": 0.9753,
      "step": 45790
    },
    {
      "epoch": 334.3065693430657,
      "grad_norm": 9.58842658996582,
      "learning_rate": 3.328467153284672e-05,
      "loss": 1.0292,
      "step": 45800
    },
    {
      "epoch": 334.3795620437956,
      "grad_norm": 8.15675163269043,
      "learning_rate": 3.328102189781022e-05,
      "loss": 0.8123,
      "step": 45810
    },
    {
      "epoch": 334.45255474452557,
      "grad_norm": 16.33511734008789,
      "learning_rate": 3.327737226277373e-05,
      "loss": 0.9521,
      "step": 45820
    },
    {
      "epoch": 334.5255474452555,
      "grad_norm": 15.85444164276123,
      "learning_rate": 3.327372262773723e-05,
      "loss": 0.7843,
      "step": 45830
    },
    {
      "epoch": 334.5985401459854,
      "grad_norm": 5.1370930671691895,
      "learning_rate": 3.327007299270073e-05,
      "loss": 0.6271,
      "step": 45840
    },
    {
      "epoch": 334.6715328467153,
      "grad_norm": 10.178949356079102,
      "learning_rate": 3.3266423357664236e-05,
      "loss": 0.8858,
      "step": 45850
    },
    {
      "epoch": 334.7445255474453,
      "grad_norm": 0.0819133073091507,
      "learning_rate": 3.3262773722627737e-05,
      "loss": 1.0321,
      "step": 45860
    },
    {
      "epoch": 334.8175182481752,
      "grad_norm": 12.476295471191406,
      "learning_rate": 3.3259124087591244e-05,
      "loss": 1.2291,
      "step": 45870
    },
    {
      "epoch": 334.8905109489051,
      "grad_norm": 5.94968318939209,
      "learning_rate": 3.3255474452554744e-05,
      "loss": 0.8131,
      "step": 45880
    },
    {
      "epoch": 334.963503649635,
      "grad_norm": 7.111347198486328,
      "learning_rate": 3.325182481751825e-05,
      "loss": 0.7108,
      "step": 45890
    },
    {
      "epoch": 335.036496350365,
      "grad_norm": 6.2024736404418945,
      "learning_rate": 3.324817518248176e-05,
      "loss": 0.9423,
      "step": 45900
    },
    {
      "epoch": 335.1094890510949,
      "grad_norm": 10.363736152648926,
      "learning_rate": 3.324452554744525e-05,
      "loss": 0.4501,
      "step": 45910
    },
    {
      "epoch": 335.1824817518248,
      "grad_norm": 0.18946969509124756,
      "learning_rate": 3.324087591240876e-05,
      "loss": 0.8631,
      "step": 45920
    },
    {
      "epoch": 335.2554744525547,
      "grad_norm": 7.542129993438721,
      "learning_rate": 3.323722627737226e-05,
      "loss": 0.6759,
      "step": 45930
    },
    {
      "epoch": 335.3284671532847,
      "grad_norm": 16.195886611938477,
      "learning_rate": 3.323357664233577e-05,
      "loss": 0.9207,
      "step": 45940
    },
    {
      "epoch": 335.4014598540146,
      "grad_norm": 6.995041370391846,
      "learning_rate": 3.3229927007299275e-05,
      "loss": 0.8093,
      "step": 45950
    },
    {
      "epoch": 335.4744525547445,
      "grad_norm": 11.100892066955566,
      "learning_rate": 3.3226277372262775e-05,
      "loss": 1.2957,
      "step": 45960
    },
    {
      "epoch": 335.54744525547443,
      "grad_norm": 7.661750793457031,
      "learning_rate": 3.322262773722628e-05,
      "loss": 1.0842,
      "step": 45970
    },
    {
      "epoch": 335.6204379562044,
      "grad_norm": 0.8029717803001404,
      "learning_rate": 3.321897810218978e-05,
      "loss": 0.9855,
      "step": 45980
    },
    {
      "epoch": 335.6934306569343,
      "grad_norm": 17.352127075195312,
      "learning_rate": 3.3215328467153284e-05,
      "loss": 0.9104,
      "step": 45990
    },
    {
      "epoch": 335.7664233576642,
      "grad_norm": 21.40249252319336,
      "learning_rate": 3.321167883211679e-05,
      "loss": 1.3167,
      "step": 46000
    },
    {
      "epoch": 335.83941605839414,
      "grad_norm": 14.565945625305176,
      "learning_rate": 3.320802919708029e-05,
      "loss": 1.0677,
      "step": 46010
    },
    {
      "epoch": 335.9124087591241,
      "grad_norm": 10.004250526428223,
      "learning_rate": 3.32043795620438e-05,
      "loss": 0.9262,
      "step": 46020
    },
    {
      "epoch": 335.985401459854,
      "grad_norm": 8.253599166870117,
      "learning_rate": 3.32007299270073e-05,
      "loss": 1.2223,
      "step": 46030
    },
    {
      "epoch": 336.05839416058393,
      "grad_norm": 8.926933288574219,
      "learning_rate": 3.3197080291970806e-05,
      "loss": 0.5745,
      "step": 46040
    },
    {
      "epoch": 336.13138686131384,
      "grad_norm": 0.11455465853214264,
      "learning_rate": 3.319343065693431e-05,
      "loss": 0.9071,
      "step": 46050
    },
    {
      "epoch": 336.2043795620438,
      "grad_norm": 8.912216186523438,
      "learning_rate": 3.3189781021897814e-05,
      "loss": 1.3101,
      "step": 46060
    },
    {
      "epoch": 336.2773722627737,
      "grad_norm": 10.409914016723633,
      "learning_rate": 3.3186131386861315e-05,
      "loss": 1.1516,
      "step": 46070
    },
    {
      "epoch": 336.35036496350364,
      "grad_norm": 7.125893592834473,
      "learning_rate": 3.3182481751824815e-05,
      "loss": 0.7919,
      "step": 46080
    },
    {
      "epoch": 336.42335766423355,
      "grad_norm": 14.911123275756836,
      "learning_rate": 3.317883211678832e-05,
      "loss": 0.9926,
      "step": 46090
    },
    {
      "epoch": 336.4963503649635,
      "grad_norm": 0.21016444265842438,
      "learning_rate": 3.317518248175183e-05,
      "loss": 0.8813,
      "step": 46100
    },
    {
      "epoch": 336.56934306569343,
      "grad_norm": 4.080305576324463,
      "learning_rate": 3.317153284671533e-05,
      "loss": 1.0451,
      "step": 46110
    },
    {
      "epoch": 336.64233576642334,
      "grad_norm": 7.168366432189941,
      "learning_rate": 3.316788321167884e-05,
      "loss": 1.202,
      "step": 46120
    },
    {
      "epoch": 336.7153284671533,
      "grad_norm": 22.05908203125,
      "learning_rate": 3.316423357664234e-05,
      "loss": 1.1009,
      "step": 46130
    },
    {
      "epoch": 336.7883211678832,
      "grad_norm": 6.653431415557861,
      "learning_rate": 3.316058394160584e-05,
      "loss": 0.743,
      "step": 46140
    },
    {
      "epoch": 336.86131386861314,
      "grad_norm": 1.9767694473266602,
      "learning_rate": 3.3156934306569346e-05,
      "loss": 0.8786,
      "step": 46150
    },
    {
      "epoch": 336.93430656934305,
      "grad_norm": 7.666675567626953,
      "learning_rate": 3.3153284671532846e-05,
      "loss": 0.91,
      "step": 46160
    },
    {
      "epoch": 337.007299270073,
      "grad_norm": 9.191329956054688,
      "learning_rate": 3.3149635036496354e-05,
      "loss": 1.0058,
      "step": 46170
    },
    {
      "epoch": 337.08029197080293,
      "grad_norm": 6.37971305847168,
      "learning_rate": 3.3145985401459854e-05,
      "loss": 1.2559,
      "step": 46180
    },
    {
      "epoch": 337.15328467153284,
      "grad_norm": 7.020133972167969,
      "learning_rate": 3.314233576642336e-05,
      "loss": 0.9599,
      "step": 46190
    },
    {
      "epoch": 337.22627737226276,
      "grad_norm": 10.515522003173828,
      "learning_rate": 3.313868613138686e-05,
      "loss": 0.8089,
      "step": 46200
    },
    {
      "epoch": 337.2992700729927,
      "grad_norm": 18.351179122924805,
      "learning_rate": 3.313503649635037e-05,
      "loss": 1.106,
      "step": 46210
    },
    {
      "epoch": 337.37226277372264,
      "grad_norm": 0.14637142419815063,
      "learning_rate": 3.313138686131387e-05,
      "loss": 0.6093,
      "step": 46220
    },
    {
      "epoch": 337.44525547445255,
      "grad_norm": 0.25270822644233704,
      "learning_rate": 3.312773722627737e-05,
      "loss": 0.7776,
      "step": 46230
    },
    {
      "epoch": 337.51824817518246,
      "grad_norm": 12.772516250610352,
      "learning_rate": 3.312408759124088e-05,
      "loss": 1.1392,
      "step": 46240
    },
    {
      "epoch": 337.59124087591243,
      "grad_norm": 13.860258102416992,
      "learning_rate": 3.312043795620438e-05,
      "loss": 0.7785,
      "step": 46250
    },
    {
      "epoch": 337.66423357664235,
      "grad_norm": 8.15499496459961,
      "learning_rate": 3.3116788321167885e-05,
      "loss": 0.8041,
      "step": 46260
    },
    {
      "epoch": 337.73722627737226,
      "grad_norm": 11.12012767791748,
      "learning_rate": 3.311313868613139e-05,
      "loss": 1.1127,
      "step": 46270
    },
    {
      "epoch": 337.81021897810217,
      "grad_norm": 13.16025161743164,
      "learning_rate": 3.310948905109489e-05,
      "loss": 1.0433,
      "step": 46280
    },
    {
      "epoch": 337.88321167883214,
      "grad_norm": 10.394749641418457,
      "learning_rate": 3.31058394160584e-05,
      "loss": 0.8248,
      "step": 46290
    },
    {
      "epoch": 337.95620437956205,
      "grad_norm": 7.310481071472168,
      "learning_rate": 3.31021897810219e-05,
      "loss": 1.0921,
      "step": 46300
    },
    {
      "epoch": 338.02919708029196,
      "grad_norm": 15.349498748779297,
      "learning_rate": 3.30985401459854e-05,
      "loss": 1.5636,
      "step": 46310
    },
    {
      "epoch": 338.1021897810219,
      "grad_norm": 5.211724281311035,
      "learning_rate": 3.309489051094891e-05,
      "loss": 1.391,
      "step": 46320
    },
    {
      "epoch": 338.17518248175185,
      "grad_norm": 16.018171310424805,
      "learning_rate": 3.309124087591241e-05,
      "loss": 0.9169,
      "step": 46330
    },
    {
      "epoch": 338.24817518248176,
      "grad_norm": 7.148770809173584,
      "learning_rate": 3.3087591240875916e-05,
      "loss": 0.7358,
      "step": 46340
    },
    {
      "epoch": 338.3211678832117,
      "grad_norm": 4.585712909698486,
      "learning_rate": 3.308394160583942e-05,
      "loss": 0.7406,
      "step": 46350
    },
    {
      "epoch": 338.3941605839416,
      "grad_norm": 10.875346183776855,
      "learning_rate": 3.3080291970802924e-05,
      "loss": 0.891,
      "step": 46360
    },
    {
      "epoch": 338.46715328467155,
      "grad_norm": 7.5217413902282715,
      "learning_rate": 3.3076642335766424e-05,
      "loss": 0.7581,
      "step": 46370
    },
    {
      "epoch": 338.54014598540147,
      "grad_norm": 0.35762616991996765,
      "learning_rate": 3.3072992700729925e-05,
      "loss": 0.579,
      "step": 46380
    },
    {
      "epoch": 338.6131386861314,
      "grad_norm": 5.6713080406188965,
      "learning_rate": 3.306934306569343e-05,
      "loss": 0.7962,
      "step": 46390
    },
    {
      "epoch": 338.6861313868613,
      "grad_norm": 12.664642333984375,
      "learning_rate": 3.306569343065693e-05,
      "loss": 1.0866,
      "step": 46400
    },
    {
      "epoch": 338.75912408759126,
      "grad_norm": 8.712475776672363,
      "learning_rate": 3.306204379562044e-05,
      "loss": 1.0719,
      "step": 46410
    },
    {
      "epoch": 338.8321167883212,
      "grad_norm": 7.365659236907959,
      "learning_rate": 3.305839416058395e-05,
      "loss": 0.7404,
      "step": 46420
    },
    {
      "epoch": 338.9051094890511,
      "grad_norm": 13.164814949035645,
      "learning_rate": 3.305474452554745e-05,
      "loss": 0.9262,
      "step": 46430
    },
    {
      "epoch": 338.978102189781,
      "grad_norm": 5.572250843048096,
      "learning_rate": 3.3051094890510955e-05,
      "loss": 1.0346,
      "step": 46440
    },
    {
      "epoch": 339.05109489051097,
      "grad_norm": 10.480448722839355,
      "learning_rate": 3.304744525547445e-05,
      "loss": 1.2847,
      "step": 46450
    },
    {
      "epoch": 339.1240875912409,
      "grad_norm": 21.773237228393555,
      "learning_rate": 3.3043795620437956e-05,
      "loss": 0.9245,
      "step": 46460
    },
    {
      "epoch": 339.1970802919708,
      "grad_norm": 15.843602180480957,
      "learning_rate": 3.304014598540146e-05,
      "loss": 0.8699,
      "step": 46470
    },
    {
      "epoch": 339.2700729927007,
      "grad_norm": 12.469660758972168,
      "learning_rate": 3.3036496350364964e-05,
      "loss": 1.1765,
      "step": 46480
    },
    {
      "epoch": 339.3430656934307,
      "grad_norm": 6.865302562713623,
      "learning_rate": 3.303284671532847e-05,
      "loss": 0.6982,
      "step": 46490
    },
    {
      "epoch": 339.4160583941606,
      "grad_norm": 5.885509967803955,
      "learning_rate": 3.302919708029197e-05,
      "loss": 1.1138,
      "step": 46500
    },
    {
      "epoch": 339.4890510948905,
      "grad_norm": 9.969878196716309,
      "learning_rate": 3.302554744525548e-05,
      "loss": 0.899,
      "step": 46510
    },
    {
      "epoch": 339.5620437956204,
      "grad_norm": 10.93801212310791,
      "learning_rate": 3.302189781021898e-05,
      "loss": 0.9594,
      "step": 46520
    },
    {
      "epoch": 339.6350364963504,
      "grad_norm": 0.11057300865650177,
      "learning_rate": 3.301824817518248e-05,
      "loss": 0.8743,
      "step": 46530
    },
    {
      "epoch": 339.7080291970803,
      "grad_norm": 12.978252410888672,
      "learning_rate": 3.301459854014599e-05,
      "loss": 0.673,
      "step": 46540
    },
    {
      "epoch": 339.7810218978102,
      "grad_norm": 10.884305953979492,
      "learning_rate": 3.301094890510949e-05,
      "loss": 0.7328,
      "step": 46550
    },
    {
      "epoch": 339.8540145985401,
      "grad_norm": 11.861213684082031,
      "learning_rate": 3.3007299270072995e-05,
      "loss": 0.9824,
      "step": 46560
    },
    {
      "epoch": 339.9270072992701,
      "grad_norm": 6.668041229248047,
      "learning_rate": 3.30036496350365e-05,
      "loss": 1.0748,
      "step": 46570
    },
    {
      "epoch": 340.0,
      "grad_norm": 29.410898208618164,
      "learning_rate": 3.3e-05,
      "loss": 1.1294,
      "step": 46580
    },
    {
      "epoch": 340.0729927007299,
      "grad_norm": 11.157556533813477,
      "learning_rate": 3.299635036496351e-05,
      "loss": 0.6256,
      "step": 46590
    },
    {
      "epoch": 340.1459854014599,
      "grad_norm": 8.636777877807617,
      "learning_rate": 3.2992700729927004e-05,
      "loss": 1.154,
      "step": 46600
    },
    {
      "epoch": 340.2189781021898,
      "grad_norm": 0.13606500625610352,
      "learning_rate": 3.298905109489051e-05,
      "loss": 1.1039,
      "step": 46610
    },
    {
      "epoch": 340.2919708029197,
      "grad_norm": 2.418067455291748,
      "learning_rate": 3.298540145985402e-05,
      "loss": 0.9243,
      "step": 46620
    },
    {
      "epoch": 340.3649635036496,
      "grad_norm": 10.052148818969727,
      "learning_rate": 3.298175182481752e-05,
      "loss": 1.2491,
      "step": 46630
    },
    {
      "epoch": 340.4379562043796,
      "grad_norm": 12.175897598266602,
      "learning_rate": 3.2978102189781026e-05,
      "loss": 1.1975,
      "step": 46640
    },
    {
      "epoch": 340.5109489051095,
      "grad_norm": 0.10582156479358673,
      "learning_rate": 3.2974452554744526e-05,
      "loss": 0.914,
      "step": 46650
    },
    {
      "epoch": 340.5839416058394,
      "grad_norm": 5.6478447914123535,
      "learning_rate": 3.2970802919708034e-05,
      "loss": 0.721,
      "step": 46660
    },
    {
      "epoch": 340.6569343065693,
      "grad_norm": 9.37718677520752,
      "learning_rate": 3.2967153284671534e-05,
      "loss": 0.9195,
      "step": 46670
    },
    {
      "epoch": 340.7299270072993,
      "grad_norm": 5.691414833068848,
      "learning_rate": 3.2963503649635035e-05,
      "loss": 0.8406,
      "step": 46680
    },
    {
      "epoch": 340.8029197080292,
      "grad_norm": 15.274445533752441,
      "learning_rate": 3.295985401459854e-05,
      "loss": 0.8558,
      "step": 46690
    },
    {
      "epoch": 340.8759124087591,
      "grad_norm": 7.17036247253418,
      "learning_rate": 3.295620437956204e-05,
      "loss": 1.0188,
      "step": 46700
    },
    {
      "epoch": 340.94890510948903,
      "grad_norm": 0.12533853948116302,
      "learning_rate": 3.295255474452555e-05,
      "loss": 1.0709,
      "step": 46710
    },
    {
      "epoch": 341.021897810219,
      "grad_norm": 7.965298652648926,
      "learning_rate": 3.294890510948905e-05,
      "loss": 0.7151,
      "step": 46720
    },
    {
      "epoch": 341.0948905109489,
      "grad_norm": 12.70936393737793,
      "learning_rate": 3.294525547445256e-05,
      "loss": 0.9802,
      "step": 46730
    },
    {
      "epoch": 341.1678832116788,
      "grad_norm": 0.14469678699970245,
      "learning_rate": 3.2941605839416065e-05,
      "loss": 0.8618,
      "step": 46740
    },
    {
      "epoch": 341.24087591240874,
      "grad_norm": 0.18708324432373047,
      "learning_rate": 3.293795620437956e-05,
      "loss": 0.9211,
      "step": 46750
    },
    {
      "epoch": 341.3138686131387,
      "grad_norm": 7.916934013366699,
      "learning_rate": 3.2934306569343066e-05,
      "loss": 1.0663,
      "step": 46760
    },
    {
      "epoch": 341.3868613138686,
      "grad_norm": 12.377703666687012,
      "learning_rate": 3.293065693430657e-05,
      "loss": 0.9428,
      "step": 46770
    },
    {
      "epoch": 341.45985401459853,
      "grad_norm": 14.398344993591309,
      "learning_rate": 3.2927007299270073e-05,
      "loss": 0.5468,
      "step": 46780
    },
    {
      "epoch": 341.53284671532845,
      "grad_norm": 14.814737319946289,
      "learning_rate": 3.292335766423358e-05,
      "loss": 1.3758,
      "step": 46790
    },
    {
      "epoch": 341.6058394160584,
      "grad_norm": 9.986851692199707,
      "learning_rate": 3.291970802919708e-05,
      "loss": 1.1474,
      "step": 46800
    },
    {
      "epoch": 341.6788321167883,
      "grad_norm": 6.5897393226623535,
      "learning_rate": 3.291605839416059e-05,
      "loss": 0.9989,
      "step": 46810
    },
    {
      "epoch": 341.75182481751824,
      "grad_norm": 13.35101318359375,
      "learning_rate": 3.291240875912409e-05,
      "loss": 0.7555,
      "step": 46820
    },
    {
      "epoch": 341.82481751824815,
      "grad_norm": 3.5473873615264893,
      "learning_rate": 3.290875912408759e-05,
      "loss": 1.0186,
      "step": 46830
    },
    {
      "epoch": 341.8978102189781,
      "grad_norm": 3.801706314086914,
      "learning_rate": 3.29051094890511e-05,
      "loss": 0.8531,
      "step": 46840
    },
    {
      "epoch": 341.97080291970804,
      "grad_norm": 5.345264911651611,
      "learning_rate": 3.29014598540146e-05,
      "loss": 0.9727,
      "step": 46850
    },
    {
      "epoch": 342.04379562043795,
      "grad_norm": 13.941753387451172,
      "learning_rate": 3.2897810218978104e-05,
      "loss": 0.6337,
      "step": 46860
    },
    {
      "epoch": 342.11678832116786,
      "grad_norm": 0.10857276618480682,
      "learning_rate": 3.2894160583941605e-05,
      "loss": 1.0095,
      "step": 46870
    },
    {
      "epoch": 342.18978102189783,
      "grad_norm": 7.406919956207275,
      "learning_rate": 3.289051094890511e-05,
      "loss": 1.0465,
      "step": 46880
    },
    {
      "epoch": 342.26277372262774,
      "grad_norm": 8.138114929199219,
      "learning_rate": 3.288686131386862e-05,
      "loss": 0.8954,
      "step": 46890
    },
    {
      "epoch": 342.33576642335765,
      "grad_norm": 16.803499221801758,
      "learning_rate": 3.288321167883212e-05,
      "loss": 0.9382,
      "step": 46900
    },
    {
      "epoch": 342.40875912408757,
      "grad_norm": 6.853786468505859,
      "learning_rate": 3.287956204379562e-05,
      "loss": 0.7936,
      "step": 46910
    },
    {
      "epoch": 342.48175182481754,
      "grad_norm": 9.280312538146973,
      "learning_rate": 3.287591240875912e-05,
      "loss": 1.0367,
      "step": 46920
    },
    {
      "epoch": 342.55474452554745,
      "grad_norm": 14.307425498962402,
      "learning_rate": 3.287226277372263e-05,
      "loss": 1.2619,
      "step": 46930
    },
    {
      "epoch": 342.62773722627736,
      "grad_norm": 7.409158229827881,
      "learning_rate": 3.2868613138686136e-05,
      "loss": 0.6671,
      "step": 46940
    },
    {
      "epoch": 342.7007299270073,
      "grad_norm": 16.012144088745117,
      "learning_rate": 3.2864963503649636e-05,
      "loss": 0.9271,
      "step": 46950
    },
    {
      "epoch": 342.77372262773724,
      "grad_norm": 11.141924858093262,
      "learning_rate": 3.286131386861314e-05,
      "loss": 0.8618,
      "step": 46960
    },
    {
      "epoch": 342.84671532846716,
      "grad_norm": 14.84171199798584,
      "learning_rate": 3.2857664233576644e-05,
      "loss": 0.9781,
      "step": 46970
    },
    {
      "epoch": 342.91970802919707,
      "grad_norm": 11.52983570098877,
      "learning_rate": 3.2854014598540144e-05,
      "loss": 0.761,
      "step": 46980
    },
    {
      "epoch": 342.992700729927,
      "grad_norm": 18.60531997680664,
      "learning_rate": 3.285036496350365e-05,
      "loss": 1.1791,
      "step": 46990
    },
    {
      "epoch": 343.06569343065695,
      "grad_norm": 0.22624213993549347,
      "learning_rate": 3.284671532846715e-05,
      "loss": 0.8515,
      "step": 47000
    },
    {
      "epoch": 343.13868613138686,
      "grad_norm": 6.960736274719238,
      "learning_rate": 3.284306569343066e-05,
      "loss": 1.1628,
      "step": 47010
    },
    {
      "epoch": 343.2116788321168,
      "grad_norm": 11.803603172302246,
      "learning_rate": 3.283941605839416e-05,
      "loss": 1.056,
      "step": 47020
    },
    {
      "epoch": 343.2846715328467,
      "grad_norm": 10.383550643920898,
      "learning_rate": 3.283576642335767e-05,
      "loss": 1.2006,
      "step": 47030
    },
    {
      "epoch": 343.35766423357666,
      "grad_norm": 12.342610359191895,
      "learning_rate": 3.2832116788321174e-05,
      "loss": 0.9115,
      "step": 47040
    },
    {
      "epoch": 343.43065693430657,
      "grad_norm": 19.59241485595703,
      "learning_rate": 3.2828467153284675e-05,
      "loss": 1.2496,
      "step": 47050
    },
    {
      "epoch": 343.5036496350365,
      "grad_norm": 12.553874015808105,
      "learning_rate": 3.2824817518248175e-05,
      "loss": 0.5281,
      "step": 47060
    },
    {
      "epoch": 343.57664233576645,
      "grad_norm": 0.13956688344478607,
      "learning_rate": 3.2821167883211676e-05,
      "loss": 0.6553,
      "step": 47070
    },
    {
      "epoch": 343.64963503649636,
      "grad_norm": 5.791538238525391,
      "learning_rate": 3.281751824817518e-05,
      "loss": 0.7758,
      "step": 47080
    },
    {
      "epoch": 343.7226277372263,
      "grad_norm": 4.60298490524292,
      "learning_rate": 3.281386861313869e-05,
      "loss": 0.8091,
      "step": 47090
    },
    {
      "epoch": 343.7956204379562,
      "grad_norm": 5.810788154602051,
      "learning_rate": 3.281021897810219e-05,
      "loss": 0.8678,
      "step": 47100
    },
    {
      "epoch": 343.86861313868616,
      "grad_norm": 7.433046340942383,
      "learning_rate": 3.28065693430657e-05,
      "loss": 1.0133,
      "step": 47110
    },
    {
      "epoch": 343.94160583941607,
      "grad_norm": 10.199735641479492,
      "learning_rate": 3.28029197080292e-05,
      "loss": 1.0957,
      "step": 47120
    },
    {
      "epoch": 344.014598540146,
      "grad_norm": 3.9666080474853516,
      "learning_rate": 3.2799270072992706e-05,
      "loss": 0.7127,
      "step": 47130
    },
    {
      "epoch": 344.0875912408759,
      "grad_norm": 9.873796463012695,
      "learning_rate": 3.2795620437956206e-05,
      "loss": 0.6223,
      "step": 47140
    },
    {
      "epoch": 344.16058394160586,
      "grad_norm": 9.008418083190918,
      "learning_rate": 3.279197080291971e-05,
      "loss": 0.8363,
      "step": 47150
    },
    {
      "epoch": 344.2335766423358,
      "grad_norm": 14.059215545654297,
      "learning_rate": 3.2788321167883214e-05,
      "loss": 1.1637,
      "step": 47160
    },
    {
      "epoch": 344.3065693430657,
      "grad_norm": 11.553581237792969,
      "learning_rate": 3.2784671532846715e-05,
      "loss": 1.3498,
      "step": 47170
    },
    {
      "epoch": 344.3795620437956,
      "grad_norm": 12.652802467346191,
      "learning_rate": 3.278102189781022e-05,
      "loss": 0.5546,
      "step": 47180
    },
    {
      "epoch": 344.45255474452557,
      "grad_norm": 0.2516757845878601,
      "learning_rate": 3.277737226277372e-05,
      "loss": 0.7643,
      "step": 47190
    },
    {
      "epoch": 344.5255474452555,
      "grad_norm": 5.298453330993652,
      "learning_rate": 3.277372262773723e-05,
      "loss": 0.8962,
      "step": 47200
    },
    {
      "epoch": 344.5985401459854,
      "grad_norm": 5.758236408233643,
      "learning_rate": 3.277007299270073e-05,
      "loss": 0.8566,
      "step": 47210
    },
    {
      "epoch": 344.6715328467153,
      "grad_norm": 13.03016471862793,
      "learning_rate": 3.276642335766423e-05,
      "loss": 1.1088,
      "step": 47220
    },
    {
      "epoch": 344.7445255474453,
      "grad_norm": 12.19864559173584,
      "learning_rate": 3.276277372262774e-05,
      "loss": 1.0817,
      "step": 47230
    },
    {
      "epoch": 344.8175182481752,
      "grad_norm": 0.14253297448158264,
      "learning_rate": 3.2759124087591245e-05,
      "loss": 0.7704,
      "step": 47240
    },
    {
      "epoch": 344.8905109489051,
      "grad_norm": 10.629762649536133,
      "learning_rate": 3.2755474452554746e-05,
      "loss": 1.2305,
      "step": 47250
    },
    {
      "epoch": 344.963503649635,
      "grad_norm": 7.867127418518066,
      "learning_rate": 3.275182481751825e-05,
      "loss": 0.8735,
      "step": 47260
    },
    {
      "epoch": 345.036496350365,
      "grad_norm": 11.454730033874512,
      "learning_rate": 3.2748175182481753e-05,
      "loss": 1.4039,
      "step": 47270
    },
    {
      "epoch": 345.1094890510949,
      "grad_norm": 16.27793312072754,
      "learning_rate": 3.274452554744526e-05,
      "loss": 0.7314,
      "step": 47280
    },
    {
      "epoch": 345.1824817518248,
      "grad_norm": 0.2162739783525467,
      "learning_rate": 3.274087591240876e-05,
      "loss": 0.8509,
      "step": 47290
    },
    {
      "epoch": 345.2554744525547,
      "grad_norm": 4.867625713348389,
      "learning_rate": 3.273722627737226e-05,
      "loss": 0.9303,
      "step": 47300
    },
    {
      "epoch": 345.3284671532847,
      "grad_norm": 12.58703327178955,
      "learning_rate": 3.273357664233577e-05,
      "loss": 0.8507,
      "step": 47310
    },
    {
      "epoch": 345.4014598540146,
      "grad_norm": 0.07584887742996216,
      "learning_rate": 3.272992700729927e-05,
      "loss": 0.6644,
      "step": 47320
    },
    {
      "epoch": 345.4744525547445,
      "grad_norm": 0.26162758469581604,
      "learning_rate": 3.272627737226278e-05,
      "loss": 0.7274,
      "step": 47330
    },
    {
      "epoch": 345.54744525547443,
      "grad_norm": 12.172607421875,
      "learning_rate": 3.272262773722628e-05,
      "loss": 1.0478,
      "step": 47340
    },
    {
      "epoch": 345.6204379562044,
      "grad_norm": 9.173101425170898,
      "learning_rate": 3.2718978102189785e-05,
      "loss": 0.916,
      "step": 47350
    },
    {
      "epoch": 345.6934306569343,
      "grad_norm": 8.61671257019043,
      "learning_rate": 3.271532846715329e-05,
      "loss": 0.7854,
      "step": 47360
    },
    {
      "epoch": 345.7664233576642,
      "grad_norm": 11.225318908691406,
      "learning_rate": 3.2711678832116786e-05,
      "loss": 1.1473,
      "step": 47370
    },
    {
      "epoch": 345.83941605839414,
      "grad_norm": 9.215463638305664,
      "learning_rate": 3.270802919708029e-05,
      "loss": 1.4897,
      "step": 47380
    },
    {
      "epoch": 345.9124087591241,
      "grad_norm": 19.452360153198242,
      "learning_rate": 3.270437956204379e-05,
      "loss": 1.174,
      "step": 47390
    },
    {
      "epoch": 345.985401459854,
      "grad_norm": 6.700230121612549,
      "learning_rate": 3.27007299270073e-05,
      "loss": 0.7327,
      "step": 47400
    },
    {
      "epoch": 346.05839416058393,
      "grad_norm": 13.372808456420898,
      "learning_rate": 3.269708029197081e-05,
      "loss": 0.7941,
      "step": 47410
    },
    {
      "epoch": 346.13138686131384,
      "grad_norm": 0.4606628119945526,
      "learning_rate": 3.269343065693431e-05,
      "loss": 0.7399,
      "step": 47420
    },
    {
      "epoch": 346.2043795620438,
      "grad_norm": 7.505642414093018,
      "learning_rate": 3.2689781021897816e-05,
      "loss": 1.2301,
      "step": 47430
    },
    {
      "epoch": 346.2773722627737,
      "grad_norm": 9.590885162353516,
      "learning_rate": 3.2686131386861316e-05,
      "loss": 0.9077,
      "step": 47440
    },
    {
      "epoch": 346.35036496350364,
      "grad_norm": 10.582208633422852,
      "learning_rate": 3.2682481751824817e-05,
      "loss": 0.8083,
      "step": 47450
    },
    {
      "epoch": 346.42335766423355,
      "grad_norm": 11.680459976196289,
      "learning_rate": 3.2678832116788324e-05,
      "loss": 1.1084,
      "step": 47460
    },
    {
      "epoch": 346.4963503649635,
      "grad_norm": 7.737306118011475,
      "learning_rate": 3.2675182481751824e-05,
      "loss": 0.7958,
      "step": 47470
    },
    {
      "epoch": 346.56934306569343,
      "grad_norm": 9.401204109191895,
      "learning_rate": 3.267153284671533e-05,
      "loss": 0.6118,
      "step": 47480
    },
    {
      "epoch": 346.64233576642334,
      "grad_norm": 11.303604125976562,
      "learning_rate": 3.266788321167883e-05,
      "loss": 1.2871,
      "step": 47490
    },
    {
      "epoch": 346.7153284671533,
      "grad_norm": 7.473776340484619,
      "learning_rate": 3.266423357664234e-05,
      "loss": 0.8148,
      "step": 47500
    },
    {
      "epoch": 346.7883211678832,
      "grad_norm": 12.310269355773926,
      "learning_rate": 3.266058394160585e-05,
      "loss": 0.9347,
      "step": 47510
    },
    {
      "epoch": 346.86131386861314,
      "grad_norm": 9.526863098144531,
      "learning_rate": 3.265693430656934e-05,
      "loss": 0.9817,
      "step": 47520
    },
    {
      "epoch": 346.93430656934305,
      "grad_norm": 7.162234306335449,
      "learning_rate": 3.265328467153285e-05,
      "loss": 1.0417,
      "step": 47530
    },
    {
      "epoch": 347.007299270073,
      "grad_norm": 10.532526016235352,
      "learning_rate": 3.264963503649635e-05,
      "loss": 0.9434,
      "step": 47540
    },
    {
      "epoch": 347.08029197080293,
      "grad_norm": 12.972744941711426,
      "learning_rate": 3.2645985401459855e-05,
      "loss": 0.9323,
      "step": 47550
    },
    {
      "epoch": 347.15328467153284,
      "grad_norm": 7.832374095916748,
      "learning_rate": 3.264233576642336e-05,
      "loss": 0.6053,
      "step": 47560
    },
    {
      "epoch": 347.22627737226276,
      "grad_norm": 4.994765758514404,
      "learning_rate": 3.263868613138686e-05,
      "loss": 1.1996,
      "step": 47570
    },
    {
      "epoch": 347.2992700729927,
      "grad_norm": 12.711499214172363,
      "learning_rate": 3.263503649635037e-05,
      "loss": 1.0492,
      "step": 47580
    },
    {
      "epoch": 347.37226277372264,
      "grad_norm": 0.17480702698230743,
      "learning_rate": 3.263138686131387e-05,
      "loss": 1.2577,
      "step": 47590
    },
    {
      "epoch": 347.44525547445255,
      "grad_norm": 5.562989234924316,
      "learning_rate": 3.262773722627737e-05,
      "loss": 0.8693,
      "step": 47600
    },
    {
      "epoch": 347.51824817518246,
      "grad_norm": 3.7087488174438477,
      "learning_rate": 3.262408759124088e-05,
      "loss": 1.0659,
      "step": 47610
    },
    {
      "epoch": 347.59124087591243,
      "grad_norm": 10.006705284118652,
      "learning_rate": 3.262043795620438e-05,
      "loss": 0.8506,
      "step": 47620
    },
    {
      "epoch": 347.66423357664235,
      "grad_norm": 7.6210174560546875,
      "learning_rate": 3.2616788321167886e-05,
      "loss": 1.0286,
      "step": 47630
    },
    {
      "epoch": 347.73722627737226,
      "grad_norm": 3.988323211669922,
      "learning_rate": 3.261313868613139e-05,
      "loss": 1.1317,
      "step": 47640
    },
    {
      "epoch": 347.81021897810217,
      "grad_norm": 0.20585185289382935,
      "learning_rate": 3.2609489051094894e-05,
      "loss": 1.0173,
      "step": 47650
    },
    {
      "epoch": 347.88321167883214,
      "grad_norm": 9.006490707397461,
      "learning_rate": 3.2605839416058395e-05,
      "loss": 0.651,
      "step": 47660
    },
    {
      "epoch": 347.95620437956205,
      "grad_norm": 9.50193977355957,
      "learning_rate": 3.2602189781021895e-05,
      "loss": 0.7134,
      "step": 47670
    },
    {
      "epoch": 348.02919708029196,
      "grad_norm": 8.034126281738281,
      "learning_rate": 3.25985401459854e-05,
      "loss": 0.9186,
      "step": 47680
    },
    {
      "epoch": 348.1021897810219,
      "grad_norm": 11.555071830749512,
      "learning_rate": 3.25948905109489e-05,
      "loss": 1.1651,
      "step": 47690
    },
    {
      "epoch": 348.17518248175185,
      "grad_norm": 22.91459083557129,
      "learning_rate": 3.259124087591241e-05,
      "loss": 0.8208,
      "step": 47700
    },
    {
      "epoch": 348.24817518248176,
      "grad_norm": 10.021567344665527,
      "learning_rate": 3.258759124087592e-05,
      "loss": 0.88,
      "step": 47710
    },
    {
      "epoch": 348.3211678832117,
      "grad_norm": 6.121493816375732,
      "learning_rate": 3.258394160583942e-05,
      "loss": 1.3562,
      "step": 47720
    },
    {
      "epoch": 348.3941605839416,
      "grad_norm": 5.0846991539001465,
      "learning_rate": 3.2580291970802925e-05,
      "loss": 0.7461,
      "step": 47730
    },
    {
      "epoch": 348.46715328467155,
      "grad_norm": 0.18391060829162598,
      "learning_rate": 3.2576642335766426e-05,
      "loss": 0.8033,
      "step": 47740
    },
    {
      "epoch": 348.54014598540147,
      "grad_norm": 12.030807495117188,
      "learning_rate": 3.2572992700729926e-05,
      "loss": 0.6663,
      "step": 47750
    },
    {
      "epoch": 348.6131386861314,
      "grad_norm": 10.831217765808105,
      "learning_rate": 3.2569343065693434e-05,
      "loss": 1.0611,
      "step": 47760
    },
    {
      "epoch": 348.6861313868613,
      "grad_norm": 10.316060066223145,
      "learning_rate": 3.2565693430656934e-05,
      "loss": 1.081,
      "step": 47770
    },
    {
      "epoch": 348.75912408759126,
      "grad_norm": 8.387333869934082,
      "learning_rate": 3.256204379562044e-05,
      "loss": 1.1743,
      "step": 47780
    },
    {
      "epoch": 348.8321167883212,
      "grad_norm": 12.644346237182617,
      "learning_rate": 3.255839416058394e-05,
      "loss": 1.0535,
      "step": 47790
    },
    {
      "epoch": 348.9051094890511,
      "grad_norm": 12.179039001464844,
      "learning_rate": 3.255474452554745e-05,
      "loss": 0.7162,
      "step": 47800
    },
    {
      "epoch": 348.978102189781,
      "grad_norm": 13.002693176269531,
      "learning_rate": 3.255109489051095e-05,
      "loss": 0.9496,
      "step": 47810
    },
    {
      "epoch": 349.05109489051097,
      "grad_norm": 10.350252151489258,
      "learning_rate": 3.254744525547446e-05,
      "loss": 0.8622,
      "step": 47820
    },
    {
      "epoch": 349.1240875912409,
      "grad_norm": 17.87599754333496,
      "learning_rate": 3.254379562043796e-05,
      "loss": 0.9611,
      "step": 47830
    },
    {
      "epoch": 349.1970802919708,
      "grad_norm": 17.12323760986328,
      "learning_rate": 3.254014598540146e-05,
      "loss": 1.0902,
      "step": 47840
    },
    {
      "epoch": 349.2700729927007,
      "grad_norm": 14.960172653198242,
      "learning_rate": 3.2536496350364965e-05,
      "loss": 0.9997,
      "step": 47850
    },
    {
      "epoch": 349.3430656934307,
      "grad_norm": 17.23408317565918,
      "learning_rate": 3.2532846715328466e-05,
      "loss": 1.0203,
      "step": 47860
    },
    {
      "epoch": 349.4160583941606,
      "grad_norm": 8.204887390136719,
      "learning_rate": 3.252919708029197e-05,
      "loss": 0.736,
      "step": 47870
    },
    {
      "epoch": 349.4890510948905,
      "grad_norm": 10.792553901672363,
      "learning_rate": 3.252554744525548e-05,
      "loss": 0.6257,
      "step": 47880
    },
    {
      "epoch": 349.5620437956204,
      "grad_norm": 7.1978888511657715,
      "learning_rate": 3.252189781021898e-05,
      "loss": 0.9491,
      "step": 47890
    },
    {
      "epoch": 349.6350364963504,
      "grad_norm": 0.10994172841310501,
      "learning_rate": 3.251824817518248e-05,
      "loss": 0.8228,
      "step": 47900
    },
    {
      "epoch": 349.7080291970803,
      "grad_norm": 7.742530345916748,
      "learning_rate": 3.251459854014599e-05,
      "loss": 1.3353,
      "step": 47910
    },
    {
      "epoch": 349.7810218978102,
      "grad_norm": 8.235705375671387,
      "learning_rate": 3.251094890510949e-05,
      "loss": 1.157,
      "step": 47920
    },
    {
      "epoch": 349.8540145985401,
      "grad_norm": 0.07466471195220947,
      "learning_rate": 3.2507299270072996e-05,
      "loss": 0.9422,
      "step": 47930
    },
    {
      "epoch": 349.9270072992701,
      "grad_norm": 7.448034286499023,
      "learning_rate": 3.25036496350365e-05,
      "loss": 0.4862,
      "step": 47940
    },
    {
      "epoch": 350.0,
      "grad_norm": 34.56075668334961,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 1.3127,
      "step": 47950
    },
    {
      "epoch": 350.0729927007299,
      "grad_norm": 10.21849536895752,
      "learning_rate": 3.2496350364963504e-05,
      "loss": 0.9082,
      "step": 47960
    },
    {
      "epoch": 350.1459854014599,
      "grad_norm": 0.21828627586364746,
      "learning_rate": 3.249270072992701e-05,
      "loss": 0.3103,
      "step": 47970
    },
    {
      "epoch": 350.2189781021898,
      "grad_norm": 0.12311854958534241,
      "learning_rate": 3.248905109489051e-05,
      "loss": 0.8116,
      "step": 47980
    },
    {
      "epoch": 350.2919708029197,
      "grad_norm": 14.00621509552002,
      "learning_rate": 3.248540145985401e-05,
      "loss": 0.6862,
      "step": 47990
    },
    {
      "epoch": 350.3649635036496,
      "grad_norm": 7.811883926391602,
      "learning_rate": 3.248175182481752e-05,
      "loss": 1.0541,
      "step": 48000
    },
    {
      "epoch": 350.4379562043796,
      "grad_norm": 10.352212905883789,
      "learning_rate": 3.247810218978102e-05,
      "loss": 1.4659,
      "step": 48010
    },
    {
      "epoch": 350.5109489051095,
      "grad_norm": 8.015538215637207,
      "learning_rate": 3.247445255474453e-05,
      "loss": 1.2005,
      "step": 48020
    },
    {
      "epoch": 350.5839416058394,
      "grad_norm": 19.471954345703125,
      "learning_rate": 3.2470802919708035e-05,
      "loss": 1.1708,
      "step": 48030
    },
    {
      "epoch": 350.6569343065693,
      "grad_norm": 5.5812602043151855,
      "learning_rate": 3.2467153284671535e-05,
      "loss": 0.6586,
      "step": 48040
    },
    {
      "epoch": 350.7299270072993,
      "grad_norm": 7.064525127410889,
      "learning_rate": 3.2463503649635036e-05,
      "loss": 1.397,
      "step": 48050
    },
    {
      "epoch": 350.8029197080292,
      "grad_norm": 0.142678901553154,
      "learning_rate": 3.2459854014598536e-05,
      "loss": 0.8129,
      "step": 48060
    },
    {
      "epoch": 350.8759124087591,
      "grad_norm": 4.765305995941162,
      "learning_rate": 3.2456204379562044e-05,
      "loss": 0.5865,
      "step": 48070
    },
    {
      "epoch": 350.94890510948903,
      "grad_norm": 14.245543479919434,
      "learning_rate": 3.245255474452555e-05,
      "loss": 1.0282,
      "step": 48080
    },
    {
      "epoch": 351.021897810219,
      "grad_norm": 9.643077850341797,
      "learning_rate": 3.244890510948905e-05,
      "loss": 1.1241,
      "step": 48090
    },
    {
      "epoch": 351.0948905109489,
      "grad_norm": 5.639673233032227,
      "learning_rate": 3.244525547445256e-05,
      "loss": 0.911,
      "step": 48100
    },
    {
      "epoch": 351.1678832116788,
      "grad_norm": 3.5613460540771484,
      "learning_rate": 3.244160583941606e-05,
      "loss": 1.043,
      "step": 48110
    },
    {
      "epoch": 351.24087591240874,
      "grad_norm": 7.290017604827881,
      "learning_rate": 3.2437956204379567e-05,
      "loss": 1.1209,
      "step": 48120
    },
    {
      "epoch": 351.3138686131387,
      "grad_norm": 19.18227767944336,
      "learning_rate": 3.243430656934307e-05,
      "loss": 0.8113,
      "step": 48130
    },
    {
      "epoch": 351.3868613138686,
      "grad_norm": 6.707211971282959,
      "learning_rate": 3.243065693430657e-05,
      "loss": 0.7495,
      "step": 48140
    },
    {
      "epoch": 351.45985401459853,
      "grad_norm": 12.216535568237305,
      "learning_rate": 3.2427007299270075e-05,
      "loss": 0.9669,
      "step": 48150
    },
    {
      "epoch": 351.53284671532845,
      "grad_norm": 9.282815933227539,
      "learning_rate": 3.2423357664233575e-05,
      "loss": 0.8949,
      "step": 48160
    },
    {
      "epoch": 351.6058394160584,
      "grad_norm": 15.903878211975098,
      "learning_rate": 3.241970802919708e-05,
      "loss": 1.225,
      "step": 48170
    },
    {
      "epoch": 351.6788321167883,
      "grad_norm": 0.339201956987381,
      "learning_rate": 3.241605839416059e-05,
      "loss": 1.1884,
      "step": 48180
    },
    {
      "epoch": 351.75182481751824,
      "grad_norm": 7.897024154663086,
      "learning_rate": 3.241240875912409e-05,
      "loss": 1.3272,
      "step": 48190
    },
    {
      "epoch": 351.82481751824815,
      "grad_norm": 5.687875747680664,
      "learning_rate": 3.24087591240876e-05,
      "loss": 1.0062,
      "step": 48200
    },
    {
      "epoch": 351.8978102189781,
      "grad_norm": 0.18921059370040894,
      "learning_rate": 3.240510948905109e-05,
      "loss": 0.7344,
      "step": 48210
    },
    {
      "epoch": 351.97080291970804,
      "grad_norm": 5.48629093170166,
      "learning_rate": 3.24014598540146e-05,
      "loss": 0.6622,
      "step": 48220
    },
    {
      "epoch": 352.04379562043795,
      "grad_norm": 4.4129486083984375,
      "learning_rate": 3.2397810218978106e-05,
      "loss": 0.8885,
      "step": 48230
    },
    {
      "epoch": 352.11678832116786,
      "grad_norm": 8.664454460144043,
      "learning_rate": 3.2394160583941606e-05,
      "loss": 1.0484,
      "step": 48240
    },
    {
      "epoch": 352.18978102189783,
      "grad_norm": 9.175630569458008,
      "learning_rate": 3.2390510948905114e-05,
      "loss": 0.815,
      "step": 48250
    },
    {
      "epoch": 352.26277372262774,
      "grad_norm": 0.12966424226760864,
      "learning_rate": 3.2386861313868614e-05,
      "loss": 0.8349,
      "step": 48260
    },
    {
      "epoch": 352.33576642335765,
      "grad_norm": 7.980519771575928,
      "learning_rate": 3.238321167883212e-05,
      "loss": 0.84,
      "step": 48270
    },
    {
      "epoch": 352.40875912408757,
      "grad_norm": 0.09695129096508026,
      "learning_rate": 3.237956204379562e-05,
      "loss": 0.6917,
      "step": 48280
    },
    {
      "epoch": 352.48175182481754,
      "grad_norm": 15.19487476348877,
      "learning_rate": 3.237591240875912e-05,
      "loss": 1.0543,
      "step": 48290
    },
    {
      "epoch": 352.55474452554745,
      "grad_norm": 7.445407867431641,
      "learning_rate": 3.237226277372263e-05,
      "loss": 0.8936,
      "step": 48300
    },
    {
      "epoch": 352.62773722627736,
      "grad_norm": 10.813679695129395,
      "learning_rate": 3.236861313868613e-05,
      "loss": 0.9358,
      "step": 48310
    },
    {
      "epoch": 352.7007299270073,
      "grad_norm": 0.19996130466461182,
      "learning_rate": 3.236496350364964e-05,
      "loss": 1.1414,
      "step": 48320
    },
    {
      "epoch": 352.77372262773724,
      "grad_norm": 17.03522300720215,
      "learning_rate": 3.236131386861314e-05,
      "loss": 1.0599,
      "step": 48330
    },
    {
      "epoch": 352.84671532846716,
      "grad_norm": 14.20970630645752,
      "learning_rate": 3.2357664233576645e-05,
      "loss": 1.1788,
      "step": 48340
    },
    {
      "epoch": 352.91970802919707,
      "grad_norm": 6.613307952880859,
      "learning_rate": 3.235401459854015e-05,
      "loss": 1.063,
      "step": 48350
    },
    {
      "epoch": 352.992700729927,
      "grad_norm": 18.966835021972656,
      "learning_rate": 3.2350364963503646e-05,
      "loss": 0.9295,
      "step": 48360
    },
    {
      "epoch": 353.06569343065695,
      "grad_norm": 12.31647777557373,
      "learning_rate": 3.2346715328467153e-05,
      "loss": 1.2319,
      "step": 48370
    },
    {
      "epoch": 353.13868613138686,
      "grad_norm": 13.636984825134277,
      "learning_rate": 3.234306569343066e-05,
      "loss": 1.0274,
      "step": 48380
    },
    {
      "epoch": 353.2116788321168,
      "grad_norm": 10.883465766906738,
      "learning_rate": 3.233941605839416e-05,
      "loss": 0.9396,
      "step": 48390
    },
    {
      "epoch": 353.2846715328467,
      "grad_norm": 10.046541213989258,
      "learning_rate": 3.233576642335767e-05,
      "loss": 0.6711,
      "step": 48400
    },
    {
      "epoch": 353.35766423357666,
      "grad_norm": 15.984220504760742,
      "learning_rate": 3.233211678832117e-05,
      "loss": 1.121,
      "step": 48410
    },
    {
      "epoch": 353.43065693430657,
      "grad_norm": 0.1555425077676773,
      "learning_rate": 3.2328467153284676e-05,
      "loss": 0.2907,
      "step": 48420
    },
    {
      "epoch": 353.5036496350365,
      "grad_norm": 5.192658424377441,
      "learning_rate": 3.232481751824818e-05,
      "loss": 0.7857,
      "step": 48430
    },
    {
      "epoch": 353.57664233576645,
      "grad_norm": 13.82968521118164,
      "learning_rate": 3.232116788321168e-05,
      "loss": 1.1158,
      "step": 48440
    },
    {
      "epoch": 353.64963503649636,
      "grad_norm": 11.49158000946045,
      "learning_rate": 3.2317518248175184e-05,
      "loss": 0.9986,
      "step": 48450
    },
    {
      "epoch": 353.7226277372263,
      "grad_norm": 8.49223804473877,
      "learning_rate": 3.2313868613138685e-05,
      "loss": 1.0233,
      "step": 48460
    },
    {
      "epoch": 353.7956204379562,
      "grad_norm": 15.472003936767578,
      "learning_rate": 3.231021897810219e-05,
      "loss": 1.0352,
      "step": 48470
    },
    {
      "epoch": 353.86861313868616,
      "grad_norm": 16.417940139770508,
      "learning_rate": 3.230656934306569e-05,
      "loss": 0.9211,
      "step": 48480
    },
    {
      "epoch": 353.94160583941607,
      "grad_norm": 5.691848278045654,
      "learning_rate": 3.23029197080292e-05,
      "loss": 1.1885,
      "step": 48490
    },
    {
      "epoch": 354.014598540146,
      "grad_norm": 8.440202713012695,
      "learning_rate": 3.229927007299271e-05,
      "loss": 1.171,
      "step": 48500
    },
    {
      "epoch": 354.0875912408759,
      "grad_norm": 20.070735931396484,
      "learning_rate": 3.22956204379562e-05,
      "loss": 1.1207,
      "step": 48510
    },
    {
      "epoch": 354.16058394160586,
      "grad_norm": 0.12159959226846695,
      "learning_rate": 3.229197080291971e-05,
      "loss": 1.4435,
      "step": 48520
    },
    {
      "epoch": 354.2335766423358,
      "grad_norm": 11.924103736877441,
      "learning_rate": 3.228832116788321e-05,
      "loss": 1.389,
      "step": 48530
    },
    {
      "epoch": 354.3065693430657,
      "grad_norm": 5.0621490478515625,
      "learning_rate": 3.2284671532846716e-05,
      "loss": 0.6525,
      "step": 48540
    },
    {
      "epoch": 354.3795620437956,
      "grad_norm": 7.749653339385986,
      "learning_rate": 3.228102189781022e-05,
      "loss": 1.0098,
      "step": 48550
    },
    {
      "epoch": 354.45255474452557,
      "grad_norm": 0.17176784574985504,
      "learning_rate": 3.2277372262773724e-05,
      "loss": 0.6876,
      "step": 48560
    },
    {
      "epoch": 354.5255474452555,
      "grad_norm": 7.448625087738037,
      "learning_rate": 3.227372262773723e-05,
      "loss": 1.0778,
      "step": 48570
    },
    {
      "epoch": 354.5985401459854,
      "grad_norm": 15.105411529541016,
      "learning_rate": 3.227007299270073e-05,
      "loss": 0.7575,
      "step": 48580
    },
    {
      "epoch": 354.6715328467153,
      "grad_norm": 9.454968452453613,
      "learning_rate": 3.226642335766423e-05,
      "loss": 0.6536,
      "step": 48590
    },
    {
      "epoch": 354.7445255474453,
      "grad_norm": 7.68449592590332,
      "learning_rate": 3.226277372262774e-05,
      "loss": 0.885,
      "step": 48600
    },
    {
      "epoch": 354.8175182481752,
      "grad_norm": 5.050325393676758,
      "learning_rate": 3.225912408759124e-05,
      "loss": 0.6301,
      "step": 48610
    },
    {
      "epoch": 354.8905109489051,
      "grad_norm": 16.865209579467773,
      "learning_rate": 3.225547445255475e-05,
      "loss": 1.1216,
      "step": 48620
    },
    {
      "epoch": 354.963503649635,
      "grad_norm": 0.07136044651269913,
      "learning_rate": 3.225182481751825e-05,
      "loss": 0.7856,
      "step": 48630
    },
    {
      "epoch": 355.036496350365,
      "grad_norm": 10.544360160827637,
      "learning_rate": 3.2248175182481755e-05,
      "loss": 0.9234,
      "step": 48640
    },
    {
      "epoch": 355.1094890510949,
      "grad_norm": 0.1266213208436966,
      "learning_rate": 3.224452554744526e-05,
      "loss": 0.7439,
      "step": 48650
    },
    {
      "epoch": 355.1824817518248,
      "grad_norm": 8.35484504699707,
      "learning_rate": 3.224087591240876e-05,
      "loss": 1.3596,
      "step": 48660
    },
    {
      "epoch": 355.2554744525547,
      "grad_norm": 6.978023052215576,
      "learning_rate": 3.223722627737226e-05,
      "loss": 1.0474,
      "step": 48670
    },
    {
      "epoch": 355.3284671532847,
      "grad_norm": 13.488536834716797,
      "learning_rate": 3.2233576642335764e-05,
      "loss": 0.9201,
      "step": 48680
    },
    {
      "epoch": 355.4014598540146,
      "grad_norm": 0.3063713312149048,
      "learning_rate": 3.222992700729927e-05,
      "loss": 1.0364,
      "step": 48690
    },
    {
      "epoch": 355.4744525547445,
      "grad_norm": 16.70117950439453,
      "learning_rate": 3.222627737226278e-05,
      "loss": 0.7912,
      "step": 48700
    },
    {
      "epoch": 355.54744525547443,
      "grad_norm": 14.132659912109375,
      "learning_rate": 3.222262773722628e-05,
      "loss": 0.9694,
      "step": 48710
    },
    {
      "epoch": 355.6204379562044,
      "grad_norm": 7.3337531089782715,
      "learning_rate": 3.2218978102189786e-05,
      "loss": 1.0354,
      "step": 48720
    },
    {
      "epoch": 355.6934306569343,
      "grad_norm": 6.250690460205078,
      "learning_rate": 3.2215328467153286e-05,
      "loss": 0.9135,
      "step": 48730
    },
    {
      "epoch": 355.7664233576642,
      "grad_norm": 0.10320014506578445,
      "learning_rate": 3.221167883211679e-05,
      "loss": 0.5461,
      "step": 48740
    },
    {
      "epoch": 355.83941605839414,
      "grad_norm": 0.15364181995391846,
      "learning_rate": 3.2208029197080294e-05,
      "loss": 0.9758,
      "step": 48750
    },
    {
      "epoch": 355.9124087591241,
      "grad_norm": 8.1038818359375,
      "learning_rate": 3.2204379562043795e-05,
      "loss": 1.1043,
      "step": 48760
    },
    {
      "epoch": 355.985401459854,
      "grad_norm": 10.085783004760742,
      "learning_rate": 3.22007299270073e-05,
      "loss": 0.8576,
      "step": 48770
    },
    {
      "epoch": 356.05839416058393,
      "grad_norm": 7.367708683013916,
      "learning_rate": 3.21970802919708e-05,
      "loss": 0.8149,
      "step": 48780
    },
    {
      "epoch": 356.13138686131384,
      "grad_norm": 5.364766597747803,
      "learning_rate": 3.219343065693431e-05,
      "loss": 0.7856,
      "step": 48790
    },
    {
      "epoch": 356.2043795620438,
      "grad_norm": 8.624406814575195,
      "learning_rate": 3.218978102189781e-05,
      "loss": 1.0356,
      "step": 48800
    },
    {
      "epoch": 356.2773722627737,
      "grad_norm": 13.79465103149414,
      "learning_rate": 3.218613138686132e-05,
      "loss": 0.6681,
      "step": 48810
    },
    {
      "epoch": 356.35036496350364,
      "grad_norm": 14.868948936462402,
      "learning_rate": 3.218248175182482e-05,
      "loss": 0.9483,
      "step": 48820
    },
    {
      "epoch": 356.42335766423355,
      "grad_norm": 7.934999942779541,
      "learning_rate": 3.217883211678832e-05,
      "loss": 0.9817,
      "step": 48830
    },
    {
      "epoch": 356.4963503649635,
      "grad_norm": 11.95181941986084,
      "learning_rate": 3.2175182481751826e-05,
      "loss": 0.9108,
      "step": 48840
    },
    {
      "epoch": 356.56934306569343,
      "grad_norm": 10.680886268615723,
      "learning_rate": 3.217153284671533e-05,
      "loss": 1.0571,
      "step": 48850
    },
    {
      "epoch": 356.64233576642334,
      "grad_norm": 5.09337854385376,
      "learning_rate": 3.2167883211678834e-05,
      "loss": 1.1744,
      "step": 48860
    },
    {
      "epoch": 356.7153284671533,
      "grad_norm": 13.960529327392578,
      "learning_rate": 3.216423357664234e-05,
      "loss": 0.8508,
      "step": 48870
    },
    {
      "epoch": 356.7883211678832,
      "grad_norm": 13.03183364868164,
      "learning_rate": 3.216058394160584e-05,
      "loss": 0.8949,
      "step": 48880
    },
    {
      "epoch": 356.86131386861314,
      "grad_norm": 9.563532829284668,
      "learning_rate": 3.215693430656935e-05,
      "loss": 0.9492,
      "step": 48890
    },
    {
      "epoch": 356.93430656934305,
      "grad_norm": 7.700943470001221,
      "learning_rate": 3.215328467153285e-05,
      "loss": 1.1781,
      "step": 48900
    },
    {
      "epoch": 357.007299270073,
      "grad_norm": 6.641282081604004,
      "learning_rate": 3.214963503649635e-05,
      "loss": 0.797,
      "step": 48910
    },
    {
      "epoch": 357.08029197080293,
      "grad_norm": 6.641536712646484,
      "learning_rate": 3.214598540145986e-05,
      "loss": 0.8039,
      "step": 48920
    },
    {
      "epoch": 357.15328467153284,
      "grad_norm": 5.168832302093506,
      "learning_rate": 3.214233576642336e-05,
      "loss": 0.9091,
      "step": 48930
    },
    {
      "epoch": 357.22627737226276,
      "grad_norm": 0.2924782335758209,
      "learning_rate": 3.2138686131386865e-05,
      "loss": 0.9459,
      "step": 48940
    },
    {
      "epoch": 357.2992700729927,
      "grad_norm": 8.371018409729004,
      "learning_rate": 3.2135036496350365e-05,
      "loss": 1.1474,
      "step": 48950
    },
    {
      "epoch": 357.37226277372264,
      "grad_norm": 6.0726118087768555,
      "learning_rate": 3.213138686131387e-05,
      "loss": 0.6587,
      "step": 48960
    },
    {
      "epoch": 357.44525547445255,
      "grad_norm": 7.529797554016113,
      "learning_rate": 3.212773722627737e-05,
      "loss": 1.0836,
      "step": 48970
    },
    {
      "epoch": 357.51824817518246,
      "grad_norm": 14.228494644165039,
      "learning_rate": 3.212408759124087e-05,
      "loss": 1.1117,
      "step": 48980
    },
    {
      "epoch": 357.59124087591243,
      "grad_norm": 0.1666528582572937,
      "learning_rate": 3.212043795620438e-05,
      "loss": 0.6905,
      "step": 48990
    },
    {
      "epoch": 357.66423357664235,
      "grad_norm": 8.777073860168457,
      "learning_rate": 3.211678832116788e-05,
      "loss": 0.7992,
      "step": 49000
    },
    {
      "epoch": 357.73722627737226,
      "grad_norm": 0.0730709359049797,
      "learning_rate": 3.211313868613139e-05,
      "loss": 0.9442,
      "step": 49010
    },
    {
      "epoch": 357.81021897810217,
      "grad_norm": 7.552303791046143,
      "learning_rate": 3.2109489051094896e-05,
      "loss": 1.3192,
      "step": 49020
    },
    {
      "epoch": 357.88321167883214,
      "grad_norm": 13.627802848815918,
      "learning_rate": 3.2105839416058396e-05,
      "loss": 0.949,
      "step": 49030
    },
    {
      "epoch": 357.95620437956205,
      "grad_norm": 10.332969665527344,
      "learning_rate": 3.2102189781021903e-05,
      "loss": 0.7372,
      "step": 49040
    },
    {
      "epoch": 358.02919708029196,
      "grad_norm": 11.041092872619629,
      "learning_rate": 3.2098540145985404e-05,
      "loss": 0.6794,
      "step": 49050
    },
    {
      "epoch": 358.1021897810219,
      "grad_norm": 7.684726238250732,
      "learning_rate": 3.2094890510948904e-05,
      "loss": 1.0334,
      "step": 49060
    },
    {
      "epoch": 358.17518248175185,
      "grad_norm": 0.2540247142314911,
      "learning_rate": 3.209124087591241e-05,
      "loss": 0.8383,
      "step": 49070
    },
    {
      "epoch": 358.24817518248176,
      "grad_norm": 10.845361709594727,
      "learning_rate": 3.208759124087591e-05,
      "loss": 0.6685,
      "step": 49080
    },
    {
      "epoch": 358.3211678832117,
      "grad_norm": 9.963532447814941,
      "learning_rate": 3.208394160583942e-05,
      "loss": 0.9906,
      "step": 49090
    },
    {
      "epoch": 358.3941605839416,
      "grad_norm": 12.29944896697998,
      "learning_rate": 3.208029197080292e-05,
      "loss": 0.977,
      "step": 49100
    },
    {
      "epoch": 358.46715328467155,
      "grad_norm": 6.18002462387085,
      "learning_rate": 3.207664233576643e-05,
      "loss": 0.8943,
      "step": 49110
    },
    {
      "epoch": 358.54014598540147,
      "grad_norm": 8.294288635253906,
      "learning_rate": 3.207299270072993e-05,
      "loss": 0.6007,
      "step": 49120
    },
    {
      "epoch": 358.6131386861314,
      "grad_norm": 15.6871919631958,
      "learning_rate": 3.206934306569343e-05,
      "loss": 1.0399,
      "step": 49130
    },
    {
      "epoch": 358.6861313868613,
      "grad_norm": 11.594470977783203,
      "learning_rate": 3.2065693430656935e-05,
      "loss": 1.3126,
      "step": 49140
    },
    {
      "epoch": 358.75912408759126,
      "grad_norm": 5.765832901000977,
      "learning_rate": 3.2062043795620436e-05,
      "loss": 0.943,
      "step": 49150
    },
    {
      "epoch": 358.8321167883212,
      "grad_norm": 12.37214469909668,
      "learning_rate": 3.205839416058394e-05,
      "loss": 0.9982,
      "step": 49160
    },
    {
      "epoch": 358.9051094890511,
      "grad_norm": 6.71587610244751,
      "learning_rate": 3.205474452554745e-05,
      "loss": 0.6384,
      "step": 49170
    },
    {
      "epoch": 358.978102189781,
      "grad_norm": 12.950060844421387,
      "learning_rate": 3.205109489051095e-05,
      "loss": 1.3496,
      "step": 49180
    },
    {
      "epoch": 359.05109489051097,
      "grad_norm": 13.39363956451416,
      "learning_rate": 3.204744525547446e-05,
      "loss": 0.9993,
      "step": 49190
    },
    {
      "epoch": 359.1240875912409,
      "grad_norm": 16.17625617980957,
      "learning_rate": 3.204379562043795e-05,
      "loss": 0.5369,
      "step": 49200
    },
    {
      "epoch": 359.1970802919708,
      "grad_norm": 12.141400337219238,
      "learning_rate": 3.204014598540146e-05,
      "loss": 1.1124,
      "step": 49210
    },
    {
      "epoch": 359.2700729927007,
      "grad_norm": 8.605594635009766,
      "learning_rate": 3.2036496350364967e-05,
      "loss": 1.0761,
      "step": 49220
    },
    {
      "epoch": 359.3430656934307,
      "grad_norm": 16.888141632080078,
      "learning_rate": 3.203284671532847e-05,
      "loss": 1.0327,
      "step": 49230
    },
    {
      "epoch": 359.4160583941606,
      "grad_norm": 9.86422061920166,
      "learning_rate": 3.2029197080291974e-05,
      "loss": 0.8595,
      "step": 49240
    },
    {
      "epoch": 359.4890510948905,
      "grad_norm": 10.724428176879883,
      "learning_rate": 3.2025547445255475e-05,
      "loss": 0.8815,
      "step": 49250
    },
    {
      "epoch": 359.5620437956204,
      "grad_norm": 11.060537338256836,
      "learning_rate": 3.202189781021898e-05,
      "loss": 0.8098,
      "step": 49260
    },
    {
      "epoch": 359.6350364963504,
      "grad_norm": 10.379388809204102,
      "learning_rate": 3.201824817518248e-05,
      "loss": 1.0514,
      "step": 49270
    },
    {
      "epoch": 359.7080291970803,
      "grad_norm": 0.13642388582229614,
      "learning_rate": 3.201459854014598e-05,
      "loss": 1.051,
      "step": 49280
    },
    {
      "epoch": 359.7810218978102,
      "grad_norm": 8.81495475769043,
      "learning_rate": 3.201094890510949e-05,
      "loss": 0.9249,
      "step": 49290
    },
    {
      "epoch": 359.8540145985401,
      "grad_norm": 9.761150360107422,
      "learning_rate": 3.200729927007299e-05,
      "loss": 0.8902,
      "step": 49300
    },
    {
      "epoch": 359.9270072992701,
      "grad_norm": 17.961055755615234,
      "learning_rate": 3.20036496350365e-05,
      "loss": 1.0402,
      "step": 49310
    },
    {
      "epoch": 360.0,
      "grad_norm": 20.54482078552246,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.8422,
      "step": 49320
    },
    {
      "epoch": 360.0729927007299,
      "grad_norm": 9.749378204345703,
      "learning_rate": 3.1996350364963506e-05,
      "loss": 0.74,
      "step": 49330
    },
    {
      "epoch": 360.1459854014599,
      "grad_norm": 11.996052742004395,
      "learning_rate": 3.199270072992701e-05,
      "loss": 0.5522,
      "step": 49340
    },
    {
      "epoch": 360.2189781021898,
      "grad_norm": 0.12043923139572144,
      "learning_rate": 3.198905109489051e-05,
      "loss": 1.2436,
      "step": 49350
    },
    {
      "epoch": 360.2919708029197,
      "grad_norm": 18.8351993560791,
      "learning_rate": 3.1985401459854014e-05,
      "loss": 0.7035,
      "step": 49360
    },
    {
      "epoch": 360.3649635036496,
      "grad_norm": 14.176340103149414,
      "learning_rate": 3.198175182481752e-05,
      "loss": 1.1966,
      "step": 49370
    },
    {
      "epoch": 360.4379562043796,
      "grad_norm": 12.86313247680664,
      "learning_rate": 3.197810218978102e-05,
      "loss": 1.0242,
      "step": 49380
    },
    {
      "epoch": 360.5109489051095,
      "grad_norm": 0.14420080184936523,
      "learning_rate": 3.197445255474453e-05,
      "loss": 0.8964,
      "step": 49390
    },
    {
      "epoch": 360.5839416058394,
      "grad_norm": 12.063698768615723,
      "learning_rate": 3.197080291970803e-05,
      "loss": 1.4466,
      "step": 49400
    },
    {
      "epoch": 360.6569343065693,
      "grad_norm": 15.246335983276367,
      "learning_rate": 3.196715328467154e-05,
      "loss": 0.9253,
      "step": 49410
    },
    {
      "epoch": 360.7299270072993,
      "grad_norm": 10.043696403503418,
      "learning_rate": 3.196350364963504e-05,
      "loss": 0.832,
      "step": 49420
    },
    {
      "epoch": 360.8029197080292,
      "grad_norm": 0.24596360325813293,
      "learning_rate": 3.195985401459854e-05,
      "loss": 0.8188,
      "step": 49430
    },
    {
      "epoch": 360.8759124087591,
      "grad_norm": 6.235339164733887,
      "learning_rate": 3.1956204379562045e-05,
      "loss": 0.9447,
      "step": 49440
    },
    {
      "epoch": 360.94890510948903,
      "grad_norm": 12.850851058959961,
      "learning_rate": 3.1952554744525546e-05,
      "loss": 1.0395,
      "step": 49450
    },
    {
      "epoch": 361.021897810219,
      "grad_norm": 18.203426361083984,
      "learning_rate": 3.194890510948905e-05,
      "loss": 1.0115,
      "step": 49460
    },
    {
      "epoch": 361.0948905109489,
      "grad_norm": 13.075557708740234,
      "learning_rate": 3.1945255474452553e-05,
      "loss": 0.7362,
      "step": 49470
    },
    {
      "epoch": 361.1678832116788,
      "grad_norm": 20.839599609375,
      "learning_rate": 3.194160583941606e-05,
      "loss": 1.4344,
      "step": 49480
    },
    {
      "epoch": 361.24087591240874,
      "grad_norm": 10.232637405395508,
      "learning_rate": 3.193795620437957e-05,
      "loss": 1.1066,
      "step": 49490
    },
    {
      "epoch": 361.3138686131387,
      "grad_norm": 7.648991584777832,
      "learning_rate": 3.193430656934307e-05,
      "loss": 0.6306,
      "step": 49500
    },
    {
      "epoch": 361.3868613138686,
      "grad_norm": 9.789355278015137,
      "learning_rate": 3.193065693430657e-05,
      "loss": 0.7639,
      "step": 49510
    },
    {
      "epoch": 361.45985401459853,
      "grad_norm": 6.9652509689331055,
      "learning_rate": 3.1927007299270076e-05,
      "loss": 0.645,
      "step": 49520
    },
    {
      "epoch": 361.53284671532845,
      "grad_norm": 7.794588565826416,
      "learning_rate": 3.192335766423358e-05,
      "loss": 1.0242,
      "step": 49530
    },
    {
      "epoch": 361.6058394160584,
      "grad_norm": 5.488055229187012,
      "learning_rate": 3.1919708029197084e-05,
      "loss": 0.8409,
      "step": 49540
    },
    {
      "epoch": 361.6788321167883,
      "grad_norm": 16.06083106994629,
      "learning_rate": 3.1916058394160584e-05,
      "loss": 1.0637,
      "step": 49550
    },
    {
      "epoch": 361.75182481751824,
      "grad_norm": 10.504034996032715,
      "learning_rate": 3.191240875912409e-05,
      "loss": 1.0011,
      "step": 49560
    },
    {
      "epoch": 361.82481751824815,
      "grad_norm": 13.246685028076172,
      "learning_rate": 3.190875912408759e-05,
      "loss": 0.5941,
      "step": 49570
    },
    {
      "epoch": 361.8978102189781,
      "grad_norm": 8.663016319274902,
      "learning_rate": 3.190510948905109e-05,
      "loss": 1.0452,
      "step": 49580
    },
    {
      "epoch": 361.97080291970804,
      "grad_norm": 9.516059875488281,
      "learning_rate": 3.19014598540146e-05,
      "loss": 0.8142,
      "step": 49590
    },
    {
      "epoch": 362.04379562043795,
      "grad_norm": 0.2847290337085724,
      "learning_rate": 3.18978102189781e-05,
      "loss": 0.5368,
      "step": 49600
    },
    {
      "epoch": 362.11678832116786,
      "grad_norm": 7.895339012145996,
      "learning_rate": 3.189416058394161e-05,
      "loss": 0.8145,
      "step": 49610
    },
    {
      "epoch": 362.18978102189783,
      "grad_norm": 17.22361183166504,
      "learning_rate": 3.189051094890511e-05,
      "loss": 0.8407,
      "step": 49620
    },
    {
      "epoch": 362.26277372262774,
      "grad_norm": 7.438689231872559,
      "learning_rate": 3.1886861313868616e-05,
      "loss": 0.4291,
      "step": 49630
    },
    {
      "epoch": 362.33576642335765,
      "grad_norm": 7.979064464569092,
      "learning_rate": 3.188321167883212e-05,
      "loss": 1.248,
      "step": 49640
    },
    {
      "epoch": 362.40875912408757,
      "grad_norm": 12.93465805053711,
      "learning_rate": 3.187956204379562e-05,
      "loss": 1.1495,
      "step": 49650
    },
    {
      "epoch": 362.48175182481754,
      "grad_norm": 8.15575122833252,
      "learning_rate": 3.1875912408759124e-05,
      "loss": 0.6517,
      "step": 49660
    },
    {
      "epoch": 362.55474452554745,
      "grad_norm": 0.10551980882883072,
      "learning_rate": 3.1872262773722624e-05,
      "loss": 0.6802,
      "step": 49670
    },
    {
      "epoch": 362.62773722627736,
      "grad_norm": 7.252989292144775,
      "learning_rate": 3.186861313868613e-05,
      "loss": 1.1241,
      "step": 49680
    },
    {
      "epoch": 362.7007299270073,
      "grad_norm": 10.099325180053711,
      "learning_rate": 3.186496350364964e-05,
      "loss": 1.0662,
      "step": 49690
    },
    {
      "epoch": 362.77372262773724,
      "grad_norm": 8.055387496948242,
      "learning_rate": 3.186131386861314e-05,
      "loss": 1.1131,
      "step": 49700
    },
    {
      "epoch": 362.84671532846716,
      "grad_norm": 10.282883644104004,
      "learning_rate": 3.1857664233576647e-05,
      "loss": 0.8668,
      "step": 49710
    },
    {
      "epoch": 362.91970802919707,
      "grad_norm": 4.778285503387451,
      "learning_rate": 3.185401459854015e-05,
      "loss": 1.3305,
      "step": 49720
    },
    {
      "epoch": 362.992700729927,
      "grad_norm": 13.326870918273926,
      "learning_rate": 3.1850364963503654e-05,
      "loss": 1.1401,
      "step": 49730
    },
    {
      "epoch": 363.06569343065695,
      "grad_norm": 5.272108554840088,
      "learning_rate": 3.1846715328467155e-05,
      "loss": 0.9694,
      "step": 49740
    },
    {
      "epoch": 363.13868613138686,
      "grad_norm": 0.10725255310535431,
      "learning_rate": 3.1843065693430655e-05,
      "loss": 0.6831,
      "step": 49750
    },
    {
      "epoch": 363.2116788321168,
      "grad_norm": 10.785645484924316,
      "learning_rate": 3.183941605839416e-05,
      "loss": 0.9258,
      "step": 49760
    },
    {
      "epoch": 363.2846715328467,
      "grad_norm": 17.235193252563477,
      "learning_rate": 3.183576642335766e-05,
      "loss": 0.9757,
      "step": 49770
    },
    {
      "epoch": 363.35766423357666,
      "grad_norm": 6.825772285461426,
      "learning_rate": 3.183211678832117e-05,
      "loss": 1.1238,
      "step": 49780
    },
    {
      "epoch": 363.43065693430657,
      "grad_norm": 13.979215621948242,
      "learning_rate": 3.182846715328468e-05,
      "loss": 0.9323,
      "step": 49790
    },
    {
      "epoch": 363.5036496350365,
      "grad_norm": 9.85527515411377,
      "learning_rate": 3.182481751824818e-05,
      "loss": 1.1068,
      "step": 49800
    },
    {
      "epoch": 363.57664233576645,
      "grad_norm": 16.077075958251953,
      "learning_rate": 3.182116788321168e-05,
      "loss": 0.831,
      "step": 49810
    },
    {
      "epoch": 363.64963503649636,
      "grad_norm": 0.06621237844228745,
      "learning_rate": 3.181751824817518e-05,
      "loss": 0.7161,
      "step": 49820
    },
    {
      "epoch": 363.7226277372263,
      "grad_norm": 13.905400276184082,
      "learning_rate": 3.1813868613138686e-05,
      "loss": 1.5123,
      "step": 49830
    },
    {
      "epoch": 363.7956204379562,
      "grad_norm": 10.396040916442871,
      "learning_rate": 3.1810218978102194e-05,
      "loss": 0.6975,
      "step": 49840
    },
    {
      "epoch": 363.86861313868616,
      "grad_norm": 5.974424362182617,
      "learning_rate": 3.1806569343065694e-05,
      "loss": 0.7831,
      "step": 49850
    },
    {
      "epoch": 363.94160583941607,
      "grad_norm": 0.3964942693710327,
      "learning_rate": 3.18029197080292e-05,
      "loss": 0.6774,
      "step": 49860
    },
    {
      "epoch": 364.014598540146,
      "grad_norm": 5.777148723602295,
      "learning_rate": 3.17992700729927e-05,
      "loss": 1.2603,
      "step": 49870
    },
    {
      "epoch": 364.0875912408759,
      "grad_norm": 0.12102582305669785,
      "learning_rate": 3.179562043795621e-05,
      "loss": 0.5285,
      "step": 49880
    },
    {
      "epoch": 364.16058394160586,
      "grad_norm": 14.721484184265137,
      "learning_rate": 3.179197080291971e-05,
      "loss": 1.1117,
      "step": 49890
    },
    {
      "epoch": 364.2335766423358,
      "grad_norm": 8.167704582214355,
      "learning_rate": 3.178832116788321e-05,
      "loss": 0.8792,
      "step": 49900
    },
    {
      "epoch": 364.3065693430657,
      "grad_norm": 9.573124885559082,
      "learning_rate": 3.178467153284672e-05,
      "loss": 0.9592,
      "step": 49910
    },
    {
      "epoch": 364.3795620437956,
      "grad_norm": 7.692161560058594,
      "learning_rate": 3.178102189781022e-05,
      "loss": 0.6395,
      "step": 49920
    },
    {
      "epoch": 364.45255474452557,
      "grad_norm": 15.935521125793457,
      "learning_rate": 3.1777372262773725e-05,
      "loss": 1.2181,
      "step": 49930
    },
    {
      "epoch": 364.5255474452555,
      "grad_norm": 10.709769248962402,
      "learning_rate": 3.1773722627737226e-05,
      "loss": 1.3306,
      "step": 49940
    },
    {
      "epoch": 364.5985401459854,
      "grad_norm": 8.619574546813965,
      "learning_rate": 3.177007299270073e-05,
      "loss": 1.1251,
      "step": 49950
    },
    {
      "epoch": 364.6715328467153,
      "grad_norm": 0.45993539690971375,
      "learning_rate": 3.176642335766424e-05,
      "loss": 1.3712,
      "step": 49960
    },
    {
      "epoch": 364.7445255474453,
      "grad_norm": 6.995452880859375,
      "learning_rate": 3.1762773722627734e-05,
      "loss": 0.6082,
      "step": 49970
    },
    {
      "epoch": 364.8175182481752,
      "grad_norm": 5.358656406402588,
      "learning_rate": 3.175912408759124e-05,
      "loss": 0.6844,
      "step": 49980
    },
    {
      "epoch": 364.8905109489051,
      "grad_norm": 0.15053187310695648,
      "learning_rate": 3.175547445255475e-05,
      "loss": 1.04,
      "step": 49990
    },
    {
      "epoch": 364.963503649635,
      "grad_norm": 15.89952564239502,
      "learning_rate": 3.175182481751825e-05,
      "loss": 1.099,
      "step": 50000
    },
    {
      "epoch": 365.036496350365,
      "grad_norm": 0.20988208055496216,
      "learning_rate": 3.1748175182481756e-05,
      "loss": 0.6963,
      "step": 50010
    },
    {
      "epoch": 365.1094890510949,
      "grad_norm": 9.158852577209473,
      "learning_rate": 3.174452554744526e-05,
      "loss": 0.6845,
      "step": 50020
    },
    {
      "epoch": 365.1824817518248,
      "grad_norm": 15.177465438842773,
      "learning_rate": 3.1740875912408764e-05,
      "loss": 1.1094,
      "step": 50030
    },
    {
      "epoch": 365.2554744525547,
      "grad_norm": 10.9109468460083,
      "learning_rate": 3.1737226277372265e-05,
      "loss": 0.9796,
      "step": 50040
    },
    {
      "epoch": 365.3284671532847,
      "grad_norm": 7.908440113067627,
      "learning_rate": 3.1733576642335765e-05,
      "loss": 0.8085,
      "step": 50050
    },
    {
      "epoch": 365.4014598540146,
      "grad_norm": 7.29054594039917,
      "learning_rate": 3.172992700729927e-05,
      "loss": 0.7365,
      "step": 50060
    },
    {
      "epoch": 365.4744525547445,
      "grad_norm": 9.445352554321289,
      "learning_rate": 3.172627737226277e-05,
      "loss": 0.7372,
      "step": 50070
    },
    {
      "epoch": 365.54744525547443,
      "grad_norm": 7.778528690338135,
      "learning_rate": 3.172262773722628e-05,
      "loss": 0.8264,
      "step": 50080
    },
    {
      "epoch": 365.6204379562044,
      "grad_norm": 15.190709114074707,
      "learning_rate": 3.171897810218978e-05,
      "loss": 1.1842,
      "step": 50090
    },
    {
      "epoch": 365.6934306569343,
      "grad_norm": 9.75594425201416,
      "learning_rate": 3.171532846715329e-05,
      "loss": 0.7946,
      "step": 50100
    },
    {
      "epoch": 365.7664233576642,
      "grad_norm": 10.762961387634277,
      "learning_rate": 3.1711678832116795e-05,
      "loss": 1.1294,
      "step": 50110
    },
    {
      "epoch": 365.83941605839414,
      "grad_norm": 12.347737312316895,
      "learning_rate": 3.170802919708029e-05,
      "loss": 1.3287,
      "step": 50120
    },
    {
      "epoch": 365.9124087591241,
      "grad_norm": 0.08578290045261383,
      "learning_rate": 3.1704379562043796e-05,
      "loss": 1.1167,
      "step": 50130
    },
    {
      "epoch": 365.985401459854,
      "grad_norm": 12.014494895935059,
      "learning_rate": 3.1700729927007297e-05,
      "loss": 0.7463,
      "step": 50140
    },
    {
      "epoch": 366.05839416058393,
      "grad_norm": 16.597509384155273,
      "learning_rate": 3.1697080291970804e-05,
      "loss": 1.2639,
      "step": 50150
    },
    {
      "epoch": 366.13138686131384,
      "grad_norm": 0.1540258526802063,
      "learning_rate": 3.169343065693431e-05,
      "loss": 0.8638,
      "step": 50160
    },
    {
      "epoch": 366.2043795620438,
      "grad_norm": 12.712926864624023,
      "learning_rate": 3.168978102189781e-05,
      "loss": 1.0297,
      "step": 50170
    },
    {
      "epoch": 366.2773722627737,
      "grad_norm": 8.059176445007324,
      "learning_rate": 3.168613138686132e-05,
      "loss": 0.8532,
      "step": 50180
    },
    {
      "epoch": 366.35036496350364,
      "grad_norm": 15.383625984191895,
      "learning_rate": 3.168248175182482e-05,
      "loss": 1.1228,
      "step": 50190
    },
    {
      "epoch": 366.42335766423355,
      "grad_norm": 5.368478775024414,
      "learning_rate": 3.167883211678832e-05,
      "loss": 0.6869,
      "step": 50200
    },
    {
      "epoch": 366.4963503649635,
      "grad_norm": 7.5867462158203125,
      "learning_rate": 3.167518248175183e-05,
      "loss": 0.5194,
      "step": 50210
    },
    {
      "epoch": 366.56934306569343,
      "grad_norm": 7.722568988800049,
      "learning_rate": 3.167153284671533e-05,
      "loss": 1.0747,
      "step": 50220
    },
    {
      "epoch": 366.64233576642334,
      "grad_norm": 9.996359825134277,
      "learning_rate": 3.1667883211678835e-05,
      "loss": 1.3287,
      "step": 50230
    },
    {
      "epoch": 366.7153284671533,
      "grad_norm": 3.9829044342041016,
      "learning_rate": 3.1664233576642335e-05,
      "loss": 0.8188,
      "step": 50240
    },
    {
      "epoch": 366.7883211678832,
      "grad_norm": 8.965468406677246,
      "learning_rate": 3.166058394160584e-05,
      "loss": 1.0279,
      "step": 50250
    },
    {
      "epoch": 366.86131386861314,
      "grad_norm": 11.839055061340332,
      "learning_rate": 3.165693430656935e-05,
      "loss": 1.1814,
      "step": 50260
    },
    {
      "epoch": 366.93430656934305,
      "grad_norm": 4.404118061065674,
      "learning_rate": 3.1653284671532844e-05,
      "loss": 0.7091,
      "step": 50270
    },
    {
      "epoch": 367.007299270073,
      "grad_norm": 8.051762580871582,
      "learning_rate": 3.164963503649635e-05,
      "loss": 0.5012,
      "step": 50280
    },
    {
      "epoch": 367.08029197080293,
      "grad_norm": 11.063131332397461,
      "learning_rate": 3.164598540145985e-05,
      "loss": 1.0622,
      "step": 50290
    },
    {
      "epoch": 367.15328467153284,
      "grad_norm": 15.697418212890625,
      "learning_rate": 3.164233576642336e-05,
      "loss": 1.1904,
      "step": 50300
    },
    {
      "epoch": 367.22627737226276,
      "grad_norm": 0.9224211573600769,
      "learning_rate": 3.1638686131386866e-05,
      "loss": 0.9551,
      "step": 50310
    },
    {
      "epoch": 367.2992700729927,
      "grad_norm": 10.120760917663574,
      "learning_rate": 3.1635036496350366e-05,
      "loss": 0.5759,
      "step": 50320
    },
    {
      "epoch": 367.37226277372264,
      "grad_norm": 17.96396255493164,
      "learning_rate": 3.1631386861313874e-05,
      "loss": 1.2619,
      "step": 50330
    },
    {
      "epoch": 367.44525547445255,
      "grad_norm": 13.84099006652832,
      "learning_rate": 3.1627737226277374e-05,
      "loss": 0.6937,
      "step": 50340
    },
    {
      "epoch": 367.51824817518246,
      "grad_norm": 10.64480209350586,
      "learning_rate": 3.1624087591240875e-05,
      "loss": 1.0371,
      "step": 50350
    },
    {
      "epoch": 367.59124087591243,
      "grad_norm": 5.807624340057373,
      "learning_rate": 3.162043795620438e-05,
      "loss": 1.393,
      "step": 50360
    },
    {
      "epoch": 367.66423357664235,
      "grad_norm": 0.12165758013725281,
      "learning_rate": 3.161678832116788e-05,
      "loss": 0.6933,
      "step": 50370
    },
    {
      "epoch": 367.73722627737226,
      "grad_norm": 6.991878509521484,
      "learning_rate": 3.161313868613139e-05,
      "loss": 0.9519,
      "step": 50380
    },
    {
      "epoch": 367.81021897810217,
      "grad_norm": 17.00600814819336,
      "learning_rate": 3.160948905109489e-05,
      "loss": 0.7755,
      "step": 50390
    },
    {
      "epoch": 367.88321167883214,
      "grad_norm": 12.537286758422852,
      "learning_rate": 3.16058394160584e-05,
      "loss": 0.6661,
      "step": 50400
    },
    {
      "epoch": 367.95620437956205,
      "grad_norm": 0.07112929970026016,
      "learning_rate": 3.1602189781021905e-05,
      "loss": 0.7752,
      "step": 50410
    },
    {
      "epoch": 368.02919708029196,
      "grad_norm": 0.19808751344680786,
      "learning_rate": 3.15985401459854e-05,
      "loss": 0.7036,
      "step": 50420
    },
    {
      "epoch": 368.1021897810219,
      "grad_norm": 14.423664093017578,
      "learning_rate": 3.1594890510948906e-05,
      "loss": 1.0565,
      "step": 50430
    },
    {
      "epoch": 368.17518248175185,
      "grad_norm": 3.471435546875,
      "learning_rate": 3.1591240875912406e-05,
      "loss": 0.9807,
      "step": 50440
    },
    {
      "epoch": 368.24817518248176,
      "grad_norm": 6.295623779296875,
      "learning_rate": 3.1587591240875914e-05,
      "loss": 1.3096,
      "step": 50450
    },
    {
      "epoch": 368.3211678832117,
      "grad_norm": 0.41589415073394775,
      "learning_rate": 3.158394160583942e-05,
      "loss": 0.5195,
      "step": 50460
    },
    {
      "epoch": 368.3941605839416,
      "grad_norm": 9.764609336853027,
      "learning_rate": 3.158029197080292e-05,
      "loss": 0.8506,
      "step": 50470
    },
    {
      "epoch": 368.46715328467155,
      "grad_norm": 7.341850757598877,
      "learning_rate": 3.157664233576643e-05,
      "loss": 1.0293,
      "step": 50480
    },
    {
      "epoch": 368.54014598540147,
      "grad_norm": 9.158645629882812,
      "learning_rate": 3.157299270072993e-05,
      "loss": 1.1875,
      "step": 50490
    },
    {
      "epoch": 368.6131386861314,
      "grad_norm": 10.692317008972168,
      "learning_rate": 3.156934306569343e-05,
      "loss": 0.9377,
      "step": 50500
    },
    {
      "epoch": 368.6861313868613,
      "grad_norm": 0.20435866713523865,
      "learning_rate": 3.156569343065694e-05,
      "loss": 1.0604,
      "step": 50510
    },
    {
      "epoch": 368.75912408759126,
      "grad_norm": 5.705259799957275,
      "learning_rate": 3.156204379562044e-05,
      "loss": 1.0657,
      "step": 50520
    },
    {
      "epoch": 368.8321167883212,
      "grad_norm": 0.18509100377559662,
      "learning_rate": 3.1558394160583945e-05,
      "loss": 0.5937,
      "step": 50530
    },
    {
      "epoch": 368.9051094890511,
      "grad_norm": 8.246541023254395,
      "learning_rate": 3.1554744525547445e-05,
      "loss": 0.7516,
      "step": 50540
    },
    {
      "epoch": 368.978102189781,
      "grad_norm": 13.46927547454834,
      "learning_rate": 3.155109489051095e-05,
      "loss": 1.0077,
      "step": 50550
    },
    {
      "epoch": 369.05109489051097,
      "grad_norm": 7.36732816696167,
      "learning_rate": 3.154744525547445e-05,
      "loss": 0.7596,
      "step": 50560
    },
    {
      "epoch": 369.1240875912409,
      "grad_norm": 7.64673376083374,
      "learning_rate": 3.154379562043796e-05,
      "loss": 0.9812,
      "step": 50570
    },
    {
      "epoch": 369.1970802919708,
      "grad_norm": 0.2011927217245102,
      "learning_rate": 3.154014598540146e-05,
      "loss": 0.8917,
      "step": 50580
    },
    {
      "epoch": 369.2700729927007,
      "grad_norm": 16.41237449645996,
      "learning_rate": 3.153649635036496e-05,
      "loss": 1.177,
      "step": 50590
    },
    {
      "epoch": 369.3430656934307,
      "grad_norm": 0.13936592638492584,
      "learning_rate": 3.153284671532847e-05,
      "loss": 0.9218,
      "step": 50600
    },
    {
      "epoch": 369.4160583941606,
      "grad_norm": 10.62679672241211,
      "learning_rate": 3.152919708029197e-05,
      "loss": 0.8981,
      "step": 50610
    },
    {
      "epoch": 369.4890510948905,
      "grad_norm": 9.289124488830566,
      "learning_rate": 3.1525547445255476e-05,
      "loss": 0.7696,
      "step": 50620
    },
    {
      "epoch": 369.5620437956204,
      "grad_norm": 0.5521198511123657,
      "learning_rate": 3.1521897810218983e-05,
      "loss": 0.7419,
      "step": 50630
    },
    {
      "epoch": 369.6350364963504,
      "grad_norm": 13.80611515045166,
      "learning_rate": 3.1518248175182484e-05,
      "loss": 0.646,
      "step": 50640
    },
    {
      "epoch": 369.7080291970803,
      "grad_norm": 9.992951393127441,
      "learning_rate": 3.1514598540145984e-05,
      "loss": 0.7511,
      "step": 50650
    },
    {
      "epoch": 369.7810218978102,
      "grad_norm": 10.758820533752441,
      "learning_rate": 3.151094890510949e-05,
      "loss": 1.0773,
      "step": 50660
    },
    {
      "epoch": 369.8540145985401,
      "grad_norm": 6.357024192810059,
      "learning_rate": 3.150729927007299e-05,
      "loss": 1.1914,
      "step": 50670
    },
    {
      "epoch": 369.9270072992701,
      "grad_norm": 0.20061293244361877,
      "learning_rate": 3.15036496350365e-05,
      "loss": 1.2739,
      "step": 50680
    },
    {
      "epoch": 370.0,
      "grad_norm": 13.912642478942871,
      "learning_rate": 3.15e-05,
      "loss": 0.6231,
      "step": 50690
    },
    {
      "epoch": 370.0729927007299,
      "grad_norm": 7.386929988861084,
      "learning_rate": 3.149635036496351e-05,
      "loss": 0.8073,
      "step": 50700
    },
    {
      "epoch": 370.1459854014599,
      "grad_norm": 10.097343444824219,
      "learning_rate": 3.149270072992701e-05,
      "loss": 0.9867,
      "step": 50710
    },
    {
      "epoch": 370.2189781021898,
      "grad_norm": 10.075005531311035,
      "learning_rate": 3.1489051094890515e-05,
      "loss": 1.1865,
      "step": 50720
    },
    {
      "epoch": 370.2919708029197,
      "grad_norm": 8.6329984664917,
      "learning_rate": 3.1485401459854015e-05,
      "loss": 1.0079,
      "step": 50730
    },
    {
      "epoch": 370.3649635036496,
      "grad_norm": 0.3092406690120697,
      "learning_rate": 3.1481751824817516e-05,
      "loss": 0.8765,
      "step": 50740
    },
    {
      "epoch": 370.4379562043796,
      "grad_norm": 9.268441200256348,
      "learning_rate": 3.147810218978102e-05,
      "loss": 0.8506,
      "step": 50750
    },
    {
      "epoch": 370.5109489051095,
      "grad_norm": 14.256258010864258,
      "learning_rate": 3.1474452554744524e-05,
      "loss": 0.8861,
      "step": 50760
    },
    {
      "epoch": 370.5839416058394,
      "grad_norm": 11.162665367126465,
      "learning_rate": 3.147080291970803e-05,
      "loss": 1.1575,
      "step": 50770
    },
    {
      "epoch": 370.6569343065693,
      "grad_norm": 20.320405960083008,
      "learning_rate": 3.146715328467154e-05,
      "loss": 1.2718,
      "step": 50780
    },
    {
      "epoch": 370.7299270072993,
      "grad_norm": 12.039809226989746,
      "learning_rate": 3.146350364963504e-05,
      "loss": 0.6866,
      "step": 50790
    },
    {
      "epoch": 370.8029197080292,
      "grad_norm": 10.417628288269043,
      "learning_rate": 3.1459854014598546e-05,
      "loss": 0.8194,
      "step": 50800
    },
    {
      "epoch": 370.8759124087591,
      "grad_norm": 0.17932285368442535,
      "learning_rate": 3.145620437956204e-05,
      "loss": 0.8831,
      "step": 50810
    },
    {
      "epoch": 370.94890510948903,
      "grad_norm": 13.344736099243164,
      "learning_rate": 3.145255474452555e-05,
      "loss": 0.643,
      "step": 50820
    },
    {
      "epoch": 371.021897810219,
      "grad_norm": 4.608072280883789,
      "learning_rate": 3.1448905109489054e-05,
      "loss": 1.007,
      "step": 50830
    },
    {
      "epoch": 371.0948905109489,
      "grad_norm": 8.68644905090332,
      "learning_rate": 3.1445255474452555e-05,
      "loss": 0.6622,
      "step": 50840
    },
    {
      "epoch": 371.1678832116788,
      "grad_norm": 13.991456985473633,
      "learning_rate": 3.144160583941606e-05,
      "loss": 0.6522,
      "step": 50850
    },
    {
      "epoch": 371.24087591240874,
      "grad_norm": 7.855353355407715,
      "learning_rate": 3.143795620437956e-05,
      "loss": 1.0291,
      "step": 50860
    },
    {
      "epoch": 371.3138686131387,
      "grad_norm": 11.53267765045166,
      "learning_rate": 3.143430656934307e-05,
      "loss": 0.8584,
      "step": 50870
    },
    {
      "epoch": 371.3868613138686,
      "grad_norm": 18.00858497619629,
      "learning_rate": 3.143065693430657e-05,
      "loss": 1.0234,
      "step": 50880
    },
    {
      "epoch": 371.45985401459853,
      "grad_norm": 0.1481642872095108,
      "learning_rate": 3.142700729927007e-05,
      "loss": 0.7411,
      "step": 50890
    },
    {
      "epoch": 371.53284671532845,
      "grad_norm": 8.877346992492676,
      "learning_rate": 3.142335766423358e-05,
      "loss": 1.0173,
      "step": 50900
    },
    {
      "epoch": 371.6058394160584,
      "grad_norm": 8.228533744812012,
      "learning_rate": 3.141970802919708e-05,
      "loss": 0.8221,
      "step": 50910
    },
    {
      "epoch": 371.6788321167883,
      "grad_norm": 7.15998649597168,
      "learning_rate": 3.1416058394160586e-05,
      "loss": 0.6121,
      "step": 50920
    },
    {
      "epoch": 371.75182481751824,
      "grad_norm": 13.846552848815918,
      "learning_rate": 3.141240875912409e-05,
      "loss": 1.2347,
      "step": 50930
    },
    {
      "epoch": 371.82481751824815,
      "grad_norm": 9.462176322937012,
      "learning_rate": 3.1408759124087594e-05,
      "loss": 0.9626,
      "step": 50940
    },
    {
      "epoch": 371.8978102189781,
      "grad_norm": 6.48668909072876,
      "learning_rate": 3.14051094890511e-05,
      "loss": 0.9205,
      "step": 50950
    },
    {
      "epoch": 371.97080291970804,
      "grad_norm": 14.062177658081055,
      "learning_rate": 3.1401459854014595e-05,
      "loss": 1.01,
      "step": 50960
    },
    {
      "epoch": 372.04379562043795,
      "grad_norm": 7.476742744445801,
      "learning_rate": 3.13978102189781e-05,
      "loss": 0.9798,
      "step": 50970
    },
    {
      "epoch": 372.11678832116786,
      "grad_norm": 10.776819229125977,
      "learning_rate": 3.139416058394161e-05,
      "loss": 0.9287,
      "step": 50980
    },
    {
      "epoch": 372.18978102189783,
      "grad_norm": 0.4294898509979248,
      "learning_rate": 3.139051094890511e-05,
      "loss": 0.5962,
      "step": 50990
    },
    {
      "epoch": 372.26277372262774,
      "grad_norm": 8.436259269714355,
      "learning_rate": 3.138686131386862e-05,
      "loss": 0.8688,
      "step": 51000
    },
    {
      "epoch": 372.33576642335765,
      "grad_norm": 0.10910187661647797,
      "learning_rate": 3.138321167883212e-05,
      "loss": 0.8112,
      "step": 51010
    },
    {
      "epoch": 372.40875912408757,
      "grad_norm": 5.699277400970459,
      "learning_rate": 3.1379562043795625e-05,
      "loss": 1.119,
      "step": 51020
    },
    {
      "epoch": 372.48175182481754,
      "grad_norm": 1.4289419651031494,
      "learning_rate": 3.1375912408759125e-05,
      "loss": 0.847,
      "step": 51030
    },
    {
      "epoch": 372.55474452554745,
      "grad_norm": 17.84149169921875,
      "learning_rate": 3.1372262773722626e-05,
      "loss": 1.5405,
      "step": 51040
    },
    {
      "epoch": 372.62773722627736,
      "grad_norm": 0.29306432604789734,
      "learning_rate": 3.136861313868613e-05,
      "loss": 0.6238,
      "step": 51050
    },
    {
      "epoch": 372.7007299270073,
      "grad_norm": 11.031704902648926,
      "learning_rate": 3.1364963503649633e-05,
      "loss": 1.1342,
      "step": 51060
    },
    {
      "epoch": 372.77372262773724,
      "grad_norm": 9.978734016418457,
      "learning_rate": 3.136131386861314e-05,
      "loss": 1.2303,
      "step": 51070
    },
    {
      "epoch": 372.84671532846716,
      "grad_norm": 15.67658805847168,
      "learning_rate": 3.135766423357665e-05,
      "loss": 0.9713,
      "step": 51080
    },
    {
      "epoch": 372.91970802919707,
      "grad_norm": 10.916742324829102,
      "learning_rate": 3.135401459854015e-05,
      "loss": 0.7833,
      "step": 51090
    },
    {
      "epoch": 372.992700729927,
      "grad_norm": 0.09074677526950836,
      "learning_rate": 3.1350364963503656e-05,
      "loss": 0.7986,
      "step": 51100
    },
    {
      "epoch": 373.06569343065695,
      "grad_norm": 9.50825309753418,
      "learning_rate": 3.134671532846715e-05,
      "loss": 1.1098,
      "step": 51110
    },
    {
      "epoch": 373.13868613138686,
      "grad_norm": 13.483826637268066,
      "learning_rate": 3.134306569343066e-05,
      "loss": 0.9657,
      "step": 51120
    },
    {
      "epoch": 373.2116788321168,
      "grad_norm": 0.09280534833669662,
      "learning_rate": 3.1339416058394164e-05,
      "loss": 0.5933,
      "step": 51130
    },
    {
      "epoch": 373.2846715328467,
      "grad_norm": 6.415770530700684,
      "learning_rate": 3.1335766423357664e-05,
      "loss": 1.0548,
      "step": 51140
    },
    {
      "epoch": 373.35766423357666,
      "grad_norm": 9.75599479675293,
      "learning_rate": 3.133211678832117e-05,
      "loss": 0.6446,
      "step": 51150
    },
    {
      "epoch": 373.43065693430657,
      "grad_norm": 13.870862007141113,
      "learning_rate": 3.132846715328467e-05,
      "loss": 0.9008,
      "step": 51160
    },
    {
      "epoch": 373.5036496350365,
      "grad_norm": 15.048110008239746,
      "learning_rate": 3.132481751824818e-05,
      "loss": 0.9148,
      "step": 51170
    },
    {
      "epoch": 373.57664233576645,
      "grad_norm": 9.31627368927002,
      "learning_rate": 3.132116788321168e-05,
      "loss": 1.0177,
      "step": 51180
    },
    {
      "epoch": 373.64963503649636,
      "grad_norm": 12.081982612609863,
      "learning_rate": 3.131751824817518e-05,
      "loss": 0.6674,
      "step": 51190
    },
    {
      "epoch": 373.7226277372263,
      "grad_norm": 20.677352905273438,
      "learning_rate": 3.131386861313869e-05,
      "loss": 0.8346,
      "step": 51200
    },
    {
      "epoch": 373.7956204379562,
      "grad_norm": 11.543420791625977,
      "learning_rate": 3.131021897810219e-05,
      "loss": 1.233,
      "step": 51210
    },
    {
      "epoch": 373.86861313868616,
      "grad_norm": 0.7884581685066223,
      "learning_rate": 3.1306569343065696e-05,
      "loss": 0.9098,
      "step": 51220
    },
    {
      "epoch": 373.94160583941607,
      "grad_norm": 11.890439987182617,
      "learning_rate": 3.1302919708029196e-05,
      "loss": 0.9844,
      "step": 51230
    },
    {
      "epoch": 374.014598540146,
      "grad_norm": 7.468318939208984,
      "learning_rate": 3.12992700729927e-05,
      "loss": 1.328,
      "step": 51240
    },
    {
      "epoch": 374.0875912408759,
      "grad_norm": 0.8152748346328735,
      "learning_rate": 3.129562043795621e-05,
      "loss": 0.2979,
      "step": 51250
    },
    {
      "epoch": 374.16058394160586,
      "grad_norm": 7.859915733337402,
      "learning_rate": 3.129197080291971e-05,
      "loss": 1.045,
      "step": 51260
    },
    {
      "epoch": 374.2335766423358,
      "grad_norm": 12.7865571975708,
      "learning_rate": 3.128832116788321e-05,
      "loss": 1.1992,
      "step": 51270
    },
    {
      "epoch": 374.3065693430657,
      "grad_norm": 7.182492256164551,
      "learning_rate": 3.128467153284672e-05,
      "loss": 1.0843,
      "step": 51280
    },
    {
      "epoch": 374.3795620437956,
      "grad_norm": 11.738469123840332,
      "learning_rate": 3.128102189781022e-05,
      "loss": 0.9344,
      "step": 51290
    },
    {
      "epoch": 374.45255474452557,
      "grad_norm": 11.616368293762207,
      "learning_rate": 3.127737226277373e-05,
      "loss": 0.8242,
      "step": 51300
    },
    {
      "epoch": 374.5255474452555,
      "grad_norm": 10.72181224822998,
      "learning_rate": 3.127372262773723e-05,
      "loss": 0.9428,
      "step": 51310
    },
    {
      "epoch": 374.5985401459854,
      "grad_norm": 3.8446013927459717,
      "learning_rate": 3.1270072992700734e-05,
      "loss": 0.826,
      "step": 51320
    },
    {
      "epoch": 374.6715328467153,
      "grad_norm": 15.294623374938965,
      "learning_rate": 3.1266423357664235e-05,
      "loss": 1.1716,
      "step": 51330
    },
    {
      "epoch": 374.7445255474453,
      "grad_norm": 8.553050994873047,
      "learning_rate": 3.1262773722627735e-05,
      "loss": 0.8211,
      "step": 51340
    },
    {
      "epoch": 374.8175182481752,
      "grad_norm": 8.872354507446289,
      "learning_rate": 3.125912408759124e-05,
      "loss": 0.8013,
      "step": 51350
    },
    {
      "epoch": 374.8905109489051,
      "grad_norm": 0.18569189310073853,
      "learning_rate": 3.125547445255474e-05,
      "loss": 0.9569,
      "step": 51360
    },
    {
      "epoch": 374.963503649635,
      "grad_norm": 3.1458771228790283,
      "learning_rate": 3.125182481751825e-05,
      "loss": 1.0793,
      "step": 51370
    },
    {
      "epoch": 375.036496350365,
      "grad_norm": 6.590932846069336,
      "learning_rate": 3.124817518248175e-05,
      "loss": 1.0702,
      "step": 51380
    },
    {
      "epoch": 375.1094890510949,
      "grad_norm": 14.173554420471191,
      "learning_rate": 3.124452554744526e-05,
      "loss": 0.9647,
      "step": 51390
    },
    {
      "epoch": 375.1824817518248,
      "grad_norm": 13.70517349243164,
      "learning_rate": 3.1240875912408765e-05,
      "loss": 1.2528,
      "step": 51400
    },
    {
      "epoch": 375.2554744525547,
      "grad_norm": 9.213227272033691,
      "learning_rate": 3.1237226277372266e-05,
      "loss": 0.6851,
      "step": 51410
    },
    {
      "epoch": 375.3284671532847,
      "grad_norm": 7.541343688964844,
      "learning_rate": 3.1233576642335766e-05,
      "loss": 1.0258,
      "step": 51420
    },
    {
      "epoch": 375.4014598540146,
      "grad_norm": 8.63965892791748,
      "learning_rate": 3.122992700729927e-05,
      "loss": 1.2098,
      "step": 51430
    },
    {
      "epoch": 375.4744525547445,
      "grad_norm": 12.374354362487793,
      "learning_rate": 3.1226277372262774e-05,
      "loss": 1.2192,
      "step": 51440
    },
    {
      "epoch": 375.54744525547443,
      "grad_norm": 0.17307494580745697,
      "learning_rate": 3.122262773722628e-05,
      "loss": 0.7082,
      "step": 51450
    },
    {
      "epoch": 375.6204379562044,
      "grad_norm": 11.729351043701172,
      "learning_rate": 3.121897810218978e-05,
      "loss": 1.0467,
      "step": 51460
    },
    {
      "epoch": 375.6934306569343,
      "grad_norm": 8.388524055480957,
      "learning_rate": 3.121532846715329e-05,
      "loss": 0.7003,
      "step": 51470
    },
    {
      "epoch": 375.7664233576642,
      "grad_norm": 7.171483039855957,
      "learning_rate": 3.121167883211679e-05,
      "loss": 0.6865,
      "step": 51480
    },
    {
      "epoch": 375.83941605839414,
      "grad_norm": 9.391030311584473,
      "learning_rate": 3.120802919708029e-05,
      "loss": 0.8188,
      "step": 51490
    },
    {
      "epoch": 375.9124087591241,
      "grad_norm": 9.097537994384766,
      "learning_rate": 3.12043795620438e-05,
      "loss": 1.0192,
      "step": 51500
    },
    {
      "epoch": 375.985401459854,
      "grad_norm": 5.002603530883789,
      "learning_rate": 3.12007299270073e-05,
      "loss": 0.7798,
      "step": 51510
    },
    {
      "epoch": 376.05839416058393,
      "grad_norm": 12.657176971435547,
      "learning_rate": 3.1197080291970805e-05,
      "loss": 1.1051,
      "step": 51520
    },
    {
      "epoch": 376.13138686131384,
      "grad_norm": 0.2613832652568817,
      "learning_rate": 3.1193430656934306e-05,
      "loss": 0.8458,
      "step": 51530
    },
    {
      "epoch": 376.2043795620438,
      "grad_norm": 0.0978148952126503,
      "learning_rate": 3.118978102189781e-05,
      "loss": 0.8141,
      "step": 51540
    },
    {
      "epoch": 376.2773722627737,
      "grad_norm": 6.312929153442383,
      "learning_rate": 3.118613138686132e-05,
      "loss": 1.0156,
      "step": 51550
    },
    {
      "epoch": 376.35036496350364,
      "grad_norm": 1.7079695463180542,
      "learning_rate": 3.118248175182482e-05,
      "loss": 0.7693,
      "step": 51560
    },
    {
      "epoch": 376.42335766423355,
      "grad_norm": 11.33247184753418,
      "learning_rate": 3.117883211678832e-05,
      "loss": 0.6998,
      "step": 51570
    },
    {
      "epoch": 376.4963503649635,
      "grad_norm": 6.264952659606934,
      "learning_rate": 3.117518248175182e-05,
      "loss": 0.7984,
      "step": 51580
    },
    {
      "epoch": 376.56934306569343,
      "grad_norm": 0.12729550898075104,
      "learning_rate": 3.117153284671533e-05,
      "loss": 0.9129,
      "step": 51590
    },
    {
      "epoch": 376.64233576642334,
      "grad_norm": 12.428380012512207,
      "learning_rate": 3.1167883211678836e-05,
      "loss": 1.4888,
      "step": 51600
    },
    {
      "epoch": 376.7153284671533,
      "grad_norm": 14.137909889221191,
      "learning_rate": 3.116423357664234e-05,
      "loss": 0.9245,
      "step": 51610
    },
    {
      "epoch": 376.7883211678832,
      "grad_norm": 0.22912287712097168,
      "learning_rate": 3.1160583941605844e-05,
      "loss": 0.8581,
      "step": 51620
    },
    {
      "epoch": 376.86131386861314,
      "grad_norm": 10.477127075195312,
      "learning_rate": 3.1156934306569345e-05,
      "loss": 0.8128,
      "step": 51630
    },
    {
      "epoch": 376.93430656934305,
      "grad_norm": 12.466079711914062,
      "learning_rate": 3.115328467153285e-05,
      "loss": 1.0834,
      "step": 51640
    },
    {
      "epoch": 377.007299270073,
      "grad_norm": 8.3516206741333,
      "learning_rate": 3.114963503649635e-05,
      "loss": 1.1445,
      "step": 51650
    },
    {
      "epoch": 377.08029197080293,
      "grad_norm": 5.974073886871338,
      "learning_rate": 3.114598540145985e-05,
      "loss": 0.684,
      "step": 51660
    },
    {
      "epoch": 377.15328467153284,
      "grad_norm": 14.42000675201416,
      "learning_rate": 3.114233576642336e-05,
      "loss": 1.3829,
      "step": 51670
    },
    {
      "epoch": 377.22627737226276,
      "grad_norm": 1.3213804960250854,
      "learning_rate": 3.113868613138686e-05,
      "loss": 0.5135,
      "step": 51680
    },
    {
      "epoch": 377.2992700729927,
      "grad_norm": 18.137964248657227,
      "learning_rate": 3.113503649635037e-05,
      "loss": 0.9886,
      "step": 51690
    },
    {
      "epoch": 377.37226277372264,
      "grad_norm": 7.94149112701416,
      "learning_rate": 3.113138686131387e-05,
      "loss": 1.0076,
      "step": 51700
    },
    {
      "epoch": 377.44525547445255,
      "grad_norm": 8.02988338470459,
      "learning_rate": 3.1127737226277376e-05,
      "loss": 1.2739,
      "step": 51710
    },
    {
      "epoch": 377.51824817518246,
      "grad_norm": 10.531390190124512,
      "learning_rate": 3.1124087591240876e-05,
      "loss": 0.999,
      "step": 51720
    },
    {
      "epoch": 377.59124087591243,
      "grad_norm": 0.17759206891059875,
      "learning_rate": 3.112043795620438e-05,
      "loss": 0.9608,
      "step": 51730
    },
    {
      "epoch": 377.66423357664235,
      "grad_norm": 11.066939353942871,
      "learning_rate": 3.1116788321167884e-05,
      "loss": 1.1504,
      "step": 51740
    },
    {
      "epoch": 377.73722627737226,
      "grad_norm": 0.09322123974561691,
      "learning_rate": 3.111313868613139e-05,
      "loss": 0.6387,
      "step": 51750
    },
    {
      "epoch": 377.81021897810217,
      "grad_norm": 5.781429290771484,
      "learning_rate": 3.110948905109489e-05,
      "loss": 0.7224,
      "step": 51760
    },
    {
      "epoch": 377.88321167883214,
      "grad_norm": 14.909784317016602,
      "learning_rate": 3.11058394160584e-05,
      "loss": 0.7867,
      "step": 51770
    },
    {
      "epoch": 377.95620437956205,
      "grad_norm": 5.902006149291992,
      "learning_rate": 3.11021897810219e-05,
      "loss": 0.8723,
      "step": 51780
    },
    {
      "epoch": 378.02919708029196,
      "grad_norm": 14.936263084411621,
      "learning_rate": 3.109854014598541e-05,
      "loss": 0.7241,
      "step": 51790
    },
    {
      "epoch": 378.1021897810219,
      "grad_norm": 13.374577522277832,
      "learning_rate": 3.109489051094891e-05,
      "loss": 0.8631,
      "step": 51800
    },
    {
      "epoch": 378.17518248175185,
      "grad_norm": 7.686380863189697,
      "learning_rate": 3.109124087591241e-05,
      "loss": 0.676,
      "step": 51810
    },
    {
      "epoch": 378.24817518248176,
      "grad_norm": 11.211469650268555,
      "learning_rate": 3.1087591240875915e-05,
      "loss": 1.2334,
      "step": 51820
    },
    {
      "epoch": 378.3211678832117,
      "grad_norm": 14.164994239807129,
      "learning_rate": 3.1083941605839415e-05,
      "loss": 0.9081,
      "step": 51830
    },
    {
      "epoch": 378.3941605839416,
      "grad_norm": 9.786287307739258,
      "learning_rate": 3.108029197080292e-05,
      "loss": 0.867,
      "step": 51840
    },
    {
      "epoch": 378.46715328467155,
      "grad_norm": 12.607547760009766,
      "learning_rate": 3.107664233576642e-05,
      "loss": 1.0391,
      "step": 51850
    },
    {
      "epoch": 378.54014598540147,
      "grad_norm": 13.101949691772461,
      "learning_rate": 3.107299270072993e-05,
      "loss": 1.2042,
      "step": 51860
    },
    {
      "epoch": 378.6131386861314,
      "grad_norm": 4.366309642791748,
      "learning_rate": 3.106934306569344e-05,
      "loss": 0.4039,
      "step": 51870
    },
    {
      "epoch": 378.6861313868613,
      "grad_norm": 0.3167673647403717,
      "learning_rate": 3.106569343065693e-05,
      "loss": 1.0542,
      "step": 51880
    },
    {
      "epoch": 378.75912408759126,
      "grad_norm": 6.979676246643066,
      "learning_rate": 3.106204379562044e-05,
      "loss": 0.9854,
      "step": 51890
    },
    {
      "epoch": 378.8321167883212,
      "grad_norm": 12.694911003112793,
      "learning_rate": 3.105839416058394e-05,
      "loss": 1.0774,
      "step": 51900
    },
    {
      "epoch": 378.9051094890511,
      "grad_norm": 6.023703098297119,
      "learning_rate": 3.1054744525547447e-05,
      "loss": 0.8319,
      "step": 51910
    },
    {
      "epoch": 378.978102189781,
      "grad_norm": 14.203775405883789,
      "learning_rate": 3.1051094890510954e-05,
      "loss": 0.8459,
      "step": 51920
    },
    {
      "epoch": 379.05109489051097,
      "grad_norm": 7.633858680725098,
      "learning_rate": 3.1047445255474454e-05,
      "loss": 1.1957,
      "step": 51930
    },
    {
      "epoch": 379.1240875912409,
      "grad_norm": 7.660355567932129,
      "learning_rate": 3.104379562043796e-05,
      "loss": 0.8923,
      "step": 51940
    },
    {
      "epoch": 379.1970802919708,
      "grad_norm": 1.3395121097564697,
      "learning_rate": 3.104014598540146e-05,
      "loss": 1.0646,
      "step": 51950
    },
    {
      "epoch": 379.2700729927007,
      "grad_norm": 0.18939468264579773,
      "learning_rate": 3.103649635036496e-05,
      "loss": 1.1998,
      "step": 51960
    },
    {
      "epoch": 379.3430656934307,
      "grad_norm": 12.038727760314941,
      "learning_rate": 3.103284671532847e-05,
      "loss": 0.9098,
      "step": 51970
    },
    {
      "epoch": 379.4160583941606,
      "grad_norm": 11.438860893249512,
      "learning_rate": 3.102919708029197e-05,
      "loss": 1.0552,
      "step": 51980
    },
    {
      "epoch": 379.4890510948905,
      "grad_norm": 6.366479396820068,
      "learning_rate": 3.102554744525548e-05,
      "loss": 0.7737,
      "step": 51990
    },
    {
      "epoch": 379.5620437956204,
      "grad_norm": 1.3553966283798218,
      "learning_rate": 3.102189781021898e-05,
      "loss": 1.0081,
      "step": 52000
    },
    {
      "epoch": 379.6350364963504,
      "grad_norm": 14.85537338256836,
      "learning_rate": 3.1018248175182485e-05,
      "loss": 0.8843,
      "step": 52010
    },
    {
      "epoch": 379.7080291970803,
      "grad_norm": 5.7058515548706055,
      "learning_rate": 3.101459854014599e-05,
      "loss": 0.3631,
      "step": 52020
    },
    {
      "epoch": 379.7810218978102,
      "grad_norm": 15.236906051635742,
      "learning_rate": 3.1010948905109486e-05,
      "loss": 1.0772,
      "step": 52030
    },
    {
      "epoch": 379.8540145985401,
      "grad_norm": 9.169576644897461,
      "learning_rate": 3.1007299270072994e-05,
      "loss": 0.9231,
      "step": 52040
    },
    {
      "epoch": 379.9270072992701,
      "grad_norm": 0.19139227271080017,
      "learning_rate": 3.1003649635036494e-05,
      "loss": 0.8271,
      "step": 52050
    },
    {
      "epoch": 380.0,
      "grad_norm": 0.09010202437639236,
      "learning_rate": 3.1e-05,
      "loss": 1.0567,
      "step": 52060
    },
    {
      "epoch": 380.0729927007299,
      "grad_norm": 11.4967679977417,
      "learning_rate": 3.099635036496351e-05,
      "loss": 0.9644,
      "step": 52070
    },
    {
      "epoch": 380.1459854014599,
      "grad_norm": 13.613948822021484,
      "learning_rate": 3.099270072992701e-05,
      "loss": 0.8716,
      "step": 52080
    },
    {
      "epoch": 380.2189781021898,
      "grad_norm": 6.603096008300781,
      "learning_rate": 3.0989051094890516e-05,
      "loss": 1.0248,
      "step": 52090
    },
    {
      "epoch": 380.2919708029197,
      "grad_norm": 0.20684406161308289,
      "learning_rate": 3.098540145985402e-05,
      "loss": 0.7781,
      "step": 52100
    },
    {
      "epoch": 380.3649635036496,
      "grad_norm": 15.006052017211914,
      "learning_rate": 3.098175182481752e-05,
      "loss": 1.0185,
      "step": 52110
    },
    {
      "epoch": 380.4379562043796,
      "grad_norm": 15.01729965209961,
      "learning_rate": 3.0978102189781025e-05,
      "loss": 1.0961,
      "step": 52120
    },
    {
      "epoch": 380.5109489051095,
      "grad_norm": 0.07184293121099472,
      "learning_rate": 3.0974452554744525e-05,
      "loss": 0.6163,
      "step": 52130
    },
    {
      "epoch": 380.5839416058394,
      "grad_norm": 8.208890914916992,
      "learning_rate": 3.097080291970803e-05,
      "loss": 1.3827,
      "step": 52140
    },
    {
      "epoch": 380.6569343065693,
      "grad_norm": 6.38566780090332,
      "learning_rate": 3.096715328467153e-05,
      "loss": 0.9205,
      "step": 52150
    },
    {
      "epoch": 380.7299270072993,
      "grad_norm": 0.14305001497268677,
      "learning_rate": 3.096350364963504e-05,
      "loss": 0.9293,
      "step": 52160
    },
    {
      "epoch": 380.8029197080292,
      "grad_norm": 0.17357990145683289,
      "learning_rate": 3.095985401459854e-05,
      "loss": 0.766,
      "step": 52170
    },
    {
      "epoch": 380.8759124087591,
      "grad_norm": 14.535750389099121,
      "learning_rate": 3.095620437956204e-05,
      "loss": 0.9594,
      "step": 52180
    },
    {
      "epoch": 380.94890510948903,
      "grad_norm": 0.06644053012132645,
      "learning_rate": 3.095255474452555e-05,
      "loss": 0.8499,
      "step": 52190
    },
    {
      "epoch": 381.021897810219,
      "grad_norm": 5.872950553894043,
      "learning_rate": 3.094890510948905e-05,
      "loss": 0.8543,
      "step": 52200
    },
    {
      "epoch": 381.0948905109489,
      "grad_norm": 12.556626319885254,
      "learning_rate": 3.0945255474452556e-05,
      "loss": 0.9231,
      "step": 52210
    },
    {
      "epoch": 381.1678832116788,
      "grad_norm": 7.408047676086426,
      "learning_rate": 3.0941605839416063e-05,
      "loss": 0.7363,
      "step": 52220
    },
    {
      "epoch": 381.24087591240874,
      "grad_norm": 12.501715660095215,
      "learning_rate": 3.0937956204379564e-05,
      "loss": 1.1414,
      "step": 52230
    },
    {
      "epoch": 381.3138686131387,
      "grad_norm": 6.78616189956665,
      "learning_rate": 3.093430656934307e-05,
      "loss": 0.9556,
      "step": 52240
    },
    {
      "epoch": 381.3868613138686,
      "grad_norm": 15.717488288879395,
      "learning_rate": 3.093065693430657e-05,
      "loss": 0.9276,
      "step": 52250
    },
    {
      "epoch": 381.45985401459853,
      "grad_norm": 0.2436618208885193,
      "learning_rate": 3.092700729927007e-05,
      "loss": 1.3809,
      "step": 52260
    },
    {
      "epoch": 381.53284671532845,
      "grad_norm": 0.16367679834365845,
      "learning_rate": 3.092335766423358e-05,
      "loss": 1.2379,
      "step": 52270
    },
    {
      "epoch": 381.6058394160584,
      "grad_norm": 10.514592170715332,
      "learning_rate": 3.091970802919708e-05,
      "loss": 0.8867,
      "step": 52280
    },
    {
      "epoch": 381.6788321167883,
      "grad_norm": 6.437477111816406,
      "learning_rate": 3.091605839416059e-05,
      "loss": 0.929,
      "step": 52290
    },
    {
      "epoch": 381.75182481751824,
      "grad_norm": 13.692380905151367,
      "learning_rate": 3.091240875912409e-05,
      "loss": 0.8487,
      "step": 52300
    },
    {
      "epoch": 381.82481751824815,
      "grad_norm": 10.037652969360352,
      "learning_rate": 3.0908759124087595e-05,
      "loss": 0.5766,
      "step": 52310
    },
    {
      "epoch": 381.8978102189781,
      "grad_norm": 5.029109001159668,
      "learning_rate": 3.0905109489051096e-05,
      "loss": 0.7076,
      "step": 52320
    },
    {
      "epoch": 381.97080291970804,
      "grad_norm": 7.590976238250732,
      "learning_rate": 3.09014598540146e-05,
      "loss": 1.0833,
      "step": 52330
    },
    {
      "epoch": 382.04379562043795,
      "grad_norm": 12.45588493347168,
      "learning_rate": 3.08978102189781e-05,
      "loss": 0.9367,
      "step": 52340
    },
    {
      "epoch": 382.11678832116786,
      "grad_norm": 11.370468139648438,
      "learning_rate": 3.0894160583941604e-05,
      "loss": 0.862,
      "step": 52350
    },
    {
      "epoch": 382.18978102189783,
      "grad_norm": 6.446566581726074,
      "learning_rate": 3.089051094890511e-05,
      "loss": 1.4724,
      "step": 52360
    },
    {
      "epoch": 382.26277372262774,
      "grad_norm": 2.2680575847625732,
      "learning_rate": 3.088686131386861e-05,
      "loss": 0.8971,
      "step": 52370
    },
    {
      "epoch": 382.33576642335765,
      "grad_norm": 0.08432451635599136,
      "learning_rate": 3.088321167883212e-05,
      "loss": 0.6963,
      "step": 52380
    },
    {
      "epoch": 382.40875912408757,
      "grad_norm": 14.467812538146973,
      "learning_rate": 3.0879562043795626e-05,
      "loss": 1.1944,
      "step": 52390
    },
    {
      "epoch": 382.48175182481754,
      "grad_norm": 1.4112972021102905,
      "learning_rate": 3.0875912408759127e-05,
      "loss": 0.6588,
      "step": 52400
    },
    {
      "epoch": 382.55474452554745,
      "grad_norm": 10.926383018493652,
      "learning_rate": 3.087226277372263e-05,
      "loss": 0.9248,
      "step": 52410
    },
    {
      "epoch": 382.62773722627736,
      "grad_norm": 5.515419006347656,
      "learning_rate": 3.0868613138686134e-05,
      "loss": 0.7946,
      "step": 52420
    },
    {
      "epoch": 382.7007299270073,
      "grad_norm": 21.036361694335938,
      "learning_rate": 3.0864963503649635e-05,
      "loss": 0.7857,
      "step": 52430
    },
    {
      "epoch": 382.77372262773724,
      "grad_norm": 5.728272438049316,
      "learning_rate": 3.086131386861314e-05,
      "loss": 0.8785,
      "step": 52440
    },
    {
      "epoch": 382.84671532846716,
      "grad_norm": 0.09439496695995331,
      "learning_rate": 3.085766423357664e-05,
      "loss": 1.532,
      "step": 52450
    },
    {
      "epoch": 382.91970802919707,
      "grad_norm": 0.13140569627285004,
      "learning_rate": 3.085401459854015e-05,
      "loss": 0.7317,
      "step": 52460
    },
    {
      "epoch": 382.992700729927,
      "grad_norm": 17.324453353881836,
      "learning_rate": 3.085036496350365e-05,
      "loss": 0.9958,
      "step": 52470
    },
    {
      "epoch": 383.06569343065695,
      "grad_norm": 0.0923718810081482,
      "learning_rate": 3.084671532846716e-05,
      "loss": 1.0233,
      "step": 52480
    },
    {
      "epoch": 383.13868613138686,
      "grad_norm": 9.477563858032227,
      "learning_rate": 3.084306569343066e-05,
      "loss": 0.688,
      "step": 52490
    },
    {
      "epoch": 383.2116788321168,
      "grad_norm": 9.366436958312988,
      "learning_rate": 3.083941605839416e-05,
      "loss": 1.284,
      "step": 52500
    },
    {
      "epoch": 383.2846715328467,
      "grad_norm": 10.89637279510498,
      "learning_rate": 3.0835766423357666e-05,
      "loss": 1.1213,
      "step": 52510
    },
    {
      "epoch": 383.35766423357666,
      "grad_norm": 9.499722480773926,
      "learning_rate": 3.0832116788321166e-05,
      "loss": 1.1861,
      "step": 52520
    },
    {
      "epoch": 383.43065693430657,
      "grad_norm": 8.100373268127441,
      "learning_rate": 3.0828467153284674e-05,
      "loss": 1.1998,
      "step": 52530
    },
    {
      "epoch": 383.5036496350365,
      "grad_norm": 11.842647552490234,
      "learning_rate": 3.082481751824818e-05,
      "loss": 0.8494,
      "step": 52540
    },
    {
      "epoch": 383.57664233576645,
      "grad_norm": 9.214620590209961,
      "learning_rate": 3.082116788321168e-05,
      "loss": 1.0539,
      "step": 52550
    },
    {
      "epoch": 383.64963503649636,
      "grad_norm": 13.550141334533691,
      "learning_rate": 3.081751824817518e-05,
      "loss": 0.5964,
      "step": 52560
    },
    {
      "epoch": 383.7226277372263,
      "grad_norm": 0.051091693341732025,
      "learning_rate": 3.081386861313868e-05,
      "loss": 0.9672,
      "step": 52570
    },
    {
      "epoch": 383.7956204379562,
      "grad_norm": 13.770084381103516,
      "learning_rate": 3.081021897810219e-05,
      "loss": 0.7448,
      "step": 52580
    },
    {
      "epoch": 383.86861313868616,
      "grad_norm": 6.449160099029541,
      "learning_rate": 3.08065693430657e-05,
      "loss": 1.1787,
      "step": 52590
    },
    {
      "epoch": 383.94160583941607,
      "grad_norm": 9.271693229675293,
      "learning_rate": 3.08029197080292e-05,
      "loss": 0.9957,
      "step": 52600
    },
    {
      "epoch": 384.014598540146,
      "grad_norm": 13.948432922363281,
      "learning_rate": 3.0799270072992705e-05,
      "loss": 0.5987,
      "step": 52610
    },
    {
      "epoch": 384.0875912408759,
      "grad_norm": 8.173749923706055,
      "learning_rate": 3.0795620437956205e-05,
      "loss": 1.3339,
      "step": 52620
    },
    {
      "epoch": 384.16058394160586,
      "grad_norm": 0.11947862803936005,
      "learning_rate": 3.079197080291971e-05,
      "loss": 0.8244,
      "step": 52630
    },
    {
      "epoch": 384.2335766423358,
      "grad_norm": 0.7826727628707886,
      "learning_rate": 3.078832116788321e-05,
      "loss": 0.9494,
      "step": 52640
    },
    {
      "epoch": 384.3065693430657,
      "grad_norm": 16.765064239501953,
      "learning_rate": 3.0784671532846713e-05,
      "loss": 0.6804,
      "step": 52650
    },
    {
      "epoch": 384.3795620437956,
      "grad_norm": 20.118587493896484,
      "learning_rate": 3.078102189781022e-05,
      "loss": 1.0442,
      "step": 52660
    },
    {
      "epoch": 384.45255474452557,
      "grad_norm": 10.227998733520508,
      "learning_rate": 3.077737226277372e-05,
      "loss": 0.7049,
      "step": 52670
    },
    {
      "epoch": 384.5255474452555,
      "grad_norm": 8.119904518127441,
      "learning_rate": 3.077372262773723e-05,
      "loss": 0.9289,
      "step": 52680
    },
    {
      "epoch": 384.5985401459854,
      "grad_norm": 12.24202823638916,
      "learning_rate": 3.0770072992700736e-05,
      "loss": 0.8266,
      "step": 52690
    },
    {
      "epoch": 384.6715328467153,
      "grad_norm": 0.23538120090961456,
      "learning_rate": 3.0766423357664236e-05,
      "loss": 0.8991,
      "step": 52700
    },
    {
      "epoch": 384.7445255474453,
      "grad_norm": 9.839681625366211,
      "learning_rate": 3.0762773722627744e-05,
      "loss": 0.8662,
      "step": 52710
    },
    {
      "epoch": 384.8175182481752,
      "grad_norm": 11.47840404510498,
      "learning_rate": 3.075912408759124e-05,
      "loss": 1.0167,
      "step": 52720
    },
    {
      "epoch": 384.8905109489051,
      "grad_norm": 13.359991073608398,
      "learning_rate": 3.0755474452554745e-05,
      "loss": 1.0783,
      "step": 52730
    },
    {
      "epoch": 384.963503649635,
      "grad_norm": 0.11866328865289688,
      "learning_rate": 3.075182481751825e-05,
      "loss": 0.7706,
      "step": 52740
    },
    {
      "epoch": 385.036496350365,
      "grad_norm": 0.28485333919525146,
      "learning_rate": 3.074817518248175e-05,
      "loss": 0.8349,
      "step": 52750
    },
    {
      "epoch": 385.1094890510949,
      "grad_norm": 7.402097225189209,
      "learning_rate": 3.074452554744526e-05,
      "loss": 1.2503,
      "step": 52760
    },
    {
      "epoch": 385.1824817518248,
      "grad_norm": 10.202815055847168,
      "learning_rate": 3.074087591240876e-05,
      "loss": 0.6574,
      "step": 52770
    },
    {
      "epoch": 385.2554744525547,
      "grad_norm": 11.472168922424316,
      "learning_rate": 3.073722627737227e-05,
      "loss": 1.098,
      "step": 52780
    },
    {
      "epoch": 385.3284671532847,
      "grad_norm": 11.513774871826172,
      "learning_rate": 3.073357664233577e-05,
      "loss": 0.6404,
      "step": 52790
    },
    {
      "epoch": 385.4014598540146,
      "grad_norm": 8.26507568359375,
      "learning_rate": 3.072992700729927e-05,
      "loss": 0.7443,
      "step": 52800
    },
    {
      "epoch": 385.4744525547445,
      "grad_norm": 7.136737823486328,
      "learning_rate": 3.0726277372262776e-05,
      "loss": 0.6878,
      "step": 52810
    },
    {
      "epoch": 385.54744525547443,
      "grad_norm": 0.1329936683177948,
      "learning_rate": 3.0722627737226276e-05,
      "loss": 1.1191,
      "step": 52820
    },
    {
      "epoch": 385.6204379562044,
      "grad_norm": 14.48595142364502,
      "learning_rate": 3.071897810218978e-05,
      "loss": 0.9672,
      "step": 52830
    },
    {
      "epoch": 385.6934306569343,
      "grad_norm": 0.21428367495536804,
      "learning_rate": 3.0715328467153284e-05,
      "loss": 0.9949,
      "step": 52840
    },
    {
      "epoch": 385.7664233576642,
      "grad_norm": 0.19079355895519257,
      "learning_rate": 3.071167883211679e-05,
      "loss": 0.9466,
      "step": 52850
    },
    {
      "epoch": 385.83941605839414,
      "grad_norm": 7.844901084899902,
      "learning_rate": 3.07080291970803e-05,
      "loss": 0.4275,
      "step": 52860
    },
    {
      "epoch": 385.9124087591241,
      "grad_norm": 7.702360153198242,
      "learning_rate": 3.070437956204379e-05,
      "loss": 0.9571,
      "step": 52870
    },
    {
      "epoch": 385.985401459854,
      "grad_norm": 11.98007583618164,
      "learning_rate": 3.07007299270073e-05,
      "loss": 1.0923,
      "step": 52880
    },
    {
      "epoch": 386.05839416058393,
      "grad_norm": 7.5761823654174805,
      "learning_rate": 3.069708029197081e-05,
      "loss": 1.0685,
      "step": 52890
    },
    {
      "epoch": 386.13138686131384,
      "grad_norm": 8.537726402282715,
      "learning_rate": 3.069343065693431e-05,
      "loss": 0.8262,
      "step": 52900
    },
    {
      "epoch": 386.2043795620438,
      "grad_norm": 15.367185592651367,
      "learning_rate": 3.0689781021897814e-05,
      "loss": 0.6086,
      "step": 52910
    },
    {
      "epoch": 386.2773722627737,
      "grad_norm": 15.47451400756836,
      "learning_rate": 3.0686131386861315e-05,
      "loss": 1.0129,
      "step": 52920
    },
    {
      "epoch": 386.35036496350364,
      "grad_norm": 13.42294979095459,
      "learning_rate": 3.068248175182482e-05,
      "loss": 1.2142,
      "step": 52930
    },
    {
      "epoch": 386.42335766423355,
      "grad_norm": 5.7621283531188965,
      "learning_rate": 3.067883211678832e-05,
      "loss": 0.7837,
      "step": 52940
    },
    {
      "epoch": 386.4963503649635,
      "grad_norm": 18.4200382232666,
      "learning_rate": 3.067518248175182e-05,
      "loss": 1.0167,
      "step": 52950
    },
    {
      "epoch": 386.56934306569343,
      "grad_norm": 0.08214499801397324,
      "learning_rate": 3.067153284671533e-05,
      "loss": 0.4753,
      "step": 52960
    },
    {
      "epoch": 386.64233576642334,
      "grad_norm": 13.458846092224121,
      "learning_rate": 3.066788321167883e-05,
      "loss": 1.0877,
      "step": 52970
    },
    {
      "epoch": 386.7153284671533,
      "grad_norm": 11.65816593170166,
      "learning_rate": 3.066423357664234e-05,
      "loss": 0.8134,
      "step": 52980
    },
    {
      "epoch": 386.7883211678832,
      "grad_norm": 0.12124409526586533,
      "learning_rate": 3.066058394160584e-05,
      "loss": 0.7078,
      "step": 52990
    },
    {
      "epoch": 386.86131386861314,
      "grad_norm": 6.8891520500183105,
      "learning_rate": 3.0656934306569346e-05,
      "loss": 1.0648,
      "step": 53000
    },
    {
      "epoch": 386.93430656934305,
      "grad_norm": 10.082266807556152,
      "learning_rate": 3.065328467153285e-05,
      "loss": 1.128,
      "step": 53010
    },
    {
      "epoch": 387.007299270073,
      "grad_norm": 14.265966415405273,
      "learning_rate": 3.064963503649635e-05,
      "loss": 1.1701,
      "step": 53020
    },
    {
      "epoch": 387.08029197080293,
      "grad_norm": 5.089621067047119,
      "learning_rate": 3.0645985401459854e-05,
      "loss": 0.703,
      "step": 53030
    },
    {
      "epoch": 387.15328467153284,
      "grad_norm": 9.885777473449707,
      "learning_rate": 3.0642335766423355e-05,
      "loss": 0.9371,
      "step": 53040
    },
    {
      "epoch": 387.22627737226276,
      "grad_norm": 10.057998657226562,
      "learning_rate": 3.063868613138686e-05,
      "loss": 0.7099,
      "step": 53050
    },
    {
      "epoch": 387.2992700729927,
      "grad_norm": 19.907968521118164,
      "learning_rate": 3.063503649635037e-05,
      "loss": 1.2772,
      "step": 53060
    },
    {
      "epoch": 387.37226277372264,
      "grad_norm": 11.979438781738281,
      "learning_rate": 3.063138686131387e-05,
      "loss": 0.8406,
      "step": 53070
    },
    {
      "epoch": 387.44525547445255,
      "grad_norm": 9.712946891784668,
      "learning_rate": 3.062773722627738e-05,
      "loss": 0.9175,
      "step": 53080
    },
    {
      "epoch": 387.51824817518246,
      "grad_norm": 1.2863754034042358,
      "learning_rate": 3.062408759124088e-05,
      "loss": 0.6857,
      "step": 53090
    },
    {
      "epoch": 387.59124087591243,
      "grad_norm": 5.787317752838135,
      "learning_rate": 3.062043795620438e-05,
      "loss": 0.7143,
      "step": 53100
    },
    {
      "epoch": 387.66423357664235,
      "grad_norm": 0.10281025618314743,
      "learning_rate": 3.0616788321167885e-05,
      "loss": 0.9549,
      "step": 53110
    },
    {
      "epoch": 387.73722627737226,
      "grad_norm": 13.885214805603027,
      "learning_rate": 3.0613138686131386e-05,
      "loss": 1.2222,
      "step": 53120
    },
    {
      "epoch": 387.81021897810217,
      "grad_norm": 7.1020307540893555,
      "learning_rate": 3.060948905109489e-05,
      "loss": 1.0976,
      "step": 53130
    },
    {
      "epoch": 387.88321167883214,
      "grad_norm": 12.354164123535156,
      "learning_rate": 3.0605839416058394e-05,
      "loss": 0.6912,
      "step": 53140
    },
    {
      "epoch": 387.95620437956205,
      "grad_norm": 14.231590270996094,
      "learning_rate": 3.06021897810219e-05,
      "loss": 1.0949,
      "step": 53150
    },
    {
      "epoch": 388.02919708029196,
      "grad_norm": 15.441779136657715,
      "learning_rate": 3.059854014598541e-05,
      "loss": 1.0009,
      "step": 53160
    },
    {
      "epoch": 388.1021897810219,
      "grad_norm": 5.556922912597656,
      "learning_rate": 3.059489051094891e-05,
      "loss": 0.8073,
      "step": 53170
    },
    {
      "epoch": 388.17518248175185,
      "grad_norm": 5.451104164123535,
      "learning_rate": 3.059124087591241e-05,
      "loss": 0.6551,
      "step": 53180
    },
    {
      "epoch": 388.24817518248176,
      "grad_norm": 0.21377217769622803,
      "learning_rate": 3.058759124087591e-05,
      "loss": 0.6105,
      "step": 53190
    },
    {
      "epoch": 388.3211678832117,
      "grad_norm": 10.789692878723145,
      "learning_rate": 3.058394160583942e-05,
      "loss": 0.8226,
      "step": 53200
    },
    {
      "epoch": 388.3941605839416,
      "grad_norm": 6.129809856414795,
      "learning_rate": 3.0580291970802924e-05,
      "loss": 1.0709,
      "step": 53210
    },
    {
      "epoch": 388.46715328467155,
      "grad_norm": 15.242424011230469,
      "learning_rate": 3.0576642335766425e-05,
      "loss": 0.8974,
      "step": 53220
    },
    {
      "epoch": 388.54014598540147,
      "grad_norm": 13.917901039123535,
      "learning_rate": 3.057299270072993e-05,
      "loss": 0.7896,
      "step": 53230
    },
    {
      "epoch": 388.6131386861314,
      "grad_norm": 10.703451156616211,
      "learning_rate": 3.056934306569343e-05,
      "loss": 1.0865,
      "step": 53240
    },
    {
      "epoch": 388.6861313868613,
      "grad_norm": 12.867568016052246,
      "learning_rate": 3.056569343065693e-05,
      "loss": 0.9913,
      "step": 53250
    },
    {
      "epoch": 388.75912408759126,
      "grad_norm": 0.10760363191366196,
      "learning_rate": 3.056204379562044e-05,
      "loss": 0.9638,
      "step": 53260
    },
    {
      "epoch": 388.8321167883212,
      "grad_norm": 5.240244388580322,
      "learning_rate": 3.055839416058394e-05,
      "loss": 1.1298,
      "step": 53270
    },
    {
      "epoch": 388.9051094890511,
      "grad_norm": 11.485957145690918,
      "learning_rate": 3.055474452554745e-05,
      "loss": 1.0691,
      "step": 53280
    },
    {
      "epoch": 388.978102189781,
      "grad_norm": 6.52330207824707,
      "learning_rate": 3.055109489051095e-05,
      "loss": 1.3015,
      "step": 53290
    },
    {
      "epoch": 389.05109489051097,
      "grad_norm": 9.54346752166748,
      "learning_rate": 3.0547445255474456e-05,
      "loss": 0.9903,
      "step": 53300
    },
    {
      "epoch": 389.1240875912409,
      "grad_norm": 9.879653930664062,
      "learning_rate": 3.0543795620437956e-05,
      "loss": 0.7717,
      "step": 53310
    },
    {
      "epoch": 389.1970802919708,
      "grad_norm": 3.8597888946533203,
      "learning_rate": 3.0540145985401463e-05,
      "loss": 0.7062,
      "step": 53320
    },
    {
      "epoch": 389.2700729927007,
      "grad_norm": 9.354389190673828,
      "learning_rate": 3.0536496350364964e-05,
      "loss": 0.8779,
      "step": 53330
    },
    {
      "epoch": 389.3430656934307,
      "grad_norm": 7.888070106506348,
      "learning_rate": 3.0532846715328464e-05,
      "loss": 0.847,
      "step": 53340
    },
    {
      "epoch": 389.4160583941606,
      "grad_norm": 9.743812561035156,
      "learning_rate": 3.052919708029197e-05,
      "loss": 1.0406,
      "step": 53350
    },
    {
      "epoch": 389.4890510948905,
      "grad_norm": 8.0924711227417,
      "learning_rate": 3.052554744525548e-05,
      "loss": 0.7604,
      "step": 53360
    },
    {
      "epoch": 389.5620437956204,
      "grad_norm": 0.12337004393339157,
      "learning_rate": 3.052189781021898e-05,
      "loss": 1.2724,
      "step": 53370
    },
    {
      "epoch": 389.6350364963504,
      "grad_norm": 8.70433521270752,
      "learning_rate": 3.051824817518249e-05,
      "loss": 1.1797,
      "step": 53380
    },
    {
      "epoch": 389.7080291970803,
      "grad_norm": 0.25893232226371765,
      "learning_rate": 3.0514598540145987e-05,
      "loss": 0.7207,
      "step": 53390
    },
    {
      "epoch": 389.7810218978102,
      "grad_norm": 19.899423599243164,
      "learning_rate": 3.0510948905109494e-05,
      "loss": 0.785,
      "step": 53400
    },
    {
      "epoch": 389.8540145985401,
      "grad_norm": 0.16851840913295746,
      "learning_rate": 3.050729927007299e-05,
      "loss": 0.6336,
      "step": 53410
    },
    {
      "epoch": 389.9270072992701,
      "grad_norm": 15.040874481201172,
      "learning_rate": 3.0503649635036495e-05,
      "loss": 1.287,
      "step": 53420
    },
    {
      "epoch": 390.0,
      "grad_norm": 16.986976623535156,
      "learning_rate": 3.05e-05,
      "loss": 1.0585,
      "step": 53430
    },
    {
      "epoch": 390.0729927007299,
      "grad_norm": 7.393978118896484,
      "learning_rate": 3.0496350364963507e-05,
      "loss": 0.6465,
      "step": 53440
    },
    {
      "epoch": 390.1459854014599,
      "grad_norm": 14.552056312561035,
      "learning_rate": 3.049270072992701e-05,
      "loss": 1.1621,
      "step": 53450
    },
    {
      "epoch": 390.2189781021898,
      "grad_norm": 16.616683959960938,
      "learning_rate": 3.0489051094890514e-05,
      "loss": 1.2628,
      "step": 53460
    },
    {
      "epoch": 390.2919708029197,
      "grad_norm": 8.49636173248291,
      "learning_rate": 3.0485401459854018e-05,
      "loss": 1.178,
      "step": 53470
    },
    {
      "epoch": 390.3649635036496,
      "grad_norm": 0.2711445391178131,
      "learning_rate": 3.048175182481752e-05,
      "loss": 0.9312,
      "step": 53480
    },
    {
      "epoch": 390.4379562043796,
      "grad_norm": 0.1178567036986351,
      "learning_rate": 3.0478102189781023e-05,
      "loss": 0.8676,
      "step": 53490
    },
    {
      "epoch": 390.5109489051095,
      "grad_norm": 10.962557792663574,
      "learning_rate": 3.0474452554744527e-05,
      "loss": 0.9433,
      "step": 53500
    },
    {
      "epoch": 390.5839416058394,
      "grad_norm": 8.060405731201172,
      "learning_rate": 3.047080291970803e-05,
      "loss": 1.0569,
      "step": 53510
    },
    {
      "epoch": 390.6569343065693,
      "grad_norm": 9.815657615661621,
      "learning_rate": 3.0467153284671534e-05,
      "loss": 0.6222,
      "step": 53520
    },
    {
      "epoch": 390.7299270072993,
      "grad_norm": 9.11962890625,
      "learning_rate": 3.0463503649635038e-05,
      "loss": 0.8206,
      "step": 53530
    },
    {
      "epoch": 390.8029197080292,
      "grad_norm": 8.364294052124023,
      "learning_rate": 3.0459854014598542e-05,
      "loss": 1.2067,
      "step": 53540
    },
    {
      "epoch": 390.8759124087591,
      "grad_norm": 15.392120361328125,
      "learning_rate": 3.045620437956205e-05,
      "loss": 0.9709,
      "step": 53550
    },
    {
      "epoch": 390.94890510948903,
      "grad_norm": 7.861448764801025,
      "learning_rate": 3.0452554744525546e-05,
      "loss": 0.8078,
      "step": 53560
    },
    {
      "epoch": 391.021897810219,
      "grad_norm": 7.583528518676758,
      "learning_rate": 3.044890510948905e-05,
      "loss": 0.4787,
      "step": 53570
    },
    {
      "epoch": 391.0948905109489,
      "grad_norm": 15.967777252197266,
      "learning_rate": 3.0445255474452554e-05,
      "loss": 1.1793,
      "step": 53580
    },
    {
      "epoch": 391.1678832116788,
      "grad_norm": 11.9183931350708,
      "learning_rate": 3.0441605839416058e-05,
      "loss": 0.7788,
      "step": 53590
    },
    {
      "epoch": 391.24087591240874,
      "grad_norm": 11.190400123596191,
      "learning_rate": 3.0437956204379565e-05,
      "loss": 0.7661,
      "step": 53600
    },
    {
      "epoch": 391.3138686131387,
      "grad_norm": 12.655533790588379,
      "learning_rate": 3.043430656934307e-05,
      "loss": 1.0622,
      "step": 53610
    },
    {
      "epoch": 391.3868613138686,
      "grad_norm": 10.115148544311523,
      "learning_rate": 3.0430656934306573e-05,
      "loss": 0.6279,
      "step": 53620
    },
    {
      "epoch": 391.45985401459853,
      "grad_norm": 6.185652732849121,
      "learning_rate": 3.0427007299270077e-05,
      "loss": 1.0503,
      "step": 53630
    },
    {
      "epoch": 391.53284671532845,
      "grad_norm": 10.11681079864502,
      "learning_rate": 3.0423357664233578e-05,
      "loss": 1.4165,
      "step": 53640
    },
    {
      "epoch": 391.6058394160584,
      "grad_norm": 11.55864143371582,
      "learning_rate": 3.041970802919708e-05,
      "loss": 1.0509,
      "step": 53650
    },
    {
      "epoch": 391.6788321167883,
      "grad_norm": 7.478349208831787,
      "learning_rate": 3.0416058394160585e-05,
      "loss": 0.6502,
      "step": 53660
    },
    {
      "epoch": 391.75182481751824,
      "grad_norm": 13.26931381225586,
      "learning_rate": 3.041240875912409e-05,
      "loss": 0.9726,
      "step": 53670
    },
    {
      "epoch": 391.82481751824815,
      "grad_norm": 6.821488857269287,
      "learning_rate": 3.0408759124087593e-05,
      "loss": 0.6965,
      "step": 53680
    },
    {
      "epoch": 391.8978102189781,
      "grad_norm": 17.148773193359375,
      "learning_rate": 3.0405109489051097e-05,
      "loss": 1.2816,
      "step": 53690
    },
    {
      "epoch": 391.97080291970804,
      "grad_norm": 8.6697359085083,
      "learning_rate": 3.04014598540146e-05,
      "loss": 0.8202,
      "step": 53700
    },
    {
      "epoch": 392.04379562043795,
      "grad_norm": 8.614547729492188,
      "learning_rate": 3.03978102189781e-05,
      "loss": 0.7702,
      "step": 53710
    },
    {
      "epoch": 392.11678832116786,
      "grad_norm": 1.4935756921768188,
      "learning_rate": 3.0394160583941605e-05,
      "loss": 0.977,
      "step": 53720
    },
    {
      "epoch": 392.18978102189783,
      "grad_norm": 5.768991947174072,
      "learning_rate": 3.039051094890511e-05,
      "loss": 0.7767,
      "step": 53730
    },
    {
      "epoch": 392.26277372262774,
      "grad_norm": 9.468351364135742,
      "learning_rate": 3.0386861313868613e-05,
      "loss": 0.9005,
      "step": 53740
    },
    {
      "epoch": 392.33576642335765,
      "grad_norm": 0.07030736654996872,
      "learning_rate": 3.038321167883212e-05,
      "loss": 0.5984,
      "step": 53750
    },
    {
      "epoch": 392.40875912408757,
      "grad_norm": 6.71672248840332,
      "learning_rate": 3.0379562043795624e-05,
      "loss": 0.9403,
      "step": 53760
    },
    {
      "epoch": 392.48175182481754,
      "grad_norm": 7.300270080566406,
      "learning_rate": 3.0375912408759128e-05,
      "loss": 1.1976,
      "step": 53770
    },
    {
      "epoch": 392.55474452554745,
      "grad_norm": 17.210205078125,
      "learning_rate": 3.0372262773722632e-05,
      "loss": 0.6251,
      "step": 53780
    },
    {
      "epoch": 392.62773722627736,
      "grad_norm": 23.10416030883789,
      "learning_rate": 3.036861313868613e-05,
      "loss": 0.74,
      "step": 53790
    },
    {
      "epoch": 392.7007299270073,
      "grad_norm": 9.45919132232666,
      "learning_rate": 3.0364963503649636e-05,
      "loss": 1.055,
      "step": 53800
    },
    {
      "epoch": 392.77372262773724,
      "grad_norm": 16.443504333496094,
      "learning_rate": 3.036131386861314e-05,
      "loss": 1.5777,
      "step": 53810
    },
    {
      "epoch": 392.84671532846716,
      "grad_norm": 9.198728561401367,
      "learning_rate": 3.0357664233576644e-05,
      "loss": 0.8145,
      "step": 53820
    },
    {
      "epoch": 392.91970802919707,
      "grad_norm": 11.860106468200684,
      "learning_rate": 3.0354014598540148e-05,
      "loss": 0.8044,
      "step": 53830
    },
    {
      "epoch": 392.992700729927,
      "grad_norm": 13.587075233459473,
      "learning_rate": 3.0350364963503652e-05,
      "loss": 1.0122,
      "step": 53840
    },
    {
      "epoch": 393.06569343065695,
      "grad_norm": 15.482526779174805,
      "learning_rate": 3.0346715328467156e-05,
      "loss": 0.961,
      "step": 53850
    },
    {
      "epoch": 393.13868613138686,
      "grad_norm": 7.96188497543335,
      "learning_rate": 3.0343065693430656e-05,
      "loss": 1.0636,
      "step": 53860
    },
    {
      "epoch": 393.2116788321168,
      "grad_norm": 7.9280314445495605,
      "learning_rate": 3.033941605839416e-05,
      "loss": 0.6569,
      "step": 53870
    },
    {
      "epoch": 393.2846715328467,
      "grad_norm": 17.36688804626465,
      "learning_rate": 3.0335766423357664e-05,
      "loss": 1.0636,
      "step": 53880
    },
    {
      "epoch": 393.35766423357666,
      "grad_norm": 6.953378200531006,
      "learning_rate": 3.0332116788321168e-05,
      "loss": 0.9346,
      "step": 53890
    },
    {
      "epoch": 393.43065693430657,
      "grad_norm": 14.721413612365723,
      "learning_rate": 3.032846715328467e-05,
      "loss": 0.999,
      "step": 53900
    },
    {
      "epoch": 393.5036496350365,
      "grad_norm": 12.939574241638184,
      "learning_rate": 3.032481751824818e-05,
      "loss": 0.7722,
      "step": 53910
    },
    {
      "epoch": 393.57664233576645,
      "grad_norm": 10.193140983581543,
      "learning_rate": 3.0321167883211683e-05,
      "loss": 0.6426,
      "step": 53920
    },
    {
      "epoch": 393.64963503649636,
      "grad_norm": 6.988604545593262,
      "learning_rate": 3.0317518248175187e-05,
      "loss": 0.8631,
      "step": 53930
    },
    {
      "epoch": 393.7226277372263,
      "grad_norm": 7.151669502258301,
      "learning_rate": 3.0313868613138684e-05,
      "loss": 1.0505,
      "step": 53940
    },
    {
      "epoch": 393.7956204379562,
      "grad_norm": 6.232300758361816,
      "learning_rate": 3.031021897810219e-05,
      "loss": 1.145,
      "step": 53950
    },
    {
      "epoch": 393.86861313868616,
      "grad_norm": 8.617712020874023,
      "learning_rate": 3.0306569343065695e-05,
      "loss": 1.0071,
      "step": 53960
    },
    {
      "epoch": 393.94160583941607,
      "grad_norm": 0.10918319225311279,
      "learning_rate": 3.03029197080292e-05,
      "loss": 0.9962,
      "step": 53970
    },
    {
      "epoch": 394.014598540146,
      "grad_norm": 10.150083541870117,
      "learning_rate": 3.0299270072992703e-05,
      "loss": 0.8468,
      "step": 53980
    },
    {
      "epoch": 394.0875912408759,
      "grad_norm": 7.600297927856445,
      "learning_rate": 3.0295620437956207e-05,
      "loss": 0.6072,
      "step": 53990
    },
    {
      "epoch": 394.16058394160586,
      "grad_norm": 0.21015161275863647,
      "learning_rate": 3.029197080291971e-05,
      "loss": 1.294,
      "step": 54000
    },
    {
      "epoch": 394.2335766423358,
      "grad_norm": 7.119481086730957,
      "learning_rate": 3.0288321167883214e-05,
      "loss": 1.1287,
      "step": 54010
    },
    {
      "epoch": 394.3065693430657,
      "grad_norm": 10.759302139282227,
      "learning_rate": 3.0284671532846715e-05,
      "loss": 0.5061,
      "step": 54020
    },
    {
      "epoch": 394.3795620437956,
      "grad_norm": 6.10264253616333,
      "learning_rate": 3.028102189781022e-05,
      "loss": 0.9558,
      "step": 54030
    },
    {
      "epoch": 394.45255474452557,
      "grad_norm": 17.051494598388672,
      "learning_rate": 3.0277372262773723e-05,
      "loss": 0.9829,
      "step": 54040
    },
    {
      "epoch": 394.5255474452555,
      "grad_norm": 5.7304205894470215,
      "learning_rate": 3.0273722627737227e-05,
      "loss": 1.0152,
      "step": 54050
    },
    {
      "epoch": 394.5985401459854,
      "grad_norm": 0.08243349194526672,
      "learning_rate": 3.027007299270073e-05,
      "loss": 0.9572,
      "step": 54060
    },
    {
      "epoch": 394.6715328467153,
      "grad_norm": 0.14629459381103516,
      "learning_rate": 3.0266423357664238e-05,
      "loss": 0.8322,
      "step": 54070
    },
    {
      "epoch": 394.7445255474453,
      "grad_norm": 14.969488143920898,
      "learning_rate": 3.026277372262774e-05,
      "loss": 1.0989,
      "step": 54080
    },
    {
      "epoch": 394.8175182481752,
      "grad_norm": 0.17266619205474854,
      "learning_rate": 3.025912408759124e-05,
      "loss": 1.0822,
      "step": 54090
    },
    {
      "epoch": 394.8905109489051,
      "grad_norm": 18.799360275268555,
      "learning_rate": 3.0255474452554743e-05,
      "loss": 0.8668,
      "step": 54100
    },
    {
      "epoch": 394.963503649635,
      "grad_norm": 11.487515449523926,
      "learning_rate": 3.025182481751825e-05,
      "loss": 0.9265,
      "step": 54110
    },
    {
      "epoch": 395.036496350365,
      "grad_norm": 6.034682273864746,
      "learning_rate": 3.0248175182481754e-05,
      "loss": 0.7291,
      "step": 54120
    },
    {
      "epoch": 395.1094890510949,
      "grad_norm": 16.941402435302734,
      "learning_rate": 3.0244525547445258e-05,
      "loss": 1.4373,
      "step": 54130
    },
    {
      "epoch": 395.1824817518248,
      "grad_norm": 16.02777671813965,
      "learning_rate": 3.024087591240876e-05,
      "loss": 0.8262,
      "step": 54140
    },
    {
      "epoch": 395.2554744525547,
      "grad_norm": 19.219161987304688,
      "learning_rate": 3.0237226277372265e-05,
      "loss": 0.9023,
      "step": 54150
    },
    {
      "epoch": 395.3284671532847,
      "grad_norm": 7.584995269775391,
      "learning_rate": 3.023357664233577e-05,
      "loss": 0.9667,
      "step": 54160
    },
    {
      "epoch": 395.4014598540146,
      "grad_norm": 10.809917449951172,
      "learning_rate": 3.022992700729927e-05,
      "loss": 0.6888,
      "step": 54170
    },
    {
      "epoch": 395.4744525547445,
      "grad_norm": 11.751788139343262,
      "learning_rate": 3.0226277372262774e-05,
      "loss": 0.875,
      "step": 54180
    },
    {
      "epoch": 395.54744525547443,
      "grad_norm": 7.9817023277282715,
      "learning_rate": 3.0222627737226277e-05,
      "loss": 0.5966,
      "step": 54190
    },
    {
      "epoch": 395.6204379562044,
      "grad_norm": 9.369027137756348,
      "learning_rate": 3.021897810218978e-05,
      "loss": 0.7322,
      "step": 54200
    },
    {
      "epoch": 395.6934306569343,
      "grad_norm": 13.451878547668457,
      "learning_rate": 3.0215328467153285e-05,
      "loss": 1.1296,
      "step": 54210
    },
    {
      "epoch": 395.7664233576642,
      "grad_norm": 13.026347160339355,
      "learning_rate": 3.0211678832116793e-05,
      "loss": 0.8998,
      "step": 54220
    },
    {
      "epoch": 395.83941605839414,
      "grad_norm": 0.2689895033836365,
      "learning_rate": 3.0208029197080296e-05,
      "loss": 0.8357,
      "step": 54230
    },
    {
      "epoch": 395.9124087591241,
      "grad_norm": 0.09277353435754776,
      "learning_rate": 3.02043795620438e-05,
      "loss": 0.9578,
      "step": 54240
    },
    {
      "epoch": 395.985401459854,
      "grad_norm": 5.222011566162109,
      "learning_rate": 3.0200729927007297e-05,
      "loss": 1.0363,
      "step": 54250
    },
    {
      "epoch": 396.05839416058393,
      "grad_norm": 7.529045104980469,
      "learning_rate": 3.01970802919708e-05,
      "loss": 0.643,
      "step": 54260
    },
    {
      "epoch": 396.13138686131384,
      "grad_norm": 10.834539413452148,
      "learning_rate": 3.019343065693431e-05,
      "loss": 1.0711,
      "step": 54270
    },
    {
      "epoch": 396.2043795620438,
      "grad_norm": 9.21463680267334,
      "learning_rate": 3.0189781021897812e-05,
      "loss": 1.1257,
      "step": 54280
    },
    {
      "epoch": 396.2773722627737,
      "grad_norm": 5.939979553222656,
      "learning_rate": 3.0186131386861316e-05,
      "loss": 1.0394,
      "step": 54290
    },
    {
      "epoch": 396.35036496350364,
      "grad_norm": 17.200944900512695,
      "learning_rate": 3.018248175182482e-05,
      "loss": 0.9991,
      "step": 54300
    },
    {
      "epoch": 396.42335766423355,
      "grad_norm": 8.366756439208984,
      "learning_rate": 3.0178832116788324e-05,
      "loss": 0.8726,
      "step": 54310
    },
    {
      "epoch": 396.4963503649635,
      "grad_norm": 13.900303840637207,
      "learning_rate": 3.0175182481751825e-05,
      "loss": 0.9852,
      "step": 54320
    },
    {
      "epoch": 396.56934306569343,
      "grad_norm": 9.631930351257324,
      "learning_rate": 3.017153284671533e-05,
      "loss": 0.4516,
      "step": 54330
    },
    {
      "epoch": 396.64233576642334,
      "grad_norm": 8.099897384643555,
      "learning_rate": 3.0167883211678832e-05,
      "loss": 1.0665,
      "step": 54340
    },
    {
      "epoch": 396.7153284671533,
      "grad_norm": 17.9448299407959,
      "learning_rate": 3.0164233576642336e-05,
      "loss": 0.8036,
      "step": 54350
    },
    {
      "epoch": 396.7883211678832,
      "grad_norm": 11.452327728271484,
      "learning_rate": 3.016058394160584e-05,
      "loss": 0.9158,
      "step": 54360
    },
    {
      "epoch": 396.86131386861314,
      "grad_norm": 8.579863548278809,
      "learning_rate": 3.0156934306569344e-05,
      "loss": 0.8111,
      "step": 54370
    },
    {
      "epoch": 396.93430656934305,
      "grad_norm": 0.0988997295498848,
      "learning_rate": 3.015328467153285e-05,
      "loss": 0.983,
      "step": 54380
    },
    {
      "epoch": 397.007299270073,
      "grad_norm": 13.455649375915527,
      "learning_rate": 3.0149635036496355e-05,
      "loss": 1.1699,
      "step": 54390
    },
    {
      "epoch": 397.08029197080293,
      "grad_norm": 8.594429016113281,
      "learning_rate": 3.0145985401459852e-05,
      "loss": 0.8489,
      "step": 54400
    },
    {
      "epoch": 397.15328467153284,
      "grad_norm": 9.054977416992188,
      "learning_rate": 3.0142335766423356e-05,
      "loss": 1.0179,
      "step": 54410
    },
    {
      "epoch": 397.22627737226276,
      "grad_norm": 9.717170715332031,
      "learning_rate": 3.0138686131386863e-05,
      "loss": 0.7745,
      "step": 54420
    },
    {
      "epoch": 397.2992700729927,
      "grad_norm": 15.11027717590332,
      "learning_rate": 3.0135036496350367e-05,
      "loss": 1.0521,
      "step": 54430
    },
    {
      "epoch": 397.37226277372264,
      "grad_norm": 13.337474822998047,
      "learning_rate": 3.013138686131387e-05,
      "loss": 0.6277,
      "step": 54440
    },
    {
      "epoch": 397.44525547445255,
      "grad_norm": 7.387268543243408,
      "learning_rate": 3.0127737226277375e-05,
      "loss": 0.8603,
      "step": 54450
    },
    {
      "epoch": 397.51824817518246,
      "grad_norm": 9.322356224060059,
      "learning_rate": 3.012408759124088e-05,
      "loss": 0.8634,
      "step": 54460
    },
    {
      "epoch": 397.59124087591243,
      "grad_norm": 14.898468017578125,
      "learning_rate": 3.0120437956204383e-05,
      "loss": 0.97,
      "step": 54470
    },
    {
      "epoch": 397.66423357664235,
      "grad_norm": 0.10958708822727203,
      "learning_rate": 3.0116788321167883e-05,
      "loss": 0.762,
      "step": 54480
    },
    {
      "epoch": 397.73722627737226,
      "grad_norm": 23.189407348632812,
      "learning_rate": 3.0113138686131387e-05,
      "loss": 0.8003,
      "step": 54490
    },
    {
      "epoch": 397.81021897810217,
      "grad_norm": 6.9874444007873535,
      "learning_rate": 3.010948905109489e-05,
      "loss": 0.6105,
      "step": 54500
    },
    {
      "epoch": 397.88321167883214,
      "grad_norm": 9.553321838378906,
      "learning_rate": 3.0105839416058395e-05,
      "loss": 1.2904,
      "step": 54510
    },
    {
      "epoch": 397.95620437956205,
      "grad_norm": 7.232911109924316,
      "learning_rate": 3.01021897810219e-05,
      "loss": 0.956,
      "step": 54520
    },
    {
      "epoch": 398.02919708029196,
      "grad_norm": 10.179986953735352,
      "learning_rate": 3.0098540145985403e-05,
      "loss": 1.1415,
      "step": 54530
    },
    {
      "epoch": 398.1021897810219,
      "grad_norm": 8.680708885192871,
      "learning_rate": 3.009489051094891e-05,
      "loss": 1.0678,
      "step": 54540
    },
    {
      "epoch": 398.17518248175185,
      "grad_norm": 12.053340911865234,
      "learning_rate": 3.0091240875912407e-05,
      "loss": 1.0439,
      "step": 54550
    },
    {
      "epoch": 398.24817518248176,
      "grad_norm": 8.554119110107422,
      "learning_rate": 3.008759124087591e-05,
      "loss": 0.8901,
      "step": 54560
    },
    {
      "epoch": 398.3211678832117,
      "grad_norm": 1.6664509773254395,
      "learning_rate": 3.0083941605839415e-05,
      "loss": 0.7841,
      "step": 54570
    },
    {
      "epoch": 398.3941605839416,
      "grad_norm": 0.06047866493463516,
      "learning_rate": 3.0080291970802922e-05,
      "loss": 0.5465,
      "step": 54580
    },
    {
      "epoch": 398.46715328467155,
      "grad_norm": 11.47007942199707,
      "learning_rate": 3.0076642335766426e-05,
      "loss": 0.6993,
      "step": 54590
    },
    {
      "epoch": 398.54014598540147,
      "grad_norm": 19.19444465637207,
      "learning_rate": 3.007299270072993e-05,
      "loss": 1.3417,
      "step": 54600
    },
    {
      "epoch": 398.6131386861314,
      "grad_norm": 0.0881572887301445,
      "learning_rate": 3.0069343065693434e-05,
      "loss": 0.8138,
      "step": 54610
    },
    {
      "epoch": 398.6861313868613,
      "grad_norm": 14.407912254333496,
      "learning_rate": 3.0065693430656938e-05,
      "loss": 0.8508,
      "step": 54620
    },
    {
      "epoch": 398.75912408759126,
      "grad_norm": 8.426005363464355,
      "learning_rate": 3.0062043795620438e-05,
      "loss": 0.8122,
      "step": 54630
    },
    {
      "epoch": 398.8321167883212,
      "grad_norm": 14.03278636932373,
      "learning_rate": 3.0058394160583942e-05,
      "loss": 1.0353,
      "step": 54640
    },
    {
      "epoch": 398.9051094890511,
      "grad_norm": 7.532580375671387,
      "learning_rate": 3.0054744525547446e-05,
      "loss": 1.1537,
      "step": 54650
    },
    {
      "epoch": 398.978102189781,
      "grad_norm": 6.459158897399902,
      "learning_rate": 3.005109489051095e-05,
      "loss": 1.2423,
      "step": 54660
    },
    {
      "epoch": 399.05109489051097,
      "grad_norm": 4.579959869384766,
      "learning_rate": 3.0047445255474454e-05,
      "loss": 0.5909,
      "step": 54670
    },
    {
      "epoch": 399.1240875912409,
      "grad_norm": 0.1291552484035492,
      "learning_rate": 3.0043795620437958e-05,
      "loss": 1.139,
      "step": 54680
    },
    {
      "epoch": 399.1970802919708,
      "grad_norm": 6.8386759757995605,
      "learning_rate": 3.0040145985401465e-05,
      "loss": 1.0808,
      "step": 54690
    },
    {
      "epoch": 399.2700729927007,
      "grad_norm": 6.493782997131348,
      "learning_rate": 3.003649635036497e-05,
      "loss": 0.6466,
      "step": 54700
    },
    {
      "epoch": 399.3430656934307,
      "grad_norm": 7.276368618011475,
      "learning_rate": 3.0032846715328466e-05,
      "loss": 1.0782,
      "step": 54710
    },
    {
      "epoch": 399.4160583941606,
      "grad_norm": 9.00342082977295,
      "learning_rate": 3.002919708029197e-05,
      "loss": 0.8426,
      "step": 54720
    },
    {
      "epoch": 399.4890510948905,
      "grad_norm": 9.923859596252441,
      "learning_rate": 3.0025547445255474e-05,
      "loss": 1.1853,
      "step": 54730
    },
    {
      "epoch": 399.5620437956204,
      "grad_norm": 5.226893424987793,
      "learning_rate": 3.002189781021898e-05,
      "loss": 1.1559,
      "step": 54740
    },
    {
      "epoch": 399.6350364963504,
      "grad_norm": 16.6968994140625,
      "learning_rate": 3.0018248175182485e-05,
      "loss": 0.859,
      "step": 54750
    },
    {
      "epoch": 399.7080291970803,
      "grad_norm": 21.726591110229492,
      "learning_rate": 3.001459854014599e-05,
      "loss": 1.1405,
      "step": 54760
    },
    {
      "epoch": 399.7810218978102,
      "grad_norm": 9.28013801574707,
      "learning_rate": 3.0010948905109493e-05,
      "loss": 0.6646,
      "step": 54770
    },
    {
      "epoch": 399.8540145985401,
      "grad_norm": 0.28344473242759705,
      "learning_rate": 3.0007299270072993e-05,
      "loss": 0.8291,
      "step": 54780
    },
    {
      "epoch": 399.9270072992701,
      "grad_norm": 6.630227088928223,
      "learning_rate": 3.0003649635036497e-05,
      "loss": 0.4751,
      "step": 54790
    },
    {
      "epoch": 400.0,
      "grad_norm": 0.10695812851190567,
      "learning_rate": 3e-05,
      "loss": 0.8211,
      "step": 54800
    },
    {
      "epoch": 400.0729927007299,
      "grad_norm": 9.561347961425781,
      "learning_rate": 2.9996350364963505e-05,
      "loss": 0.8989,
      "step": 54810
    },
    {
      "epoch": 400.1459854014599,
      "grad_norm": 5.719290733337402,
      "learning_rate": 2.999270072992701e-05,
      "loss": 0.7385,
      "step": 54820
    },
    {
      "epoch": 400.2189781021898,
      "grad_norm": 0.13195686042308807,
      "learning_rate": 2.9989051094890512e-05,
      "loss": 0.5615,
      "step": 54830
    },
    {
      "epoch": 400.2919708029197,
      "grad_norm": 0.25632286071777344,
      "learning_rate": 2.9985401459854016e-05,
      "loss": 0.6469,
      "step": 54840
    },
    {
      "epoch": 400.3649635036496,
      "grad_norm": 8.908780097961426,
      "learning_rate": 2.9981751824817524e-05,
      "loss": 1.0547,
      "step": 54850
    },
    {
      "epoch": 400.4379562043796,
      "grad_norm": 10.873025894165039,
      "learning_rate": 2.997810218978102e-05,
      "loss": 1.0179,
      "step": 54860
    },
    {
      "epoch": 400.5109489051095,
      "grad_norm": 12.646267890930176,
      "learning_rate": 2.9974452554744525e-05,
      "loss": 0.9394,
      "step": 54870
    },
    {
      "epoch": 400.5839416058394,
      "grad_norm": 7.36752462387085,
      "learning_rate": 2.997080291970803e-05,
      "loss": 1.0513,
      "step": 54880
    },
    {
      "epoch": 400.6569343065693,
      "grad_norm": 6.278110980987549,
      "learning_rate": 2.9967153284671536e-05,
      "loss": 1.0635,
      "step": 54890
    },
    {
      "epoch": 400.7299270072993,
      "grad_norm": 15.972332954406738,
      "learning_rate": 2.996350364963504e-05,
      "loss": 1.2262,
      "step": 54900
    },
    {
      "epoch": 400.8029197080292,
      "grad_norm": 18.71457290649414,
      "learning_rate": 2.9959854014598543e-05,
      "loss": 0.6945,
      "step": 54910
    },
    {
      "epoch": 400.8759124087591,
      "grad_norm": 9.517508506774902,
      "learning_rate": 2.9956204379562047e-05,
      "loss": 0.7972,
      "step": 54920
    },
    {
      "epoch": 400.94890510948903,
      "grad_norm": 11.62142276763916,
      "learning_rate": 2.9952554744525544e-05,
      "loss": 1.1646,
      "step": 54930
    },
    {
      "epoch": 401.021897810219,
      "grad_norm": 13.23473834991455,
      "learning_rate": 2.9948905109489052e-05,
      "loss": 0.9162,
      "step": 54940
    },
    {
      "epoch": 401.0948905109489,
      "grad_norm": 15.154946327209473,
      "learning_rate": 2.9945255474452556e-05,
      "loss": 1.1362,
      "step": 54950
    },
    {
      "epoch": 401.1678832116788,
      "grad_norm": 5.59853982925415,
      "learning_rate": 2.994160583941606e-05,
      "loss": 0.9374,
      "step": 54960
    },
    {
      "epoch": 401.24087591240874,
      "grad_norm": 7.342308044433594,
      "learning_rate": 2.9937956204379563e-05,
      "loss": 0.825,
      "step": 54970
    },
    {
      "epoch": 401.3138686131387,
      "grad_norm": 10.365538597106934,
      "learning_rate": 2.9934306569343067e-05,
      "loss": 1.0988,
      "step": 54980
    },
    {
      "epoch": 401.3868613138686,
      "grad_norm": 4.787918567657471,
      "learning_rate": 2.993065693430657e-05,
      "loss": 0.7805,
      "step": 54990
    },
    {
      "epoch": 401.45985401459853,
      "grad_norm": 4.753120422363281,
      "learning_rate": 2.992700729927008e-05,
      "loss": 0.4309,
      "step": 55000
    },
    {
      "epoch": 401.53284671532845,
      "grad_norm": 19.097156524658203,
      "learning_rate": 2.9923357664233576e-05,
      "loss": 1.2372,
      "step": 55010
    },
    {
      "epoch": 401.6058394160584,
      "grad_norm": 13.820669174194336,
      "learning_rate": 2.991970802919708e-05,
      "loss": 0.727,
      "step": 55020
    },
    {
      "epoch": 401.6788321167883,
      "grad_norm": 0.09334590286016464,
      "learning_rate": 2.9916058394160583e-05,
      "loss": 0.9456,
      "step": 55030
    },
    {
      "epoch": 401.75182481751824,
      "grad_norm": 17.29244613647461,
      "learning_rate": 2.9912408759124087e-05,
      "loss": 1.1028,
      "step": 55040
    },
    {
      "epoch": 401.82481751824815,
      "grad_norm": 10.292257308959961,
      "learning_rate": 2.9908759124087594e-05,
      "loss": 1.107,
      "step": 55050
    },
    {
      "epoch": 401.8978102189781,
      "grad_norm": 1.0811890363693237,
      "learning_rate": 2.99051094890511e-05,
      "loss": 0.9405,
      "step": 55060
    },
    {
      "epoch": 401.97080291970804,
      "grad_norm": 9.354171752929688,
      "learning_rate": 2.9901459854014602e-05,
      "loss": 0.8317,
      "step": 55070
    },
    {
      "epoch": 402.04379562043795,
      "grad_norm": 5.7919745445251465,
      "learning_rate": 2.9897810218978106e-05,
      "loss": 1.1594,
      "step": 55080
    },
    {
      "epoch": 402.11678832116786,
      "grad_norm": 5.6477131843566895,
      "learning_rate": 2.9894160583941607e-05,
      "loss": 1.0222,
      "step": 55090
    },
    {
      "epoch": 402.18978102189783,
      "grad_norm": 7.70977783203125,
      "learning_rate": 2.989051094890511e-05,
      "loss": 1.0378,
      "step": 55100
    },
    {
      "epoch": 402.26277372262774,
      "grad_norm": 11.439779281616211,
      "learning_rate": 2.9886861313868614e-05,
      "loss": 0.5801,
      "step": 55110
    },
    {
      "epoch": 402.33576642335765,
      "grad_norm": 0.2472541332244873,
      "learning_rate": 2.9883211678832118e-05,
      "loss": 1.0927,
      "step": 55120
    },
    {
      "epoch": 402.40875912408757,
      "grad_norm": 8.641456604003906,
      "learning_rate": 2.9879562043795622e-05,
      "loss": 1.2509,
      "step": 55130
    },
    {
      "epoch": 402.48175182481754,
      "grad_norm": 13.964714050292969,
      "learning_rate": 2.9875912408759126e-05,
      "loss": 0.7837,
      "step": 55140
    },
    {
      "epoch": 402.55474452554745,
      "grad_norm": 8.118122100830078,
      "learning_rate": 2.987226277372263e-05,
      "loss": 0.7074,
      "step": 55150
    },
    {
      "epoch": 402.62773722627736,
      "grad_norm": 14.082832336425781,
      "learning_rate": 2.986861313868613e-05,
      "loss": 0.9975,
      "step": 55160
    },
    {
      "epoch": 402.7007299270073,
      "grad_norm": 11.034299850463867,
      "learning_rate": 2.9864963503649634e-05,
      "loss": 1.0753,
      "step": 55170
    },
    {
      "epoch": 402.77372262773724,
      "grad_norm": 8.676141738891602,
      "learning_rate": 2.9861313868613138e-05,
      "loss": 0.8963,
      "step": 55180
    },
    {
      "epoch": 402.84671532846716,
      "grad_norm": 14.463120460510254,
      "learning_rate": 2.9857664233576642e-05,
      "loss": 0.9576,
      "step": 55190
    },
    {
      "epoch": 402.91970802919707,
      "grad_norm": 9.498193740844727,
      "learning_rate": 2.985401459854015e-05,
      "loss": 1.1506,
      "step": 55200
    },
    {
      "epoch": 402.992700729927,
      "grad_norm": 0.0675286203622818,
      "learning_rate": 2.9850364963503653e-05,
      "loss": 0.5262,
      "step": 55210
    },
    {
      "epoch": 403.06569343065695,
      "grad_norm": 12.488863945007324,
      "learning_rate": 2.9846715328467157e-05,
      "loss": 1.1813,
      "step": 55220
    },
    {
      "epoch": 403.13868613138686,
      "grad_norm": 7.1537628173828125,
      "learning_rate": 2.984306569343066e-05,
      "loss": 0.726,
      "step": 55230
    },
    {
      "epoch": 403.2116788321168,
      "grad_norm": 0.09486031532287598,
      "learning_rate": 2.9839416058394158e-05,
      "loss": 0.7562,
      "step": 55240
    },
    {
      "epoch": 403.2846715328467,
      "grad_norm": 7.653226375579834,
      "learning_rate": 2.9835766423357665e-05,
      "loss": 0.8292,
      "step": 55250
    },
    {
      "epoch": 403.35766423357666,
      "grad_norm": 0.1781153827905655,
      "learning_rate": 2.983211678832117e-05,
      "loss": 1.517,
      "step": 55260
    },
    {
      "epoch": 403.43065693430657,
      "grad_norm": 7.858768463134766,
      "learning_rate": 2.9828467153284673e-05,
      "loss": 0.7285,
      "step": 55270
    },
    {
      "epoch": 403.5036496350365,
      "grad_norm": 0.12605033814907074,
      "learning_rate": 2.9824817518248177e-05,
      "loss": 0.9039,
      "step": 55280
    },
    {
      "epoch": 403.57664233576645,
      "grad_norm": 6.996054172515869,
      "learning_rate": 2.982116788321168e-05,
      "loss": 0.997,
      "step": 55290
    },
    {
      "epoch": 403.64963503649636,
      "grad_norm": 9.780019760131836,
      "learning_rate": 2.9817518248175185e-05,
      "loss": 0.7083,
      "step": 55300
    },
    {
      "epoch": 403.7226277372263,
      "grad_norm": 5.25396203994751,
      "learning_rate": 2.981386861313869e-05,
      "loss": 0.5201,
      "step": 55310
    },
    {
      "epoch": 403.7956204379562,
      "grad_norm": 9.189799308776855,
      "learning_rate": 2.981021897810219e-05,
      "loss": 1.0199,
      "step": 55320
    },
    {
      "epoch": 403.86861313868616,
      "grad_norm": 8.238616943359375,
      "learning_rate": 2.9806569343065693e-05,
      "loss": 1.0373,
      "step": 55330
    },
    {
      "epoch": 403.94160583941607,
      "grad_norm": 13.620631217956543,
      "learning_rate": 2.9802919708029197e-05,
      "loss": 0.9859,
      "step": 55340
    },
    {
      "epoch": 404.014598540146,
      "grad_norm": 0.2165360301733017,
      "learning_rate": 2.97992700729927e-05,
      "loss": 0.8529,
      "step": 55350
    },
    {
      "epoch": 404.0875912408759,
      "grad_norm": 0.42388761043548584,
      "learning_rate": 2.9795620437956208e-05,
      "loss": 0.4855,
      "step": 55360
    },
    {
      "epoch": 404.16058394160586,
      "grad_norm": 0.3603249788284302,
      "learning_rate": 2.9791970802919712e-05,
      "loss": 1.0509,
      "step": 55370
    },
    {
      "epoch": 404.2335766423358,
      "grad_norm": 17.57247543334961,
      "learning_rate": 2.9788321167883216e-05,
      "loss": 0.9713,
      "step": 55380
    },
    {
      "epoch": 404.3065693430657,
      "grad_norm": 13.667634963989258,
      "learning_rate": 2.9784671532846713e-05,
      "loss": 1.145,
      "step": 55390
    },
    {
      "epoch": 404.3795620437956,
      "grad_norm": 17.66074562072754,
      "learning_rate": 2.9781021897810217e-05,
      "loss": 0.7082,
      "step": 55400
    },
    {
      "epoch": 404.45255474452557,
      "grad_norm": 9.218831062316895,
      "learning_rate": 2.9777372262773724e-05,
      "loss": 1.0952,
      "step": 55410
    },
    {
      "epoch": 404.5255474452555,
      "grad_norm": 8.340847969055176,
      "learning_rate": 2.9773722627737228e-05,
      "loss": 0.8984,
      "step": 55420
    },
    {
      "epoch": 404.5985401459854,
      "grad_norm": 11.02440357208252,
      "learning_rate": 2.9770072992700732e-05,
      "loss": 0.8751,
      "step": 55430
    },
    {
      "epoch": 404.6715328467153,
      "grad_norm": 0.18417856097221375,
      "learning_rate": 2.9766423357664236e-05,
      "loss": 1.3605,
      "step": 55440
    },
    {
      "epoch": 404.7445255474453,
      "grad_norm": 7.574159622192383,
      "learning_rate": 2.976277372262774e-05,
      "loss": 0.7521,
      "step": 55450
    },
    {
      "epoch": 404.8175182481752,
      "grad_norm": 18.370136260986328,
      "learning_rate": 2.9759124087591243e-05,
      "loss": 1.0555,
      "step": 55460
    },
    {
      "epoch": 404.8905109489051,
      "grad_norm": 0.09030412137508392,
      "learning_rate": 2.9755474452554744e-05,
      "loss": 0.7687,
      "step": 55470
    },
    {
      "epoch": 404.963503649635,
      "grad_norm": 6.335390567779541,
      "learning_rate": 2.9751824817518248e-05,
      "loss": 1.0316,
      "step": 55480
    },
    {
      "epoch": 405.036496350365,
      "grad_norm": 10.193312644958496,
      "learning_rate": 2.9748175182481752e-05,
      "loss": 0.867,
      "step": 55490
    },
    {
      "epoch": 405.1094890510949,
      "grad_norm": 15.335732460021973,
      "learning_rate": 2.9744525547445256e-05,
      "loss": 1.2941,
      "step": 55500
    },
    {
      "epoch": 405.1824817518248,
      "grad_norm": 6.528996467590332,
      "learning_rate": 2.974087591240876e-05,
      "loss": 0.8356,
      "step": 55510
    },
    {
      "epoch": 405.2554744525547,
      "grad_norm": 10.76917839050293,
      "learning_rate": 2.9737226277372267e-05,
      "loss": 0.9379,
      "step": 55520
    },
    {
      "epoch": 405.3284671532847,
      "grad_norm": 5.75209379196167,
      "learning_rate": 2.973357664233577e-05,
      "loss": 0.632,
      "step": 55530
    },
    {
      "epoch": 405.4014598540146,
      "grad_norm": 14.26656436920166,
      "learning_rate": 2.9729927007299275e-05,
      "loss": 0.7608,
      "step": 55540
    },
    {
      "epoch": 405.4744525547445,
      "grad_norm": 10.188203811645508,
      "learning_rate": 2.972627737226277e-05,
      "loss": 0.8812,
      "step": 55550
    },
    {
      "epoch": 405.54744525547443,
      "grad_norm": 9.654725074768066,
      "learning_rate": 2.972262773722628e-05,
      "loss": 1.0627,
      "step": 55560
    },
    {
      "epoch": 405.6204379562044,
      "grad_norm": 9.43522834777832,
      "learning_rate": 2.9718978102189783e-05,
      "loss": 0.9293,
      "step": 55570
    },
    {
      "epoch": 405.6934306569343,
      "grad_norm": 13.58579158782959,
      "learning_rate": 2.9715328467153287e-05,
      "loss": 0.7857,
      "step": 55580
    },
    {
      "epoch": 405.7664233576642,
      "grad_norm": 15.071649551391602,
      "learning_rate": 2.971167883211679e-05,
      "loss": 0.9485,
      "step": 55590
    },
    {
      "epoch": 405.83941605839414,
      "grad_norm": 11.480996131896973,
      "learning_rate": 2.9708029197080294e-05,
      "loss": 1.1076,
      "step": 55600
    },
    {
      "epoch": 405.9124087591241,
      "grad_norm": 11.424622535705566,
      "learning_rate": 2.9704379562043798e-05,
      "loss": 0.9758,
      "step": 55610
    },
    {
      "epoch": 405.985401459854,
      "grad_norm": 11.290801048278809,
      "learning_rate": 2.97007299270073e-05,
      "loss": 0.8753,
      "step": 55620
    },
    {
      "epoch": 406.05839416058393,
      "grad_norm": 7.67962646484375,
      "learning_rate": 2.9697080291970803e-05,
      "loss": 0.8508,
      "step": 55630
    },
    {
      "epoch": 406.13138686131384,
      "grad_norm": 13.672109603881836,
      "learning_rate": 2.9693430656934307e-05,
      "loss": 1.0411,
      "step": 55640
    },
    {
      "epoch": 406.2043795620438,
      "grad_norm": 0.48070913553237915,
      "learning_rate": 2.968978102189781e-05,
      "loss": 0.7072,
      "step": 55650
    },
    {
      "epoch": 406.2773722627737,
      "grad_norm": 12.560626029968262,
      "learning_rate": 2.9686131386861314e-05,
      "loss": 0.8166,
      "step": 55660
    },
    {
      "epoch": 406.35036496350364,
      "grad_norm": 10.612264633178711,
      "learning_rate": 2.968248175182482e-05,
      "loss": 0.8981,
      "step": 55670
    },
    {
      "epoch": 406.42335766423355,
      "grad_norm": 10.790380477905273,
      "learning_rate": 2.9678832116788325e-05,
      "loss": 0.9882,
      "step": 55680
    },
    {
      "epoch": 406.4963503649635,
      "grad_norm": 15.133249282836914,
      "learning_rate": 2.967518248175183e-05,
      "loss": 0.8187,
      "step": 55690
    },
    {
      "epoch": 406.56934306569343,
      "grad_norm": 6.259970664978027,
      "learning_rate": 2.9671532846715326e-05,
      "loss": 0.7565,
      "step": 55700
    },
    {
      "epoch": 406.64233576642334,
      "grad_norm": 12.661198616027832,
      "learning_rate": 2.966788321167883e-05,
      "loss": 1.2848,
      "step": 55710
    },
    {
      "epoch": 406.7153284671533,
      "grad_norm": 13.871744155883789,
      "learning_rate": 2.9664233576642338e-05,
      "loss": 0.8856,
      "step": 55720
    },
    {
      "epoch": 406.7883211678832,
      "grad_norm": 11.298615455627441,
      "learning_rate": 2.966058394160584e-05,
      "loss": 1.2877,
      "step": 55730
    },
    {
      "epoch": 406.86131386861314,
      "grad_norm": 13.074870109558105,
      "learning_rate": 2.9656934306569345e-05,
      "loss": 0.9189,
      "step": 55740
    },
    {
      "epoch": 406.93430656934305,
      "grad_norm": 4.808034896850586,
      "learning_rate": 2.965328467153285e-05,
      "loss": 1.0787,
      "step": 55750
    },
    {
      "epoch": 407.007299270073,
      "grad_norm": 8.598932266235352,
      "learning_rate": 2.9649635036496353e-05,
      "loss": 0.9742,
      "step": 55760
    },
    {
      "epoch": 407.08029197080293,
      "grad_norm": 0.08972985297441483,
      "learning_rate": 2.9645985401459857e-05,
      "loss": 0.8137,
      "step": 55770
    },
    {
      "epoch": 407.15328467153284,
      "grad_norm": 3.7313807010650635,
      "learning_rate": 2.9642335766423358e-05,
      "loss": 1.1145,
      "step": 55780
    },
    {
      "epoch": 407.22627737226276,
      "grad_norm": 7.271947860717773,
      "learning_rate": 2.963868613138686e-05,
      "loss": 0.8751,
      "step": 55790
    },
    {
      "epoch": 407.2992700729927,
      "grad_norm": 9.29676342010498,
      "learning_rate": 2.9635036496350365e-05,
      "loss": 0.7359,
      "step": 55800
    },
    {
      "epoch": 407.37226277372264,
      "grad_norm": 0.12557446956634521,
      "learning_rate": 2.963138686131387e-05,
      "loss": 0.8591,
      "step": 55810
    },
    {
      "epoch": 407.44525547445255,
      "grad_norm": 11.190035820007324,
      "learning_rate": 2.9627737226277373e-05,
      "loss": 0.9518,
      "step": 55820
    },
    {
      "epoch": 407.51824817518246,
      "grad_norm": 14.47092342376709,
      "learning_rate": 2.962408759124088e-05,
      "loss": 1.1047,
      "step": 55830
    },
    {
      "epoch": 407.59124087591243,
      "grad_norm": 7.423578262329102,
      "learning_rate": 2.9620437956204384e-05,
      "loss": 0.8928,
      "step": 55840
    },
    {
      "epoch": 407.66423357664235,
      "grad_norm": 8.761351585388184,
      "learning_rate": 2.961678832116788e-05,
      "loss": 1.0038,
      "step": 55850
    },
    {
      "epoch": 407.73722627737226,
      "grad_norm": 7.547175884246826,
      "learning_rate": 2.9613138686131385e-05,
      "loss": 0.8624,
      "step": 55860
    },
    {
      "epoch": 407.81021897810217,
      "grad_norm": 10.0313720703125,
      "learning_rate": 2.9609489051094892e-05,
      "loss": 0.8972,
      "step": 55870
    },
    {
      "epoch": 407.88321167883214,
      "grad_norm": 5.3813323974609375,
      "learning_rate": 2.9605839416058396e-05,
      "loss": 0.9932,
      "step": 55880
    },
    {
      "epoch": 407.95620437956205,
      "grad_norm": 11.259536743164062,
      "learning_rate": 2.96021897810219e-05,
      "loss": 1.1697,
      "step": 55890
    },
    {
      "epoch": 408.02919708029196,
      "grad_norm": 15.526815414428711,
      "learning_rate": 2.9598540145985404e-05,
      "loss": 1.0965,
      "step": 55900
    },
    {
      "epoch": 408.1021897810219,
      "grad_norm": 12.872289657592773,
      "learning_rate": 2.9594890510948908e-05,
      "loss": 1.1182,
      "step": 55910
    },
    {
      "epoch": 408.17518248175185,
      "grad_norm": 11.124765396118164,
      "learning_rate": 2.9591240875912412e-05,
      "loss": 0.8862,
      "step": 55920
    },
    {
      "epoch": 408.24817518248176,
      "grad_norm": 0.1828077882528305,
      "learning_rate": 2.9587591240875912e-05,
      "loss": 0.7254,
      "step": 55930
    },
    {
      "epoch": 408.3211678832117,
      "grad_norm": 9.11104965209961,
      "learning_rate": 2.9583941605839416e-05,
      "loss": 0.9763,
      "step": 55940
    },
    {
      "epoch": 408.3941605839416,
      "grad_norm": 0.16216912865638733,
      "learning_rate": 2.958029197080292e-05,
      "loss": 1.0422,
      "step": 55950
    },
    {
      "epoch": 408.46715328467155,
      "grad_norm": 0.4793257713317871,
      "learning_rate": 2.9576642335766424e-05,
      "loss": 0.9195,
      "step": 55960
    },
    {
      "epoch": 408.54014598540147,
      "grad_norm": 11.507426261901855,
      "learning_rate": 2.9572992700729928e-05,
      "loss": 0.6406,
      "step": 55970
    },
    {
      "epoch": 408.6131386861314,
      "grad_norm": 10.33760929107666,
      "learning_rate": 2.9569343065693432e-05,
      "loss": 0.736,
      "step": 55980
    },
    {
      "epoch": 408.6861313868613,
      "grad_norm": 9.481866836547852,
      "learning_rate": 2.956569343065694e-05,
      "loss": 0.7547,
      "step": 55990
    },
    {
      "epoch": 408.75912408759126,
      "grad_norm": 0.2733772397041321,
      "learning_rate": 2.9562043795620443e-05,
      "loss": 0.9659,
      "step": 56000
    },
    {
      "epoch": 408.8321167883212,
      "grad_norm": 16.044105529785156,
      "learning_rate": 2.955839416058394e-05,
      "loss": 0.7763,
      "step": 56010
    },
    {
      "epoch": 408.9051094890511,
      "grad_norm": 8.191916465759277,
      "learning_rate": 2.9554744525547444e-05,
      "loss": 1.3626,
      "step": 56020
    },
    {
      "epoch": 408.978102189781,
      "grad_norm": 0.30801859498023987,
      "learning_rate": 2.955109489051095e-05,
      "loss": 0.7811,
      "step": 56030
    },
    {
      "epoch": 409.05109489051097,
      "grad_norm": 12.9562406539917,
      "learning_rate": 2.9547445255474455e-05,
      "loss": 1.509,
      "step": 56040
    },
    {
      "epoch": 409.1240875912409,
      "grad_norm": 8.307173728942871,
      "learning_rate": 2.954379562043796e-05,
      "loss": 1.0692,
      "step": 56050
    },
    {
      "epoch": 409.1970802919708,
      "grad_norm": 7.402287483215332,
      "learning_rate": 2.9540145985401463e-05,
      "loss": 1.0584,
      "step": 56060
    },
    {
      "epoch": 409.2700729927007,
      "grad_norm": 16.201108932495117,
      "learning_rate": 2.9536496350364967e-05,
      "loss": 0.8213,
      "step": 56070
    },
    {
      "epoch": 409.3430656934307,
      "grad_norm": 6.344813823699951,
      "learning_rate": 2.9532846715328467e-05,
      "loss": 1.0116,
      "step": 56080
    },
    {
      "epoch": 409.4160583941606,
      "grad_norm": 3.0400283336639404,
      "learning_rate": 2.952919708029197e-05,
      "loss": 0.6992,
      "step": 56090
    },
    {
      "epoch": 409.4890510948905,
      "grad_norm": 18.072799682617188,
      "learning_rate": 2.9525547445255475e-05,
      "loss": 0.9401,
      "step": 56100
    },
    {
      "epoch": 409.5620437956204,
      "grad_norm": 15.939334869384766,
      "learning_rate": 2.952189781021898e-05,
      "loss": 1.1188,
      "step": 56110
    },
    {
      "epoch": 409.6350364963504,
      "grad_norm": 10.215394973754883,
      "learning_rate": 2.9518248175182483e-05,
      "loss": 0.9026,
      "step": 56120
    },
    {
      "epoch": 409.7080291970803,
      "grad_norm": 0.18649490177631378,
      "learning_rate": 2.9514598540145987e-05,
      "loss": 0.4221,
      "step": 56130
    },
    {
      "epoch": 409.7810218978102,
      "grad_norm": 8.800447463989258,
      "learning_rate": 2.9510948905109494e-05,
      "loss": 1.0426,
      "step": 56140
    },
    {
      "epoch": 409.8540145985401,
      "grad_norm": 11.269248008728027,
      "learning_rate": 2.9507299270072998e-05,
      "loss": 0.8257,
      "step": 56150
    },
    {
      "epoch": 409.9270072992701,
      "grad_norm": 5.8346381187438965,
      "learning_rate": 2.9503649635036495e-05,
      "loss": 0.6192,
      "step": 56160
    },
    {
      "epoch": 410.0,
      "grad_norm": 0.08461347222328186,
      "learning_rate": 2.95e-05,
      "loss": 0.7567,
      "step": 56170
    },
    {
      "epoch": 410.0729927007299,
      "grad_norm": 19.41046142578125,
      "learning_rate": 2.9496350364963503e-05,
      "loss": 1.0281,
      "step": 56180
    },
    {
      "epoch": 410.1459854014599,
      "grad_norm": 6.918904781341553,
      "learning_rate": 2.949270072992701e-05,
      "loss": 0.7263,
      "step": 56190
    },
    {
      "epoch": 410.2189781021898,
      "grad_norm": 16.960235595703125,
      "learning_rate": 2.9489051094890514e-05,
      "loss": 1.001,
      "step": 56200
    },
    {
      "epoch": 410.2919708029197,
      "grad_norm": 4.468496799468994,
      "learning_rate": 2.9485401459854018e-05,
      "loss": 0.7591,
      "step": 56210
    },
    {
      "epoch": 410.3649635036496,
      "grad_norm": 0.08637164533138275,
      "learning_rate": 2.948175182481752e-05,
      "loss": 0.821,
      "step": 56220
    },
    {
      "epoch": 410.4379562043796,
      "grad_norm": 9.861421585083008,
      "learning_rate": 2.9478102189781022e-05,
      "loss": 1.0896,
      "step": 56230
    },
    {
      "epoch": 410.5109489051095,
      "grad_norm": 0.18058745563030243,
      "learning_rate": 2.9474452554744526e-05,
      "loss": 1.3312,
      "step": 56240
    },
    {
      "epoch": 410.5839416058394,
      "grad_norm": 5.518578052520752,
      "learning_rate": 2.947080291970803e-05,
      "loss": 0.9671,
      "step": 56250
    },
    {
      "epoch": 410.6569343065693,
      "grad_norm": 0.06417416781187057,
      "learning_rate": 2.9467153284671534e-05,
      "loss": 0.439,
      "step": 56260
    },
    {
      "epoch": 410.7299270072993,
      "grad_norm": 0.9050008058547974,
      "learning_rate": 2.9463503649635038e-05,
      "loss": 0.747,
      "step": 56270
    },
    {
      "epoch": 410.8029197080292,
      "grad_norm": 9.395174026489258,
      "learning_rate": 2.945985401459854e-05,
      "loss": 0.8842,
      "step": 56280
    },
    {
      "epoch": 410.8759124087591,
      "grad_norm": 7.641489028930664,
      "learning_rate": 2.9456204379562045e-05,
      "loss": 1.0043,
      "step": 56290
    },
    {
      "epoch": 410.94890510948903,
      "grad_norm": 20.278400421142578,
      "learning_rate": 2.9452554744525553e-05,
      "loss": 1.1179,
      "step": 56300
    },
    {
      "epoch": 411.021897810219,
      "grad_norm": 8.511836051940918,
      "learning_rate": 2.944890510948905e-05,
      "loss": 0.5616,
      "step": 56310
    },
    {
      "epoch": 411.0948905109489,
      "grad_norm": 9.620946884155273,
      "learning_rate": 2.9445255474452554e-05,
      "loss": 1.0569,
      "step": 56320
    },
    {
      "epoch": 411.1678832116788,
      "grad_norm": 15.206459999084473,
      "learning_rate": 2.9441605839416058e-05,
      "loss": 0.9903,
      "step": 56330
    },
    {
      "epoch": 411.24087591240874,
      "grad_norm": 0.1314006894826889,
      "learning_rate": 2.9437956204379565e-05,
      "loss": 0.9458,
      "step": 56340
    },
    {
      "epoch": 411.3138686131387,
      "grad_norm": 10.705123901367188,
      "learning_rate": 2.943430656934307e-05,
      "loss": 0.815,
      "step": 56350
    },
    {
      "epoch": 411.3868613138686,
      "grad_norm": 0.04080461338162422,
      "learning_rate": 2.9430656934306573e-05,
      "loss": 0.9063,
      "step": 56360
    },
    {
      "epoch": 411.45985401459853,
      "grad_norm": 12.489628791809082,
      "learning_rate": 2.9427007299270076e-05,
      "loss": 0.9832,
      "step": 56370
    },
    {
      "epoch": 411.53284671532845,
      "grad_norm": 9.4368314743042,
      "learning_rate": 2.942335766423358e-05,
      "loss": 1.2094,
      "step": 56380
    },
    {
      "epoch": 411.6058394160584,
      "grad_norm": 11.80140495300293,
      "learning_rate": 2.941970802919708e-05,
      "loss": 1.1484,
      "step": 56390
    },
    {
      "epoch": 411.6788321167883,
      "grad_norm": 9.698060035705566,
      "learning_rate": 2.9416058394160585e-05,
      "loss": 0.817,
      "step": 56400
    },
    {
      "epoch": 411.75182481751824,
      "grad_norm": 10.59758472442627,
      "learning_rate": 2.941240875912409e-05,
      "loss": 0.9569,
      "step": 56410
    },
    {
      "epoch": 411.82481751824815,
      "grad_norm": 11.129403114318848,
      "learning_rate": 2.9408759124087592e-05,
      "loss": 1.1684,
      "step": 56420
    },
    {
      "epoch": 411.8978102189781,
      "grad_norm": 12.657992362976074,
      "learning_rate": 2.9405109489051096e-05,
      "loss": 0.834,
      "step": 56430
    },
    {
      "epoch": 411.97080291970804,
      "grad_norm": 17.57923126220703,
      "learning_rate": 2.94014598540146e-05,
      "loss": 0.8859,
      "step": 56440
    },
    {
      "epoch": 412.04379562043795,
      "grad_norm": 0.07477514445781708,
      "learning_rate": 2.9397810218978104e-05,
      "loss": 0.5522,
      "step": 56450
    },
    {
      "epoch": 412.11678832116786,
      "grad_norm": 7.619359493255615,
      "learning_rate": 2.9394160583941605e-05,
      "loss": 0.6652,
      "step": 56460
    },
    {
      "epoch": 412.18978102189783,
      "grad_norm": 12.333624839782715,
      "learning_rate": 2.939051094890511e-05,
      "loss": 0.6598,
      "step": 56470
    },
    {
      "epoch": 412.26277372262774,
      "grad_norm": 5.436819553375244,
      "learning_rate": 2.9386861313868612e-05,
      "loss": 0.9425,
      "step": 56480
    },
    {
      "epoch": 412.33576642335765,
      "grad_norm": 12.831543922424316,
      "learning_rate": 2.9383211678832116e-05,
      "loss": 0.8238,
      "step": 56490
    },
    {
      "epoch": 412.40875912408757,
      "grad_norm": 12.354036331176758,
      "learning_rate": 2.9379562043795624e-05,
      "loss": 0.9732,
      "step": 56500
    },
    {
      "epoch": 412.48175182481754,
      "grad_norm": 15.551246643066406,
      "learning_rate": 2.9375912408759127e-05,
      "loss": 0.9686,
      "step": 56510
    },
    {
      "epoch": 412.55474452554745,
      "grad_norm": 10.10670280456543,
      "learning_rate": 2.937226277372263e-05,
      "loss": 0.962,
      "step": 56520
    },
    {
      "epoch": 412.62773722627736,
      "grad_norm": 5.950155258178711,
      "learning_rate": 2.9368613138686135e-05,
      "loss": 1.203,
      "step": 56530
    },
    {
      "epoch": 412.7007299270073,
      "grad_norm": 13.601503372192383,
      "learning_rate": 2.9364963503649636e-05,
      "loss": 1.1011,
      "step": 56540
    },
    {
      "epoch": 412.77372262773724,
      "grad_norm": 13.367745399475098,
      "learning_rate": 2.936131386861314e-05,
      "loss": 0.75,
      "step": 56550
    },
    {
      "epoch": 412.84671532846716,
      "grad_norm": 11.306836128234863,
      "learning_rate": 2.9357664233576643e-05,
      "loss": 0.7591,
      "step": 56560
    },
    {
      "epoch": 412.91970802919707,
      "grad_norm": 9.06998062133789,
      "learning_rate": 2.9354014598540147e-05,
      "loss": 0.9922,
      "step": 56570
    },
    {
      "epoch": 412.992700729927,
      "grad_norm": 12.825653076171875,
      "learning_rate": 2.935036496350365e-05,
      "loss": 1.1374,
      "step": 56580
    },
    {
      "epoch": 413.06569343065695,
      "grad_norm": 11.105520248413086,
      "learning_rate": 2.9346715328467155e-05,
      "loss": 0.6082,
      "step": 56590
    },
    {
      "epoch": 413.13868613138686,
      "grad_norm": 8.434504508972168,
      "learning_rate": 2.934306569343066e-05,
      "loss": 0.9508,
      "step": 56600
    },
    {
      "epoch": 413.2116788321168,
      "grad_norm": 14.356146812438965,
      "learning_rate": 2.9339416058394166e-05,
      "loss": 0.9966,
      "step": 56610
    },
    {
      "epoch": 413.2846715328467,
      "grad_norm": 14.098179817199707,
      "learning_rate": 2.9335766423357663e-05,
      "loss": 0.9822,
      "step": 56620
    },
    {
      "epoch": 413.35766423357666,
      "grad_norm": 4.106388092041016,
      "learning_rate": 2.9332116788321167e-05,
      "loss": 0.7153,
      "step": 56630
    },
    {
      "epoch": 413.43065693430657,
      "grad_norm": 7.0792317390441895,
      "learning_rate": 2.932846715328467e-05,
      "loss": 0.994,
      "step": 56640
    },
    {
      "epoch": 413.5036496350365,
      "grad_norm": 0.08828262984752655,
      "learning_rate": 2.9324817518248175e-05,
      "loss": 0.644,
      "step": 56650
    },
    {
      "epoch": 413.57664233576645,
      "grad_norm": 8.107723236083984,
      "learning_rate": 2.9321167883211682e-05,
      "loss": 1.1541,
      "step": 56660
    },
    {
      "epoch": 413.64963503649636,
      "grad_norm": 19.024391174316406,
      "learning_rate": 2.9317518248175186e-05,
      "loss": 0.9662,
      "step": 56670
    },
    {
      "epoch": 413.7226277372263,
      "grad_norm": 9.439891815185547,
      "learning_rate": 2.931386861313869e-05,
      "loss": 0.8815,
      "step": 56680
    },
    {
      "epoch": 413.7956204379562,
      "grad_norm": 18.229143142700195,
      "learning_rate": 2.9310218978102187e-05,
      "loss": 1.0074,
      "step": 56690
    },
    {
      "epoch": 413.86861313868616,
      "grad_norm": 14.301612854003906,
      "learning_rate": 2.9306569343065694e-05,
      "loss": 1.0017,
      "step": 56700
    },
    {
      "epoch": 413.94160583941607,
      "grad_norm": 0.7071470022201538,
      "learning_rate": 2.9302919708029198e-05,
      "loss": 1.4449,
      "step": 56710
    },
    {
      "epoch": 414.014598540146,
      "grad_norm": 7.483428478240967,
      "learning_rate": 2.9299270072992702e-05,
      "loss": 1.0097,
      "step": 56720
    },
    {
      "epoch": 414.0875912408759,
      "grad_norm": 6.918495178222656,
      "learning_rate": 2.9295620437956206e-05,
      "loss": 0.9878,
      "step": 56730
    },
    {
      "epoch": 414.16058394160586,
      "grad_norm": 8.068794250488281,
      "learning_rate": 2.929197080291971e-05,
      "loss": 0.8246,
      "step": 56740
    },
    {
      "epoch": 414.2335766423358,
      "grad_norm": 16.716176986694336,
      "learning_rate": 2.9288321167883214e-05,
      "loss": 1.346,
      "step": 56750
    },
    {
      "epoch": 414.3065693430657,
      "grad_norm": 7.515222072601318,
      "learning_rate": 2.9284671532846718e-05,
      "loss": 1.4231,
      "step": 56760
    },
    {
      "epoch": 414.3795620437956,
      "grad_norm": 0.29795101284980774,
      "learning_rate": 2.9281021897810218e-05,
      "loss": 0.7024,
      "step": 56770
    },
    {
      "epoch": 414.45255474452557,
      "grad_norm": 11.154290199279785,
      "learning_rate": 2.9277372262773722e-05,
      "loss": 0.8855,
      "step": 56780
    },
    {
      "epoch": 414.5255474452555,
      "grad_norm": 21.07124900817871,
      "learning_rate": 2.9273722627737226e-05,
      "loss": 0.8456,
      "step": 56790
    },
    {
      "epoch": 414.5985401459854,
      "grad_norm": 10.004461288452148,
      "learning_rate": 2.927007299270073e-05,
      "loss": 0.6977,
      "step": 56800
    },
    {
      "epoch": 414.6715328467153,
      "grad_norm": 9.514286994934082,
      "learning_rate": 2.9266423357664237e-05,
      "loss": 0.7308,
      "step": 56810
    },
    {
      "epoch": 414.7445255474453,
      "grad_norm": 0.2814556062221527,
      "learning_rate": 2.926277372262774e-05,
      "loss": 0.8675,
      "step": 56820
    },
    {
      "epoch": 414.8175182481752,
      "grad_norm": 7.779979705810547,
      "learning_rate": 2.9259124087591245e-05,
      "loss": 0.829,
      "step": 56830
    },
    {
      "epoch": 414.8905109489051,
      "grad_norm": 0.5484228134155273,
      "learning_rate": 2.925547445255475e-05,
      "loss": 0.6355,
      "step": 56840
    },
    {
      "epoch": 414.963503649635,
      "grad_norm": 0.2877102196216583,
      "learning_rate": 2.9251824817518246e-05,
      "loss": 0.9085,
      "step": 56850
    },
    {
      "epoch": 415.036496350365,
      "grad_norm": 0.17825506627559662,
      "learning_rate": 2.9248175182481753e-05,
      "loss": 1.079,
      "step": 56860
    },
    {
      "epoch": 415.1094890510949,
      "grad_norm": 10.115100860595703,
      "learning_rate": 2.9244525547445257e-05,
      "loss": 0.6588,
      "step": 56870
    },
    {
      "epoch": 415.1824817518248,
      "grad_norm": 10.691006660461426,
      "learning_rate": 2.924087591240876e-05,
      "loss": 1.0271,
      "step": 56880
    },
    {
      "epoch": 415.2554744525547,
      "grad_norm": 10.368239402770996,
      "learning_rate": 2.9237226277372265e-05,
      "loss": 0.8086,
      "step": 56890
    },
    {
      "epoch": 415.3284671532847,
      "grad_norm": 13.480457305908203,
      "learning_rate": 2.923357664233577e-05,
      "loss": 1.2533,
      "step": 56900
    },
    {
      "epoch": 415.4014598540146,
      "grad_norm": 8.559762001037598,
      "learning_rate": 2.9229927007299273e-05,
      "loss": 0.8049,
      "step": 56910
    },
    {
      "epoch": 415.4744525547445,
      "grad_norm": 9.294093132019043,
      "learning_rate": 2.9226277372262773e-05,
      "loss": 0.8123,
      "step": 56920
    },
    {
      "epoch": 415.54744525547443,
      "grad_norm": 0.09702455997467041,
      "learning_rate": 2.9222627737226277e-05,
      "loss": 0.8475,
      "step": 56930
    },
    {
      "epoch": 415.6204379562044,
      "grad_norm": 13.858418464660645,
      "learning_rate": 2.921897810218978e-05,
      "loss": 0.875,
      "step": 56940
    },
    {
      "epoch": 415.6934306569343,
      "grad_norm": 0.4238918125629425,
      "learning_rate": 2.9215328467153285e-05,
      "loss": 0.7219,
      "step": 56950
    },
    {
      "epoch": 415.7664233576642,
      "grad_norm": 11.482895851135254,
      "learning_rate": 2.921167883211679e-05,
      "loss": 0.5512,
      "step": 56960
    },
    {
      "epoch": 415.83941605839414,
      "grad_norm": 10.74686050415039,
      "learning_rate": 2.9208029197080296e-05,
      "loss": 1.2087,
      "step": 56970
    },
    {
      "epoch": 415.9124087591241,
      "grad_norm": 0.12904493510723114,
      "learning_rate": 2.92043795620438e-05,
      "loss": 1.2891,
      "step": 56980
    },
    {
      "epoch": 415.985401459854,
      "grad_norm": 10.69461727142334,
      "learning_rate": 2.9200729927007304e-05,
      "loss": 1.2429,
      "step": 56990
    },
    {
      "epoch": 416.05839416058393,
      "grad_norm": 7.445224761962891,
      "learning_rate": 2.91970802919708e-05,
      "loss": 0.6439,
      "step": 57000
    },
    {
      "epoch": 416.13138686131384,
      "grad_norm": 7.200806617736816,
      "learning_rate": 2.9193430656934308e-05,
      "loss": 1.0188,
      "step": 57010
    },
    {
      "epoch": 416.2043795620438,
      "grad_norm": 18.33772087097168,
      "learning_rate": 2.9189781021897812e-05,
      "loss": 1.1022,
      "step": 57020
    },
    {
      "epoch": 416.2773722627737,
      "grad_norm": 13.772842407226562,
      "learning_rate": 2.9186131386861316e-05,
      "loss": 1.2665,
      "step": 57030
    },
    {
      "epoch": 416.35036496350364,
      "grad_norm": 0.07309509068727493,
      "learning_rate": 2.918248175182482e-05,
      "loss": 0.4277,
      "step": 57040
    },
    {
      "epoch": 416.42335766423355,
      "grad_norm": 11.491174697875977,
      "learning_rate": 2.9178832116788323e-05,
      "loss": 0.8782,
      "step": 57050
    },
    {
      "epoch": 416.4963503649635,
      "grad_norm": 0.16271300613880157,
      "learning_rate": 2.9175182481751827e-05,
      "loss": 0.8154,
      "step": 57060
    },
    {
      "epoch": 416.56934306569343,
      "grad_norm": 13.12806224822998,
      "learning_rate": 2.917153284671533e-05,
      "loss": 1.3261,
      "step": 57070
    },
    {
      "epoch": 416.64233576642334,
      "grad_norm": 12.092511177062988,
      "learning_rate": 2.9167883211678832e-05,
      "loss": 1.1382,
      "step": 57080
    },
    {
      "epoch": 416.7153284671533,
      "grad_norm": 7.613460063934326,
      "learning_rate": 2.9164233576642336e-05,
      "loss": 0.9589,
      "step": 57090
    },
    {
      "epoch": 416.7883211678832,
      "grad_norm": 9.155649185180664,
      "learning_rate": 2.916058394160584e-05,
      "loss": 0.7142,
      "step": 57100
    },
    {
      "epoch": 416.86131386861314,
      "grad_norm": 8.159263610839844,
      "learning_rate": 2.9156934306569343e-05,
      "loss": 0.9919,
      "step": 57110
    },
    {
      "epoch": 416.93430656934305,
      "grad_norm": 9.059532165527344,
      "learning_rate": 2.9153284671532847e-05,
      "loss": 0.8936,
      "step": 57120
    },
    {
      "epoch": 417.007299270073,
      "grad_norm": 7.89966344833374,
      "learning_rate": 2.9149635036496355e-05,
      "loss": 1.0389,
      "step": 57130
    },
    {
      "epoch": 417.08029197080293,
      "grad_norm": 14.276159286499023,
      "learning_rate": 2.914598540145986e-05,
      "loss": 0.6597,
      "step": 57140
    },
    {
      "epoch": 417.15328467153284,
      "grad_norm": 0.07235598564147949,
      "learning_rate": 2.9142335766423356e-05,
      "loss": 0.7759,
      "step": 57150
    },
    {
      "epoch": 417.22627737226276,
      "grad_norm": 10.872593879699707,
      "learning_rate": 2.913868613138686e-05,
      "loss": 1.0996,
      "step": 57160
    },
    {
      "epoch": 417.2992700729927,
      "grad_norm": 14.005732536315918,
      "learning_rate": 2.9135036496350367e-05,
      "loss": 0.9823,
      "step": 57170
    },
    {
      "epoch": 417.37226277372264,
      "grad_norm": 10.929408073425293,
      "learning_rate": 2.913138686131387e-05,
      "loss": 1.012,
      "step": 57180
    },
    {
      "epoch": 417.44525547445255,
      "grad_norm": 9.951157569885254,
      "learning_rate": 2.9127737226277374e-05,
      "loss": 1.0502,
      "step": 57190
    },
    {
      "epoch": 417.51824817518246,
      "grad_norm": 9.853716850280762,
      "learning_rate": 2.912408759124088e-05,
      "loss": 0.7078,
      "step": 57200
    },
    {
      "epoch": 417.59124087591243,
      "grad_norm": 15.817082405090332,
      "learning_rate": 2.9120437956204382e-05,
      "loss": 1.1207,
      "step": 57210
    },
    {
      "epoch": 417.66423357664235,
      "grad_norm": 12.561431884765625,
      "learning_rate": 2.9116788321167886e-05,
      "loss": 1.0823,
      "step": 57220
    },
    {
      "epoch": 417.73722627737226,
      "grad_norm": 15.474845886230469,
      "learning_rate": 2.9113138686131387e-05,
      "loss": 0.6934,
      "step": 57230
    },
    {
      "epoch": 417.81021897810217,
      "grad_norm": 6.271997928619385,
      "learning_rate": 2.910948905109489e-05,
      "loss": 0.721,
      "step": 57240
    },
    {
      "epoch": 417.88321167883214,
      "grad_norm": 10.142468452453613,
      "learning_rate": 2.9105839416058394e-05,
      "loss": 0.8234,
      "step": 57250
    },
    {
      "epoch": 417.95620437956205,
      "grad_norm": 19.07552146911621,
      "learning_rate": 2.9102189781021898e-05,
      "loss": 1.1181,
      "step": 57260
    },
    {
      "epoch": 418.02919708029196,
      "grad_norm": 7.495983123779297,
      "learning_rate": 2.9098540145985402e-05,
      "loss": 0.709,
      "step": 57270
    },
    {
      "epoch": 418.1021897810219,
      "grad_norm": 0.18143068253993988,
      "learning_rate": 2.909489051094891e-05,
      "loss": 1.1011,
      "step": 57280
    },
    {
      "epoch": 418.17518248175185,
      "grad_norm": 10.217772483825684,
      "learning_rate": 2.9091240875912413e-05,
      "loss": 0.7252,
      "step": 57290
    },
    {
      "epoch": 418.24817518248176,
      "grad_norm": 7.163802146911621,
      "learning_rate": 2.908759124087591e-05,
      "loss": 1.175,
      "step": 57300
    },
    {
      "epoch": 418.3211678832117,
      "grad_norm": 10.846829414367676,
      "learning_rate": 2.9083941605839414e-05,
      "loss": 1.0574,
      "step": 57310
    },
    {
      "epoch": 418.3941605839416,
      "grad_norm": 0.2273707091808319,
      "learning_rate": 2.9080291970802918e-05,
      "loss": 0.806,
      "step": 57320
    },
    {
      "epoch": 418.46715328467155,
      "grad_norm": 5.7960405349731445,
      "learning_rate": 2.9076642335766425e-05,
      "loss": 0.7582,
      "step": 57330
    },
    {
      "epoch": 418.54014598540147,
      "grad_norm": 12.967935562133789,
      "learning_rate": 2.907299270072993e-05,
      "loss": 1.0247,
      "step": 57340
    },
    {
      "epoch": 418.6131386861314,
      "grad_norm": 2.085552453994751,
      "learning_rate": 2.9069343065693433e-05,
      "loss": 0.454,
      "step": 57350
    },
    {
      "epoch": 418.6861313868613,
      "grad_norm": 9.911338806152344,
      "learning_rate": 2.9065693430656937e-05,
      "loss": 0.8911,
      "step": 57360
    },
    {
      "epoch": 418.75912408759126,
      "grad_norm": 7.640313148498535,
      "learning_rate": 2.906204379562044e-05,
      "loss": 1.1018,
      "step": 57370
    },
    {
      "epoch": 418.8321167883212,
      "grad_norm": 7.7278218269348145,
      "learning_rate": 2.905839416058394e-05,
      "loss": 0.948,
      "step": 57380
    },
    {
      "epoch": 418.9051094890511,
      "grad_norm": 7.462913513183594,
      "learning_rate": 2.9054744525547445e-05,
      "loss": 0.9447,
      "step": 57390
    },
    {
      "epoch": 418.978102189781,
      "grad_norm": 7.51472282409668,
      "learning_rate": 2.905109489051095e-05,
      "loss": 1.1322,
      "step": 57400
    },
    {
      "epoch": 419.05109489051097,
      "grad_norm": 4.8559136390686035,
      "learning_rate": 2.9047445255474453e-05,
      "loss": 1.0498,
      "step": 57410
    },
    {
      "epoch": 419.1240875912409,
      "grad_norm": 16.88701629638672,
      "learning_rate": 2.9043795620437957e-05,
      "loss": 0.9524,
      "step": 57420
    },
    {
      "epoch": 419.1970802919708,
      "grad_norm": 15.32056713104248,
      "learning_rate": 2.904014598540146e-05,
      "loss": 1.1187,
      "step": 57430
    },
    {
      "epoch": 419.2700729927007,
      "grad_norm": 7.464096546173096,
      "learning_rate": 2.9036496350364968e-05,
      "loss": 1.2831,
      "step": 57440
    },
    {
      "epoch": 419.3430656934307,
      "grad_norm": 2.469728708267212,
      "learning_rate": 2.9032846715328472e-05,
      "loss": 0.8576,
      "step": 57450
    },
    {
      "epoch": 419.4160583941606,
      "grad_norm": 8.111011505126953,
      "learning_rate": 2.902919708029197e-05,
      "loss": 0.6365,
      "step": 57460
    },
    {
      "epoch": 419.4890510948905,
      "grad_norm": 9.984496116638184,
      "learning_rate": 2.9025547445255473e-05,
      "loss": 0.9063,
      "step": 57470
    },
    {
      "epoch": 419.5620437956204,
      "grad_norm": 13.33204174041748,
      "learning_rate": 2.902189781021898e-05,
      "loss": 1.1856,
      "step": 57480
    },
    {
      "epoch": 419.6350364963504,
      "grad_norm": 11.115364074707031,
      "learning_rate": 2.9018248175182484e-05,
      "loss": 0.9157,
      "step": 57490
    },
    {
      "epoch": 419.7080291970803,
      "grad_norm": 5.597195625305176,
      "learning_rate": 2.9014598540145988e-05,
      "loss": 0.7151,
      "step": 57500
    },
    {
      "epoch": 419.7810218978102,
      "grad_norm": 3.284545660018921,
      "learning_rate": 2.9010948905109492e-05,
      "loss": 0.6953,
      "step": 57510
    },
    {
      "epoch": 419.8540145985401,
      "grad_norm": 0.9861524105072021,
      "learning_rate": 2.9007299270072996e-05,
      "loss": 0.7674,
      "step": 57520
    },
    {
      "epoch": 419.9270072992701,
      "grad_norm": 10.81778335571289,
      "learning_rate": 2.9003649635036496e-05,
      "loss": 0.7956,
      "step": 57530
    },
    {
      "epoch": 420.0,
      "grad_norm": 14.026912689208984,
      "learning_rate": 2.9e-05,
      "loss": 1.0211,
      "step": 57540
    },
    {
      "epoch": 420.0729927007299,
      "grad_norm": 15.688271522521973,
      "learning_rate": 2.8996350364963504e-05,
      "loss": 0.9605,
      "step": 57550
    },
    {
      "epoch": 420.1459854014599,
      "grad_norm": 6.511525630950928,
      "learning_rate": 2.8992700729927008e-05,
      "loss": 0.9299,
      "step": 57560
    },
    {
      "epoch": 420.2189781021898,
      "grad_norm": 11.01485538482666,
      "learning_rate": 2.8989051094890512e-05,
      "loss": 1.3638,
      "step": 57570
    },
    {
      "epoch": 420.2919708029197,
      "grad_norm": 9.297320365905762,
      "learning_rate": 2.8985401459854016e-05,
      "loss": 0.9421,
      "step": 57580
    },
    {
      "epoch": 420.3649635036496,
      "grad_norm": 15.750301361083984,
      "learning_rate": 2.898175182481752e-05,
      "loss": 1.0792,
      "step": 57590
    },
    {
      "epoch": 420.4379562043796,
      "grad_norm": 0.16104085743427277,
      "learning_rate": 2.8978102189781027e-05,
      "loss": 0.3888,
      "step": 57600
    },
    {
      "epoch": 420.5109489051095,
      "grad_norm": 17.442026138305664,
      "learning_rate": 2.8974452554744524e-05,
      "loss": 0.7421,
      "step": 57610
    },
    {
      "epoch": 420.5839416058394,
      "grad_norm": 5.910627841949463,
      "learning_rate": 2.8970802919708028e-05,
      "loss": 0.6787,
      "step": 57620
    },
    {
      "epoch": 420.6569343065693,
      "grad_norm": 9.623229026794434,
      "learning_rate": 2.8967153284671532e-05,
      "loss": 0.752,
      "step": 57630
    },
    {
      "epoch": 420.7299270072993,
      "grad_norm": 12.582633018493652,
      "learning_rate": 2.896350364963504e-05,
      "loss": 0.7807,
      "step": 57640
    },
    {
      "epoch": 420.8029197080292,
      "grad_norm": 8.900683403015137,
      "learning_rate": 2.8959854014598543e-05,
      "loss": 0.9156,
      "step": 57650
    },
    {
      "epoch": 420.8759124087591,
      "grad_norm": 20.25230598449707,
      "learning_rate": 2.8956204379562047e-05,
      "loss": 1.0995,
      "step": 57660
    },
    {
      "epoch": 420.94890510948903,
      "grad_norm": 11.390013694763184,
      "learning_rate": 2.895255474452555e-05,
      "loss": 0.9669,
      "step": 57670
    },
    {
      "epoch": 421.021897810219,
      "grad_norm": 4.556490421295166,
      "learning_rate": 2.8948905109489055e-05,
      "loss": 1.011,
      "step": 57680
    },
    {
      "epoch": 421.0948905109489,
      "grad_norm": 8.777048110961914,
      "learning_rate": 2.8945255474452555e-05,
      "loss": 0.5535,
      "step": 57690
    },
    {
      "epoch": 421.1678832116788,
      "grad_norm": 9.849164962768555,
      "learning_rate": 2.894160583941606e-05,
      "loss": 0.6673,
      "step": 57700
    },
    {
      "epoch": 421.24087591240874,
      "grad_norm": 15.480914115905762,
      "learning_rate": 2.8937956204379563e-05,
      "loss": 0.9844,
      "step": 57710
    },
    {
      "epoch": 421.3138686131387,
      "grad_norm": 9.288443565368652,
      "learning_rate": 2.8934306569343067e-05,
      "loss": 0.9541,
      "step": 57720
    },
    {
      "epoch": 421.3868613138686,
      "grad_norm": 0.05673321336507797,
      "learning_rate": 2.893065693430657e-05,
      "loss": 0.9357,
      "step": 57730
    },
    {
      "epoch": 421.45985401459853,
      "grad_norm": 11.797894477844238,
      "learning_rate": 2.8927007299270074e-05,
      "loss": 0.9786,
      "step": 57740
    },
    {
      "epoch": 421.53284671532845,
      "grad_norm": 10.093707084655762,
      "learning_rate": 2.8923357664233582e-05,
      "loss": 1.3873,
      "step": 57750
    },
    {
      "epoch": 421.6058394160584,
      "grad_norm": 8.095151901245117,
      "learning_rate": 2.891970802919708e-05,
      "loss": 0.6608,
      "step": 57760
    },
    {
      "epoch": 421.6788321167883,
      "grad_norm": 9.411539077758789,
      "learning_rate": 2.8916058394160583e-05,
      "loss": 1.2922,
      "step": 57770
    },
    {
      "epoch": 421.75182481751824,
      "grad_norm": 16.510587692260742,
      "learning_rate": 2.8912408759124087e-05,
      "loss": 1.2457,
      "step": 57780
    },
    {
      "epoch": 421.82481751824815,
      "grad_norm": 0.502378523349762,
      "learning_rate": 2.890875912408759e-05,
      "loss": 0.9282,
      "step": 57790
    },
    {
      "epoch": 421.8978102189781,
      "grad_norm": 14.515226364135742,
      "learning_rate": 2.8905109489051098e-05,
      "loss": 0.9931,
      "step": 57800
    },
    {
      "epoch": 421.97080291970804,
      "grad_norm": 7.960822582244873,
      "learning_rate": 2.89014598540146e-05,
      "loss": 0.8492,
      "step": 57810
    },
    {
      "epoch": 422.04379562043795,
      "grad_norm": 12.760079383850098,
      "learning_rate": 2.8897810218978105e-05,
      "loss": 1.1621,
      "step": 57820
    },
    {
      "epoch": 422.11678832116786,
      "grad_norm": 7.445893287658691,
      "learning_rate": 2.889416058394161e-05,
      "loss": 0.6389,
      "step": 57830
    },
    {
      "epoch": 422.18978102189783,
      "grad_norm": 7.737799167633057,
      "learning_rate": 2.889051094890511e-05,
      "loss": 1.0116,
      "step": 57840
    },
    {
      "epoch": 422.26277372262774,
      "grad_norm": 11.013545036315918,
      "learning_rate": 2.8886861313868614e-05,
      "loss": 1.2327,
      "step": 57850
    },
    {
      "epoch": 422.33576642335765,
      "grad_norm": 10.355932235717773,
      "learning_rate": 2.8883211678832118e-05,
      "loss": 1.0096,
      "step": 57860
    },
    {
      "epoch": 422.40875912408757,
      "grad_norm": 8.567854881286621,
      "learning_rate": 2.887956204379562e-05,
      "loss": 0.7942,
      "step": 57870
    },
    {
      "epoch": 422.48175182481754,
      "grad_norm": 13.080989837646484,
      "learning_rate": 2.8875912408759125e-05,
      "loss": 0.7997,
      "step": 57880
    },
    {
      "epoch": 422.55474452554745,
      "grad_norm": 10.710149765014648,
      "learning_rate": 2.887226277372263e-05,
      "loss": 0.9324,
      "step": 57890
    },
    {
      "epoch": 422.62773722627736,
      "grad_norm": 6.40375280380249,
      "learning_rate": 2.8868613138686133e-05,
      "loss": 1.0695,
      "step": 57900
    },
    {
      "epoch": 422.7007299270073,
      "grad_norm": 9.010268211364746,
      "learning_rate": 2.886496350364964e-05,
      "loss": 0.6598,
      "step": 57910
    },
    {
      "epoch": 422.77372262773724,
      "grad_norm": 8.316086769104004,
      "learning_rate": 2.8861313868613138e-05,
      "loss": 1.1121,
      "step": 57920
    },
    {
      "epoch": 422.84671532846716,
      "grad_norm": 20.385623931884766,
      "learning_rate": 2.885766423357664e-05,
      "loss": 1.1914,
      "step": 57930
    },
    {
      "epoch": 422.91970802919707,
      "grad_norm": 11.31613826751709,
      "learning_rate": 2.8854014598540145e-05,
      "loss": 0.7279,
      "step": 57940
    },
    {
      "epoch": 422.992700729927,
      "grad_norm": 8.51816177368164,
      "learning_rate": 2.8850364963503653e-05,
      "loss": 0.531,
      "step": 57950
    },
    {
      "epoch": 423.06569343065695,
      "grad_norm": 5.468365669250488,
      "learning_rate": 2.8846715328467156e-05,
      "loss": 0.8783,
      "step": 57960
    },
    {
      "epoch": 423.13868613138686,
      "grad_norm": 0.5624083876609802,
      "learning_rate": 2.884306569343066e-05,
      "loss": 0.7236,
      "step": 57970
    },
    {
      "epoch": 423.2116788321168,
      "grad_norm": 22.697044372558594,
      "learning_rate": 2.8839416058394164e-05,
      "loss": 0.8304,
      "step": 57980
    },
    {
      "epoch": 423.2846715328467,
      "grad_norm": 6.0830206871032715,
      "learning_rate": 2.883576642335766e-05,
      "loss": 0.6381,
      "step": 57990
    },
    {
      "epoch": 423.35766423357666,
      "grad_norm": 8.588777542114258,
      "learning_rate": 2.883211678832117e-05,
      "loss": 1.0136,
      "step": 58000
    },
    {
      "epoch": 423.43065693430657,
      "grad_norm": 5.705842971801758,
      "learning_rate": 2.8828467153284672e-05,
      "loss": 0.4618,
      "step": 58010
    },
    {
      "epoch": 423.5036496350365,
      "grad_norm": 0.614619791507721,
      "learning_rate": 2.8824817518248176e-05,
      "loss": 0.8846,
      "step": 58020
    },
    {
      "epoch": 423.57664233576645,
      "grad_norm": 11.990039825439453,
      "learning_rate": 2.882116788321168e-05,
      "loss": 1.1708,
      "step": 58030
    },
    {
      "epoch": 423.64963503649636,
      "grad_norm": 13.120893478393555,
      "learning_rate": 2.8817518248175184e-05,
      "loss": 1.144,
      "step": 58040
    },
    {
      "epoch": 423.7226277372263,
      "grad_norm": 7.202369213104248,
      "learning_rate": 2.8813868613138688e-05,
      "loss": 0.9455,
      "step": 58050
    },
    {
      "epoch": 423.7956204379562,
      "grad_norm": 14.286041259765625,
      "learning_rate": 2.8810218978102195e-05,
      "loss": 0.9406,
      "step": 58060
    },
    {
      "epoch": 423.86861313868616,
      "grad_norm": 9.428045272827148,
      "learning_rate": 2.8806569343065692e-05,
      "loss": 1.1677,
      "step": 58070
    },
    {
      "epoch": 423.94160583941607,
      "grad_norm": 0.25839748978614807,
      "learning_rate": 2.8802919708029196e-05,
      "loss": 1.081,
      "step": 58080
    },
    {
      "epoch": 424.014598540146,
      "grad_norm": 14.185218811035156,
      "learning_rate": 2.87992700729927e-05,
      "loss": 1.0028,
      "step": 58090
    },
    {
      "epoch": 424.0875912408759,
      "grad_norm": 5.298643112182617,
      "learning_rate": 2.8795620437956204e-05,
      "loss": 0.7385,
      "step": 58100
    },
    {
      "epoch": 424.16058394160586,
      "grad_norm": 7.73491907119751,
      "learning_rate": 2.879197080291971e-05,
      "loss": 0.9004,
      "step": 58110
    },
    {
      "epoch": 424.2335766423358,
      "grad_norm": 10.88690185546875,
      "learning_rate": 2.8788321167883215e-05,
      "loss": 0.6975,
      "step": 58120
    },
    {
      "epoch": 424.3065693430657,
      "grad_norm": 4.745779991149902,
      "learning_rate": 2.878467153284672e-05,
      "loss": 0.4835,
      "step": 58130
    },
    {
      "epoch": 424.3795620437956,
      "grad_norm": 6.422109127044678,
      "learning_rate": 2.8781021897810223e-05,
      "loss": 1.1573,
      "step": 58140
    },
    {
      "epoch": 424.45255474452557,
      "grad_norm": 0.13611769676208496,
      "learning_rate": 2.8777372262773723e-05,
      "loss": 0.748,
      "step": 58150
    },
    {
      "epoch": 424.5255474452555,
      "grad_norm": 4.647215366363525,
      "learning_rate": 2.8773722627737227e-05,
      "loss": 0.8159,
      "step": 58160
    },
    {
      "epoch": 424.5985401459854,
      "grad_norm": 0.3056508004665375,
      "learning_rate": 2.877007299270073e-05,
      "loss": 0.981,
      "step": 58170
    },
    {
      "epoch": 424.6715328467153,
      "grad_norm": 8.201896667480469,
      "learning_rate": 2.8766423357664235e-05,
      "loss": 0.9602,
      "step": 58180
    },
    {
      "epoch": 424.7445255474453,
      "grad_norm": 7.14711856842041,
      "learning_rate": 2.876277372262774e-05,
      "loss": 0.9139,
      "step": 58190
    },
    {
      "epoch": 424.8175182481752,
      "grad_norm": 15.422111511230469,
      "learning_rate": 2.8759124087591243e-05,
      "loss": 1.1101,
      "step": 58200
    },
    {
      "epoch": 424.8905109489051,
      "grad_norm": 10.518123626708984,
      "learning_rate": 2.8755474452554747e-05,
      "loss": 0.8122,
      "step": 58210
    },
    {
      "epoch": 424.963503649635,
      "grad_norm": 9.361303329467773,
      "learning_rate": 2.8751824817518247e-05,
      "loss": 1.329,
      "step": 58220
    },
    {
      "epoch": 425.036496350365,
      "grad_norm": 15.185588836669922,
      "learning_rate": 2.874817518248175e-05,
      "loss": 0.8156,
      "step": 58230
    },
    {
      "epoch": 425.1094890510949,
      "grad_norm": 17.838300704956055,
      "learning_rate": 2.8744525547445255e-05,
      "loss": 0.8408,
      "step": 58240
    },
    {
      "epoch": 425.1824817518248,
      "grad_norm": 10.338685989379883,
      "learning_rate": 2.874087591240876e-05,
      "loss": 0.5371,
      "step": 58250
    },
    {
      "epoch": 425.2554744525547,
      "grad_norm": 10.579901695251465,
      "learning_rate": 2.8737226277372266e-05,
      "loss": 0.9926,
      "step": 58260
    },
    {
      "epoch": 425.3284671532847,
      "grad_norm": 0.12218794971704483,
      "learning_rate": 2.873357664233577e-05,
      "loss": 0.7641,
      "step": 58270
    },
    {
      "epoch": 425.4014598540146,
      "grad_norm": 10.332995414733887,
      "learning_rate": 2.8729927007299274e-05,
      "loss": 1.0394,
      "step": 58280
    },
    {
      "epoch": 425.4744525547445,
      "grad_norm": 7.105661392211914,
      "learning_rate": 2.8726277372262778e-05,
      "loss": 0.792,
      "step": 58290
    },
    {
      "epoch": 425.54744525547443,
      "grad_norm": 10.358241081237793,
      "learning_rate": 2.8722627737226275e-05,
      "loss": 1.525,
      "step": 58300
    },
    {
      "epoch": 425.6204379562044,
      "grad_norm": 0.09672780334949493,
      "learning_rate": 2.8718978102189782e-05,
      "loss": 0.864,
      "step": 58310
    },
    {
      "epoch": 425.6934306569343,
      "grad_norm": 11.902433395385742,
      "learning_rate": 2.8715328467153286e-05,
      "loss": 1.1167,
      "step": 58320
    },
    {
      "epoch": 425.7664233576642,
      "grad_norm": 11.743572235107422,
      "learning_rate": 2.871167883211679e-05,
      "loss": 0.6422,
      "step": 58330
    },
    {
      "epoch": 425.83941605839414,
      "grad_norm": 11.701760292053223,
      "learning_rate": 2.8708029197080294e-05,
      "loss": 1.2594,
      "step": 58340
    },
    {
      "epoch": 425.9124087591241,
      "grad_norm": 1.407454252243042,
      "learning_rate": 2.8704379562043798e-05,
      "loss": 0.4468,
      "step": 58350
    },
    {
      "epoch": 425.985401459854,
      "grad_norm": 10.481873512268066,
      "learning_rate": 2.87007299270073e-05,
      "loss": 1.0514,
      "step": 58360
    },
    {
      "epoch": 426.05839416058393,
      "grad_norm": 0.230425164103508,
      "learning_rate": 2.8697080291970805e-05,
      "loss": 0.7598,
      "step": 58370
    },
    {
      "epoch": 426.13138686131384,
      "grad_norm": 10.317614555358887,
      "learning_rate": 2.8693430656934306e-05,
      "loss": 0.8572,
      "step": 58380
    },
    {
      "epoch": 426.2043795620438,
      "grad_norm": 11.78420352935791,
      "learning_rate": 2.868978102189781e-05,
      "loss": 1.1243,
      "step": 58390
    },
    {
      "epoch": 426.2773722627737,
      "grad_norm": 3.907773971557617,
      "learning_rate": 2.8686131386861314e-05,
      "loss": 0.6657,
      "step": 58400
    },
    {
      "epoch": 426.35036496350364,
      "grad_norm": 12.122048377990723,
      "learning_rate": 2.8682481751824818e-05,
      "loss": 0.6753,
      "step": 58410
    },
    {
      "epoch": 426.42335766423355,
      "grad_norm": 0.1422048807144165,
      "learning_rate": 2.8678832116788325e-05,
      "loss": 0.4078,
      "step": 58420
    },
    {
      "epoch": 426.4963503649635,
      "grad_norm": 0.11979527026414871,
      "learning_rate": 2.867518248175183e-05,
      "loss": 1.0545,
      "step": 58430
    },
    {
      "epoch": 426.56934306569343,
      "grad_norm": 10.240117073059082,
      "learning_rate": 2.8671532846715333e-05,
      "loss": 0.6369,
      "step": 58440
    },
    {
      "epoch": 426.64233576642334,
      "grad_norm": 15.98549747467041,
      "learning_rate": 2.866788321167883e-05,
      "loss": 1.2994,
      "step": 58450
    },
    {
      "epoch": 426.7153284671533,
      "grad_norm": 14.349849700927734,
      "learning_rate": 2.8664233576642334e-05,
      "loss": 1.1008,
      "step": 58460
    },
    {
      "epoch": 426.7883211678832,
      "grad_norm": 10.284964561462402,
      "learning_rate": 2.866058394160584e-05,
      "loss": 1.0665,
      "step": 58470
    },
    {
      "epoch": 426.86131386861314,
      "grad_norm": 8.257701873779297,
      "learning_rate": 2.8656934306569345e-05,
      "loss": 1.0656,
      "step": 58480
    },
    {
      "epoch": 426.93430656934305,
      "grad_norm": 19.428813934326172,
      "learning_rate": 2.865328467153285e-05,
      "loss": 1.4327,
      "step": 58490
    },
    {
      "epoch": 427.007299270073,
      "grad_norm": 0.21450258791446686,
      "learning_rate": 2.8649635036496353e-05,
      "loss": 0.9151,
      "step": 58500
    },
    {
      "epoch": 427.08029197080293,
      "grad_norm": 8.626108169555664,
      "learning_rate": 2.8645985401459856e-05,
      "loss": 0.826,
      "step": 58510
    },
    {
      "epoch": 427.15328467153284,
      "grad_norm": 15.73029613494873,
      "learning_rate": 2.864233576642336e-05,
      "loss": 0.9866,
      "step": 58520
    },
    {
      "epoch": 427.22627737226276,
      "grad_norm": 6.568179130554199,
      "learning_rate": 2.863868613138686e-05,
      "loss": 0.6529,
      "step": 58530
    },
    {
      "epoch": 427.2992700729927,
      "grad_norm": 6.470004558563232,
      "learning_rate": 2.8635036496350365e-05,
      "loss": 0.7114,
      "step": 58540
    },
    {
      "epoch": 427.37226277372264,
      "grad_norm": 11.628997802734375,
      "learning_rate": 2.863138686131387e-05,
      "loss": 0.8456,
      "step": 58550
    },
    {
      "epoch": 427.44525547445255,
      "grad_norm": 7.580678939819336,
      "learning_rate": 2.8627737226277372e-05,
      "loss": 0.717,
      "step": 58560
    },
    {
      "epoch": 427.51824817518246,
      "grad_norm": 0.26205703616142273,
      "learning_rate": 2.8624087591240876e-05,
      "loss": 1.127,
      "step": 58570
    },
    {
      "epoch": 427.59124087591243,
      "grad_norm": 15.54060173034668,
      "learning_rate": 2.8620437956204384e-05,
      "loss": 0.932,
      "step": 58580
    },
    {
      "epoch": 427.66423357664235,
      "grad_norm": 9.972033500671387,
      "learning_rate": 2.8616788321167887e-05,
      "loss": 1.0087,
      "step": 58590
    },
    {
      "epoch": 427.73722627737226,
      "grad_norm": 12.450143814086914,
      "learning_rate": 2.8613138686131385e-05,
      "loss": 1.0265,
      "step": 58600
    },
    {
      "epoch": 427.81021897810217,
      "grad_norm": 11.418684005737305,
      "learning_rate": 2.860948905109489e-05,
      "loss": 0.7378,
      "step": 58610
    },
    {
      "epoch": 427.88321167883214,
      "grad_norm": 10.108004570007324,
      "learning_rate": 2.8605839416058396e-05,
      "loss": 1.2502,
      "step": 58620
    },
    {
      "epoch": 427.95620437956205,
      "grad_norm": 19.45328140258789,
      "learning_rate": 2.86021897810219e-05,
      "loss": 1.2089,
      "step": 58630
    },
    {
      "epoch": 428.02919708029196,
      "grad_norm": 5.930868148803711,
      "learning_rate": 2.8598540145985404e-05,
      "loss": 0.9366,
      "step": 58640
    },
    {
      "epoch": 428.1021897810219,
      "grad_norm": 11.630592346191406,
      "learning_rate": 2.8594890510948907e-05,
      "loss": 0.797,
      "step": 58650
    },
    {
      "epoch": 428.17518248175185,
      "grad_norm": 15.186419486999512,
      "learning_rate": 2.859124087591241e-05,
      "loss": 1.2048,
      "step": 58660
    },
    {
      "epoch": 428.24817518248176,
      "grad_norm": 6.588407516479492,
      "learning_rate": 2.8587591240875915e-05,
      "loss": 0.7551,
      "step": 58670
    },
    {
      "epoch": 428.3211678832117,
      "grad_norm": 11.864110946655273,
      "learning_rate": 2.8583941605839416e-05,
      "loss": 0.8622,
      "step": 58680
    },
    {
      "epoch": 428.3941605839416,
      "grad_norm": 14.7810697555542,
      "learning_rate": 2.858029197080292e-05,
      "loss": 0.4831,
      "step": 58690
    },
    {
      "epoch": 428.46715328467155,
      "grad_norm": 9.811505317687988,
      "learning_rate": 2.8576642335766423e-05,
      "loss": 1.0261,
      "step": 58700
    },
    {
      "epoch": 428.54014598540147,
      "grad_norm": 9.62498664855957,
      "learning_rate": 2.8572992700729927e-05,
      "loss": 1.1802,
      "step": 58710
    },
    {
      "epoch": 428.6131386861314,
      "grad_norm": 0.08808387815952301,
      "learning_rate": 2.856934306569343e-05,
      "loss": 0.7491,
      "step": 58720
    },
    {
      "epoch": 428.6861313868613,
      "grad_norm": 6.0182647705078125,
      "learning_rate": 2.856569343065694e-05,
      "loss": 1.1497,
      "step": 58730
    },
    {
      "epoch": 428.75912408759126,
      "grad_norm": 0.08061165362596512,
      "learning_rate": 2.8562043795620442e-05,
      "loss": 0.6848,
      "step": 58740
    },
    {
      "epoch": 428.8321167883212,
      "grad_norm": 21.467464447021484,
      "learning_rate": 2.8558394160583946e-05,
      "loss": 1.0975,
      "step": 58750
    },
    {
      "epoch": 428.9051094890511,
      "grad_norm": 11.698232650756836,
      "learning_rate": 2.8554744525547443e-05,
      "loss": 0.8631,
      "step": 58760
    },
    {
      "epoch": 428.978102189781,
      "grad_norm": 9.395479202270508,
      "learning_rate": 2.8551094890510947e-05,
      "loss": 0.8886,
      "step": 58770
    },
    {
      "epoch": 429.05109489051097,
      "grad_norm": 12.994667053222656,
      "learning_rate": 2.8547445255474454e-05,
      "loss": 0.9712,
      "step": 58780
    },
    {
      "epoch": 429.1240875912409,
      "grad_norm": 5.880415439605713,
      "learning_rate": 2.854379562043796e-05,
      "loss": 0.9369,
      "step": 58790
    },
    {
      "epoch": 429.1970802919708,
      "grad_norm": 9.626374244689941,
      "learning_rate": 2.8540145985401462e-05,
      "loss": 0.761,
      "step": 58800
    },
    {
      "epoch": 429.2700729927007,
      "grad_norm": 5.953519821166992,
      "learning_rate": 2.8536496350364966e-05,
      "loss": 0.637,
      "step": 58810
    },
    {
      "epoch": 429.3430656934307,
      "grad_norm": 8.957845687866211,
      "learning_rate": 2.853284671532847e-05,
      "loss": 1.0339,
      "step": 58820
    },
    {
      "epoch": 429.4160583941606,
      "grad_norm": 13.375298500061035,
      "learning_rate": 2.852919708029197e-05,
      "loss": 1.0527,
      "step": 58830
    },
    {
      "epoch": 429.4890510948905,
      "grad_norm": 8.370485305786133,
      "learning_rate": 2.8525547445255474e-05,
      "loss": 0.894,
      "step": 58840
    },
    {
      "epoch": 429.5620437956204,
      "grad_norm": 9.213174819946289,
      "learning_rate": 2.8521897810218978e-05,
      "loss": 0.7542,
      "step": 58850
    },
    {
      "epoch": 429.6350364963504,
      "grad_norm": 14.234047889709473,
      "learning_rate": 2.8518248175182482e-05,
      "loss": 0.9339,
      "step": 58860
    },
    {
      "epoch": 429.7080291970803,
      "grad_norm": 6.76301383972168,
      "learning_rate": 2.8514598540145986e-05,
      "loss": 0.9945,
      "step": 58870
    },
    {
      "epoch": 429.7810218978102,
      "grad_norm": 6.736331939697266,
      "learning_rate": 2.851094890510949e-05,
      "loss": 0.7652,
      "step": 58880
    },
    {
      "epoch": 429.8540145985401,
      "grad_norm": 15.36837100982666,
      "learning_rate": 2.8507299270072997e-05,
      "loss": 1.0912,
      "step": 58890
    },
    {
      "epoch": 429.9270072992701,
      "grad_norm": 16.192392349243164,
      "learning_rate": 2.85036496350365e-05,
      "loss": 0.9429,
      "step": 58900
    },
    {
      "epoch": 430.0,
      "grad_norm": 0.15295784175395966,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.0289,
      "step": 58910
    },
    {
      "epoch": 430.0729927007299,
      "grad_norm": 0.1436384916305542,
      "learning_rate": 2.8496350364963502e-05,
      "loss": 0.7118,
      "step": 58920
    },
    {
      "epoch": 430.1459854014599,
      "grad_norm": 7.873467922210693,
      "learning_rate": 2.849270072992701e-05,
      "loss": 0.8746,
      "step": 58930
    },
    {
      "epoch": 430.2189781021898,
      "grad_norm": 9.451543807983398,
      "learning_rate": 2.8489051094890513e-05,
      "loss": 0.8097,
      "step": 58940
    },
    {
      "epoch": 430.2919708029197,
      "grad_norm": 8.723697662353516,
      "learning_rate": 2.8485401459854017e-05,
      "loss": 0.7013,
      "step": 58950
    },
    {
      "epoch": 430.3649635036496,
      "grad_norm": 8.740857124328613,
      "learning_rate": 2.848175182481752e-05,
      "loss": 0.7386,
      "step": 58960
    },
    {
      "epoch": 430.4379562043796,
      "grad_norm": 13.515458106994629,
      "learning_rate": 2.8478102189781025e-05,
      "loss": 1.0831,
      "step": 58970
    },
    {
      "epoch": 430.5109489051095,
      "grad_norm": 11.291422843933105,
      "learning_rate": 2.847445255474453e-05,
      "loss": 0.9385,
      "step": 58980
    },
    {
      "epoch": 430.5839416058394,
      "grad_norm": 0.08110527694225311,
      "learning_rate": 2.847080291970803e-05,
      "loss": 0.8983,
      "step": 58990
    },
    {
      "epoch": 430.6569343065693,
      "grad_norm": 13.981657981872559,
      "learning_rate": 2.8467153284671533e-05,
      "loss": 1.0406,
      "step": 59000
    },
    {
      "epoch": 430.7299270072993,
      "grad_norm": 17.73578453063965,
      "learning_rate": 2.8463503649635037e-05,
      "loss": 1.5,
      "step": 59010
    },
    {
      "epoch": 430.8029197080292,
      "grad_norm": 13.295591354370117,
      "learning_rate": 2.845985401459854e-05,
      "loss": 0.9241,
      "step": 59020
    },
    {
      "epoch": 430.8759124087591,
      "grad_norm": 0.06584691256284714,
      "learning_rate": 2.8456204379562045e-05,
      "loss": 0.8823,
      "step": 59030
    },
    {
      "epoch": 430.94890510948903,
      "grad_norm": 6.637505054473877,
      "learning_rate": 2.845255474452555e-05,
      "loss": 1.1,
      "step": 59040
    },
    {
      "epoch": 431.021897810219,
      "grad_norm": 5.460838317871094,
      "learning_rate": 2.8448905109489056e-05,
      "loss": 0.8006,
      "step": 59050
    },
    {
      "epoch": 431.0948905109489,
      "grad_norm": 8.612236022949219,
      "learning_rate": 2.8445255474452553e-05,
      "loss": 1.0819,
      "step": 59060
    },
    {
      "epoch": 431.1678832116788,
      "grad_norm": 12.015430450439453,
      "learning_rate": 2.8441605839416057e-05,
      "loss": 0.752,
      "step": 59070
    },
    {
      "epoch": 431.24087591240874,
      "grad_norm": 9.152298927307129,
      "learning_rate": 2.843795620437956e-05,
      "loss": 0.8381,
      "step": 59080
    },
    {
      "epoch": 431.3138686131387,
      "grad_norm": 0.05520610511302948,
      "learning_rate": 2.8434306569343068e-05,
      "loss": 0.8856,
      "step": 59090
    },
    {
      "epoch": 431.3868613138686,
      "grad_norm": 12.40887451171875,
      "learning_rate": 2.8430656934306572e-05,
      "loss": 0.8232,
      "step": 59100
    },
    {
      "epoch": 431.45985401459853,
      "grad_norm": 16.88749122619629,
      "learning_rate": 2.8427007299270076e-05,
      "loss": 0.7586,
      "step": 59110
    },
    {
      "epoch": 431.53284671532845,
      "grad_norm": 13.692727088928223,
      "learning_rate": 2.842335766423358e-05,
      "loss": 0.8801,
      "step": 59120
    },
    {
      "epoch": 431.6058394160584,
      "grad_norm": 9.62940502166748,
      "learning_rate": 2.8419708029197084e-05,
      "loss": 1.0346,
      "step": 59130
    },
    {
      "epoch": 431.6788321167883,
      "grad_norm": 5.195396423339844,
      "learning_rate": 2.8416058394160584e-05,
      "loss": 1.1357,
      "step": 59140
    },
    {
      "epoch": 431.75182481751824,
      "grad_norm": 9.426066398620605,
      "learning_rate": 2.8412408759124088e-05,
      "loss": 0.8073,
      "step": 59150
    },
    {
      "epoch": 431.82481751824815,
      "grad_norm": 14.080010414123535,
      "learning_rate": 2.8408759124087592e-05,
      "loss": 0.6783,
      "step": 59160
    },
    {
      "epoch": 431.8978102189781,
      "grad_norm": 7.927516937255859,
      "learning_rate": 2.8405109489051096e-05,
      "loss": 1.1657,
      "step": 59170
    },
    {
      "epoch": 431.97080291970804,
      "grad_norm": 13.693500518798828,
      "learning_rate": 2.84014598540146e-05,
      "loss": 1.1742,
      "step": 59180
    },
    {
      "epoch": 432.04379562043795,
      "grad_norm": 4.944979667663574,
      "learning_rate": 2.8397810218978104e-05,
      "loss": 0.6977,
      "step": 59190
    },
    {
      "epoch": 432.11678832116786,
      "grad_norm": 0.19432395696640015,
      "learning_rate": 2.839416058394161e-05,
      "loss": 0.4898,
      "step": 59200
    },
    {
      "epoch": 432.18978102189783,
      "grad_norm": 14.725032806396484,
      "learning_rate": 2.8390510948905115e-05,
      "loss": 0.571,
      "step": 59210
    },
    {
      "epoch": 432.26277372262774,
      "grad_norm": 15.085978507995605,
      "learning_rate": 2.8386861313868612e-05,
      "loss": 1.264,
      "step": 59220
    },
    {
      "epoch": 432.33576642335765,
      "grad_norm": 9.278717994689941,
      "learning_rate": 2.8383211678832116e-05,
      "loss": 1.1685,
      "step": 59230
    },
    {
      "epoch": 432.40875912408757,
      "grad_norm": 0.1537351757287979,
      "learning_rate": 2.837956204379562e-05,
      "loss": 0.8102,
      "step": 59240
    },
    {
      "epoch": 432.48175182481754,
      "grad_norm": 11.180749893188477,
      "learning_rate": 2.8375912408759127e-05,
      "loss": 1.1079,
      "step": 59250
    },
    {
      "epoch": 432.55474452554745,
      "grad_norm": 14.10682201385498,
      "learning_rate": 2.837226277372263e-05,
      "loss": 0.8056,
      "step": 59260
    },
    {
      "epoch": 432.62773722627736,
      "grad_norm": 7.395777225494385,
      "learning_rate": 2.8368613138686135e-05,
      "loss": 0.8748,
      "step": 59270
    },
    {
      "epoch": 432.7007299270073,
      "grad_norm": 15.049991607666016,
      "learning_rate": 2.836496350364964e-05,
      "loss": 1.0734,
      "step": 59280
    },
    {
      "epoch": 432.77372262773724,
      "grad_norm": 10.744996070861816,
      "learning_rate": 2.836131386861314e-05,
      "loss": 0.9623,
      "step": 59290
    },
    {
      "epoch": 432.84671532846716,
      "grad_norm": 9.45709228515625,
      "learning_rate": 2.8357664233576643e-05,
      "loss": 0.9634,
      "step": 59300
    },
    {
      "epoch": 432.91970802919707,
      "grad_norm": 5.876845836639404,
      "learning_rate": 2.8354014598540147e-05,
      "loss": 1.0221,
      "step": 59310
    },
    {
      "epoch": 432.992700729927,
      "grad_norm": 11.182653427124023,
      "learning_rate": 2.835036496350365e-05,
      "loss": 1.0497,
      "step": 59320
    },
    {
      "epoch": 433.06569343065695,
      "grad_norm": 0.0859723910689354,
      "learning_rate": 2.8346715328467154e-05,
      "loss": 1.0957,
      "step": 59330
    },
    {
      "epoch": 433.13868613138686,
      "grad_norm": 5.482611179351807,
      "learning_rate": 2.834306569343066e-05,
      "loss": 1.0453,
      "step": 59340
    },
    {
      "epoch": 433.2116788321168,
      "grad_norm": 7.593780517578125,
      "learning_rate": 2.8339416058394162e-05,
      "loss": 0.7321,
      "step": 59350
    },
    {
      "epoch": 433.2846715328467,
      "grad_norm": 6.876106262207031,
      "learning_rate": 2.833576642335767e-05,
      "loss": 1.0093,
      "step": 59360
    },
    {
      "epoch": 433.35766423357666,
      "grad_norm": 21.976505279541016,
      "learning_rate": 2.8332116788321167e-05,
      "loss": 0.7445,
      "step": 59370
    },
    {
      "epoch": 433.43065693430657,
      "grad_norm": 8.192078590393066,
      "learning_rate": 2.832846715328467e-05,
      "loss": 0.822,
      "step": 59380
    },
    {
      "epoch": 433.5036496350365,
      "grad_norm": 12.826272010803223,
      "learning_rate": 2.8324817518248174e-05,
      "loss": 0.818,
      "step": 59390
    },
    {
      "epoch": 433.57664233576645,
      "grad_norm": 9.728001594543457,
      "learning_rate": 2.832116788321168e-05,
      "loss": 1.0696,
      "step": 59400
    },
    {
      "epoch": 433.64963503649636,
      "grad_norm": 0.11373744159936905,
      "learning_rate": 2.8317518248175186e-05,
      "loss": 0.8028,
      "step": 59410
    },
    {
      "epoch": 433.7226277372263,
      "grad_norm": 9.990058898925781,
      "learning_rate": 2.831386861313869e-05,
      "loss": 0.7432,
      "step": 59420
    },
    {
      "epoch": 433.7956204379562,
      "grad_norm": 19.620868682861328,
      "learning_rate": 2.8310218978102193e-05,
      "loss": 1.1806,
      "step": 59430
    },
    {
      "epoch": 433.86861313868616,
      "grad_norm": 0.17832383513450623,
      "learning_rate": 2.8306569343065697e-05,
      "loss": 0.8608,
      "step": 59440
    },
    {
      "epoch": 433.94160583941607,
      "grad_norm": 18.68206787109375,
      "learning_rate": 2.8302919708029198e-05,
      "loss": 0.8583,
      "step": 59450
    },
    {
      "epoch": 434.014598540146,
      "grad_norm": 8.640371322631836,
      "learning_rate": 2.82992700729927e-05,
      "loss": 0.7098,
      "step": 59460
    },
    {
      "epoch": 434.0875912408759,
      "grad_norm": 6.505380153656006,
      "learning_rate": 2.8295620437956205e-05,
      "loss": 1.089,
      "step": 59470
    },
    {
      "epoch": 434.16058394160586,
      "grad_norm": 0.6565868854522705,
      "learning_rate": 2.829197080291971e-05,
      "loss": 0.8997,
      "step": 59480
    },
    {
      "epoch": 434.2335766423358,
      "grad_norm": 13.040692329406738,
      "learning_rate": 2.8288321167883213e-05,
      "loss": 0.8471,
      "step": 59490
    },
    {
      "epoch": 434.3065693430657,
      "grad_norm": 12.232552528381348,
      "learning_rate": 2.8284671532846717e-05,
      "loss": 0.8333,
      "step": 59500
    },
    {
      "epoch": 434.3795620437956,
      "grad_norm": 6.038928031921387,
      "learning_rate": 2.828102189781022e-05,
      "loss": 0.9747,
      "step": 59510
    },
    {
      "epoch": 434.45255474452557,
      "grad_norm": 9.148640632629395,
      "learning_rate": 2.827737226277372e-05,
      "loss": 0.9636,
      "step": 59520
    },
    {
      "epoch": 434.5255474452555,
      "grad_norm": 11.021799087524414,
      "learning_rate": 2.8273722627737225e-05,
      "loss": 1.0958,
      "step": 59530
    },
    {
      "epoch": 434.5985401459854,
      "grad_norm": 9.368698120117188,
      "learning_rate": 2.827007299270073e-05,
      "loss": 0.8543,
      "step": 59540
    },
    {
      "epoch": 434.6715328467153,
      "grad_norm": 9.681764602661133,
      "learning_rate": 2.8266423357664233e-05,
      "loss": 0.581,
      "step": 59550
    },
    {
      "epoch": 434.7445255474453,
      "grad_norm": 10.009681701660156,
      "learning_rate": 2.826277372262774e-05,
      "loss": 1.1367,
      "step": 59560
    },
    {
      "epoch": 434.8175182481752,
      "grad_norm": 15.988136291503906,
      "learning_rate": 2.8259124087591244e-05,
      "loss": 0.9737,
      "step": 59570
    },
    {
      "epoch": 434.8905109489051,
      "grad_norm": 11.540833473205566,
      "learning_rate": 2.8255474452554748e-05,
      "loss": 0.7698,
      "step": 59580
    },
    {
      "epoch": 434.963503649635,
      "grad_norm": 9.633912086486816,
      "learning_rate": 2.8251824817518252e-05,
      "loss": 0.9556,
      "step": 59590
    },
    {
      "epoch": 435.036496350365,
      "grad_norm": 6.999073028564453,
      "learning_rate": 2.8248175182481753e-05,
      "loss": 0.7292,
      "step": 59600
    },
    {
      "epoch": 435.1094890510949,
      "grad_norm": 5.088659286499023,
      "learning_rate": 2.8244525547445256e-05,
      "loss": 0.4683,
      "step": 59610
    },
    {
      "epoch": 435.1824817518248,
      "grad_norm": 16.32686424255371,
      "learning_rate": 2.824087591240876e-05,
      "loss": 1.3688,
      "step": 59620
    },
    {
      "epoch": 435.2554744525547,
      "grad_norm": 6.65565824508667,
      "learning_rate": 2.8237226277372264e-05,
      "loss": 0.789,
      "step": 59630
    },
    {
      "epoch": 435.3284671532847,
      "grad_norm": 20.41322898864746,
      "learning_rate": 2.8233576642335768e-05,
      "loss": 0.7672,
      "step": 59640
    },
    {
      "epoch": 435.4014598540146,
      "grad_norm": 7.5855712890625,
      "learning_rate": 2.8229927007299272e-05,
      "loss": 0.8191,
      "step": 59650
    },
    {
      "epoch": 435.4744525547445,
      "grad_norm": 10.786338806152344,
      "learning_rate": 2.8226277372262776e-05,
      "loss": 1.2167,
      "step": 59660
    },
    {
      "epoch": 435.54744525547443,
      "grad_norm": 7.652407169342041,
      "learning_rate": 2.8222627737226276e-05,
      "loss": 0.9005,
      "step": 59670
    },
    {
      "epoch": 435.6204379562044,
      "grad_norm": 0.10092827677726746,
      "learning_rate": 2.821897810218978e-05,
      "loss": 0.66,
      "step": 59680
    },
    {
      "epoch": 435.6934306569343,
      "grad_norm": 0.14637881517410278,
      "learning_rate": 2.8215328467153284e-05,
      "loss": 0.6364,
      "step": 59690
    },
    {
      "epoch": 435.7664233576642,
      "grad_norm": 6.008213043212891,
      "learning_rate": 2.8211678832116788e-05,
      "loss": 1.4124,
      "step": 59700
    },
    {
      "epoch": 435.83941605839414,
      "grad_norm": 8.916472434997559,
      "learning_rate": 2.8208029197080292e-05,
      "loss": 0.7502,
      "step": 59710
    },
    {
      "epoch": 435.9124087591241,
      "grad_norm": 0.06453954428434372,
      "learning_rate": 2.82043795620438e-05,
      "loss": 1.0556,
      "step": 59720
    },
    {
      "epoch": 435.985401459854,
      "grad_norm": 10.492281913757324,
      "learning_rate": 2.8200729927007303e-05,
      "loss": 1.0946,
      "step": 59730
    },
    {
      "epoch": 436.05839416058393,
      "grad_norm": 11.993378639221191,
      "learning_rate": 2.8197080291970807e-05,
      "loss": 1.1772,
      "step": 59740
    },
    {
      "epoch": 436.13138686131384,
      "grad_norm": 14.140127182006836,
      "learning_rate": 2.8193430656934304e-05,
      "loss": 1.0487,
      "step": 59750
    },
    {
      "epoch": 436.2043795620438,
      "grad_norm": 10.723809242248535,
      "learning_rate": 2.818978102189781e-05,
      "loss": 0.6265,
      "step": 59760
    },
    {
      "epoch": 436.2773722627737,
      "grad_norm": 0.2109733372926712,
      "learning_rate": 2.8186131386861315e-05,
      "loss": 1.1685,
      "step": 59770
    },
    {
      "epoch": 436.35036496350364,
      "grad_norm": 7.372091770172119,
      "learning_rate": 2.818248175182482e-05,
      "loss": 0.9152,
      "step": 59780
    },
    {
      "epoch": 436.42335766423355,
      "grad_norm": 7.058280944824219,
      "learning_rate": 2.8178832116788323e-05,
      "loss": 0.8293,
      "step": 59790
    },
    {
      "epoch": 436.4963503649635,
      "grad_norm": 17.170076370239258,
      "learning_rate": 2.8175182481751827e-05,
      "loss": 0.709,
      "step": 59800
    },
    {
      "epoch": 436.56934306569343,
      "grad_norm": 13.468782424926758,
      "learning_rate": 2.817153284671533e-05,
      "loss": 0.7624,
      "step": 59810
    },
    {
      "epoch": 436.64233576642334,
      "grad_norm": 0.0896490290760994,
      "learning_rate": 2.8167883211678835e-05,
      "loss": 0.67,
      "step": 59820
    },
    {
      "epoch": 436.7153284671533,
      "grad_norm": 0.10071509331464767,
      "learning_rate": 2.8164233576642335e-05,
      "loss": 0.9447,
      "step": 59830
    },
    {
      "epoch": 436.7883211678832,
      "grad_norm": 0.05533856526017189,
      "learning_rate": 2.816058394160584e-05,
      "loss": 1.0936,
      "step": 59840
    },
    {
      "epoch": 436.86131386861314,
      "grad_norm": 8.107815742492676,
      "learning_rate": 2.8156934306569343e-05,
      "loss": 0.9335,
      "step": 59850
    },
    {
      "epoch": 436.93430656934305,
      "grad_norm": 9.86523723602295,
      "learning_rate": 2.8153284671532847e-05,
      "loss": 1.001,
      "step": 59860
    },
    {
      "epoch": 437.007299270073,
      "grad_norm": 0.12000839412212372,
      "learning_rate": 2.8149635036496354e-05,
      "loss": 0.5975,
      "step": 59870
    },
    {
      "epoch": 437.08029197080293,
      "grad_norm": 13.888293266296387,
      "learning_rate": 2.8145985401459858e-05,
      "loss": 0.9488,
      "step": 59880
    },
    {
      "epoch": 437.15328467153284,
      "grad_norm": 7.466854572296143,
      "learning_rate": 2.8142335766423362e-05,
      "loss": 0.8162,
      "step": 59890
    },
    {
      "epoch": 437.22627737226276,
      "grad_norm": 0.03820420056581497,
      "learning_rate": 2.813868613138686e-05,
      "loss": 0.6119,
      "step": 59900
    },
    {
      "epoch": 437.2992700729927,
      "grad_norm": 8.994850158691406,
      "learning_rate": 2.8135036496350363e-05,
      "loss": 0.5312,
      "step": 59910
    },
    {
      "epoch": 437.37226277372264,
      "grad_norm": 13.90816593170166,
      "learning_rate": 2.813138686131387e-05,
      "loss": 0.8337,
      "step": 59920
    },
    {
      "epoch": 437.44525547445255,
      "grad_norm": 18.109525680541992,
      "learning_rate": 2.8127737226277374e-05,
      "loss": 1.2113,
      "step": 59930
    },
    {
      "epoch": 437.51824817518246,
      "grad_norm": 9.464950561523438,
      "learning_rate": 2.8124087591240878e-05,
      "loss": 1.0519,
      "step": 59940
    },
    {
      "epoch": 437.59124087591243,
      "grad_norm": 6.940184116363525,
      "learning_rate": 2.812043795620438e-05,
      "loss": 1.312,
      "step": 59950
    },
    {
      "epoch": 437.66423357664235,
      "grad_norm": 12.19698715209961,
      "learning_rate": 2.8116788321167886e-05,
      "loss": 0.8815,
      "step": 59960
    },
    {
      "epoch": 437.73722627737226,
      "grad_norm": 15.367951393127441,
      "learning_rate": 2.811313868613139e-05,
      "loss": 1.0516,
      "step": 59970
    },
    {
      "epoch": 437.81021897810217,
      "grad_norm": 14.741527557373047,
      "learning_rate": 2.810948905109489e-05,
      "loss": 0.8985,
      "step": 59980
    },
    {
      "epoch": 437.88321167883214,
      "grad_norm": 0.19287492334842682,
      "learning_rate": 2.8105839416058394e-05,
      "loss": 0.7241,
      "step": 59990
    },
    {
      "epoch": 437.95620437956205,
      "grad_norm": 7.761124134063721,
      "learning_rate": 2.8102189781021898e-05,
      "loss": 1.0611,
      "step": 60000
    },
    {
      "epoch": 438.02919708029196,
      "grad_norm": 0.06536181271076202,
      "learning_rate": 2.80985401459854e-05,
      "loss": 1.0304,
      "step": 60010
    },
    {
      "epoch": 438.1021897810219,
      "grad_norm": 0.07882273942232132,
      "learning_rate": 2.8094890510948905e-05,
      "loss": 0.6943,
      "step": 60020
    },
    {
      "epoch": 438.17518248175185,
      "grad_norm": 11.451141357421875,
      "learning_rate": 2.8091240875912413e-05,
      "loss": 0.668,
      "step": 60030
    },
    {
      "epoch": 438.24817518248176,
      "grad_norm": 0.042756907641887665,
      "learning_rate": 2.8087591240875917e-05,
      "loss": 0.7722,
      "step": 60040
    },
    {
      "epoch": 438.3211678832117,
      "grad_norm": 12.139315605163574,
      "learning_rate": 2.808394160583942e-05,
      "loss": 0.7903,
      "step": 60050
    },
    {
      "epoch": 438.3941605839416,
      "grad_norm": 0.07712499052286148,
      "learning_rate": 2.8080291970802918e-05,
      "loss": 0.8378,
      "step": 60060
    },
    {
      "epoch": 438.46715328467155,
      "grad_norm": 10.497712135314941,
      "learning_rate": 2.8076642335766425e-05,
      "loss": 1.184,
      "step": 60070
    },
    {
      "epoch": 438.54014598540147,
      "grad_norm": 8.456486701965332,
      "learning_rate": 2.807299270072993e-05,
      "loss": 1.0825,
      "step": 60080
    },
    {
      "epoch": 438.6131386861314,
      "grad_norm": 7.789129257202148,
      "learning_rate": 2.8069343065693433e-05,
      "loss": 0.8886,
      "step": 60090
    },
    {
      "epoch": 438.6861313868613,
      "grad_norm": 8.155623435974121,
      "learning_rate": 2.8065693430656936e-05,
      "loss": 1.1229,
      "step": 60100
    },
    {
      "epoch": 438.75912408759126,
      "grad_norm": 11.622352600097656,
      "learning_rate": 2.806204379562044e-05,
      "loss": 1.3309,
      "step": 60110
    },
    {
      "epoch": 438.8321167883212,
      "grad_norm": 0.052351802587509155,
      "learning_rate": 2.8058394160583944e-05,
      "loss": 0.7975,
      "step": 60120
    },
    {
      "epoch": 438.9051094890511,
      "grad_norm": 8.922517776489258,
      "learning_rate": 2.8054744525547445e-05,
      "loss": 1.0685,
      "step": 60130
    },
    {
      "epoch": 438.978102189781,
      "grad_norm": 17.456096649169922,
      "learning_rate": 2.805109489051095e-05,
      "loss": 0.7807,
      "step": 60140
    },
    {
      "epoch": 439.05109489051097,
      "grad_norm": 0.06831631064414978,
      "learning_rate": 2.8047445255474452e-05,
      "loss": 0.6698,
      "step": 60150
    },
    {
      "epoch": 439.1240875912409,
      "grad_norm": 7.278043270111084,
      "learning_rate": 2.8043795620437956e-05,
      "loss": 0.6262,
      "step": 60160
    },
    {
      "epoch": 439.1970802919708,
      "grad_norm": 13.946026802062988,
      "learning_rate": 2.804014598540146e-05,
      "loss": 0.9647,
      "step": 60170
    },
    {
      "epoch": 439.2700729927007,
      "grad_norm": 10.552833557128906,
      "learning_rate": 2.8036496350364964e-05,
      "loss": 1.0549,
      "step": 60180
    },
    {
      "epoch": 439.3430656934307,
      "grad_norm": 0.17708657681941986,
      "learning_rate": 2.803284671532847e-05,
      "loss": 0.734,
      "step": 60190
    },
    {
      "epoch": 439.4160583941606,
      "grad_norm": 7.08843994140625,
      "learning_rate": 2.8029197080291975e-05,
      "loss": 0.7985,
      "step": 60200
    },
    {
      "epoch": 439.4890510948905,
      "grad_norm": 12.583460807800293,
      "learning_rate": 2.8025547445255472e-05,
      "loss": 1.1415,
      "step": 60210
    },
    {
      "epoch": 439.5620437956204,
      "grad_norm": 10.448210716247559,
      "learning_rate": 2.8021897810218976e-05,
      "loss": 0.8392,
      "step": 60220
    },
    {
      "epoch": 439.6350364963504,
      "grad_norm": 13.26796817779541,
      "learning_rate": 2.8018248175182484e-05,
      "loss": 1.1579,
      "step": 60230
    },
    {
      "epoch": 439.7080291970803,
      "grad_norm": 12.559494972229004,
      "learning_rate": 2.8014598540145987e-05,
      "loss": 1.2365,
      "step": 60240
    },
    {
      "epoch": 439.7810218978102,
      "grad_norm": 6.030876636505127,
      "learning_rate": 2.801094890510949e-05,
      "loss": 1.4503,
      "step": 60250
    },
    {
      "epoch": 439.8540145985401,
      "grad_norm": 0.15974441170692444,
      "learning_rate": 2.8007299270072995e-05,
      "loss": 0.7249,
      "step": 60260
    },
    {
      "epoch": 439.9270072992701,
      "grad_norm": 5.389672756195068,
      "learning_rate": 2.80036496350365e-05,
      "loss": 0.5689,
      "step": 60270
    },
    {
      "epoch": 440.0,
      "grad_norm": 15.698148727416992,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.7183,
      "step": 60280
    },
    {
      "epoch": 440.0729927007299,
      "grad_norm": 9.659774780273438,
      "learning_rate": 2.7996350364963503e-05,
      "loss": 0.9556,
      "step": 60290
    },
    {
      "epoch": 440.1459854014599,
      "grad_norm": 6.398787021636963,
      "learning_rate": 2.7992700729927007e-05,
      "loss": 0.7725,
      "step": 60300
    },
    {
      "epoch": 440.2189781021898,
      "grad_norm": 16.718727111816406,
      "learning_rate": 2.798905109489051e-05,
      "loss": 0.9838,
      "step": 60310
    },
    {
      "epoch": 440.2919708029197,
      "grad_norm": 5.993836879730225,
      "learning_rate": 2.7985401459854015e-05,
      "loss": 1.1692,
      "step": 60320
    },
    {
      "epoch": 440.3649635036496,
      "grad_norm": 12.048095703125,
      "learning_rate": 2.798175182481752e-05,
      "loss": 1.0845,
      "step": 60330
    },
    {
      "epoch": 440.4379562043796,
      "grad_norm": 7.259559154510498,
      "learning_rate": 2.7978102189781026e-05,
      "loss": 1.0928,
      "step": 60340
    },
    {
      "epoch": 440.5109489051095,
      "grad_norm": 6.522943496704102,
      "learning_rate": 2.797445255474453e-05,
      "loss": 1.1037,
      "step": 60350
    },
    {
      "epoch": 440.5839416058394,
      "grad_norm": 0.08858480304479599,
      "learning_rate": 2.7970802919708027e-05,
      "loss": 0.9831,
      "step": 60360
    },
    {
      "epoch": 440.6569343065693,
      "grad_norm": 6.220311641693115,
      "learning_rate": 2.796715328467153e-05,
      "loss": 0.8393,
      "step": 60370
    },
    {
      "epoch": 440.7299270072993,
      "grad_norm": 6.226583957672119,
      "learning_rate": 2.7963503649635035e-05,
      "loss": 0.6234,
      "step": 60380
    },
    {
      "epoch": 440.8029197080292,
      "grad_norm": 0.11798129975795746,
      "learning_rate": 2.7959854014598542e-05,
      "loss": 0.5666,
      "step": 60390
    },
    {
      "epoch": 440.8759124087591,
      "grad_norm": 17.219327926635742,
      "learning_rate": 2.7956204379562046e-05,
      "loss": 1.0513,
      "step": 60400
    },
    {
      "epoch": 440.94890510948903,
      "grad_norm": 8.603825569152832,
      "learning_rate": 2.795255474452555e-05,
      "loss": 0.711,
      "step": 60410
    },
    {
      "epoch": 441.021897810219,
      "grad_norm": 9.938329696655273,
      "learning_rate": 2.7948905109489054e-05,
      "loss": 1.2519,
      "step": 60420
    },
    {
      "epoch": 441.0948905109489,
      "grad_norm": 4.027266502380371,
      "learning_rate": 2.7945255474452558e-05,
      "loss": 0.6174,
      "step": 60430
    },
    {
      "epoch": 441.1678832116788,
      "grad_norm": 7.511291027069092,
      "learning_rate": 2.794160583941606e-05,
      "loss": 0.7844,
      "step": 60440
    },
    {
      "epoch": 441.24087591240874,
      "grad_norm": 12.014360427856445,
      "learning_rate": 2.7937956204379562e-05,
      "loss": 1.024,
      "step": 60450
    },
    {
      "epoch": 441.3138686131387,
      "grad_norm": 7.388351917266846,
      "learning_rate": 2.7934306569343066e-05,
      "loss": 0.8033,
      "step": 60460
    },
    {
      "epoch": 441.3868613138686,
      "grad_norm": 12.00539779663086,
      "learning_rate": 2.793065693430657e-05,
      "loss": 1.1886,
      "step": 60470
    },
    {
      "epoch": 441.45985401459853,
      "grad_norm": 19.78590202331543,
      "learning_rate": 2.7927007299270074e-05,
      "loss": 0.9681,
      "step": 60480
    },
    {
      "epoch": 441.53284671532845,
      "grad_norm": 10.953605651855469,
      "learning_rate": 2.7923357664233578e-05,
      "loss": 1.3009,
      "step": 60490
    },
    {
      "epoch": 441.6058394160584,
      "grad_norm": 10.717358589172363,
      "learning_rate": 2.7919708029197085e-05,
      "loss": 0.9982,
      "step": 60500
    },
    {
      "epoch": 441.6788321167883,
      "grad_norm": 0.09482463449239731,
      "learning_rate": 2.791605839416059e-05,
      "loss": 0.9306,
      "step": 60510
    },
    {
      "epoch": 441.75182481751824,
      "grad_norm": 11.23622989654541,
      "learning_rate": 2.7912408759124086e-05,
      "loss": 0.7653,
      "step": 60520
    },
    {
      "epoch": 441.82481751824815,
      "grad_norm": 10.942220687866211,
      "learning_rate": 2.790875912408759e-05,
      "loss": 1.0208,
      "step": 60530
    },
    {
      "epoch": 441.8978102189781,
      "grad_norm": 0.17915278673171997,
      "learning_rate": 2.7905109489051097e-05,
      "loss": 0.8573,
      "step": 60540
    },
    {
      "epoch": 441.97080291970804,
      "grad_norm": 9.2091703414917,
      "learning_rate": 2.79014598540146e-05,
      "loss": 0.7607,
      "step": 60550
    },
    {
      "epoch": 442.04379562043795,
      "grad_norm": 9.685388565063477,
      "learning_rate": 2.7897810218978105e-05,
      "loss": 1.0116,
      "step": 60560
    },
    {
      "epoch": 442.11678832116786,
      "grad_norm": 7.6310319900512695,
      "learning_rate": 2.789416058394161e-05,
      "loss": 0.8612,
      "step": 60570
    },
    {
      "epoch": 442.18978102189783,
      "grad_norm": 8.137319564819336,
      "learning_rate": 2.7890510948905113e-05,
      "loss": 1.1017,
      "step": 60580
    },
    {
      "epoch": 442.26277372262774,
      "grad_norm": 6.649929523468018,
      "learning_rate": 2.7886861313868613e-05,
      "loss": 1.0573,
      "step": 60590
    },
    {
      "epoch": 442.33576642335765,
      "grad_norm": 12.578045845031738,
      "learning_rate": 2.7883211678832117e-05,
      "loss": 0.9657,
      "step": 60600
    },
    {
      "epoch": 442.40875912408757,
      "grad_norm": 13.077818870544434,
      "learning_rate": 2.787956204379562e-05,
      "loss": 0.8534,
      "step": 60610
    },
    {
      "epoch": 442.48175182481754,
      "grad_norm": 7.450507640838623,
      "learning_rate": 2.7875912408759125e-05,
      "loss": 1.0795,
      "step": 60620
    },
    {
      "epoch": 442.55474452554745,
      "grad_norm": 14.811882972717285,
      "learning_rate": 2.787226277372263e-05,
      "loss": 1.0895,
      "step": 60630
    },
    {
      "epoch": 442.62773722627736,
      "grad_norm": 14.560480117797852,
      "learning_rate": 2.7868613138686133e-05,
      "loss": 0.9581,
      "step": 60640
    },
    {
      "epoch": 442.7007299270073,
      "grad_norm": 0.06519340723752975,
      "learning_rate": 2.7864963503649636e-05,
      "loss": 0.4497,
      "step": 60650
    },
    {
      "epoch": 442.77372262773724,
      "grad_norm": 0.07837606221437454,
      "learning_rate": 2.7861313868613144e-05,
      "loss": 0.7358,
      "step": 60660
    },
    {
      "epoch": 442.84671532846716,
      "grad_norm": 15.099123001098633,
      "learning_rate": 2.785766423357664e-05,
      "loss": 0.9416,
      "step": 60670
    },
    {
      "epoch": 442.91970802919707,
      "grad_norm": 2.654123306274414,
      "learning_rate": 2.7854014598540145e-05,
      "loss": 0.7196,
      "step": 60680
    },
    {
      "epoch": 442.992700729927,
      "grad_norm": 9.872360229492188,
      "learning_rate": 2.785036496350365e-05,
      "loss": 1.0169,
      "step": 60690
    },
    {
      "epoch": 443.06569343065695,
      "grad_norm": 23.277599334716797,
      "learning_rate": 2.7846715328467156e-05,
      "loss": 0.7958,
      "step": 60700
    },
    {
      "epoch": 443.13868613138686,
      "grad_norm": 14.103316307067871,
      "learning_rate": 2.784306569343066e-05,
      "loss": 1.1467,
      "step": 60710
    },
    {
      "epoch": 443.2116788321168,
      "grad_norm": 14.707965850830078,
      "learning_rate": 2.7839416058394164e-05,
      "loss": 0.9938,
      "step": 60720
    },
    {
      "epoch": 443.2846715328467,
      "grad_norm": 10.219470024108887,
      "learning_rate": 2.7835766423357668e-05,
      "loss": 0.8797,
      "step": 60730
    },
    {
      "epoch": 443.35766423357666,
      "grad_norm": 10.532343864440918,
      "learning_rate": 2.783211678832117e-05,
      "loss": 0.8441,
      "step": 60740
    },
    {
      "epoch": 443.43065693430657,
      "grad_norm": 6.489919185638428,
      "learning_rate": 2.7828467153284672e-05,
      "loss": 0.7758,
      "step": 60750
    },
    {
      "epoch": 443.5036496350365,
      "grad_norm": 11.593700408935547,
      "learning_rate": 2.7824817518248176e-05,
      "loss": 0.8056,
      "step": 60760
    },
    {
      "epoch": 443.57664233576645,
      "grad_norm": 11.625551223754883,
      "learning_rate": 2.782116788321168e-05,
      "loss": 0.829,
      "step": 60770
    },
    {
      "epoch": 443.64963503649636,
      "grad_norm": 9.741599082946777,
      "learning_rate": 2.7817518248175184e-05,
      "loss": 0.9712,
      "step": 60780
    },
    {
      "epoch": 443.7226277372263,
      "grad_norm": 9.410415649414062,
      "learning_rate": 2.7813868613138687e-05,
      "loss": 1.0283,
      "step": 60790
    },
    {
      "epoch": 443.7956204379562,
      "grad_norm": 9.965529441833496,
      "learning_rate": 2.781021897810219e-05,
      "loss": 1.0598,
      "step": 60800
    },
    {
      "epoch": 443.86861313868616,
      "grad_norm": 0.20987142622470856,
      "learning_rate": 2.78065693430657e-05,
      "loss": 0.6849,
      "step": 60810
    },
    {
      "epoch": 443.94160583941607,
      "grad_norm": 0.36776861548423767,
      "learning_rate": 2.7802919708029196e-05,
      "loss": 0.8688,
      "step": 60820
    },
    {
      "epoch": 444.014598540146,
      "grad_norm": 5.784497261047363,
      "learning_rate": 2.77992700729927e-05,
      "loss": 0.818,
      "step": 60830
    },
    {
      "epoch": 444.0875912408759,
      "grad_norm": 8.853830337524414,
      "learning_rate": 2.7795620437956203e-05,
      "loss": 0.9452,
      "step": 60840
    },
    {
      "epoch": 444.16058394160586,
      "grad_norm": 15.943936347961426,
      "learning_rate": 2.7791970802919707e-05,
      "loss": 1.0395,
      "step": 60850
    },
    {
      "epoch": 444.2335766423358,
      "grad_norm": 10.1970853805542,
      "learning_rate": 2.7788321167883215e-05,
      "loss": 0.8651,
      "step": 60860
    },
    {
      "epoch": 444.3065693430657,
      "grad_norm": 0.22792303562164307,
      "learning_rate": 2.778467153284672e-05,
      "loss": 0.9922,
      "step": 60870
    },
    {
      "epoch": 444.3795620437956,
      "grad_norm": 5.179875373840332,
      "learning_rate": 2.7781021897810222e-05,
      "loss": 0.922,
      "step": 60880
    },
    {
      "epoch": 444.45255474452557,
      "grad_norm": 6.69419527053833,
      "learning_rate": 2.7777372262773726e-05,
      "loss": 0.7236,
      "step": 60890
    },
    {
      "epoch": 444.5255474452555,
      "grad_norm": 12.60448169708252,
      "learning_rate": 2.7773722627737227e-05,
      "loss": 0.9589,
      "step": 60900
    },
    {
      "epoch": 444.5985401459854,
      "grad_norm": 8.228700637817383,
      "learning_rate": 2.777007299270073e-05,
      "loss": 0.6992,
      "step": 60910
    },
    {
      "epoch": 444.6715328467153,
      "grad_norm": 14.17287540435791,
      "learning_rate": 2.7766423357664235e-05,
      "loss": 0.9704,
      "step": 60920
    },
    {
      "epoch": 444.7445255474453,
      "grad_norm": 11.88896369934082,
      "learning_rate": 2.776277372262774e-05,
      "loss": 0.7705,
      "step": 60930
    },
    {
      "epoch": 444.8175182481752,
      "grad_norm": 11.29947280883789,
      "learning_rate": 2.7759124087591242e-05,
      "loss": 1.1298,
      "step": 60940
    },
    {
      "epoch": 444.8905109489051,
      "grad_norm": 7.924247741699219,
      "learning_rate": 2.7755474452554746e-05,
      "loss": 0.8503,
      "step": 60950
    },
    {
      "epoch": 444.963503649635,
      "grad_norm": 0.08164326101541519,
      "learning_rate": 2.775182481751825e-05,
      "loss": 0.7543,
      "step": 60960
    },
    {
      "epoch": 445.036496350365,
      "grad_norm": 10.247462272644043,
      "learning_rate": 2.774817518248175e-05,
      "loss": 1.1769,
      "step": 60970
    },
    {
      "epoch": 445.1094890510949,
      "grad_norm": 14.203887939453125,
      "learning_rate": 2.7744525547445254e-05,
      "loss": 1.0444,
      "step": 60980
    },
    {
      "epoch": 445.1824817518248,
      "grad_norm": 8.55838394165039,
      "learning_rate": 2.7740875912408758e-05,
      "loss": 0.8108,
      "step": 60990
    },
    {
      "epoch": 445.2554744525547,
      "grad_norm": 12.903576850891113,
      "learning_rate": 2.7737226277372262e-05,
      "loss": 1.153,
      "step": 61000
    },
    {
      "epoch": 445.3284671532847,
      "grad_norm": 9.076278686523438,
      "learning_rate": 2.773357664233577e-05,
      "loss": 0.8809,
      "step": 61010
    },
    {
      "epoch": 445.4014598540146,
      "grad_norm": 4.813606262207031,
      "learning_rate": 2.7729927007299273e-05,
      "loss": 0.9251,
      "step": 61020
    },
    {
      "epoch": 445.4744525547445,
      "grad_norm": 10.077750205993652,
      "learning_rate": 2.7726277372262777e-05,
      "loss": 1.0668,
      "step": 61030
    },
    {
      "epoch": 445.54744525547443,
      "grad_norm": 9.715551376342773,
      "learning_rate": 2.772262773722628e-05,
      "loss": 0.6345,
      "step": 61040
    },
    {
      "epoch": 445.6204379562044,
      "grad_norm": 18.725448608398438,
      "learning_rate": 2.7718978102189778e-05,
      "loss": 1.0519,
      "step": 61050
    },
    {
      "epoch": 445.6934306569343,
      "grad_norm": 5.4470133781433105,
      "learning_rate": 2.7715328467153285e-05,
      "loss": 1.1063,
      "step": 61060
    },
    {
      "epoch": 445.7664233576642,
      "grad_norm": 1.8811694383621216,
      "learning_rate": 2.771167883211679e-05,
      "loss": 0.8529,
      "step": 61070
    },
    {
      "epoch": 445.83941605839414,
      "grad_norm": 12.263605117797852,
      "learning_rate": 2.7708029197080293e-05,
      "loss": 1.1264,
      "step": 61080
    },
    {
      "epoch": 445.9124087591241,
      "grad_norm": 0.10042824596166611,
      "learning_rate": 2.7704379562043797e-05,
      "loss": 0.8247,
      "step": 61090
    },
    {
      "epoch": 445.985401459854,
      "grad_norm": 10.187668800354004,
      "learning_rate": 2.77007299270073e-05,
      "loss": 0.707,
      "step": 61100
    },
    {
      "epoch": 446.05839416058393,
      "grad_norm": 15.463557243347168,
      "learning_rate": 2.7697080291970805e-05,
      "loss": 1.0599,
      "step": 61110
    },
    {
      "epoch": 446.13138686131384,
      "grad_norm": 8.361725807189941,
      "learning_rate": 2.7693430656934312e-05,
      "loss": 0.7499,
      "step": 61120
    },
    {
      "epoch": 446.2043795620438,
      "grad_norm": 7.9586639404296875,
      "learning_rate": 2.768978102189781e-05,
      "loss": 1.401,
      "step": 61130
    },
    {
      "epoch": 446.2773722627737,
      "grad_norm": 6.929608345031738,
      "learning_rate": 2.7686131386861313e-05,
      "loss": 0.7132,
      "step": 61140
    },
    {
      "epoch": 446.35036496350364,
      "grad_norm": 15.103046417236328,
      "learning_rate": 2.7682481751824817e-05,
      "loss": 0.9063,
      "step": 61150
    },
    {
      "epoch": 446.42335766423355,
      "grad_norm": 13.177364349365234,
      "learning_rate": 2.767883211678832e-05,
      "loss": 0.7874,
      "step": 61160
    },
    {
      "epoch": 446.4963503649635,
      "grad_norm": 6.465117931365967,
      "learning_rate": 2.7675182481751828e-05,
      "loss": 0.7227,
      "step": 61170
    },
    {
      "epoch": 446.56934306569343,
      "grad_norm": 18.60112953186035,
      "learning_rate": 2.7671532846715332e-05,
      "loss": 0.9606,
      "step": 61180
    },
    {
      "epoch": 446.64233576642334,
      "grad_norm": 11.687060356140137,
      "learning_rate": 2.7667883211678836e-05,
      "loss": 0.8664,
      "step": 61190
    },
    {
      "epoch": 446.7153284671533,
      "grad_norm": 7.193545818328857,
      "learning_rate": 2.7664233576642333e-05,
      "loss": 1.005,
      "step": 61200
    },
    {
      "epoch": 446.7883211678832,
      "grad_norm": 0.16642384231090546,
      "learning_rate": 2.766058394160584e-05,
      "loss": 0.7528,
      "step": 61210
    },
    {
      "epoch": 446.86131386861314,
      "grad_norm": 15.886760711669922,
      "learning_rate": 2.7656934306569344e-05,
      "loss": 0.9156,
      "step": 61220
    },
    {
      "epoch": 446.93430656934305,
      "grad_norm": 11.038202285766602,
      "learning_rate": 2.7653284671532848e-05,
      "loss": 1.0535,
      "step": 61230
    },
    {
      "epoch": 447.007299270073,
      "grad_norm": 10.358030319213867,
      "learning_rate": 2.7649635036496352e-05,
      "loss": 0.8458,
      "step": 61240
    },
    {
      "epoch": 447.08029197080293,
      "grad_norm": 18.87501335144043,
      "learning_rate": 2.7645985401459856e-05,
      "loss": 1.6276,
      "step": 61250
    },
    {
      "epoch": 447.15328467153284,
      "grad_norm": 10.811341285705566,
      "learning_rate": 2.764233576642336e-05,
      "loss": 0.8025,
      "step": 61260
    },
    {
      "epoch": 447.22627737226276,
      "grad_norm": 7.712975025177002,
      "learning_rate": 2.7638686131386864e-05,
      "loss": 0.6346,
      "step": 61270
    },
    {
      "epoch": 447.2992700729927,
      "grad_norm": 9.336479187011719,
      "learning_rate": 2.7635036496350364e-05,
      "loss": 0.7836,
      "step": 61280
    },
    {
      "epoch": 447.37226277372264,
      "grad_norm": 0.1689455658197403,
      "learning_rate": 2.7631386861313868e-05,
      "loss": 0.8186,
      "step": 61290
    },
    {
      "epoch": 447.44525547445255,
      "grad_norm": 9.040891647338867,
      "learning_rate": 2.7627737226277372e-05,
      "loss": 0.8027,
      "step": 61300
    },
    {
      "epoch": 447.51824817518246,
      "grad_norm": 2.7314469814300537,
      "learning_rate": 2.7624087591240876e-05,
      "loss": 0.8437,
      "step": 61310
    },
    {
      "epoch": 447.59124087591243,
      "grad_norm": 4.992128372192383,
      "learning_rate": 2.7620437956204383e-05,
      "loss": 0.8664,
      "step": 61320
    },
    {
      "epoch": 447.66423357664235,
      "grad_norm": 0.07170596718788147,
      "learning_rate": 2.7616788321167887e-05,
      "loss": 0.7735,
      "step": 61330
    },
    {
      "epoch": 447.73722627737226,
      "grad_norm": 7.830589771270752,
      "learning_rate": 2.761313868613139e-05,
      "loss": 0.737,
      "step": 61340
    },
    {
      "epoch": 447.81021897810217,
      "grad_norm": 10.608223915100098,
      "learning_rate": 2.7609489051094895e-05,
      "loss": 0.838,
      "step": 61350
    },
    {
      "epoch": 447.88321167883214,
      "grad_norm": 7.026384353637695,
      "learning_rate": 2.7605839416058392e-05,
      "loss": 1.2905,
      "step": 61360
    },
    {
      "epoch": 447.95620437956205,
      "grad_norm": 21.863595962524414,
      "learning_rate": 2.76021897810219e-05,
      "loss": 1.0119,
      "step": 61370
    },
    {
      "epoch": 448.02919708029196,
      "grad_norm": 14.074377059936523,
      "learning_rate": 2.7598540145985403e-05,
      "loss": 1.2426,
      "step": 61380
    },
    {
      "epoch": 448.1021897810219,
      "grad_norm": 0.10808549076318741,
      "learning_rate": 2.7594890510948907e-05,
      "loss": 0.9319,
      "step": 61390
    },
    {
      "epoch": 448.17518248175185,
      "grad_norm": 10.574874877929688,
      "learning_rate": 2.759124087591241e-05,
      "loss": 1.1191,
      "step": 61400
    },
    {
      "epoch": 448.24817518248176,
      "grad_norm": 8.003288269042969,
      "learning_rate": 2.7587591240875915e-05,
      "loss": 0.86,
      "step": 61410
    },
    {
      "epoch": 448.3211678832117,
      "grad_norm": 0.0866689458489418,
      "learning_rate": 2.758394160583942e-05,
      "loss": 0.8789,
      "step": 61420
    },
    {
      "epoch": 448.3941605839416,
      "grad_norm": 3.7732644081115723,
      "learning_rate": 2.758029197080292e-05,
      "loss": 0.4523,
      "step": 61430
    },
    {
      "epoch": 448.46715328467155,
      "grad_norm": 0.10592291504144669,
      "learning_rate": 2.7576642335766423e-05,
      "loss": 0.9431,
      "step": 61440
    },
    {
      "epoch": 448.54014598540147,
      "grad_norm": 7.453519821166992,
      "learning_rate": 2.7572992700729927e-05,
      "loss": 1.163,
      "step": 61450
    },
    {
      "epoch": 448.6131386861314,
      "grad_norm": 0.07371461391448975,
      "learning_rate": 2.756934306569343e-05,
      "loss": 0.7838,
      "step": 61460
    },
    {
      "epoch": 448.6861313868613,
      "grad_norm": 12.724745750427246,
      "learning_rate": 2.7565693430656934e-05,
      "loss": 1.0316,
      "step": 61470
    },
    {
      "epoch": 448.75912408759126,
      "grad_norm": 7.041135787963867,
      "learning_rate": 2.7562043795620442e-05,
      "loss": 0.9245,
      "step": 61480
    },
    {
      "epoch": 448.8321167883212,
      "grad_norm": 20.810302734375,
      "learning_rate": 2.7558394160583946e-05,
      "loss": 1.1196,
      "step": 61490
    },
    {
      "epoch": 448.9051094890511,
      "grad_norm": 14.981718063354492,
      "learning_rate": 2.755474452554745e-05,
      "loss": 1.167,
      "step": 61500
    },
    {
      "epoch": 448.978102189781,
      "grad_norm": 0.1575469672679901,
      "learning_rate": 2.7551094890510947e-05,
      "loss": 0.7259,
      "step": 61510
    },
    {
      "epoch": 449.05109489051097,
      "grad_norm": 14.081328392028809,
      "learning_rate": 2.7547445255474454e-05,
      "loss": 0.7842,
      "step": 61520
    },
    {
      "epoch": 449.1240875912409,
      "grad_norm": 0.07375340908765793,
      "learning_rate": 2.7543795620437958e-05,
      "loss": 1.0877,
      "step": 61530
    },
    {
      "epoch": 449.1970802919708,
      "grad_norm": 9.003725051879883,
      "learning_rate": 2.754014598540146e-05,
      "loss": 0.6987,
      "step": 61540
    },
    {
      "epoch": 449.2700729927007,
      "grad_norm": 8.782567977905273,
      "learning_rate": 2.7536496350364966e-05,
      "loss": 1.0989,
      "step": 61550
    },
    {
      "epoch": 449.3430656934307,
      "grad_norm": 7.3129801750183105,
      "learning_rate": 2.753284671532847e-05,
      "loss": 0.6515,
      "step": 61560
    },
    {
      "epoch": 449.4160583941606,
      "grad_norm": 7.485255718231201,
      "learning_rate": 2.7529197080291973e-05,
      "loss": 0.7321,
      "step": 61570
    },
    {
      "epoch": 449.4890510948905,
      "grad_norm": 10.836620330810547,
      "learning_rate": 2.7525547445255477e-05,
      "loss": 1.2603,
      "step": 61580
    },
    {
      "epoch": 449.5620437956204,
      "grad_norm": 9.245806694030762,
      "learning_rate": 2.7521897810218978e-05,
      "loss": 0.6735,
      "step": 61590
    },
    {
      "epoch": 449.6350364963504,
      "grad_norm": 11.823286056518555,
      "learning_rate": 2.751824817518248e-05,
      "loss": 0.8847,
      "step": 61600
    },
    {
      "epoch": 449.7080291970803,
      "grad_norm": 0.20957788825035095,
      "learning_rate": 2.7514598540145985e-05,
      "loss": 0.7323,
      "step": 61610
    },
    {
      "epoch": 449.7810218978102,
      "grad_norm": 9.794649124145508,
      "learning_rate": 2.751094890510949e-05,
      "loss": 1.1994,
      "step": 61620
    },
    {
      "epoch": 449.8540145985401,
      "grad_norm": 8.876379013061523,
      "learning_rate": 2.7507299270072993e-05,
      "loss": 1.0278,
      "step": 61630
    },
    {
      "epoch": 449.9270072992701,
      "grad_norm": 11.510125160217285,
      "learning_rate": 2.75036496350365e-05,
      "loss": 0.994,
      "step": 61640
    },
    {
      "epoch": 450.0,
      "grad_norm": 0.41271287202835083,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.7835,
      "step": 61650
    },
    {
      "epoch": 450.0729927007299,
      "grad_norm": 8.268780708312988,
      "learning_rate": 2.74963503649635e-05,
      "loss": 1.3518,
      "step": 61660
    },
    {
      "epoch": 450.1459854014599,
      "grad_norm": 10.511051177978516,
      "learning_rate": 2.7492700729927005e-05,
      "loss": 0.9728,
      "step": 61670
    },
    {
      "epoch": 450.2189781021898,
      "grad_norm": 11.877507209777832,
      "learning_rate": 2.7489051094890513e-05,
      "loss": 0.9731,
      "step": 61680
    },
    {
      "epoch": 450.2919708029197,
      "grad_norm": 6.389105319976807,
      "learning_rate": 2.7485401459854017e-05,
      "loss": 0.7103,
      "step": 61690
    },
    {
      "epoch": 450.3649635036496,
      "grad_norm": 11.556814193725586,
      "learning_rate": 2.748175182481752e-05,
      "loss": 1.0025,
      "step": 61700
    },
    {
      "epoch": 450.4379562043796,
      "grad_norm": 7.203882694244385,
      "learning_rate": 2.7478102189781024e-05,
      "loss": 0.9166,
      "step": 61710
    },
    {
      "epoch": 450.5109489051095,
      "grad_norm": 7.184331893920898,
      "learning_rate": 2.7474452554744528e-05,
      "loss": 0.7179,
      "step": 61720
    },
    {
      "epoch": 450.5839416058394,
      "grad_norm": 6.6738176345825195,
      "learning_rate": 2.7470802919708032e-05,
      "loss": 1.0469,
      "step": 61730
    },
    {
      "epoch": 450.6569343065693,
      "grad_norm": 13.322293281555176,
      "learning_rate": 2.7467153284671533e-05,
      "loss": 0.9206,
      "step": 61740
    },
    {
      "epoch": 450.7299270072993,
      "grad_norm": 8.4890718460083,
      "learning_rate": 2.7463503649635036e-05,
      "loss": 0.8258,
      "step": 61750
    },
    {
      "epoch": 450.8029197080292,
      "grad_norm": 15.501565933227539,
      "learning_rate": 2.745985401459854e-05,
      "loss": 0.9745,
      "step": 61760
    },
    {
      "epoch": 450.8759124087591,
      "grad_norm": 0.26421764492988586,
      "learning_rate": 2.7456204379562044e-05,
      "loss": 0.704,
      "step": 61770
    },
    {
      "epoch": 450.94890510948903,
      "grad_norm": 4.318849563598633,
      "learning_rate": 2.7452554744525548e-05,
      "loss": 0.7546,
      "step": 61780
    },
    {
      "epoch": 451.021897810219,
      "grad_norm": 6.316403865814209,
      "learning_rate": 2.7448905109489055e-05,
      "loss": 1.0334,
      "step": 61790
    },
    {
      "epoch": 451.0948905109489,
      "grad_norm": 12.421666145324707,
      "learning_rate": 2.744525547445256e-05,
      "loss": 1.019,
      "step": 61800
    },
    {
      "epoch": 451.1678832116788,
      "grad_norm": 0.17951500415802002,
      "learning_rate": 2.7441605839416063e-05,
      "loss": 0.6842,
      "step": 61810
    },
    {
      "epoch": 451.24087591240874,
      "grad_norm": 17.51577377319336,
      "learning_rate": 2.743795620437956e-05,
      "loss": 0.9026,
      "step": 61820
    },
    {
      "epoch": 451.3138686131387,
      "grad_norm": 9.447041511535645,
      "learning_rate": 2.7434306569343064e-05,
      "loss": 1.0272,
      "step": 61830
    },
    {
      "epoch": 451.3868613138686,
      "grad_norm": 0.07507375627756119,
      "learning_rate": 2.743065693430657e-05,
      "loss": 0.872,
      "step": 61840
    },
    {
      "epoch": 451.45985401459853,
      "grad_norm": 8.074540138244629,
      "learning_rate": 2.7427007299270075e-05,
      "loss": 0.9163,
      "step": 61850
    },
    {
      "epoch": 451.53284671532845,
      "grad_norm": 0.10186608135700226,
      "learning_rate": 2.742335766423358e-05,
      "loss": 0.9152,
      "step": 61860
    },
    {
      "epoch": 451.6058394160584,
      "grad_norm": 0.03502028062939644,
      "learning_rate": 2.7419708029197083e-05,
      "loss": 1.1778,
      "step": 61870
    },
    {
      "epoch": 451.6788321167883,
      "grad_norm": 6.010993003845215,
      "learning_rate": 2.7416058394160587e-05,
      "loss": 0.7529,
      "step": 61880
    },
    {
      "epoch": 451.75182481751824,
      "grad_norm": 7.291983604431152,
      "learning_rate": 2.7412408759124087e-05,
      "loss": 1.1949,
      "step": 61890
    },
    {
      "epoch": 451.82481751824815,
      "grad_norm": 17.63481903076172,
      "learning_rate": 2.740875912408759e-05,
      "loss": 0.9637,
      "step": 61900
    },
    {
      "epoch": 451.8978102189781,
      "grad_norm": 0.07219510525465012,
      "learning_rate": 2.7405109489051095e-05,
      "loss": 0.9182,
      "step": 61910
    },
    {
      "epoch": 451.97080291970804,
      "grad_norm": 6.987784385681152,
      "learning_rate": 2.74014598540146e-05,
      "loss": 0.7221,
      "step": 61920
    },
    {
      "epoch": 452.04379562043795,
      "grad_norm": 10.58802318572998,
      "learning_rate": 2.7397810218978103e-05,
      "loss": 0.7815,
      "step": 61930
    },
    {
      "epoch": 452.11678832116786,
      "grad_norm": 10.463988304138184,
      "learning_rate": 2.7394160583941607e-05,
      "loss": 1.0026,
      "step": 61940
    },
    {
      "epoch": 452.18978102189783,
      "grad_norm": 0.12483259290456772,
      "learning_rate": 2.7390510948905114e-05,
      "loss": 0.4778,
      "step": 61950
    },
    {
      "epoch": 452.26277372262774,
      "grad_norm": 0.2795892655849457,
      "learning_rate": 2.7386861313868618e-05,
      "loss": 0.6759,
      "step": 61960
    },
    {
      "epoch": 452.33576642335765,
      "grad_norm": 10.189471244812012,
      "learning_rate": 2.7383211678832115e-05,
      "loss": 0.9387,
      "step": 61970
    },
    {
      "epoch": 452.40875912408757,
      "grad_norm": 18.100975036621094,
      "learning_rate": 2.737956204379562e-05,
      "loss": 0.6708,
      "step": 61980
    },
    {
      "epoch": 452.48175182481754,
      "grad_norm": 12.168412208557129,
      "learning_rate": 2.7375912408759126e-05,
      "loss": 1.4234,
      "step": 61990
    },
    {
      "epoch": 452.55474452554745,
      "grad_norm": 6.809377670288086,
      "learning_rate": 2.737226277372263e-05,
      "loss": 1.0032,
      "step": 62000
    },
    {
      "epoch": 452.62773722627736,
      "grad_norm": 12.374897956848145,
      "learning_rate": 2.7368613138686134e-05,
      "loss": 0.7951,
      "step": 62010
    },
    {
      "epoch": 452.7007299270073,
      "grad_norm": 8.5858793258667,
      "learning_rate": 2.7364963503649638e-05,
      "loss": 1.0606,
      "step": 62020
    },
    {
      "epoch": 452.77372262773724,
      "grad_norm": 0.1892627477645874,
      "learning_rate": 2.7361313868613142e-05,
      "loss": 0.8076,
      "step": 62030
    },
    {
      "epoch": 452.84671532846716,
      "grad_norm": 10.490227699279785,
      "learning_rate": 2.7357664233576642e-05,
      "loss": 1.172,
      "step": 62040
    },
    {
      "epoch": 452.91970802919707,
      "grad_norm": 14.50683307647705,
      "learning_rate": 2.7354014598540146e-05,
      "loss": 1.107,
      "step": 62050
    },
    {
      "epoch": 452.992700729927,
      "grad_norm": 6.163875102996826,
      "learning_rate": 2.735036496350365e-05,
      "loss": 0.7177,
      "step": 62060
    },
    {
      "epoch": 453.06569343065695,
      "grad_norm": 11.369275093078613,
      "learning_rate": 2.7346715328467154e-05,
      "loss": 0.6804,
      "step": 62070
    },
    {
      "epoch": 453.13868613138686,
      "grad_norm": 13.37761116027832,
      "learning_rate": 2.7343065693430658e-05,
      "loss": 1.1214,
      "step": 62080
    },
    {
      "epoch": 453.2116788321168,
      "grad_norm": 12.566609382629395,
      "learning_rate": 2.733941605839416e-05,
      "loss": 0.7515,
      "step": 62090
    },
    {
      "epoch": 453.2846715328467,
      "grad_norm": 15.144844055175781,
      "learning_rate": 2.7335766423357666e-05,
      "loss": 0.9364,
      "step": 62100
    },
    {
      "epoch": 453.35766423357666,
      "grad_norm": 7.760092735290527,
      "learning_rate": 2.7332116788321173e-05,
      "loss": 0.8961,
      "step": 62110
    },
    {
      "epoch": 453.43065693430657,
      "grad_norm": 15.856353759765625,
      "learning_rate": 2.732846715328467e-05,
      "loss": 0.7507,
      "step": 62120
    },
    {
      "epoch": 453.5036496350365,
      "grad_norm": 0.10391255468130112,
      "learning_rate": 2.7324817518248174e-05,
      "loss": 0.5523,
      "step": 62130
    },
    {
      "epoch": 453.57664233576645,
      "grad_norm": 11.366028785705566,
      "learning_rate": 2.7321167883211678e-05,
      "loss": 0.6019,
      "step": 62140
    },
    {
      "epoch": 453.64963503649636,
      "grad_norm": 9.678999900817871,
      "learning_rate": 2.7317518248175185e-05,
      "loss": 0.9969,
      "step": 62150
    },
    {
      "epoch": 453.7226277372263,
      "grad_norm": 11.65451431274414,
      "learning_rate": 2.731386861313869e-05,
      "loss": 0.9331,
      "step": 62160
    },
    {
      "epoch": 453.7956204379562,
      "grad_norm": 7.166378974914551,
      "learning_rate": 2.7310218978102193e-05,
      "loss": 0.9841,
      "step": 62170
    },
    {
      "epoch": 453.86861313868616,
      "grad_norm": 12.525702476501465,
      "learning_rate": 2.7306569343065697e-05,
      "loss": 1.3198,
      "step": 62180
    },
    {
      "epoch": 453.94160583941607,
      "grad_norm": 8.67592716217041,
      "learning_rate": 2.73029197080292e-05,
      "loss": 1.2908,
      "step": 62190
    },
    {
      "epoch": 454.014598540146,
      "grad_norm": 9.18332576751709,
      "learning_rate": 2.72992700729927e-05,
      "loss": 0.7682,
      "step": 62200
    },
    {
      "epoch": 454.0875912408759,
      "grad_norm": 13.638201713562012,
      "learning_rate": 2.7295620437956205e-05,
      "loss": 1.1278,
      "step": 62210
    },
    {
      "epoch": 454.16058394160586,
      "grad_norm": 11.81869125366211,
      "learning_rate": 2.729197080291971e-05,
      "loss": 0.9081,
      "step": 62220
    },
    {
      "epoch": 454.2335766423358,
      "grad_norm": 10.924918174743652,
      "learning_rate": 2.7288321167883213e-05,
      "loss": 1.0095,
      "step": 62230
    },
    {
      "epoch": 454.3065693430657,
      "grad_norm": 7.4998064041137695,
      "learning_rate": 2.7284671532846716e-05,
      "loss": 1.1419,
      "step": 62240
    },
    {
      "epoch": 454.3795620437956,
      "grad_norm": 11.506975173950195,
      "learning_rate": 2.728102189781022e-05,
      "loss": 0.6913,
      "step": 62250
    },
    {
      "epoch": 454.45255474452557,
      "grad_norm": 7.097955226898193,
      "learning_rate": 2.7277372262773728e-05,
      "loss": 0.8507,
      "step": 62260
    },
    {
      "epoch": 454.5255474452555,
      "grad_norm": 11.61074161529541,
      "learning_rate": 2.7273722627737225e-05,
      "loss": 0.9124,
      "step": 62270
    },
    {
      "epoch": 454.5985401459854,
      "grad_norm": 6.7581400871276855,
      "learning_rate": 2.727007299270073e-05,
      "loss": 0.6496,
      "step": 62280
    },
    {
      "epoch": 454.6715328467153,
      "grad_norm": 13.691493034362793,
      "learning_rate": 2.7266423357664233e-05,
      "loss": 0.8587,
      "step": 62290
    },
    {
      "epoch": 454.7445255474453,
      "grad_norm": 9.431902885437012,
      "learning_rate": 2.7262773722627736e-05,
      "loss": 0.7076,
      "step": 62300
    },
    {
      "epoch": 454.8175182481752,
      "grad_norm": 8.517112731933594,
      "learning_rate": 2.7259124087591244e-05,
      "loss": 1.2122,
      "step": 62310
    },
    {
      "epoch": 454.8905109489051,
      "grad_norm": 14.934576988220215,
      "learning_rate": 2.7255474452554748e-05,
      "loss": 0.8366,
      "step": 62320
    },
    {
      "epoch": 454.963503649635,
      "grad_norm": 0.13858717679977417,
      "learning_rate": 2.725182481751825e-05,
      "loss": 0.9003,
      "step": 62330
    },
    {
      "epoch": 455.036496350365,
      "grad_norm": 8.158559799194336,
      "learning_rate": 2.7248175182481755e-05,
      "loss": 0.8053,
      "step": 62340
    },
    {
      "epoch": 455.1094890510949,
      "grad_norm": 0.06459496170282364,
      "learning_rate": 2.7244525547445256e-05,
      "loss": 0.7949,
      "step": 62350
    },
    {
      "epoch": 455.1824817518248,
      "grad_norm": 8.956710815429688,
      "learning_rate": 2.724087591240876e-05,
      "loss": 0.9136,
      "step": 62360
    },
    {
      "epoch": 455.2554744525547,
      "grad_norm": 13.124624252319336,
      "learning_rate": 2.7237226277372264e-05,
      "loss": 1.0934,
      "step": 62370
    },
    {
      "epoch": 455.3284671532847,
      "grad_norm": 0.13245558738708496,
      "learning_rate": 2.7233576642335767e-05,
      "loss": 0.7357,
      "step": 62380
    },
    {
      "epoch": 455.4014598540146,
      "grad_norm": 10.366276741027832,
      "learning_rate": 2.722992700729927e-05,
      "loss": 1.2738,
      "step": 62390
    },
    {
      "epoch": 455.4744525547445,
      "grad_norm": 0.060237325727939606,
      "learning_rate": 2.7226277372262775e-05,
      "loss": 0.5005,
      "step": 62400
    },
    {
      "epoch": 455.54744525547443,
      "grad_norm": 5.248178005218506,
      "learning_rate": 2.722262773722628e-05,
      "loss": 0.855,
      "step": 62410
    },
    {
      "epoch": 455.6204379562044,
      "grad_norm": 9.973919868469238,
      "learning_rate": 2.7218978102189786e-05,
      "loss": 1.05,
      "step": 62420
    },
    {
      "epoch": 455.6934306569343,
      "grad_norm": 9.775528907775879,
      "learning_rate": 2.7215328467153283e-05,
      "loss": 0.8626,
      "step": 62430
    },
    {
      "epoch": 455.7664233576642,
      "grad_norm": 11.023041725158691,
      "learning_rate": 2.7211678832116787e-05,
      "loss": 1.0727,
      "step": 62440
    },
    {
      "epoch": 455.83941605839414,
      "grad_norm": 0.04885326325893402,
      "learning_rate": 2.720802919708029e-05,
      "loss": 0.7115,
      "step": 62450
    },
    {
      "epoch": 455.9124087591241,
      "grad_norm": 15.747127532958984,
      "learning_rate": 2.72043795620438e-05,
      "loss": 1.1979,
      "step": 62460
    },
    {
      "epoch": 455.985401459854,
      "grad_norm": 7.148270606994629,
      "learning_rate": 2.7200729927007302e-05,
      "loss": 0.9122,
      "step": 62470
    },
    {
      "epoch": 456.05839416058393,
      "grad_norm": 9.511706352233887,
      "learning_rate": 2.7197080291970806e-05,
      "loss": 0.7769,
      "step": 62480
    },
    {
      "epoch": 456.13138686131384,
      "grad_norm": 14.024781227111816,
      "learning_rate": 2.719343065693431e-05,
      "loss": 1.0334,
      "step": 62490
    },
    {
      "epoch": 456.2043795620438,
      "grad_norm": 17.18794822692871,
      "learning_rate": 2.7189781021897807e-05,
      "loss": 1.0628,
      "step": 62500
    },
    {
      "epoch": 456.2773722627737,
      "grad_norm": 18.778230667114258,
      "learning_rate": 2.7186131386861315e-05,
      "loss": 1.0145,
      "step": 62510
    },
    {
      "epoch": 456.35036496350364,
      "grad_norm": 18.272445678710938,
      "learning_rate": 2.718248175182482e-05,
      "loss": 0.6646,
      "step": 62520
    },
    {
      "epoch": 456.42335766423355,
      "grad_norm": 12.064379692077637,
      "learning_rate": 2.7178832116788322e-05,
      "loss": 1.3918,
      "step": 62530
    },
    {
      "epoch": 456.4963503649635,
      "grad_norm": 7.078822135925293,
      "learning_rate": 2.7175182481751826e-05,
      "loss": 0.4478,
      "step": 62540
    },
    {
      "epoch": 456.56934306569343,
      "grad_norm": 7.109094142913818,
      "learning_rate": 2.717153284671533e-05,
      "loss": 0.895,
      "step": 62550
    },
    {
      "epoch": 456.64233576642334,
      "grad_norm": 21.875980377197266,
      "learning_rate": 2.7167883211678834e-05,
      "loss": 1.2982,
      "step": 62560
    },
    {
      "epoch": 456.7153284671533,
      "grad_norm": 8.147760391235352,
      "learning_rate": 2.7164233576642338e-05,
      "loss": 1.0377,
      "step": 62570
    },
    {
      "epoch": 456.7883211678832,
      "grad_norm": 10.269804954528809,
      "learning_rate": 2.716058394160584e-05,
      "loss": 0.5914,
      "step": 62580
    },
    {
      "epoch": 456.86131386861314,
      "grad_norm": 9.575740814208984,
      "learning_rate": 2.7156934306569342e-05,
      "loss": 0.8453,
      "step": 62590
    },
    {
      "epoch": 456.93430656934305,
      "grad_norm": 6.854840278625488,
      "learning_rate": 2.7153284671532846e-05,
      "loss": 0.9448,
      "step": 62600
    },
    {
      "epoch": 457.007299270073,
      "grad_norm": 10.207910537719727,
      "learning_rate": 2.714963503649635e-05,
      "loss": 0.7613,
      "step": 62610
    },
    {
      "epoch": 457.08029197080293,
      "grad_norm": 9.544119834899902,
      "learning_rate": 2.7145985401459857e-05,
      "loss": 0.9645,
      "step": 62620
    },
    {
      "epoch": 457.15328467153284,
      "grad_norm": 0.032158806920051575,
      "learning_rate": 2.714233576642336e-05,
      "loss": 1.2882,
      "step": 62630
    },
    {
      "epoch": 457.22627737226276,
      "grad_norm": 6.686592102050781,
      "learning_rate": 2.7138686131386865e-05,
      "loss": 0.8706,
      "step": 62640
    },
    {
      "epoch": 457.2992700729927,
      "grad_norm": 14.941398620605469,
      "learning_rate": 2.713503649635037e-05,
      "loss": 1.2915,
      "step": 62650
    },
    {
      "epoch": 457.37226277372264,
      "grad_norm": 10.40403938293457,
      "learning_rate": 2.713138686131387e-05,
      "loss": 0.9045,
      "step": 62660
    },
    {
      "epoch": 457.44525547445255,
      "grad_norm": 4.705299377441406,
      "learning_rate": 2.7127737226277373e-05,
      "loss": 0.2315,
      "step": 62670
    },
    {
      "epoch": 457.51824817518246,
      "grad_norm": 9.632450103759766,
      "learning_rate": 2.7124087591240877e-05,
      "loss": 0.7414,
      "step": 62680
    },
    {
      "epoch": 457.59124087591243,
      "grad_norm": 0.053030140697956085,
      "learning_rate": 2.712043795620438e-05,
      "loss": 1.0602,
      "step": 62690
    },
    {
      "epoch": 457.66423357664235,
      "grad_norm": 0.09803310036659241,
      "learning_rate": 2.7116788321167885e-05,
      "loss": 0.721,
      "step": 62700
    },
    {
      "epoch": 457.73722627737226,
      "grad_norm": 11.66641902923584,
      "learning_rate": 2.711313868613139e-05,
      "loss": 1.285,
      "step": 62710
    },
    {
      "epoch": 457.81021897810217,
      "grad_norm": 11.524627685546875,
      "learning_rate": 2.7109489051094893e-05,
      "loss": 0.7502,
      "step": 62720
    },
    {
      "epoch": 457.88321167883214,
      "grad_norm": 8.083802223205566,
      "learning_rate": 2.7105839416058393e-05,
      "loss": 0.6805,
      "step": 62730
    },
    {
      "epoch": 457.95620437956205,
      "grad_norm": 13.383543014526367,
      "learning_rate": 2.7102189781021897e-05,
      "loss": 1.0492,
      "step": 62740
    },
    {
      "epoch": 458.02919708029196,
      "grad_norm": 0.10500995814800262,
      "learning_rate": 2.70985401459854e-05,
      "loss": 0.9528,
      "step": 62750
    },
    {
      "epoch": 458.1021897810219,
      "grad_norm": 1.3890595436096191,
      "learning_rate": 2.7094890510948905e-05,
      "loss": 0.5468,
      "step": 62760
    },
    {
      "epoch": 458.17518248175185,
      "grad_norm": 11.216047286987305,
      "learning_rate": 2.709124087591241e-05,
      "loss": 1.1284,
      "step": 62770
    },
    {
      "epoch": 458.24817518248176,
      "grad_norm": 13.590099334716797,
      "learning_rate": 2.7087591240875916e-05,
      "loss": 0.8346,
      "step": 62780
    },
    {
      "epoch": 458.3211678832117,
      "grad_norm": 0.09433431178331375,
      "learning_rate": 2.708394160583942e-05,
      "loss": 0.7373,
      "step": 62790
    },
    {
      "epoch": 458.3941605839416,
      "grad_norm": 10.359338760375977,
      "learning_rate": 2.7080291970802924e-05,
      "loss": 1.1468,
      "step": 62800
    },
    {
      "epoch": 458.46715328467155,
      "grad_norm": 6.1561760902404785,
      "learning_rate": 2.707664233576642e-05,
      "loss": 0.9077,
      "step": 62810
    },
    {
      "epoch": 458.54014598540147,
      "grad_norm": 1.1398028135299683,
      "learning_rate": 2.7072992700729928e-05,
      "loss": 0.8962,
      "step": 62820
    },
    {
      "epoch": 458.6131386861314,
      "grad_norm": 11.893120765686035,
      "learning_rate": 2.7069343065693432e-05,
      "loss": 0.9825,
      "step": 62830
    },
    {
      "epoch": 458.6861313868613,
      "grad_norm": 5.0885820388793945,
      "learning_rate": 2.7065693430656936e-05,
      "loss": 1.294,
      "step": 62840
    },
    {
      "epoch": 458.75912408759126,
      "grad_norm": 19.41968536376953,
      "learning_rate": 2.706204379562044e-05,
      "loss": 1.1141,
      "step": 62850
    },
    {
      "epoch": 458.8321167883212,
      "grad_norm": 10.80672836303711,
      "learning_rate": 2.7058394160583944e-05,
      "loss": 0.6456,
      "step": 62860
    },
    {
      "epoch": 458.9051094890511,
      "grad_norm": 9.293062210083008,
      "learning_rate": 2.7054744525547448e-05,
      "loss": 0.692,
      "step": 62870
    },
    {
      "epoch": 458.978102189781,
      "grad_norm": 8.877114295959473,
      "learning_rate": 2.705109489051095e-05,
      "loss": 1.159,
      "step": 62880
    },
    {
      "epoch": 459.05109489051097,
      "grad_norm": 8.792364120483398,
      "learning_rate": 2.7047445255474452e-05,
      "loss": 0.8142,
      "step": 62890
    },
    {
      "epoch": 459.1240875912409,
      "grad_norm": 0.1404539793729782,
      "learning_rate": 2.7043795620437956e-05,
      "loss": 0.947,
      "step": 62900
    },
    {
      "epoch": 459.1970802919708,
      "grad_norm": 0.05552142113447189,
      "learning_rate": 2.704014598540146e-05,
      "loss": 0.9169,
      "step": 62910
    },
    {
      "epoch": 459.2700729927007,
      "grad_norm": 9.025558471679688,
      "learning_rate": 2.7036496350364964e-05,
      "loss": 0.9289,
      "step": 62920
    },
    {
      "epoch": 459.3430656934307,
      "grad_norm": 9.477254867553711,
      "learning_rate": 2.703284671532847e-05,
      "loss": 1.2656,
      "step": 62930
    },
    {
      "epoch": 459.4160583941606,
      "grad_norm": 11.854822158813477,
      "learning_rate": 2.7029197080291975e-05,
      "loss": 0.8601,
      "step": 62940
    },
    {
      "epoch": 459.4890510948905,
      "grad_norm": 16.576648712158203,
      "learning_rate": 2.702554744525548e-05,
      "loss": 1.021,
      "step": 62950
    },
    {
      "epoch": 459.5620437956204,
      "grad_norm": 12.790718078613281,
      "learning_rate": 2.7021897810218976e-05,
      "loss": 0.6906,
      "step": 62960
    },
    {
      "epoch": 459.6350364963504,
      "grad_norm": 7.984194755554199,
      "learning_rate": 2.701824817518248e-05,
      "loss": 0.6125,
      "step": 62970
    },
    {
      "epoch": 459.7080291970803,
      "grad_norm": 17.045764923095703,
      "learning_rate": 2.7014598540145987e-05,
      "loss": 0.5917,
      "step": 62980
    },
    {
      "epoch": 459.7810218978102,
      "grad_norm": 12.602659225463867,
      "learning_rate": 2.701094890510949e-05,
      "loss": 1.3334,
      "step": 62990
    },
    {
      "epoch": 459.8540145985401,
      "grad_norm": 8.964860916137695,
      "learning_rate": 2.7007299270072995e-05,
      "loss": 0.8233,
      "step": 63000
    },
    {
      "epoch": 459.9270072992701,
      "grad_norm": 8.970245361328125,
      "learning_rate": 2.70036496350365e-05,
      "loss": 1.009,
      "step": 63010
    },
    {
      "epoch": 460.0,
      "grad_norm": 0.17769639194011688,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.8066,
      "step": 63020
    },
    {
      "epoch": 460.0729927007299,
      "grad_norm": 0.5117743611335754,
      "learning_rate": 2.6996350364963506e-05,
      "loss": 0.6361,
      "step": 63030
    },
    {
      "epoch": 460.1459854014599,
      "grad_norm": 8.365824699401855,
      "learning_rate": 2.6992700729927007e-05,
      "loss": 1.3491,
      "step": 63040
    },
    {
      "epoch": 460.2189781021898,
      "grad_norm": 13.656729698181152,
      "learning_rate": 2.698905109489051e-05,
      "loss": 0.8188,
      "step": 63050
    },
    {
      "epoch": 460.2919708029197,
      "grad_norm": 20.27859115600586,
      "learning_rate": 2.6985401459854015e-05,
      "loss": 0.7097,
      "step": 63060
    },
    {
      "epoch": 460.3649635036496,
      "grad_norm": 0.34946638345718384,
      "learning_rate": 2.698175182481752e-05,
      "loss": 1.0575,
      "step": 63070
    },
    {
      "epoch": 460.4379562043796,
      "grad_norm": 0.09582162648439407,
      "learning_rate": 2.6978102189781022e-05,
      "loss": 0.8454,
      "step": 63080
    },
    {
      "epoch": 460.5109489051095,
      "grad_norm": 4.059114456176758,
      "learning_rate": 2.697445255474453e-05,
      "loss": 0.7062,
      "step": 63090
    },
    {
      "epoch": 460.5839416058394,
      "grad_norm": 9.424210548400879,
      "learning_rate": 2.6970802919708033e-05,
      "loss": 1.1532,
      "step": 63100
    },
    {
      "epoch": 460.6569343065693,
      "grad_norm": 16.631935119628906,
      "learning_rate": 2.6967153284671537e-05,
      "loss": 0.9328,
      "step": 63110
    },
    {
      "epoch": 460.7299270072993,
      "grad_norm": 11.964669227600098,
      "learning_rate": 2.6963503649635034e-05,
      "loss": 0.9055,
      "step": 63120
    },
    {
      "epoch": 460.8029197080292,
      "grad_norm": 14.149052619934082,
      "learning_rate": 2.6959854014598542e-05,
      "loss": 1.2621,
      "step": 63130
    },
    {
      "epoch": 460.8759124087591,
      "grad_norm": 7.3393635749816895,
      "learning_rate": 2.6956204379562046e-05,
      "loss": 0.9971,
      "step": 63140
    },
    {
      "epoch": 460.94890510948903,
      "grad_norm": 16.80254554748535,
      "learning_rate": 2.695255474452555e-05,
      "loss": 1.0319,
      "step": 63150
    },
    {
      "epoch": 461.021897810219,
      "grad_norm": 10.078821182250977,
      "learning_rate": 2.6948905109489053e-05,
      "loss": 0.6664,
      "step": 63160
    },
    {
      "epoch": 461.0948905109489,
      "grad_norm": 13.843623161315918,
      "learning_rate": 2.6945255474452557e-05,
      "loss": 0.9057,
      "step": 63170
    },
    {
      "epoch": 461.1678832116788,
      "grad_norm": 9.17900562286377,
      "learning_rate": 2.694160583941606e-05,
      "loss": 1.0069,
      "step": 63180
    },
    {
      "epoch": 461.24087591240874,
      "grad_norm": 0.11567014455795288,
      "learning_rate": 2.693795620437956e-05,
      "loss": 0.7493,
      "step": 63190
    },
    {
      "epoch": 461.3138686131387,
      "grad_norm": 9.624241828918457,
      "learning_rate": 2.6934306569343065e-05,
      "loss": 1.024,
      "step": 63200
    },
    {
      "epoch": 461.3868613138686,
      "grad_norm": 0.06812714040279388,
      "learning_rate": 2.693065693430657e-05,
      "loss": 0.9279,
      "step": 63210
    },
    {
      "epoch": 461.45985401459853,
      "grad_norm": 7.643963813781738,
      "learning_rate": 2.6927007299270073e-05,
      "loss": 0.9411,
      "step": 63220
    },
    {
      "epoch": 461.53284671532845,
      "grad_norm": 10.391816139221191,
      "learning_rate": 2.6923357664233577e-05,
      "loss": 1.1374,
      "step": 63230
    },
    {
      "epoch": 461.6058394160584,
      "grad_norm": 0.09062009304761887,
      "learning_rate": 2.691970802919708e-05,
      "loss": 1.18,
      "step": 63240
    },
    {
      "epoch": 461.6788321167883,
      "grad_norm": 12.315417289733887,
      "learning_rate": 2.6916058394160588e-05,
      "loss": 0.8783,
      "step": 63250
    },
    {
      "epoch": 461.75182481751824,
      "grad_norm": 5.987644672393799,
      "learning_rate": 2.6912408759124092e-05,
      "loss": 0.765,
      "step": 63260
    },
    {
      "epoch": 461.82481751824815,
      "grad_norm": 12.691447257995605,
      "learning_rate": 2.690875912408759e-05,
      "loss": 0.7963,
      "step": 63270
    },
    {
      "epoch": 461.8978102189781,
      "grad_norm": 16.184207916259766,
      "learning_rate": 2.6905109489051093e-05,
      "loss": 0.5802,
      "step": 63280
    },
    {
      "epoch": 461.97080291970804,
      "grad_norm": 13.031302452087402,
      "learning_rate": 2.69014598540146e-05,
      "loss": 0.9693,
      "step": 63290
    },
    {
      "epoch": 462.04379562043795,
      "grad_norm": 0.08080693334341049,
      "learning_rate": 2.6897810218978104e-05,
      "loss": 0.8819,
      "step": 63300
    },
    {
      "epoch": 462.11678832116786,
      "grad_norm": 11.402830123901367,
      "learning_rate": 2.6894160583941608e-05,
      "loss": 0.9909,
      "step": 63310
    },
    {
      "epoch": 462.18978102189783,
      "grad_norm": 7.191140651702881,
      "learning_rate": 2.6890510948905112e-05,
      "loss": 1.0064,
      "step": 63320
    },
    {
      "epoch": 462.26277372262774,
      "grad_norm": 6.951117038726807,
      "learning_rate": 2.6886861313868616e-05,
      "loss": 0.5474,
      "step": 63330
    },
    {
      "epoch": 462.33576642335765,
      "grad_norm": 11.12918472290039,
      "learning_rate": 2.6883211678832116e-05,
      "loss": 0.9508,
      "step": 63340
    },
    {
      "epoch": 462.40875912408757,
      "grad_norm": 15.099682807922363,
      "learning_rate": 2.687956204379562e-05,
      "loss": 0.7677,
      "step": 63350
    },
    {
      "epoch": 462.48175182481754,
      "grad_norm": 8.070863723754883,
      "learning_rate": 2.6875912408759124e-05,
      "loss": 1.0416,
      "step": 63360
    },
    {
      "epoch": 462.55474452554745,
      "grad_norm": 16.15833282470703,
      "learning_rate": 2.6872262773722628e-05,
      "loss": 1.2988,
      "step": 63370
    },
    {
      "epoch": 462.62773722627736,
      "grad_norm": 0.12363050132989883,
      "learning_rate": 2.6868613138686132e-05,
      "loss": 1.1204,
      "step": 63380
    },
    {
      "epoch": 462.7007299270073,
      "grad_norm": 8.723688125610352,
      "learning_rate": 2.6864963503649636e-05,
      "loss": 0.5722,
      "step": 63390
    },
    {
      "epoch": 462.77372262773724,
      "grad_norm": 8.78696346282959,
      "learning_rate": 2.6861313868613143e-05,
      "loss": 0.6884,
      "step": 63400
    },
    {
      "epoch": 462.84671532846716,
      "grad_norm": 18.110130310058594,
      "learning_rate": 2.6857664233576647e-05,
      "loss": 1.2359,
      "step": 63410
    },
    {
      "epoch": 462.91970802919707,
      "grad_norm": 23.441436767578125,
      "learning_rate": 2.6854014598540144e-05,
      "loss": 1.0526,
      "step": 63420
    },
    {
      "epoch": 462.992700729927,
      "grad_norm": 7.675376892089844,
      "learning_rate": 2.6850364963503648e-05,
      "loss": 0.7886,
      "step": 63430
    },
    {
      "epoch": 463.06569343065695,
      "grad_norm": 11.023392677307129,
      "learning_rate": 2.6846715328467152e-05,
      "loss": 1.0588,
      "step": 63440
    },
    {
      "epoch": 463.13868613138686,
      "grad_norm": 14.931525230407715,
      "learning_rate": 2.684306569343066e-05,
      "loss": 1.504,
      "step": 63450
    },
    {
      "epoch": 463.2116788321168,
      "grad_norm": 11.853625297546387,
      "learning_rate": 2.6839416058394163e-05,
      "loss": 0.6284,
      "step": 63460
    },
    {
      "epoch": 463.2846715328467,
      "grad_norm": 8.009842872619629,
      "learning_rate": 2.6835766423357667e-05,
      "loss": 0.7708,
      "step": 63470
    },
    {
      "epoch": 463.35766423357666,
      "grad_norm": 6.462466239929199,
      "learning_rate": 2.683211678832117e-05,
      "loss": 0.9068,
      "step": 63480
    },
    {
      "epoch": 463.43065693430657,
      "grad_norm": 9.19713020324707,
      "learning_rate": 2.6828467153284675e-05,
      "loss": 1.0626,
      "step": 63490
    },
    {
      "epoch": 463.5036496350365,
      "grad_norm": 9.096391677856445,
      "learning_rate": 2.6824817518248175e-05,
      "loss": 1.0346,
      "step": 63500
    },
    {
      "epoch": 463.57664233576645,
      "grad_norm": 6.575547218322754,
      "learning_rate": 2.682116788321168e-05,
      "loss": 1.0508,
      "step": 63510
    },
    {
      "epoch": 463.64963503649636,
      "grad_norm": 8.359591484069824,
      "learning_rate": 2.6817518248175183e-05,
      "loss": 0.9414,
      "step": 63520
    },
    {
      "epoch": 463.7226277372263,
      "grad_norm": 11.15768051147461,
      "learning_rate": 2.6813868613138687e-05,
      "loss": 0.6956,
      "step": 63530
    },
    {
      "epoch": 463.7956204379562,
      "grad_norm": 5.57565975189209,
      "learning_rate": 2.681021897810219e-05,
      "loss": 0.7418,
      "step": 63540
    },
    {
      "epoch": 463.86861313868616,
      "grad_norm": 15.453957557678223,
      "learning_rate": 2.6806569343065695e-05,
      "loss": 0.9136,
      "step": 63550
    },
    {
      "epoch": 463.94160583941607,
      "grad_norm": 9.826606750488281,
      "learning_rate": 2.6802919708029202e-05,
      "loss": 0.9068,
      "step": 63560
    },
    {
      "epoch": 464.014598540146,
      "grad_norm": 10.45952320098877,
      "learning_rate": 2.67992700729927e-05,
      "loss": 0.6495,
      "step": 63570
    },
    {
      "epoch": 464.0875912408759,
      "grad_norm": 11.13884449005127,
      "learning_rate": 2.6795620437956203e-05,
      "loss": 1.1236,
      "step": 63580
    },
    {
      "epoch": 464.16058394160586,
      "grad_norm": 0.07693461328744888,
      "learning_rate": 2.6791970802919707e-05,
      "loss": 0.7118,
      "step": 63590
    },
    {
      "epoch": 464.2335766423358,
      "grad_norm": 9.114494323730469,
      "learning_rate": 2.6788321167883214e-05,
      "loss": 0.9562,
      "step": 63600
    },
    {
      "epoch": 464.3065693430657,
      "grad_norm": 0.06775449216365814,
      "learning_rate": 2.6784671532846718e-05,
      "loss": 0.7814,
      "step": 63610
    },
    {
      "epoch": 464.3795620437956,
      "grad_norm": 5.8078718185424805,
      "learning_rate": 2.6781021897810222e-05,
      "loss": 0.6201,
      "step": 63620
    },
    {
      "epoch": 464.45255474452557,
      "grad_norm": 4.711482524871826,
      "learning_rate": 2.6777372262773726e-05,
      "loss": 0.8617,
      "step": 63630
    },
    {
      "epoch": 464.5255474452555,
      "grad_norm": 5.662676811218262,
      "learning_rate": 2.677372262773723e-05,
      "loss": 0.8553,
      "step": 63640
    },
    {
      "epoch": 464.5985401459854,
      "grad_norm": 0.30131852626800537,
      "learning_rate": 2.677007299270073e-05,
      "loss": 0.7867,
      "step": 63650
    },
    {
      "epoch": 464.6715328467153,
      "grad_norm": 12.697803497314453,
      "learning_rate": 2.6766423357664234e-05,
      "loss": 0.8458,
      "step": 63660
    },
    {
      "epoch": 464.7445255474453,
      "grad_norm": 5.354949951171875,
      "learning_rate": 2.6762773722627738e-05,
      "loss": 1.1392,
      "step": 63670
    },
    {
      "epoch": 464.8175182481752,
      "grad_norm": 0.0758747085928917,
      "learning_rate": 2.675912408759124e-05,
      "loss": 1.0709,
      "step": 63680
    },
    {
      "epoch": 464.8905109489051,
      "grad_norm": 12.205062866210938,
      "learning_rate": 2.6755474452554746e-05,
      "loss": 1.0861,
      "step": 63690
    },
    {
      "epoch": 464.963503649635,
      "grad_norm": 9.656891822814941,
      "learning_rate": 2.675182481751825e-05,
      "loss": 0.8851,
      "step": 63700
    },
    {
      "epoch": 465.036496350365,
      "grad_norm": 9.67410945892334,
      "learning_rate": 2.6748175182481757e-05,
      "loss": 0.8254,
      "step": 63710
    },
    {
      "epoch": 465.1094890510949,
      "grad_norm": 16.798242568969727,
      "learning_rate": 2.674452554744526e-05,
      "loss": 0.7767,
      "step": 63720
    },
    {
      "epoch": 465.1824817518248,
      "grad_norm": 11.01982307434082,
      "learning_rate": 2.6740875912408758e-05,
      "loss": 1.2557,
      "step": 63730
    },
    {
      "epoch": 465.2554744525547,
      "grad_norm": 15.44852066040039,
      "learning_rate": 2.673722627737226e-05,
      "loss": 0.7014,
      "step": 63740
    },
    {
      "epoch": 465.3284671532847,
      "grad_norm": 0.04021459445357323,
      "learning_rate": 2.6733576642335765e-05,
      "loss": 0.6661,
      "step": 63750
    },
    {
      "epoch": 465.4014598540146,
      "grad_norm": 6.675116539001465,
      "learning_rate": 2.6729927007299273e-05,
      "loss": 0.9818,
      "step": 63760
    },
    {
      "epoch": 465.4744525547445,
      "grad_norm": 0.06344640254974365,
      "learning_rate": 2.6726277372262777e-05,
      "loss": 0.9204,
      "step": 63770
    },
    {
      "epoch": 465.54744525547443,
      "grad_norm": 14.53475284576416,
      "learning_rate": 2.672262773722628e-05,
      "loss": 1.1932,
      "step": 63780
    },
    {
      "epoch": 465.6204379562044,
      "grad_norm": 0.12825340032577515,
      "learning_rate": 2.6718978102189784e-05,
      "loss": 0.8128,
      "step": 63790
    },
    {
      "epoch": 465.6934306569343,
      "grad_norm": 7.93709135055542,
      "learning_rate": 2.6715328467153285e-05,
      "loss": 1.3093,
      "step": 63800
    },
    {
      "epoch": 465.7664233576642,
      "grad_norm": 11.936627388000488,
      "learning_rate": 2.671167883211679e-05,
      "loss": 0.7062,
      "step": 63810
    },
    {
      "epoch": 465.83941605839414,
      "grad_norm": 5.886032581329346,
      "learning_rate": 2.6708029197080293e-05,
      "loss": 0.8341,
      "step": 63820
    },
    {
      "epoch": 465.9124087591241,
      "grad_norm": 0.18484796583652496,
      "learning_rate": 2.6704379562043797e-05,
      "loss": 1.062,
      "step": 63830
    },
    {
      "epoch": 465.985401459854,
      "grad_norm": 10.582728385925293,
      "learning_rate": 2.67007299270073e-05,
      "loss": 0.9247,
      "step": 63840
    },
    {
      "epoch": 466.05839416058393,
      "grad_norm": 5.243549346923828,
      "learning_rate": 2.6697080291970804e-05,
      "loss": 0.8024,
      "step": 63850
    },
    {
      "epoch": 466.13138686131384,
      "grad_norm": 8.547146797180176,
      "learning_rate": 2.6693430656934308e-05,
      "loss": 0.7558,
      "step": 63860
    },
    {
      "epoch": 466.2043795620438,
      "grad_norm": 12.419970512390137,
      "learning_rate": 2.6689781021897815e-05,
      "loss": 1.0004,
      "step": 63870
    },
    {
      "epoch": 466.2773722627737,
      "grad_norm": 15.418850898742676,
      "learning_rate": 2.6686131386861313e-05,
      "loss": 0.6391,
      "step": 63880
    },
    {
      "epoch": 466.35036496350364,
      "grad_norm": 0.1410011202096939,
      "learning_rate": 2.6682481751824816e-05,
      "loss": 0.5184,
      "step": 63890
    },
    {
      "epoch": 466.42335766423355,
      "grad_norm": 15.184516906738281,
      "learning_rate": 2.667883211678832e-05,
      "loss": 1.1647,
      "step": 63900
    },
    {
      "epoch": 466.4963503649635,
      "grad_norm": 14.931611061096191,
      "learning_rate": 2.6675182481751824e-05,
      "loss": 0.9761,
      "step": 63910
    },
    {
      "epoch": 466.56934306569343,
      "grad_norm": 10.205426216125488,
      "learning_rate": 2.667153284671533e-05,
      "loss": 1.2352,
      "step": 63920
    },
    {
      "epoch": 466.64233576642334,
      "grad_norm": 17.50237464904785,
      "learning_rate": 2.6667883211678835e-05,
      "loss": 0.926,
      "step": 63930
    },
    {
      "epoch": 466.7153284671533,
      "grad_norm": 7.303473949432373,
      "learning_rate": 2.666423357664234e-05,
      "loss": 0.6829,
      "step": 63940
    },
    {
      "epoch": 466.7883211678832,
      "grad_norm": 16.50095558166504,
      "learning_rate": 2.6660583941605843e-05,
      "loss": 0.86,
      "step": 63950
    },
    {
      "epoch": 466.86131386861314,
      "grad_norm": 11.816154479980469,
      "learning_rate": 2.6656934306569344e-05,
      "loss": 0.8337,
      "step": 63960
    },
    {
      "epoch": 466.93430656934305,
      "grad_norm": 15.203283309936523,
      "learning_rate": 2.6653284671532847e-05,
      "loss": 1.518,
      "step": 63970
    },
    {
      "epoch": 467.007299270073,
      "grad_norm": 10.260231018066406,
      "learning_rate": 2.664963503649635e-05,
      "loss": 0.9513,
      "step": 63980
    },
    {
      "epoch": 467.08029197080293,
      "grad_norm": 0.06862865388393402,
      "learning_rate": 2.6645985401459855e-05,
      "loss": 1.0426,
      "step": 63990
    },
    {
      "epoch": 467.15328467153284,
      "grad_norm": 6.235811233520508,
      "learning_rate": 2.664233576642336e-05,
      "loss": 0.6878,
      "step": 64000
    },
    {
      "epoch": 467.22627737226276,
      "grad_norm": 9.377096176147461,
      "learning_rate": 2.6638686131386863e-05,
      "loss": 0.5661,
      "step": 64010
    },
    {
      "epoch": 467.2992700729927,
      "grad_norm": 7.72476053237915,
      "learning_rate": 2.6635036496350367e-05,
      "loss": 0.9725,
      "step": 64020
    },
    {
      "epoch": 467.37226277372264,
      "grad_norm": 8.063809394836426,
      "learning_rate": 2.6631386861313867e-05,
      "loss": 0.7076,
      "step": 64030
    },
    {
      "epoch": 467.44525547445255,
      "grad_norm": 10.438496589660645,
      "learning_rate": 2.662773722627737e-05,
      "loss": 0.9718,
      "step": 64040
    },
    {
      "epoch": 467.51824817518246,
      "grad_norm": 8.630352020263672,
      "learning_rate": 2.6624087591240875e-05,
      "loss": 0.9287,
      "step": 64050
    },
    {
      "epoch": 467.59124087591243,
      "grad_norm": 11.378101348876953,
      "learning_rate": 2.662043795620438e-05,
      "loss": 1.1152,
      "step": 64060
    },
    {
      "epoch": 467.66423357664235,
      "grad_norm": 4.931005954742432,
      "learning_rate": 2.6616788321167886e-05,
      "loss": 0.6602,
      "step": 64070
    },
    {
      "epoch": 467.73722627737226,
      "grad_norm": 11.15432357788086,
      "learning_rate": 2.661313868613139e-05,
      "loss": 1.118,
      "step": 64080
    },
    {
      "epoch": 467.81021897810217,
      "grad_norm": 10.672131538391113,
      "learning_rate": 2.6609489051094894e-05,
      "loss": 1.0727,
      "step": 64090
    },
    {
      "epoch": 467.88321167883214,
      "grad_norm": 0.057063907384872437,
      "learning_rate": 2.6605839416058398e-05,
      "loss": 0.5734,
      "step": 64100
    },
    {
      "epoch": 467.95620437956205,
      "grad_norm": 19.98442840576172,
      "learning_rate": 2.6602189781021895e-05,
      "loss": 1.4294,
      "step": 64110
    },
    {
      "epoch": 468.02919708029196,
      "grad_norm": 0.06946779042482376,
      "learning_rate": 2.6598540145985402e-05,
      "loss": 1.1622,
      "step": 64120
    },
    {
      "epoch": 468.1021897810219,
      "grad_norm": 8.14811897277832,
      "learning_rate": 2.6594890510948906e-05,
      "loss": 0.882,
      "step": 64130
    },
    {
      "epoch": 468.17518248175185,
      "grad_norm": 4.930707931518555,
      "learning_rate": 2.659124087591241e-05,
      "loss": 1.0076,
      "step": 64140
    },
    {
      "epoch": 468.24817518248176,
      "grad_norm": 16.997718811035156,
      "learning_rate": 2.6587591240875914e-05,
      "loss": 0.8394,
      "step": 64150
    },
    {
      "epoch": 468.3211678832117,
      "grad_norm": 10.300414085388184,
      "learning_rate": 2.6583941605839418e-05,
      "loss": 0.9882,
      "step": 64160
    },
    {
      "epoch": 468.3941605839416,
      "grad_norm": 19.099346160888672,
      "learning_rate": 2.6580291970802922e-05,
      "loss": 1.1765,
      "step": 64170
    },
    {
      "epoch": 468.46715328467155,
      "grad_norm": 12.218294143676758,
      "learning_rate": 2.657664233576643e-05,
      "loss": 0.651,
      "step": 64180
    },
    {
      "epoch": 468.54014598540147,
      "grad_norm": 8.348930358886719,
      "learning_rate": 2.6572992700729926e-05,
      "loss": 0.8142,
      "step": 64190
    },
    {
      "epoch": 468.6131386861314,
      "grad_norm": 0.118364118039608,
      "learning_rate": 2.656934306569343e-05,
      "loss": 0.8623,
      "step": 64200
    },
    {
      "epoch": 468.6861313868613,
      "grad_norm": 6.0912766456604,
      "learning_rate": 2.6565693430656934e-05,
      "loss": 0.7395,
      "step": 64210
    },
    {
      "epoch": 468.75912408759126,
      "grad_norm": 0.07856124639511108,
      "learning_rate": 2.6562043795620438e-05,
      "loss": 0.6507,
      "step": 64220
    },
    {
      "epoch": 468.8321167883212,
      "grad_norm": 8.172061920166016,
      "learning_rate": 2.6558394160583945e-05,
      "loss": 1.2502,
      "step": 64230
    },
    {
      "epoch": 468.9051094890511,
      "grad_norm": 5.288619041442871,
      "learning_rate": 2.655474452554745e-05,
      "loss": 0.8444,
      "step": 64240
    },
    {
      "epoch": 468.978102189781,
      "grad_norm": 13.966246604919434,
      "learning_rate": 2.6551094890510953e-05,
      "loss": 0.9704,
      "step": 64250
    },
    {
      "epoch": 469.05109489051097,
      "grad_norm": 4.268404006958008,
      "learning_rate": 2.654744525547445e-05,
      "loss": 0.8632,
      "step": 64260
    },
    {
      "epoch": 469.1240875912409,
      "grad_norm": 9.48042106628418,
      "learning_rate": 2.6543795620437957e-05,
      "loss": 0.6286,
      "step": 64270
    },
    {
      "epoch": 469.1970802919708,
      "grad_norm": 16.36793327331543,
      "learning_rate": 2.654014598540146e-05,
      "loss": 1.3817,
      "step": 64280
    },
    {
      "epoch": 469.2700729927007,
      "grad_norm": 9.29079532623291,
      "learning_rate": 2.6536496350364965e-05,
      "loss": 0.926,
      "step": 64290
    },
    {
      "epoch": 469.3430656934307,
      "grad_norm": 7.330062389373779,
      "learning_rate": 2.653284671532847e-05,
      "loss": 1.2189,
      "step": 64300
    },
    {
      "epoch": 469.4160583941606,
      "grad_norm": 0.027567178010940552,
      "learning_rate": 2.6529197080291973e-05,
      "loss": 0.9383,
      "step": 64310
    },
    {
      "epoch": 469.4890510948905,
      "grad_norm": 14.251285552978516,
      "learning_rate": 2.6525547445255477e-05,
      "loss": 1.1585,
      "step": 64320
    },
    {
      "epoch": 469.5620437956204,
      "grad_norm": 12.200297355651855,
      "learning_rate": 2.652189781021898e-05,
      "loss": 0.8228,
      "step": 64330
    },
    {
      "epoch": 469.6350364963504,
      "grad_norm": 7.568203926086426,
      "learning_rate": 2.651824817518248e-05,
      "loss": 0.8066,
      "step": 64340
    },
    {
      "epoch": 469.7080291970803,
      "grad_norm": 0.04997691139578819,
      "learning_rate": 2.6514598540145985e-05,
      "loss": 0.8849,
      "step": 64350
    },
    {
      "epoch": 469.7810218978102,
      "grad_norm": 0.9385387897491455,
      "learning_rate": 2.651094890510949e-05,
      "loss": 0.6196,
      "step": 64360
    },
    {
      "epoch": 469.8540145985401,
      "grad_norm": 9.11452865600586,
      "learning_rate": 2.6507299270072993e-05,
      "loss": 1.0679,
      "step": 64370
    },
    {
      "epoch": 469.9270072992701,
      "grad_norm": 8.94551944732666,
      "learning_rate": 2.65036496350365e-05,
      "loss": 1.0018,
      "step": 64380
    },
    {
      "epoch": 470.0,
      "grad_norm": 0.19618768990039825,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.4655,
      "step": 64390
    },
    {
      "epoch": 470.0729927007299,
      "grad_norm": 6.889368057250977,
      "learning_rate": 2.6496350364963508e-05,
      "loss": 0.9145,
      "step": 64400
    },
    {
      "epoch": 470.1459854014599,
      "grad_norm": 17.144638061523438,
      "learning_rate": 2.6492700729927005e-05,
      "loss": 1.2477,
      "step": 64410
    },
    {
      "epoch": 470.2189781021898,
      "grad_norm": 9.67530345916748,
      "learning_rate": 2.648905109489051e-05,
      "loss": 0.7723,
      "step": 64420
    },
    {
      "epoch": 470.2919708029197,
      "grad_norm": 0.1163790374994278,
      "learning_rate": 2.6485401459854016e-05,
      "loss": 0.7699,
      "step": 64430
    },
    {
      "epoch": 470.3649635036496,
      "grad_norm": 6.692159652709961,
      "learning_rate": 2.648175182481752e-05,
      "loss": 0.8413,
      "step": 64440
    },
    {
      "epoch": 470.4379562043796,
      "grad_norm": 7.1208038330078125,
      "learning_rate": 2.6478102189781024e-05,
      "loss": 0.8987,
      "step": 64450
    },
    {
      "epoch": 470.5109489051095,
      "grad_norm": 13.825020790100098,
      "learning_rate": 2.6474452554744528e-05,
      "loss": 1.4007,
      "step": 64460
    },
    {
      "epoch": 470.5839416058394,
      "grad_norm": 0.03898121044039726,
      "learning_rate": 2.647080291970803e-05,
      "loss": 1.08,
      "step": 64470
    },
    {
      "epoch": 470.6569343065693,
      "grad_norm": 11.541631698608398,
      "learning_rate": 2.6467153284671535e-05,
      "loss": 0.8565,
      "step": 64480
    },
    {
      "epoch": 470.7299270072993,
      "grad_norm": 10.36652946472168,
      "learning_rate": 2.6463503649635036e-05,
      "loss": 0.6843,
      "step": 64490
    },
    {
      "epoch": 470.8029197080292,
      "grad_norm": 23.9173526763916,
      "learning_rate": 2.645985401459854e-05,
      "loss": 0.7455,
      "step": 64500
    },
    {
      "epoch": 470.8759124087591,
      "grad_norm": 10.853035926818848,
      "learning_rate": 2.6456204379562044e-05,
      "loss": 1.1085,
      "step": 64510
    },
    {
      "epoch": 470.94890510948903,
      "grad_norm": 10.525189399719238,
      "learning_rate": 2.6452554744525547e-05,
      "loss": 0.7598,
      "step": 64520
    },
    {
      "epoch": 471.021897810219,
      "grad_norm": 7.768487453460693,
      "learning_rate": 2.644890510948905e-05,
      "loss": 0.9367,
      "step": 64530
    },
    {
      "epoch": 471.0948905109489,
      "grad_norm": 16.979082107543945,
      "learning_rate": 2.644525547445256e-05,
      "loss": 0.9301,
      "step": 64540
    },
    {
      "epoch": 471.1678832116788,
      "grad_norm": 0.20624372363090515,
      "learning_rate": 2.6441605839416063e-05,
      "loss": 0.4416,
      "step": 64550
    },
    {
      "epoch": 471.24087591240874,
      "grad_norm": 5.624583721160889,
      "learning_rate": 2.6437956204379566e-05,
      "loss": 0.916,
      "step": 64560
    },
    {
      "epoch": 471.3138686131387,
      "grad_norm": 7.60733699798584,
      "learning_rate": 2.6434306569343063e-05,
      "loss": 1.3637,
      "step": 64570
    },
    {
      "epoch": 471.3868613138686,
      "grad_norm": 4.837339401245117,
      "learning_rate": 2.643065693430657e-05,
      "loss": 0.9389,
      "step": 64580
    },
    {
      "epoch": 471.45985401459853,
      "grad_norm": 15.242557525634766,
      "learning_rate": 2.6427007299270075e-05,
      "loss": 1.1264,
      "step": 64590
    },
    {
      "epoch": 471.53284671532845,
      "grad_norm": 9.569677352905273,
      "learning_rate": 2.642335766423358e-05,
      "loss": 1.2231,
      "step": 64600
    },
    {
      "epoch": 471.6058394160584,
      "grad_norm": 0.16225335001945496,
      "learning_rate": 2.6419708029197082e-05,
      "loss": 0.7406,
      "step": 64610
    },
    {
      "epoch": 471.6788321167883,
      "grad_norm": 16.132862091064453,
      "learning_rate": 2.6416058394160586e-05,
      "loss": 1.1013,
      "step": 64620
    },
    {
      "epoch": 471.75182481751824,
      "grad_norm": 0.565473735332489,
      "learning_rate": 2.641240875912409e-05,
      "loss": 0.7817,
      "step": 64630
    },
    {
      "epoch": 471.82481751824815,
      "grad_norm": 0.15788757801055908,
      "learning_rate": 2.640875912408759e-05,
      "loss": 0.5381,
      "step": 64640
    },
    {
      "epoch": 471.8978102189781,
      "grad_norm": 7.51993989944458,
      "learning_rate": 2.6405109489051095e-05,
      "loss": 0.9024,
      "step": 64650
    },
    {
      "epoch": 471.97080291970804,
      "grad_norm": 7.399908542633057,
      "learning_rate": 2.64014598540146e-05,
      "loss": 0.8465,
      "step": 64660
    },
    {
      "epoch": 472.04379562043795,
      "grad_norm": 14.884370803833008,
      "learning_rate": 2.6397810218978102e-05,
      "loss": 0.7367,
      "step": 64670
    },
    {
      "epoch": 472.11678832116786,
      "grad_norm": 0.5834299921989441,
      "learning_rate": 2.6394160583941606e-05,
      "loss": 0.5025,
      "step": 64680
    },
    {
      "epoch": 472.18978102189783,
      "grad_norm": 0.0929114893078804,
      "learning_rate": 2.639051094890511e-05,
      "loss": 0.9974,
      "step": 64690
    },
    {
      "epoch": 472.26277372262774,
      "grad_norm": 12.069567680358887,
      "learning_rate": 2.6386861313868617e-05,
      "loss": 0.8717,
      "step": 64700
    },
    {
      "epoch": 472.33576642335765,
      "grad_norm": 16.460342407226562,
      "learning_rate": 2.638321167883212e-05,
      "loss": 1.2185,
      "step": 64710
    },
    {
      "epoch": 472.40875912408757,
      "grad_norm": 10.470312118530273,
      "learning_rate": 2.637956204379562e-05,
      "loss": 0.9145,
      "step": 64720
    },
    {
      "epoch": 472.48175182481754,
      "grad_norm": 8.767361640930176,
      "learning_rate": 2.6375912408759122e-05,
      "loss": 1.0457,
      "step": 64730
    },
    {
      "epoch": 472.55474452554745,
      "grad_norm": 12.020730972290039,
      "learning_rate": 2.637226277372263e-05,
      "loss": 0.7319,
      "step": 64740
    },
    {
      "epoch": 472.62773722627736,
      "grad_norm": 9.88986587524414,
      "learning_rate": 2.6368613138686133e-05,
      "loss": 0.9667,
      "step": 64750
    },
    {
      "epoch": 472.7007299270073,
      "grad_norm": 10.53748607635498,
      "learning_rate": 2.6364963503649637e-05,
      "loss": 1.1289,
      "step": 64760
    },
    {
      "epoch": 472.77372262773724,
      "grad_norm": 16.594980239868164,
      "learning_rate": 2.636131386861314e-05,
      "loss": 0.9052,
      "step": 64770
    },
    {
      "epoch": 472.84671532846716,
      "grad_norm": 7.491656303405762,
      "learning_rate": 2.6357664233576645e-05,
      "loss": 1.2825,
      "step": 64780
    },
    {
      "epoch": 472.91970802919707,
      "grad_norm": 10.47131633758545,
      "learning_rate": 2.635401459854015e-05,
      "loss": 0.7454,
      "step": 64790
    },
    {
      "epoch": 472.992700729927,
      "grad_norm": 12.896517753601074,
      "learning_rate": 2.635036496350365e-05,
      "loss": 0.9119,
      "step": 64800
    },
    {
      "epoch": 473.06569343065695,
      "grad_norm": 17.276905059814453,
      "learning_rate": 2.6346715328467153e-05,
      "loss": 0.9473,
      "step": 64810
    },
    {
      "epoch": 473.13868613138686,
      "grad_norm": 0.1080612912774086,
      "learning_rate": 2.6343065693430657e-05,
      "loss": 0.9475,
      "step": 64820
    },
    {
      "epoch": 473.2116788321168,
      "grad_norm": 0.10454638302326202,
      "learning_rate": 2.633941605839416e-05,
      "loss": 0.7293,
      "step": 64830
    },
    {
      "epoch": 473.2846715328467,
      "grad_norm": 12.498605728149414,
      "learning_rate": 2.6335766423357665e-05,
      "loss": 0.9761,
      "step": 64840
    },
    {
      "epoch": 473.35766423357666,
      "grad_norm": 6.636083126068115,
      "learning_rate": 2.6332116788321172e-05,
      "loss": 1.0974,
      "step": 64850
    },
    {
      "epoch": 473.43065693430657,
      "grad_norm": 5.068634510040283,
      "learning_rate": 2.6328467153284676e-05,
      "loss": 1.1963,
      "step": 64860
    },
    {
      "epoch": 473.5036496350365,
      "grad_norm": 7.671342849731445,
      "learning_rate": 2.6324817518248173e-05,
      "loss": 1.1093,
      "step": 64870
    },
    {
      "epoch": 473.57664233576645,
      "grad_norm": 0.04533788561820984,
      "learning_rate": 2.6321167883211677e-05,
      "loss": 0.4261,
      "step": 64880
    },
    {
      "epoch": 473.64963503649636,
      "grad_norm": 9.175829887390137,
      "learning_rate": 2.631751824817518e-05,
      "loss": 1.0087,
      "step": 64890
    },
    {
      "epoch": 473.7226277372263,
      "grad_norm": 13.1692533493042,
      "learning_rate": 2.6313868613138688e-05,
      "loss": 0.7725,
      "step": 64900
    },
    {
      "epoch": 473.7956204379562,
      "grad_norm": 13.905994415283203,
      "learning_rate": 2.6310218978102192e-05,
      "loss": 0.9443,
      "step": 64910
    },
    {
      "epoch": 473.86861313868616,
      "grad_norm": 14.338601112365723,
      "learning_rate": 2.6306569343065696e-05,
      "loss": 0.4956,
      "step": 64920
    },
    {
      "epoch": 473.94160583941607,
      "grad_norm": 13.6867094039917,
      "learning_rate": 2.63029197080292e-05,
      "loss": 1.0415,
      "step": 64930
    },
    {
      "epoch": 474.014598540146,
      "grad_norm": 10.24067497253418,
      "learning_rate": 2.6299270072992704e-05,
      "loss": 1.0699,
      "step": 64940
    },
    {
      "epoch": 474.0875912408759,
      "grad_norm": 7.521264553070068,
      "learning_rate": 2.6295620437956204e-05,
      "loss": 1.218,
      "step": 64950
    },
    {
      "epoch": 474.16058394160586,
      "grad_norm": 14.022257804870605,
      "learning_rate": 2.6291970802919708e-05,
      "loss": 0.7725,
      "step": 64960
    },
    {
      "epoch": 474.2335766423358,
      "grad_norm": 8.349639892578125,
      "learning_rate": 2.6288321167883212e-05,
      "loss": 0.9295,
      "step": 64970
    },
    {
      "epoch": 474.3065693430657,
      "grad_norm": 15.048055648803711,
      "learning_rate": 2.6284671532846716e-05,
      "loss": 0.8826,
      "step": 64980
    },
    {
      "epoch": 474.3795620437956,
      "grad_norm": 4.905030250549316,
      "learning_rate": 2.628102189781022e-05,
      "loss": 0.7529,
      "step": 64990
    },
    {
      "epoch": 474.45255474452557,
      "grad_norm": 11.94093132019043,
      "learning_rate": 2.6277372262773724e-05,
      "loss": 0.9715,
      "step": 65000
    },
    {
      "epoch": 474.5255474452555,
      "grad_norm": 0.16246047616004944,
      "learning_rate": 2.627372262773723e-05,
      "loss": 0.8059,
      "step": 65010
    },
    {
      "epoch": 474.5985401459854,
      "grad_norm": 9.15829849243164,
      "learning_rate": 2.6270072992700735e-05,
      "loss": 1.0383,
      "step": 65020
    },
    {
      "epoch": 474.6715328467153,
      "grad_norm": 15.565268516540527,
      "learning_rate": 2.6266423357664232e-05,
      "loss": 1.2937,
      "step": 65030
    },
    {
      "epoch": 474.7445255474453,
      "grad_norm": 8.535202026367188,
      "learning_rate": 2.6262773722627736e-05,
      "loss": 1.1692,
      "step": 65040
    },
    {
      "epoch": 474.8175182481752,
      "grad_norm": 7.888122081756592,
      "learning_rate": 2.6259124087591243e-05,
      "loss": 0.8398,
      "step": 65050
    },
    {
      "epoch": 474.8905109489051,
      "grad_norm": 7.760691165924072,
      "learning_rate": 2.6255474452554747e-05,
      "loss": 0.62,
      "step": 65060
    },
    {
      "epoch": 474.963503649635,
      "grad_norm": 13.361038208007812,
      "learning_rate": 2.625182481751825e-05,
      "loss": 0.9415,
      "step": 65070
    },
    {
      "epoch": 475.036496350365,
      "grad_norm": 4.602835655212402,
      "learning_rate": 2.6248175182481755e-05,
      "loss": 0.7856,
      "step": 65080
    },
    {
      "epoch": 475.1094890510949,
      "grad_norm": 12.880146980285645,
      "learning_rate": 2.624452554744526e-05,
      "loss": 1.0996,
      "step": 65090
    },
    {
      "epoch": 475.1824817518248,
      "grad_norm": 19.205190658569336,
      "learning_rate": 2.624087591240876e-05,
      "loss": 0.9095,
      "step": 65100
    },
    {
      "epoch": 475.2554744525547,
      "grad_norm": 18.68216323852539,
      "learning_rate": 2.6237226277372263e-05,
      "loss": 0.8864,
      "step": 65110
    },
    {
      "epoch": 475.3284671532847,
      "grad_norm": 9.862320899963379,
      "learning_rate": 2.6233576642335767e-05,
      "loss": 0.4713,
      "step": 65120
    },
    {
      "epoch": 475.4014598540146,
      "grad_norm": 5.713446617126465,
      "learning_rate": 2.622992700729927e-05,
      "loss": 1.1391,
      "step": 65130
    },
    {
      "epoch": 475.4744525547445,
      "grad_norm": 8.028264999389648,
      "learning_rate": 2.6226277372262775e-05,
      "loss": 1.0022,
      "step": 65140
    },
    {
      "epoch": 475.54744525547443,
      "grad_norm": 8.338090896606445,
      "learning_rate": 2.622262773722628e-05,
      "loss": 0.9868,
      "step": 65150
    },
    {
      "epoch": 475.6204379562044,
      "grad_norm": 0.1276080310344696,
      "learning_rate": 2.6218978102189782e-05,
      "loss": 1.0055,
      "step": 65160
    },
    {
      "epoch": 475.6934306569343,
      "grad_norm": 6.0258026123046875,
      "learning_rate": 2.621532846715329e-05,
      "loss": 1.0252,
      "step": 65170
    },
    {
      "epoch": 475.7664233576642,
      "grad_norm": 9.766014099121094,
      "learning_rate": 2.6211678832116787e-05,
      "loss": 0.9305,
      "step": 65180
    },
    {
      "epoch": 475.83941605839414,
      "grad_norm": 0.0367879755795002,
      "learning_rate": 2.620802919708029e-05,
      "loss": 0.8406,
      "step": 65190
    },
    {
      "epoch": 475.9124087591241,
      "grad_norm": 13.949585914611816,
      "learning_rate": 2.6204379562043795e-05,
      "loss": 0.9211,
      "step": 65200
    },
    {
      "epoch": 475.985401459854,
      "grad_norm": 10.304912567138672,
      "learning_rate": 2.6200729927007302e-05,
      "loss": 0.8001,
      "step": 65210
    },
    {
      "epoch": 476.05839416058393,
      "grad_norm": 6.842144012451172,
      "learning_rate": 2.6197080291970806e-05,
      "loss": 0.7023,
      "step": 65220
    },
    {
      "epoch": 476.13138686131384,
      "grad_norm": 20.357955932617188,
      "learning_rate": 2.619343065693431e-05,
      "loss": 0.8936,
      "step": 65230
    },
    {
      "epoch": 476.2043795620438,
      "grad_norm": 9.201560974121094,
      "learning_rate": 2.6189781021897813e-05,
      "loss": 0.9484,
      "step": 65240
    },
    {
      "epoch": 476.2773722627737,
      "grad_norm": 0.10252516716718674,
      "learning_rate": 2.6186131386861317e-05,
      "loss": 0.6971,
      "step": 65250
    },
    {
      "epoch": 476.35036496350364,
      "grad_norm": 8.16535472869873,
      "learning_rate": 2.6182481751824818e-05,
      "loss": 1.3104,
      "step": 65260
    },
    {
      "epoch": 476.42335766423355,
      "grad_norm": 8.246108055114746,
      "learning_rate": 2.6178832116788322e-05,
      "loss": 0.8652,
      "step": 65270
    },
    {
      "epoch": 476.4963503649635,
      "grad_norm": 7.822537899017334,
      "learning_rate": 2.6175182481751826e-05,
      "loss": 0.6129,
      "step": 65280
    },
    {
      "epoch": 476.56934306569343,
      "grad_norm": 19.139257431030273,
      "learning_rate": 2.617153284671533e-05,
      "loss": 1.2807,
      "step": 65290
    },
    {
      "epoch": 476.64233576642334,
      "grad_norm": 0.11342557519674301,
      "learning_rate": 2.6167883211678833e-05,
      "loss": 1.1792,
      "step": 65300
    },
    {
      "epoch": 476.7153284671533,
      "grad_norm": 10.416010856628418,
      "learning_rate": 2.6164233576642337e-05,
      "loss": 0.7255,
      "step": 65310
    },
    {
      "epoch": 476.7883211678832,
      "grad_norm": 0.44021469354629517,
      "learning_rate": 2.6160583941605845e-05,
      "loss": 0.8101,
      "step": 65320
    },
    {
      "epoch": 476.86131386861314,
      "grad_norm": 11.585732460021973,
      "learning_rate": 2.615693430656934e-05,
      "loss": 0.694,
      "step": 65330
    },
    {
      "epoch": 476.93430656934305,
      "grad_norm": 3.3079497814178467,
      "learning_rate": 2.6153284671532846e-05,
      "loss": 0.7843,
      "step": 65340
    },
    {
      "epoch": 477.007299270073,
      "grad_norm": 7.25556755065918,
      "learning_rate": 2.614963503649635e-05,
      "loss": 0.6946,
      "step": 65350
    },
    {
      "epoch": 477.08029197080293,
      "grad_norm": 8.374210357666016,
      "learning_rate": 2.6145985401459853e-05,
      "loss": 0.7882,
      "step": 65360
    },
    {
      "epoch": 477.15328467153284,
      "grad_norm": 13.240442276000977,
      "learning_rate": 2.614233576642336e-05,
      "loss": 0.4462,
      "step": 65370
    },
    {
      "epoch": 477.22627737226276,
      "grad_norm": 11.564932823181152,
      "learning_rate": 2.6138686131386864e-05,
      "loss": 0.8855,
      "step": 65380
    },
    {
      "epoch": 477.2992700729927,
      "grad_norm": 8.385625839233398,
      "learning_rate": 2.6135036496350368e-05,
      "loss": 1.1118,
      "step": 65390
    },
    {
      "epoch": 477.37226277372264,
      "grad_norm": 7.6578145027160645,
      "learning_rate": 2.6131386861313872e-05,
      "loss": 0.8713,
      "step": 65400
    },
    {
      "epoch": 477.44525547445255,
      "grad_norm": 9.68416690826416,
      "learning_rate": 2.6127737226277373e-05,
      "loss": 1.208,
      "step": 65410
    },
    {
      "epoch": 477.51824817518246,
      "grad_norm": 11.510275840759277,
      "learning_rate": 2.6124087591240877e-05,
      "loss": 0.8918,
      "step": 65420
    },
    {
      "epoch": 477.59124087591243,
      "grad_norm": 0.0725855678319931,
      "learning_rate": 2.612043795620438e-05,
      "loss": 0.824,
      "step": 65430
    },
    {
      "epoch": 477.66423357664235,
      "grad_norm": 10.42158317565918,
      "learning_rate": 2.6116788321167884e-05,
      "loss": 0.6379,
      "step": 65440
    },
    {
      "epoch": 477.73722627737226,
      "grad_norm": 19.18288230895996,
      "learning_rate": 2.6113138686131388e-05,
      "loss": 0.7784,
      "step": 65450
    },
    {
      "epoch": 477.81021897810217,
      "grad_norm": 14.80672836303711,
      "learning_rate": 2.6109489051094892e-05,
      "loss": 0.8914,
      "step": 65460
    },
    {
      "epoch": 477.88321167883214,
      "grad_norm": 9.149360656738281,
      "learning_rate": 2.6105839416058396e-05,
      "loss": 1.0141,
      "step": 65470
    },
    {
      "epoch": 477.95620437956205,
      "grad_norm": 9.034323692321777,
      "learning_rate": 2.6102189781021903e-05,
      "loss": 0.7927,
      "step": 65480
    },
    {
      "epoch": 478.02919708029196,
      "grad_norm": 9.264981269836426,
      "learning_rate": 2.60985401459854e-05,
      "loss": 1.0827,
      "step": 65490
    },
    {
      "epoch": 478.1021897810219,
      "grad_norm": 0.12850266695022583,
      "learning_rate": 2.6094890510948904e-05,
      "loss": 0.5309,
      "step": 65500
    },
    {
      "epoch": 478.17518248175185,
      "grad_norm": 7.923270225524902,
      "learning_rate": 2.6091240875912408e-05,
      "loss": 1.3616,
      "step": 65510
    },
    {
      "epoch": 478.24817518248176,
      "grad_norm": 0.05924350395798683,
      "learning_rate": 2.6087591240875915e-05,
      "loss": 0.8377,
      "step": 65520
    },
    {
      "epoch": 478.3211678832117,
      "grad_norm": 5.94718074798584,
      "learning_rate": 2.608394160583942e-05,
      "loss": 0.6828,
      "step": 65530
    },
    {
      "epoch": 478.3941605839416,
      "grad_norm": 13.263699531555176,
      "learning_rate": 2.6080291970802923e-05,
      "loss": 0.7392,
      "step": 65540
    },
    {
      "epoch": 478.46715328467155,
      "grad_norm": 15.57740592956543,
      "learning_rate": 2.6076642335766427e-05,
      "loss": 1.2842,
      "step": 65550
    },
    {
      "epoch": 478.54014598540147,
      "grad_norm": 14.295539855957031,
      "learning_rate": 2.6072992700729924e-05,
      "loss": 0.8902,
      "step": 65560
    },
    {
      "epoch": 478.6131386861314,
      "grad_norm": 11.1013765335083,
      "learning_rate": 2.606934306569343e-05,
      "loss": 0.865,
      "step": 65570
    },
    {
      "epoch": 478.6861313868613,
      "grad_norm": 9.71603775024414,
      "learning_rate": 2.6065693430656935e-05,
      "loss": 1.2311,
      "step": 65580
    },
    {
      "epoch": 478.75912408759126,
      "grad_norm": 6.9412689208984375,
      "learning_rate": 2.606204379562044e-05,
      "loss": 0.767,
      "step": 65590
    },
    {
      "epoch": 478.8321167883212,
      "grad_norm": 0.09428422152996063,
      "learning_rate": 2.6058394160583943e-05,
      "loss": 1.0184,
      "step": 65600
    },
    {
      "epoch": 478.9051094890511,
      "grad_norm": 11.149884223937988,
      "learning_rate": 2.6054744525547447e-05,
      "loss": 0.7347,
      "step": 65610
    },
    {
      "epoch": 478.978102189781,
      "grad_norm": 18.76689338684082,
      "learning_rate": 2.605109489051095e-05,
      "loss": 0.809,
      "step": 65620
    },
    {
      "epoch": 479.05109489051097,
      "grad_norm": 16.07757568359375,
      "learning_rate": 2.6047445255474455e-05,
      "loss": 1.1213,
      "step": 65630
    },
    {
      "epoch": 479.1240875912409,
      "grad_norm": 5.477715969085693,
      "learning_rate": 2.6043795620437955e-05,
      "loss": 1.0843,
      "step": 65640
    },
    {
      "epoch": 479.1970802919708,
      "grad_norm": 10.141520500183105,
      "learning_rate": 2.604014598540146e-05,
      "loss": 0.4069,
      "step": 65650
    },
    {
      "epoch": 479.2700729927007,
      "grad_norm": 5.90281343460083,
      "learning_rate": 2.6036496350364963e-05,
      "loss": 0.7795,
      "step": 65660
    },
    {
      "epoch": 479.3430656934307,
      "grad_norm": 0.0692642480134964,
      "learning_rate": 2.6032846715328467e-05,
      "loss": 0.7132,
      "step": 65670
    },
    {
      "epoch": 479.4160583941606,
      "grad_norm": 6.660403251647949,
      "learning_rate": 2.6029197080291974e-05,
      "loss": 0.7101,
      "step": 65680
    },
    {
      "epoch": 479.4890510948905,
      "grad_norm": 8.222317695617676,
      "learning_rate": 2.6025547445255478e-05,
      "loss": 1.038,
      "step": 65690
    },
    {
      "epoch": 479.5620437956204,
      "grad_norm": 9.059259414672852,
      "learning_rate": 2.6021897810218982e-05,
      "loss": 0.8413,
      "step": 65700
    },
    {
      "epoch": 479.6350364963504,
      "grad_norm": 0.058635346591472626,
      "learning_rate": 2.601824817518248e-05,
      "loss": 0.887,
      "step": 65710
    },
    {
      "epoch": 479.7080291970803,
      "grad_norm": 11.064374923706055,
      "learning_rate": 2.6014598540145986e-05,
      "loss": 1.2245,
      "step": 65720
    },
    {
      "epoch": 479.7810218978102,
      "grad_norm": 9.8787841796875,
      "learning_rate": 2.601094890510949e-05,
      "loss": 0.9714,
      "step": 65730
    },
    {
      "epoch": 479.8540145985401,
      "grad_norm": 0.10920216888189316,
      "learning_rate": 2.6007299270072994e-05,
      "loss": 0.8081,
      "step": 65740
    },
    {
      "epoch": 479.9270072992701,
      "grad_norm": 15.733312606811523,
      "learning_rate": 2.6003649635036498e-05,
      "loss": 1.1025,
      "step": 65750
    },
    {
      "epoch": 480.0,
      "grad_norm": 1.0359364748001099,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.955,
      "step": 65760
    },
    {
      "epoch": 480.0729927007299,
      "grad_norm": 11.72916030883789,
      "learning_rate": 2.5996350364963506e-05,
      "loss": 0.8892,
      "step": 65770
    },
    {
      "epoch": 480.1459854014599,
      "grad_norm": 11.776714324951172,
      "learning_rate": 2.599270072992701e-05,
      "loss": 0.9288,
      "step": 65780
    },
    {
      "epoch": 480.2189781021898,
      "grad_norm": 11.215010643005371,
      "learning_rate": 2.598905109489051e-05,
      "loss": 0.9015,
      "step": 65790
    },
    {
      "epoch": 480.2919708029197,
      "grad_norm": 8.983246803283691,
      "learning_rate": 2.5985401459854014e-05,
      "loss": 0.8019,
      "step": 65800
    },
    {
      "epoch": 480.3649635036496,
      "grad_norm": 15.156563758850098,
      "learning_rate": 2.5981751824817518e-05,
      "loss": 1.2865,
      "step": 65810
    },
    {
      "epoch": 480.4379562043796,
      "grad_norm": 4.533555030822754,
      "learning_rate": 2.5978102189781022e-05,
      "loss": 0.7085,
      "step": 65820
    },
    {
      "epoch": 480.5109489051095,
      "grad_norm": 6.359684467315674,
      "learning_rate": 2.5974452554744526e-05,
      "loss": 1.5939,
      "step": 65830
    },
    {
      "epoch": 480.5839416058394,
      "grad_norm": 0.09542770683765411,
      "learning_rate": 2.5970802919708033e-05,
      "loss": 0.516,
      "step": 65840
    },
    {
      "epoch": 480.6569343065693,
      "grad_norm": 5.833310604095459,
      "learning_rate": 2.5967153284671537e-05,
      "loss": 0.8821,
      "step": 65850
    },
    {
      "epoch": 480.7299270072993,
      "grad_norm": 10.739896774291992,
      "learning_rate": 2.596350364963504e-05,
      "loss": 0.7631,
      "step": 65860
    },
    {
      "epoch": 480.8029197080292,
      "grad_norm": 7.370450496673584,
      "learning_rate": 2.5959854014598538e-05,
      "loss": 1.135,
      "step": 65870
    },
    {
      "epoch": 480.8759124087591,
      "grad_norm": 7.59440803527832,
      "learning_rate": 2.5956204379562045e-05,
      "loss": 0.9332,
      "step": 65880
    },
    {
      "epoch": 480.94890510948903,
      "grad_norm": 14.262920379638672,
      "learning_rate": 2.595255474452555e-05,
      "loss": 0.6103,
      "step": 65890
    },
    {
      "epoch": 481.021897810219,
      "grad_norm": 10.864269256591797,
      "learning_rate": 2.5948905109489053e-05,
      "loss": 0.8257,
      "step": 65900
    },
    {
      "epoch": 481.0948905109489,
      "grad_norm": 0.06072069704532623,
      "learning_rate": 2.5945255474452557e-05,
      "loss": 0.7495,
      "step": 65910
    },
    {
      "epoch": 481.1678832116788,
      "grad_norm": 12.815192222595215,
      "learning_rate": 2.594160583941606e-05,
      "loss": 0.9737,
      "step": 65920
    },
    {
      "epoch": 481.24087591240874,
      "grad_norm": 16.090648651123047,
      "learning_rate": 2.5937956204379564e-05,
      "loss": 0.697,
      "step": 65930
    },
    {
      "epoch": 481.3138686131387,
      "grad_norm": 8.775586128234863,
      "learning_rate": 2.5934306569343065e-05,
      "loss": 0.7464,
      "step": 65940
    },
    {
      "epoch": 481.3868613138686,
      "grad_norm": 14.504337310791016,
      "learning_rate": 2.593065693430657e-05,
      "loss": 1.1107,
      "step": 65950
    },
    {
      "epoch": 481.45985401459853,
      "grad_norm": 8.778951644897461,
      "learning_rate": 2.5927007299270073e-05,
      "loss": 1.144,
      "step": 65960
    },
    {
      "epoch": 481.53284671532845,
      "grad_norm": 8.535773277282715,
      "learning_rate": 2.5923357664233577e-05,
      "loss": 1.0052,
      "step": 65970
    },
    {
      "epoch": 481.6058394160584,
      "grad_norm": 7.437418460845947,
      "learning_rate": 2.591970802919708e-05,
      "loss": 0.7697,
      "step": 65980
    },
    {
      "epoch": 481.6788321167883,
      "grad_norm": 0.03646044433116913,
      "learning_rate": 2.5916058394160588e-05,
      "loss": 0.8611,
      "step": 65990
    },
    {
      "epoch": 481.75182481751824,
      "grad_norm": 10.908883094787598,
      "learning_rate": 2.591240875912409e-05,
      "loss": 0.7084,
      "step": 66000
    },
    {
      "epoch": 481.82481751824815,
      "grad_norm": 0.0784681886434555,
      "learning_rate": 2.5908759124087595e-05,
      "loss": 0.9989,
      "step": 66010
    },
    {
      "epoch": 481.8978102189781,
      "grad_norm": 6.877508163452148,
      "learning_rate": 2.5905109489051093e-05,
      "loss": 0.9758,
      "step": 66020
    },
    {
      "epoch": 481.97080291970804,
      "grad_norm": 7.5635085105896,
      "learning_rate": 2.5901459854014596e-05,
      "loss": 1.1224,
      "step": 66030
    },
    {
      "epoch": 482.04379562043795,
      "grad_norm": 6.562747478485107,
      "learning_rate": 2.5897810218978104e-05,
      "loss": 0.5525,
      "step": 66040
    },
    {
      "epoch": 482.11678832116786,
      "grad_norm": 0.1057436540722847,
      "learning_rate": 2.5894160583941608e-05,
      "loss": 0.8512,
      "step": 66050
    },
    {
      "epoch": 482.18978102189783,
      "grad_norm": 8.044210433959961,
      "learning_rate": 2.589051094890511e-05,
      "loss": 1.0399,
      "step": 66060
    },
    {
      "epoch": 482.26277372262774,
      "grad_norm": 9.767248153686523,
      "learning_rate": 2.5886861313868615e-05,
      "loss": 1.052,
      "step": 66070
    },
    {
      "epoch": 482.33576642335765,
      "grad_norm": 8.489849090576172,
      "learning_rate": 2.588321167883212e-05,
      "loss": 0.653,
      "step": 66080
    },
    {
      "epoch": 482.40875912408757,
      "grad_norm": 12.639509201049805,
      "learning_rate": 2.5879562043795623e-05,
      "loss": 0.6541,
      "step": 66090
    },
    {
      "epoch": 482.48175182481754,
      "grad_norm": 16.020782470703125,
      "learning_rate": 2.5875912408759124e-05,
      "loss": 1.2465,
      "step": 66100
    },
    {
      "epoch": 482.55474452554745,
      "grad_norm": 15.660453796386719,
      "learning_rate": 2.5872262773722628e-05,
      "loss": 0.9869,
      "step": 66110
    },
    {
      "epoch": 482.62773722627736,
      "grad_norm": 5.755738258361816,
      "learning_rate": 2.586861313868613e-05,
      "loss": 0.7372,
      "step": 66120
    },
    {
      "epoch": 482.7007299270073,
      "grad_norm": 17.36595916748047,
      "learning_rate": 2.5864963503649635e-05,
      "loss": 0.7083,
      "step": 66130
    },
    {
      "epoch": 482.77372262773724,
      "grad_norm": 13.671662330627441,
      "learning_rate": 2.586131386861314e-05,
      "loss": 1.07,
      "step": 66140
    },
    {
      "epoch": 482.84671532846716,
      "grad_norm": 21.18169593811035,
      "learning_rate": 2.5857664233576646e-05,
      "loss": 0.9489,
      "step": 66150
    },
    {
      "epoch": 482.91970802919707,
      "grad_norm": 11.434487342834473,
      "learning_rate": 2.585401459854015e-05,
      "loss": 1.1868,
      "step": 66160
    },
    {
      "epoch": 482.992700729927,
      "grad_norm": 9.487263679504395,
      "learning_rate": 2.5850364963503647e-05,
      "loss": 0.9519,
      "step": 66170
    },
    {
      "epoch": 483.06569343065695,
      "grad_norm": 6.0841779708862305,
      "learning_rate": 2.584671532846715e-05,
      "loss": 0.9493,
      "step": 66180
    },
    {
      "epoch": 483.13868613138686,
      "grad_norm": 11.716639518737793,
      "learning_rate": 2.584306569343066e-05,
      "loss": 0.6392,
      "step": 66190
    },
    {
      "epoch": 483.2116788321168,
      "grad_norm": 8.156033515930176,
      "learning_rate": 2.5839416058394162e-05,
      "loss": 1.2772,
      "step": 66200
    },
    {
      "epoch": 483.2846715328467,
      "grad_norm": 14.387316703796387,
      "learning_rate": 2.5835766423357666e-05,
      "loss": 0.9494,
      "step": 66210
    },
    {
      "epoch": 483.35766423357666,
      "grad_norm": 0.36986568570137024,
      "learning_rate": 2.583211678832117e-05,
      "loss": 0.785,
      "step": 66220
    },
    {
      "epoch": 483.43065693430657,
      "grad_norm": 5.341452598571777,
      "learning_rate": 2.5828467153284674e-05,
      "loss": 1.1381,
      "step": 66230
    },
    {
      "epoch": 483.5036496350365,
      "grad_norm": 7.623507976531982,
      "learning_rate": 2.5824817518248178e-05,
      "loss": 1.0143,
      "step": 66240
    },
    {
      "epoch": 483.57664233576645,
      "grad_norm": 9.487092971801758,
      "learning_rate": 2.582116788321168e-05,
      "loss": 0.4831,
      "step": 66250
    },
    {
      "epoch": 483.64963503649636,
      "grad_norm": 10.120849609375,
      "learning_rate": 2.5817518248175182e-05,
      "loss": 0.9969,
      "step": 66260
    },
    {
      "epoch": 483.7226277372263,
      "grad_norm": 6.971444129943848,
      "learning_rate": 2.5813868613138686e-05,
      "loss": 0.9595,
      "step": 66270
    },
    {
      "epoch": 483.7956204379562,
      "grad_norm": 13.612963676452637,
      "learning_rate": 2.581021897810219e-05,
      "loss": 0.8176,
      "step": 66280
    },
    {
      "epoch": 483.86861313868616,
      "grad_norm": 19.5329647064209,
      "learning_rate": 2.5806569343065694e-05,
      "loss": 1.2359,
      "step": 66290
    },
    {
      "epoch": 483.94160583941607,
      "grad_norm": 19.07656478881836,
      "learning_rate": 2.5802919708029198e-05,
      "loss": 0.8769,
      "step": 66300
    },
    {
      "epoch": 484.014598540146,
      "grad_norm": 0.9050273299217224,
      "learning_rate": 2.5799270072992705e-05,
      "loss": 0.8957,
      "step": 66310
    },
    {
      "epoch": 484.0875912408759,
      "grad_norm": 8.294642448425293,
      "learning_rate": 2.579562043795621e-05,
      "loss": 0.9177,
      "step": 66320
    },
    {
      "epoch": 484.16058394160586,
      "grad_norm": 0.7301231622695923,
      "learning_rate": 2.5791970802919706e-05,
      "loss": 0.6048,
      "step": 66330
    },
    {
      "epoch": 484.2335766423358,
      "grad_norm": 1.3074336051940918,
      "learning_rate": 2.578832116788321e-05,
      "loss": 0.8362,
      "step": 66340
    },
    {
      "epoch": 484.3065693430657,
      "grad_norm": 9.98648452758789,
      "learning_rate": 2.5784671532846717e-05,
      "loss": 1.2796,
      "step": 66350
    },
    {
      "epoch": 484.3795620437956,
      "grad_norm": 9.543686866760254,
      "learning_rate": 2.578102189781022e-05,
      "loss": 0.9531,
      "step": 66360
    },
    {
      "epoch": 484.45255474452557,
      "grad_norm": 9.351444244384766,
      "learning_rate": 2.5777372262773725e-05,
      "loss": 1.2955,
      "step": 66370
    },
    {
      "epoch": 484.5255474452555,
      "grad_norm": 11.88890552520752,
      "learning_rate": 2.577372262773723e-05,
      "loss": 0.7449,
      "step": 66380
    },
    {
      "epoch": 484.5985401459854,
      "grad_norm": 10.075501441955566,
      "learning_rate": 2.5770072992700733e-05,
      "loss": 1.0898,
      "step": 66390
    },
    {
      "epoch": 484.6715328467153,
      "grad_norm": 7.138004302978516,
      "learning_rate": 2.5766423357664233e-05,
      "loss": 0.9252,
      "step": 66400
    },
    {
      "epoch": 484.7445255474453,
      "grad_norm": 1.568103551864624,
      "learning_rate": 2.5762773722627737e-05,
      "loss": 0.7326,
      "step": 66410
    },
    {
      "epoch": 484.8175182481752,
      "grad_norm": 10.806346893310547,
      "learning_rate": 2.575912408759124e-05,
      "loss": 0.5058,
      "step": 66420
    },
    {
      "epoch": 484.8905109489051,
      "grad_norm": 6.156864166259766,
      "learning_rate": 2.5755474452554745e-05,
      "loss": 1.1745,
      "step": 66430
    },
    {
      "epoch": 484.963503649635,
      "grad_norm": 7.501443386077881,
      "learning_rate": 2.575182481751825e-05,
      "loss": 0.9058,
      "step": 66440
    },
    {
      "epoch": 485.036496350365,
      "grad_norm": 4.938249588012695,
      "learning_rate": 2.5748175182481753e-05,
      "loss": 0.9985,
      "step": 66450
    },
    {
      "epoch": 485.1094890510949,
      "grad_norm": 19.139453887939453,
      "learning_rate": 2.574452554744526e-05,
      "loss": 0.895,
      "step": 66460
    },
    {
      "epoch": 485.1824817518248,
      "grad_norm": 9.103610038757324,
      "learning_rate": 2.5740875912408764e-05,
      "loss": 0.9697,
      "step": 66470
    },
    {
      "epoch": 485.2554744525547,
      "grad_norm": 10.07072925567627,
      "learning_rate": 2.573722627737226e-05,
      "loss": 0.8266,
      "step": 66480
    },
    {
      "epoch": 485.3284671532847,
      "grad_norm": 4.881515979766846,
      "learning_rate": 2.5733576642335765e-05,
      "loss": 0.6832,
      "step": 66490
    },
    {
      "epoch": 485.4014598540146,
      "grad_norm": 10.350808143615723,
      "learning_rate": 2.572992700729927e-05,
      "loss": 1.3014,
      "step": 66500
    },
    {
      "epoch": 485.4744525547445,
      "grad_norm": 12.173563003540039,
      "learning_rate": 2.5726277372262776e-05,
      "loss": 1.0353,
      "step": 66510
    },
    {
      "epoch": 485.54744525547443,
      "grad_norm": 8.695626258850098,
      "learning_rate": 2.572262773722628e-05,
      "loss": 0.7216,
      "step": 66520
    },
    {
      "epoch": 485.6204379562044,
      "grad_norm": 8.142871856689453,
      "learning_rate": 2.5718978102189784e-05,
      "loss": 0.728,
      "step": 66530
    },
    {
      "epoch": 485.6934306569343,
      "grad_norm": 10.368340492248535,
      "learning_rate": 2.5715328467153288e-05,
      "loss": 0.9796,
      "step": 66540
    },
    {
      "epoch": 485.7664233576642,
      "grad_norm": 19.427000045776367,
      "learning_rate": 2.571167883211679e-05,
      "loss": 0.5894,
      "step": 66550
    },
    {
      "epoch": 485.83941605839414,
      "grad_norm": 0.12075494229793549,
      "learning_rate": 2.5708029197080292e-05,
      "loss": 0.8774,
      "step": 66560
    },
    {
      "epoch": 485.9124087591241,
      "grad_norm": 11.135146141052246,
      "learning_rate": 2.5704379562043796e-05,
      "loss": 0.9434,
      "step": 66570
    },
    {
      "epoch": 485.985401459854,
      "grad_norm": 9.513059616088867,
      "learning_rate": 2.57007299270073e-05,
      "loss": 0.9146,
      "step": 66580
    },
    {
      "epoch": 486.05839416058393,
      "grad_norm": 6.65813684463501,
      "learning_rate": 2.5697080291970804e-05,
      "loss": 1.0472,
      "step": 66590
    },
    {
      "epoch": 486.13138686131384,
      "grad_norm": 7.119759559631348,
      "learning_rate": 2.5693430656934308e-05,
      "loss": 0.8515,
      "step": 66600
    },
    {
      "epoch": 486.2043795620438,
      "grad_norm": 12.650315284729004,
      "learning_rate": 2.568978102189781e-05,
      "loss": 0.6817,
      "step": 66610
    },
    {
      "epoch": 486.2773722627737,
      "grad_norm": 8.088382720947266,
      "learning_rate": 2.568613138686132e-05,
      "loss": 0.8637,
      "step": 66620
    },
    {
      "epoch": 486.35036496350364,
      "grad_norm": 11.49698257446289,
      "learning_rate": 2.5682481751824816e-05,
      "loss": 1.0009,
      "step": 66630
    },
    {
      "epoch": 486.42335766423355,
      "grad_norm": 0.04245707020163536,
      "learning_rate": 2.567883211678832e-05,
      "loss": 0.8343,
      "step": 66640
    },
    {
      "epoch": 486.4963503649635,
      "grad_norm": 8.894373893737793,
      "learning_rate": 2.5675182481751824e-05,
      "loss": 0.8286,
      "step": 66650
    },
    {
      "epoch": 486.56934306569343,
      "grad_norm": 7.715887069702148,
      "learning_rate": 2.567153284671533e-05,
      "loss": 1.0427,
      "step": 66660
    },
    {
      "epoch": 486.64233576642334,
      "grad_norm": 0.15073226392269135,
      "learning_rate": 2.5667883211678835e-05,
      "loss": 0.6821,
      "step": 66670
    },
    {
      "epoch": 486.7153284671533,
      "grad_norm": 0.13646657764911652,
      "learning_rate": 2.566423357664234e-05,
      "loss": 1.1459,
      "step": 66680
    },
    {
      "epoch": 486.7883211678832,
      "grad_norm": 17.251171112060547,
      "learning_rate": 2.5660583941605843e-05,
      "loss": 1.0089,
      "step": 66690
    },
    {
      "epoch": 486.86131386861314,
      "grad_norm": 13.143885612487793,
      "learning_rate": 2.5656934306569346e-05,
      "loss": 0.6962,
      "step": 66700
    },
    {
      "epoch": 486.93430656934305,
      "grad_norm": 1.7101129293441772,
      "learning_rate": 2.5653284671532847e-05,
      "loss": 0.7696,
      "step": 66710
    },
    {
      "epoch": 487.007299270073,
      "grad_norm": 0.03392495959997177,
      "learning_rate": 2.564963503649635e-05,
      "loss": 1.0673,
      "step": 66720
    },
    {
      "epoch": 487.08029197080293,
      "grad_norm": 9.690932273864746,
      "learning_rate": 2.5645985401459855e-05,
      "loss": 0.6987,
      "step": 66730
    },
    {
      "epoch": 487.15328467153284,
      "grad_norm": 8.139726638793945,
      "learning_rate": 2.564233576642336e-05,
      "loss": 1.1517,
      "step": 66740
    },
    {
      "epoch": 487.22627737226276,
      "grad_norm": 9.793585777282715,
      "learning_rate": 2.5638686131386862e-05,
      "loss": 0.9874,
      "step": 66750
    },
    {
      "epoch": 487.2992700729927,
      "grad_norm": 18.780683517456055,
      "learning_rate": 2.5635036496350366e-05,
      "loss": 0.9235,
      "step": 66760
    },
    {
      "epoch": 487.37226277372264,
      "grad_norm": 0.027005573734641075,
      "learning_rate": 2.5631386861313874e-05,
      "loss": 0.8473,
      "step": 66770
    },
    {
      "epoch": 487.44525547445255,
      "grad_norm": 6.459146022796631,
      "learning_rate": 2.562773722627737e-05,
      "loss": 0.7944,
      "step": 66780
    },
    {
      "epoch": 487.51824817518246,
      "grad_norm": 10.11041259765625,
      "learning_rate": 2.5624087591240875e-05,
      "loss": 1.0836,
      "step": 66790
    },
    {
      "epoch": 487.59124087591243,
      "grad_norm": 6.262961387634277,
      "learning_rate": 2.562043795620438e-05,
      "loss": 1.0137,
      "step": 66800
    },
    {
      "epoch": 487.66423357664235,
      "grad_norm": 10.109207153320312,
      "learning_rate": 2.5616788321167882e-05,
      "loss": 0.8468,
      "step": 66810
    },
    {
      "epoch": 487.73722627737226,
      "grad_norm": 11.096696853637695,
      "learning_rate": 2.561313868613139e-05,
      "loss": 0.7467,
      "step": 66820
    },
    {
      "epoch": 487.81021897810217,
      "grad_norm": 7.58758544921875,
      "learning_rate": 2.5609489051094893e-05,
      "loss": 1.1564,
      "step": 66830
    },
    {
      "epoch": 487.88321167883214,
      "grad_norm": 5.801240921020508,
      "learning_rate": 2.5605839416058397e-05,
      "loss": 0.8392,
      "step": 66840
    },
    {
      "epoch": 487.95620437956205,
      "grad_norm": 11.436201095581055,
      "learning_rate": 2.56021897810219e-05,
      "loss": 1.2349,
      "step": 66850
    },
    {
      "epoch": 488.02919708029196,
      "grad_norm": 8.100314140319824,
      "learning_rate": 2.5598540145985402e-05,
      "loss": 0.6782,
      "step": 66860
    },
    {
      "epoch": 488.1021897810219,
      "grad_norm": 6.6193528175354,
      "learning_rate": 2.5594890510948906e-05,
      "loss": 1.1665,
      "step": 66870
    },
    {
      "epoch": 488.17518248175185,
      "grad_norm": 0.09800666570663452,
      "learning_rate": 2.559124087591241e-05,
      "loss": 0.7346,
      "step": 66880
    },
    {
      "epoch": 488.24817518248176,
      "grad_norm": 12.748359680175781,
      "learning_rate": 2.5587591240875913e-05,
      "loss": 0.832,
      "step": 66890
    },
    {
      "epoch": 488.3211678832117,
      "grad_norm": 16.062034606933594,
      "learning_rate": 2.5583941605839417e-05,
      "loss": 0.7641,
      "step": 66900
    },
    {
      "epoch": 488.3941605839416,
      "grad_norm": 7.707645893096924,
      "learning_rate": 2.558029197080292e-05,
      "loss": 1.1462,
      "step": 66910
    },
    {
      "epoch": 488.46715328467155,
      "grad_norm": 0.038160670548677444,
      "learning_rate": 2.5576642335766425e-05,
      "loss": 0.8405,
      "step": 66920
    },
    {
      "epoch": 488.54014598540147,
      "grad_norm": 0.09158143401145935,
      "learning_rate": 2.5572992700729932e-05,
      "loss": 0.9912,
      "step": 66930
    },
    {
      "epoch": 488.6131386861314,
      "grad_norm": 22.10296058654785,
      "learning_rate": 2.556934306569343e-05,
      "loss": 0.8907,
      "step": 66940
    },
    {
      "epoch": 488.6861313868613,
      "grad_norm": 0.23732958734035492,
      "learning_rate": 2.5565693430656933e-05,
      "loss": 0.7952,
      "step": 66950
    },
    {
      "epoch": 488.75912408759126,
      "grad_norm": 6.473405361175537,
      "learning_rate": 2.5562043795620437e-05,
      "loss": 0.8365,
      "step": 66960
    },
    {
      "epoch": 488.8321167883212,
      "grad_norm": 12.397848129272461,
      "learning_rate": 2.555839416058394e-05,
      "loss": 0.8231,
      "step": 66970
    },
    {
      "epoch": 488.9051094890511,
      "grad_norm": 0.12731312215328217,
      "learning_rate": 2.555474452554745e-05,
      "loss": 0.6988,
      "step": 66980
    },
    {
      "epoch": 488.978102189781,
      "grad_norm": 15.123305320739746,
      "learning_rate": 2.5551094890510952e-05,
      "loss": 1.3493,
      "step": 66990
    },
    {
      "epoch": 489.05109489051097,
      "grad_norm": 13.397665023803711,
      "learning_rate": 2.5547445255474456e-05,
      "loss": 1.1629,
      "step": 67000
    },
    {
      "epoch": 489.1240875912409,
      "grad_norm": 11.483417510986328,
      "learning_rate": 2.5543795620437953e-05,
      "loss": 1.1969,
      "step": 67010
    },
    {
      "epoch": 489.1970802919708,
      "grad_norm": 14.718151092529297,
      "learning_rate": 2.554014598540146e-05,
      "loss": 0.8293,
      "step": 67020
    },
    {
      "epoch": 489.2700729927007,
      "grad_norm": 15.882302284240723,
      "learning_rate": 2.5536496350364964e-05,
      "loss": 1.4541,
      "step": 67030
    },
    {
      "epoch": 489.3430656934307,
      "grad_norm": 8.145829200744629,
      "learning_rate": 2.5532846715328468e-05,
      "loss": 0.7578,
      "step": 67040
    },
    {
      "epoch": 489.4160583941606,
      "grad_norm": 13.603337287902832,
      "learning_rate": 2.5529197080291972e-05,
      "loss": 0.8623,
      "step": 67050
    },
    {
      "epoch": 489.4890510948905,
      "grad_norm": 11.898037910461426,
      "learning_rate": 2.5525547445255476e-05,
      "loss": 0.9898,
      "step": 67060
    },
    {
      "epoch": 489.5620437956204,
      "grad_norm": 9.833247184753418,
      "learning_rate": 2.552189781021898e-05,
      "loss": 0.8364,
      "step": 67070
    },
    {
      "epoch": 489.6350364963504,
      "grad_norm": 9.086423873901367,
      "learning_rate": 2.5518248175182484e-05,
      "loss": 0.7916,
      "step": 67080
    },
    {
      "epoch": 489.7080291970803,
      "grad_norm": 7.860806941986084,
      "learning_rate": 2.5514598540145984e-05,
      "loss": 1.1707,
      "step": 67090
    },
    {
      "epoch": 489.7810218978102,
      "grad_norm": 4.130012512207031,
      "learning_rate": 2.5510948905109488e-05,
      "loss": 0.6865,
      "step": 67100
    },
    {
      "epoch": 489.8540145985401,
      "grad_norm": 4.686548233032227,
      "learning_rate": 2.5507299270072992e-05,
      "loss": 0.5876,
      "step": 67110
    },
    {
      "epoch": 489.9270072992701,
      "grad_norm": 6.080687522888184,
      "learning_rate": 2.5503649635036496e-05,
      "loss": 0.9203,
      "step": 67120
    },
    {
      "epoch": 490.0,
      "grad_norm": 0.28122904896736145,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.8177,
      "step": 67130
    },
    {
      "epoch": 490.0729927007299,
      "grad_norm": 6.826834678649902,
      "learning_rate": 2.5496350364963507e-05,
      "loss": 0.849,
      "step": 67140
    },
    {
      "epoch": 490.1459854014599,
      "grad_norm": 0.06752607226371765,
      "learning_rate": 2.549270072992701e-05,
      "loss": 0.7195,
      "step": 67150
    },
    {
      "epoch": 490.2189781021898,
      "grad_norm": 0.08605031669139862,
      "learning_rate": 2.5489051094890515e-05,
      "loss": 1.0116,
      "step": 67160
    },
    {
      "epoch": 490.2919708029197,
      "grad_norm": 8.046331405639648,
      "learning_rate": 2.5485401459854012e-05,
      "loss": 1.14,
      "step": 67170
    },
    {
      "epoch": 490.3649635036496,
      "grad_norm": 15.168383598327637,
      "learning_rate": 2.548175182481752e-05,
      "loss": 1.0832,
      "step": 67180
    },
    {
      "epoch": 490.4379562043796,
      "grad_norm": 15.585601806640625,
      "learning_rate": 2.5478102189781023e-05,
      "loss": 1.0223,
      "step": 67190
    },
    {
      "epoch": 490.5109489051095,
      "grad_norm": 9.445981979370117,
      "learning_rate": 2.5474452554744527e-05,
      "loss": 0.576,
      "step": 67200
    },
    {
      "epoch": 490.5839416058394,
      "grad_norm": 3.8511099815368652,
      "learning_rate": 2.547080291970803e-05,
      "loss": 0.7471,
      "step": 67210
    },
    {
      "epoch": 490.6569343065693,
      "grad_norm": 10.398355484008789,
      "learning_rate": 2.5467153284671535e-05,
      "loss": 1.0888,
      "step": 67220
    },
    {
      "epoch": 490.7299270072993,
      "grad_norm": 10.599076271057129,
      "learning_rate": 2.546350364963504e-05,
      "loss": 0.9201,
      "step": 67230
    },
    {
      "epoch": 490.8029197080292,
      "grad_norm": 10.634501457214355,
      "learning_rate": 2.545985401459854e-05,
      "loss": 0.9428,
      "step": 67240
    },
    {
      "epoch": 490.8759124087591,
      "grad_norm": 7.521707057952881,
      "learning_rate": 2.5456204379562043e-05,
      "loss": 1.1262,
      "step": 67250
    },
    {
      "epoch": 490.94890510948903,
      "grad_norm": 0.029027681797742844,
      "learning_rate": 2.5452554744525547e-05,
      "loss": 0.5448,
      "step": 67260
    },
    {
      "epoch": 491.021897810219,
      "grad_norm": 5.379491806030273,
      "learning_rate": 2.544890510948905e-05,
      "loss": 1.1359,
      "step": 67270
    },
    {
      "epoch": 491.0948905109489,
      "grad_norm": 12.631389617919922,
      "learning_rate": 2.5445255474452555e-05,
      "loss": 0.9841,
      "step": 67280
    },
    {
      "epoch": 491.1678832116788,
      "grad_norm": 21.023866653442383,
      "learning_rate": 2.5441605839416062e-05,
      "loss": 1.3006,
      "step": 67290
    },
    {
      "epoch": 491.24087591240874,
      "grad_norm": 12.663673400878906,
      "learning_rate": 2.5437956204379566e-05,
      "loss": 0.9332,
      "step": 67300
    },
    {
      "epoch": 491.3138686131387,
      "grad_norm": 9.472647666931152,
      "learning_rate": 2.543430656934307e-05,
      "loss": 1.3258,
      "step": 67310
    },
    {
      "epoch": 491.3868613138686,
      "grad_norm": 0.10310383141040802,
      "learning_rate": 2.5430656934306567e-05,
      "loss": 0.788,
      "step": 67320
    },
    {
      "epoch": 491.45985401459853,
      "grad_norm": 0.02354157716035843,
      "learning_rate": 2.5427007299270074e-05,
      "loss": 0.3239,
      "step": 67330
    },
    {
      "epoch": 491.53284671532845,
      "grad_norm": 11.097175598144531,
      "learning_rate": 2.5423357664233578e-05,
      "loss": 1.0219,
      "step": 67340
    },
    {
      "epoch": 491.6058394160584,
      "grad_norm": 7.886634826660156,
      "learning_rate": 2.5419708029197082e-05,
      "loss": 1.0632,
      "step": 67350
    },
    {
      "epoch": 491.6788321167883,
      "grad_norm": 9.845125198364258,
      "learning_rate": 2.5416058394160586e-05,
      "loss": 0.7172,
      "step": 67360
    },
    {
      "epoch": 491.75182481751824,
      "grad_norm": 9.16685962677002,
      "learning_rate": 2.541240875912409e-05,
      "loss": 0.862,
      "step": 67370
    },
    {
      "epoch": 491.82481751824815,
      "grad_norm": 5.373851776123047,
      "learning_rate": 2.5408759124087593e-05,
      "loss": 0.9832,
      "step": 67380
    },
    {
      "epoch": 491.8978102189781,
      "grad_norm": 0.057410482317209244,
      "learning_rate": 2.5405109489051097e-05,
      "loss": 0.7802,
      "step": 67390
    },
    {
      "epoch": 491.97080291970804,
      "grad_norm": 7.490756988525391,
      "learning_rate": 2.5401459854014598e-05,
      "loss": 0.9077,
      "step": 67400
    },
    {
      "epoch": 492.04379562043795,
      "grad_norm": 5.317544460296631,
      "learning_rate": 2.5397810218978102e-05,
      "loss": 0.6984,
      "step": 67410
    },
    {
      "epoch": 492.11678832116786,
      "grad_norm": 0.040295857936143875,
      "learning_rate": 2.5394160583941606e-05,
      "loss": 1.1889,
      "step": 67420
    },
    {
      "epoch": 492.18978102189783,
      "grad_norm": 13.7571382522583,
      "learning_rate": 2.539051094890511e-05,
      "loss": 1.0017,
      "step": 67430
    },
    {
      "epoch": 492.26277372262774,
      "grad_norm": 12.37373161315918,
      "learning_rate": 2.5386861313868617e-05,
      "loss": 0.8852,
      "step": 67440
    },
    {
      "epoch": 492.33576642335765,
      "grad_norm": 6.782925128936768,
      "learning_rate": 2.538321167883212e-05,
      "loss": 0.8816,
      "step": 67450
    },
    {
      "epoch": 492.40875912408757,
      "grad_norm": 13.512239456176758,
      "learning_rate": 2.5379562043795625e-05,
      "loss": 0.9068,
      "step": 67460
    },
    {
      "epoch": 492.48175182481754,
      "grad_norm": 6.9441304206848145,
      "learning_rate": 2.537591240875912e-05,
      "loss": 1.0018,
      "step": 67470
    },
    {
      "epoch": 492.55474452554745,
      "grad_norm": 10.258556365966797,
      "learning_rate": 2.5372262773722626e-05,
      "loss": 0.9132,
      "step": 67480
    },
    {
      "epoch": 492.62773722627736,
      "grad_norm": 0.0633179247379303,
      "learning_rate": 2.5368613138686133e-05,
      "loss": 0.6272,
      "step": 67490
    },
    {
      "epoch": 492.7007299270073,
      "grad_norm": 12.369730949401855,
      "learning_rate": 2.5364963503649637e-05,
      "loss": 0.9092,
      "step": 67500
    },
    {
      "epoch": 492.77372262773724,
      "grad_norm": 10.282880783081055,
      "learning_rate": 2.536131386861314e-05,
      "loss": 0.8813,
      "step": 67510
    },
    {
      "epoch": 492.84671532846716,
      "grad_norm": 13.201114654541016,
      "learning_rate": 2.5357664233576644e-05,
      "loss": 0.8612,
      "step": 67520
    },
    {
      "epoch": 492.91970802919707,
      "grad_norm": 0.07017727196216583,
      "learning_rate": 2.535401459854015e-05,
      "loss": 0.6335,
      "step": 67530
    },
    {
      "epoch": 492.992700729927,
      "grad_norm": 9.99183464050293,
      "learning_rate": 2.5350364963503652e-05,
      "loss": 1.1492,
      "step": 67540
    },
    {
      "epoch": 493.06569343065695,
      "grad_norm": 5.391963005065918,
      "learning_rate": 2.5346715328467153e-05,
      "loss": 0.8576,
      "step": 67550
    },
    {
      "epoch": 493.13868613138686,
      "grad_norm": 8.196388244628906,
      "learning_rate": 2.5343065693430657e-05,
      "loss": 0.9277,
      "step": 67560
    },
    {
      "epoch": 493.2116788321168,
      "grad_norm": 9.122604370117188,
      "learning_rate": 2.533941605839416e-05,
      "loss": 0.9096,
      "step": 67570
    },
    {
      "epoch": 493.2846715328467,
      "grad_norm": 6.289346694946289,
      "learning_rate": 2.5335766423357664e-05,
      "loss": 0.6419,
      "step": 67580
    },
    {
      "epoch": 493.35766423357666,
      "grad_norm": 10.1317138671875,
      "learning_rate": 2.5332116788321168e-05,
      "loss": 1.1519,
      "step": 67590
    },
    {
      "epoch": 493.43065693430657,
      "grad_norm": 14.77758502960205,
      "learning_rate": 2.5328467153284675e-05,
      "loss": 0.7294,
      "step": 67600
    },
    {
      "epoch": 493.5036496350365,
      "grad_norm": 9.433119773864746,
      "learning_rate": 2.532481751824818e-05,
      "loss": 0.7303,
      "step": 67610
    },
    {
      "epoch": 493.57664233576645,
      "grad_norm": 9.882879257202148,
      "learning_rate": 2.5321167883211683e-05,
      "loss": 0.5919,
      "step": 67620
    },
    {
      "epoch": 493.64963503649636,
      "grad_norm": 12.122843742370605,
      "learning_rate": 2.531751824817518e-05,
      "loss": 1.123,
      "step": 67630
    },
    {
      "epoch": 493.7226277372263,
      "grad_norm": 0.07495583593845367,
      "learning_rate": 2.5313868613138688e-05,
      "loss": 0.6934,
      "step": 67640
    },
    {
      "epoch": 493.7956204379562,
      "grad_norm": 14.064498901367188,
      "learning_rate": 2.531021897810219e-05,
      "loss": 0.922,
      "step": 67650
    },
    {
      "epoch": 493.86861313868616,
      "grad_norm": 8.248600959777832,
      "learning_rate": 2.5306569343065695e-05,
      "loss": 0.7585,
      "step": 67660
    },
    {
      "epoch": 493.94160583941607,
      "grad_norm": 11.47329044342041,
      "learning_rate": 2.53029197080292e-05,
      "loss": 1.4583,
      "step": 67670
    },
    {
      "epoch": 494.014598540146,
      "grad_norm": 2.4524970054626465,
      "learning_rate": 2.5299270072992703e-05,
      "loss": 0.8856,
      "step": 67680
    },
    {
      "epoch": 494.0875912408759,
      "grad_norm": 11.24264144897461,
      "learning_rate": 2.5295620437956207e-05,
      "loss": 1.0285,
      "step": 67690
    },
    {
      "epoch": 494.16058394160586,
      "grad_norm": 11.21537971496582,
      "learning_rate": 2.5291970802919708e-05,
      "loss": 1.0178,
      "step": 67700
    },
    {
      "epoch": 494.2335766423358,
      "grad_norm": 8.088912963867188,
      "learning_rate": 2.528832116788321e-05,
      "loss": 0.8148,
      "step": 67710
    },
    {
      "epoch": 494.3065693430657,
      "grad_norm": 8.462265968322754,
      "learning_rate": 2.5284671532846715e-05,
      "loss": 1.2746,
      "step": 67720
    },
    {
      "epoch": 494.3795620437956,
      "grad_norm": 8.613763809204102,
      "learning_rate": 2.528102189781022e-05,
      "loss": 0.7437,
      "step": 67730
    },
    {
      "epoch": 494.45255474452557,
      "grad_norm": 6.787864685058594,
      "learning_rate": 2.5277372262773723e-05,
      "loss": 0.9056,
      "step": 67740
    },
    {
      "epoch": 494.5255474452555,
      "grad_norm": 6.871419906616211,
      "learning_rate": 2.5273722627737227e-05,
      "loss": 1.0131,
      "step": 67750
    },
    {
      "epoch": 494.5985401459854,
      "grad_norm": 20.639360427856445,
      "learning_rate": 2.5270072992700734e-05,
      "loss": 1.0968,
      "step": 67760
    },
    {
      "epoch": 494.6715328467153,
      "grad_norm": 0.0794612318277359,
      "learning_rate": 2.5266423357664238e-05,
      "loss": 0.2872,
      "step": 67770
    },
    {
      "epoch": 494.7445255474453,
      "grad_norm": 5.329591274261475,
      "learning_rate": 2.5262773722627735e-05,
      "loss": 0.6983,
      "step": 67780
    },
    {
      "epoch": 494.8175182481752,
      "grad_norm": 8.80230712890625,
      "learning_rate": 2.525912408759124e-05,
      "loss": 1.0766,
      "step": 67790
    },
    {
      "epoch": 494.8905109489051,
      "grad_norm": 17.156105041503906,
      "learning_rate": 2.5255474452554746e-05,
      "loss": 0.7524,
      "step": 67800
    },
    {
      "epoch": 494.963503649635,
      "grad_norm": 16.60626983642578,
      "learning_rate": 2.525182481751825e-05,
      "loss": 1.0986,
      "step": 67810
    },
    {
      "epoch": 495.036496350365,
      "grad_norm": 11.39184284210205,
      "learning_rate": 2.5248175182481754e-05,
      "loss": 0.9685,
      "step": 67820
    },
    {
      "epoch": 495.1094890510949,
      "grad_norm": 7.687434196472168,
      "learning_rate": 2.5244525547445258e-05,
      "loss": 0.7442,
      "step": 67830
    },
    {
      "epoch": 495.1824817518248,
      "grad_norm": 12.186972618103027,
      "learning_rate": 2.5240875912408762e-05,
      "loss": 0.9346,
      "step": 67840
    },
    {
      "epoch": 495.2554744525547,
      "grad_norm": 10.234469413757324,
      "learning_rate": 2.5237226277372266e-05,
      "loss": 0.6471,
      "step": 67850
    },
    {
      "epoch": 495.3284671532847,
      "grad_norm": 4.315126419067383,
      "learning_rate": 2.5233576642335766e-05,
      "loss": 0.5551,
      "step": 67860
    },
    {
      "epoch": 495.4014598540146,
      "grad_norm": 1.6512291431427002,
      "learning_rate": 2.522992700729927e-05,
      "loss": 0.5141,
      "step": 67870
    },
    {
      "epoch": 495.4744525547445,
      "grad_norm": 9.875707626342773,
      "learning_rate": 2.5226277372262774e-05,
      "loss": 1.1624,
      "step": 67880
    },
    {
      "epoch": 495.54744525547443,
      "grad_norm": 16.651641845703125,
      "learning_rate": 2.5222627737226278e-05,
      "loss": 1.0333,
      "step": 67890
    },
    {
      "epoch": 495.6204379562044,
      "grad_norm": 5.212684631347656,
      "learning_rate": 2.5218978102189782e-05,
      "loss": 0.7331,
      "step": 67900
    },
    {
      "epoch": 495.6934306569343,
      "grad_norm": 14.698198318481445,
      "learning_rate": 2.521532846715329e-05,
      "loss": 0.9662,
      "step": 67910
    },
    {
      "epoch": 495.7664233576642,
      "grad_norm": 0.11956557631492615,
      "learning_rate": 2.5211678832116793e-05,
      "loss": 1.1375,
      "step": 67920
    },
    {
      "epoch": 495.83941605839414,
      "grad_norm": 16.946434020996094,
      "learning_rate": 2.520802919708029e-05,
      "loss": 1.2273,
      "step": 67930
    },
    {
      "epoch": 495.9124087591241,
      "grad_norm": 15.698206901550293,
      "learning_rate": 2.5204379562043794e-05,
      "loss": 0.9852,
      "step": 67940
    },
    {
      "epoch": 495.985401459854,
      "grad_norm": 10.629993438720703,
      "learning_rate": 2.5200729927007298e-05,
      "loss": 1.0169,
      "step": 67950
    },
    {
      "epoch": 496.05839416058393,
      "grad_norm": 10.270200729370117,
      "learning_rate": 2.5197080291970805e-05,
      "loss": 1.1215,
      "step": 67960
    },
    {
      "epoch": 496.13138686131384,
      "grad_norm": 14.44354248046875,
      "learning_rate": 2.519343065693431e-05,
      "loss": 1.0916,
      "step": 67970
    },
    {
      "epoch": 496.2043795620438,
      "grad_norm": 6.4694743156433105,
      "learning_rate": 2.5189781021897813e-05,
      "loss": 0.8479,
      "step": 67980
    },
    {
      "epoch": 496.2773722627737,
      "grad_norm": 0.14176908135414124,
      "learning_rate": 2.5186131386861317e-05,
      "loss": 0.6665,
      "step": 67990
    },
    {
      "epoch": 496.35036496350364,
      "grad_norm": 12.948336601257324,
      "learning_rate": 2.518248175182482e-05,
      "loss": 0.9474,
      "step": 68000
    },
    {
      "epoch": 496.42335766423355,
      "grad_norm": 6.099227428436279,
      "learning_rate": 2.517883211678832e-05,
      "loss": 0.5367,
      "step": 68010
    },
    {
      "epoch": 496.4963503649635,
      "grad_norm": 10.398514747619629,
      "learning_rate": 2.5175182481751825e-05,
      "loss": 1.2772,
      "step": 68020
    },
    {
      "epoch": 496.56934306569343,
      "grad_norm": 0.04389219358563423,
      "learning_rate": 2.517153284671533e-05,
      "loss": 0.9783,
      "step": 68030
    },
    {
      "epoch": 496.64233576642334,
      "grad_norm": 11.523964881896973,
      "learning_rate": 2.5167883211678833e-05,
      "loss": 1.0213,
      "step": 68040
    },
    {
      "epoch": 496.7153284671533,
      "grad_norm": 0.72724848985672,
      "learning_rate": 2.5164233576642337e-05,
      "loss": 1.213,
      "step": 68050
    },
    {
      "epoch": 496.7883211678832,
      "grad_norm": 0.06361495703458786,
      "learning_rate": 2.516058394160584e-05,
      "loss": 0.7089,
      "step": 68060
    },
    {
      "epoch": 496.86131386861314,
      "grad_norm": 6.419031620025635,
      "learning_rate": 2.5156934306569348e-05,
      "loss": 0.7183,
      "step": 68070
    },
    {
      "epoch": 496.93430656934305,
      "grad_norm": 0.31575533747673035,
      "learning_rate": 2.5153284671532845e-05,
      "loss": 0.6935,
      "step": 68080
    },
    {
      "epoch": 497.007299270073,
      "grad_norm": 16.197237014770508,
      "learning_rate": 2.514963503649635e-05,
      "loss": 1.095,
      "step": 68090
    },
    {
      "epoch": 497.08029197080293,
      "grad_norm": 10.385757446289062,
      "learning_rate": 2.5145985401459853e-05,
      "loss": 0.773,
      "step": 68100
    },
    {
      "epoch": 497.15328467153284,
      "grad_norm": 19.196578979492188,
      "learning_rate": 2.514233576642336e-05,
      "loss": 1.0183,
      "step": 68110
    },
    {
      "epoch": 497.22627737226276,
      "grad_norm": 13.05142879486084,
      "learning_rate": 2.5138686131386864e-05,
      "loss": 1.1977,
      "step": 68120
    },
    {
      "epoch": 497.2992700729927,
      "grad_norm": 8.621049880981445,
      "learning_rate": 2.5135036496350368e-05,
      "loss": 0.8515,
      "step": 68130
    },
    {
      "epoch": 497.37226277372264,
      "grad_norm": 7.307743072509766,
      "learning_rate": 2.513138686131387e-05,
      "loss": 0.9026,
      "step": 68140
    },
    {
      "epoch": 497.44525547445255,
      "grad_norm": 0.7727141976356506,
      "learning_rate": 2.5127737226277375e-05,
      "loss": 1.0066,
      "step": 68150
    },
    {
      "epoch": 497.51824817518246,
      "grad_norm": 8.121976852416992,
      "learning_rate": 2.5124087591240876e-05,
      "loss": 0.6829,
      "step": 68160
    },
    {
      "epoch": 497.59124087591243,
      "grad_norm": 10.089985847473145,
      "learning_rate": 2.512043795620438e-05,
      "loss": 0.8081,
      "step": 68170
    },
    {
      "epoch": 497.66423357664235,
      "grad_norm": 4.75079345703125,
      "learning_rate": 2.5116788321167884e-05,
      "loss": 0.8383,
      "step": 68180
    },
    {
      "epoch": 497.73722627737226,
      "grad_norm": 0.050194550305604935,
      "learning_rate": 2.5113138686131388e-05,
      "loss": 1.04,
      "step": 68190
    },
    {
      "epoch": 497.81021897810217,
      "grad_norm": 14.195600509643555,
      "learning_rate": 2.510948905109489e-05,
      "loss": 0.6624,
      "step": 68200
    },
    {
      "epoch": 497.88321167883214,
      "grad_norm": 11.781347274780273,
      "learning_rate": 2.5105839416058395e-05,
      "loss": 1.0106,
      "step": 68210
    },
    {
      "epoch": 497.95620437956205,
      "grad_norm": 11.118523597717285,
      "learning_rate": 2.51021897810219e-05,
      "loss": 1.0575,
      "step": 68220
    },
    {
      "epoch": 498.02919708029196,
      "grad_norm": 20.21986961364746,
      "learning_rate": 2.5098540145985407e-05,
      "loss": 1.4129,
      "step": 68230
    },
    {
      "epoch": 498.1021897810219,
      "grad_norm": 8.226366996765137,
      "learning_rate": 2.5094890510948904e-05,
      "loss": 1.3917,
      "step": 68240
    },
    {
      "epoch": 498.17518248175185,
      "grad_norm": 13.626319885253906,
      "learning_rate": 2.5091240875912408e-05,
      "loss": 0.9763,
      "step": 68250
    },
    {
      "epoch": 498.24817518248176,
      "grad_norm": 8.718311309814453,
      "learning_rate": 2.508759124087591e-05,
      "loss": 0.9028,
      "step": 68260
    },
    {
      "epoch": 498.3211678832117,
      "grad_norm": 7.2963151931762695,
      "learning_rate": 2.508394160583942e-05,
      "loss": 0.5463,
      "step": 68270
    },
    {
      "epoch": 498.3941605839416,
      "grad_norm": 9.352435111999512,
      "learning_rate": 2.5080291970802923e-05,
      "loss": 1.0002,
      "step": 68280
    },
    {
      "epoch": 498.46715328467155,
      "grad_norm": 10.564273834228516,
      "learning_rate": 2.5076642335766426e-05,
      "loss": 0.7222,
      "step": 68290
    },
    {
      "epoch": 498.54014598540147,
      "grad_norm": 7.201923370361328,
      "learning_rate": 2.507299270072993e-05,
      "loss": 0.5633,
      "step": 68300
    },
    {
      "epoch": 498.6131386861314,
      "grad_norm": 22.379499435424805,
      "learning_rate": 2.506934306569343e-05,
      "loss": 0.9354,
      "step": 68310
    },
    {
      "epoch": 498.6861313868613,
      "grad_norm": 10.927698135375977,
      "learning_rate": 2.5065693430656935e-05,
      "loss": 1.1032,
      "step": 68320
    },
    {
      "epoch": 498.75912408759126,
      "grad_norm": 13.81941032409668,
      "learning_rate": 2.506204379562044e-05,
      "loss": 0.6966,
      "step": 68330
    },
    {
      "epoch": 498.8321167883212,
      "grad_norm": 11.11605167388916,
      "learning_rate": 2.5058394160583942e-05,
      "loss": 1.2813,
      "step": 68340
    },
    {
      "epoch": 498.9051094890511,
      "grad_norm": 11.468692779541016,
      "learning_rate": 2.5054744525547446e-05,
      "loss": 0.5361,
      "step": 68350
    },
    {
      "epoch": 498.978102189781,
      "grad_norm": 19.26694107055664,
      "learning_rate": 2.505109489051095e-05,
      "loss": 0.9664,
      "step": 68360
    },
    {
      "epoch": 499.05109489051097,
      "grad_norm": 11.12136173248291,
      "learning_rate": 2.5047445255474454e-05,
      "loss": 1.0974,
      "step": 68370
    },
    {
      "epoch": 499.1240875912409,
      "grad_norm": 0.16902263462543488,
      "learning_rate": 2.504379562043796e-05,
      "loss": 0.6075,
      "step": 68380
    },
    {
      "epoch": 499.1970802919708,
      "grad_norm": 7.161880970001221,
      "learning_rate": 2.504014598540146e-05,
      "loss": 0.8593,
      "step": 68390
    },
    {
      "epoch": 499.2700729927007,
      "grad_norm": 0.1949111968278885,
      "learning_rate": 2.5036496350364962e-05,
      "loss": 0.9191,
      "step": 68400
    },
    {
      "epoch": 499.3430656934307,
      "grad_norm": 8.76457405090332,
      "learning_rate": 2.5032846715328466e-05,
      "loss": 0.8506,
      "step": 68410
    },
    {
      "epoch": 499.4160583941606,
      "grad_norm": 12.432021141052246,
      "learning_rate": 2.502919708029197e-05,
      "loss": 0.8173,
      "step": 68420
    },
    {
      "epoch": 499.4890510948905,
      "grad_norm": 9.082869529724121,
      "learning_rate": 2.5025547445255477e-05,
      "loss": 0.8164,
      "step": 68430
    },
    {
      "epoch": 499.5620437956204,
      "grad_norm": 9.282404899597168,
      "learning_rate": 2.502189781021898e-05,
      "loss": 0.7906,
      "step": 68440
    },
    {
      "epoch": 499.6350364963504,
      "grad_norm": 7.729634761810303,
      "learning_rate": 2.5018248175182485e-05,
      "loss": 0.825,
      "step": 68450
    },
    {
      "epoch": 499.7080291970803,
      "grad_norm": 12.271307945251465,
      "learning_rate": 2.501459854014599e-05,
      "loss": 0.9297,
      "step": 68460
    },
    {
      "epoch": 499.7810218978102,
      "grad_norm": 9.77152156829834,
      "learning_rate": 2.501094890510949e-05,
      "loss": 0.9462,
      "step": 68470
    },
    {
      "epoch": 499.8540145985401,
      "grad_norm": 8.633732795715332,
      "learning_rate": 2.5007299270072993e-05,
      "loss": 0.8274,
      "step": 68480
    },
    {
      "epoch": 499.9270072992701,
      "grad_norm": 17.932823181152344,
      "learning_rate": 2.5003649635036497e-05,
      "loss": 1.2932,
      "step": 68490
    },
    {
      "epoch": 500.0,
      "grad_norm": 21.71651840209961,
      "learning_rate": 2.5e-05,
      "loss": 0.8152,
      "step": 68500
    },
    {
      "epoch": 500.0729927007299,
      "grad_norm": 15.169844627380371,
      "learning_rate": 2.4996350364963505e-05,
      "loss": 1.3404,
      "step": 68510
    },
    {
      "epoch": 500.1459854014599,
      "grad_norm": 5.043963432312012,
      "learning_rate": 2.4992700729927006e-05,
      "loss": 0.7006,
      "step": 68520
    },
    {
      "epoch": 500.2189781021898,
      "grad_norm": 6.895866870880127,
      "learning_rate": 2.4989051094890513e-05,
      "loss": 0.6038,
      "step": 68530
    },
    {
      "epoch": 500.2919708029197,
      "grad_norm": 9.511248588562012,
      "learning_rate": 2.4985401459854017e-05,
      "loss": 0.981,
      "step": 68540
    },
    {
      "epoch": 500.3649635036496,
      "grad_norm": 11.555006980895996,
      "learning_rate": 2.498175182481752e-05,
      "loss": 0.6769,
      "step": 68550
    },
    {
      "epoch": 500.4379562043796,
      "grad_norm": 12.706461906433105,
      "learning_rate": 2.497810218978102e-05,
      "loss": 0.9103,
      "step": 68560
    },
    {
      "epoch": 500.5109489051095,
      "grad_norm": 11.428140640258789,
      "learning_rate": 2.4974452554744525e-05,
      "loss": 0.9623,
      "step": 68570
    },
    {
      "epoch": 500.5839416058394,
      "grad_norm": 0.09899299591779709,
      "learning_rate": 2.4970802919708032e-05,
      "loss": 0.6512,
      "step": 68580
    },
    {
      "epoch": 500.6569343065693,
      "grad_norm": 15.6007080078125,
      "learning_rate": 2.4967153284671536e-05,
      "loss": 1.0657,
      "step": 68590
    },
    {
      "epoch": 500.7299270072993,
      "grad_norm": 10.071786880493164,
      "learning_rate": 2.4963503649635037e-05,
      "loss": 1.1511,
      "step": 68600
    },
    {
      "epoch": 500.8029197080292,
      "grad_norm": 11.84583854675293,
      "learning_rate": 2.495985401459854e-05,
      "loss": 0.9449,
      "step": 68610
    },
    {
      "epoch": 500.8759124087591,
      "grad_norm": 15.679088592529297,
      "learning_rate": 2.4956204379562044e-05,
      "loss": 1.0056,
      "step": 68620
    },
    {
      "epoch": 500.94890510948903,
      "grad_norm": 0.5678260922431946,
      "learning_rate": 2.4952554744525548e-05,
      "loss": 0.8161,
      "step": 68630
    },
    {
      "epoch": 501.021897810219,
      "grad_norm": 14.78829574584961,
      "learning_rate": 2.4948905109489052e-05,
      "loss": 1.0331,
      "step": 68640
    },
    {
      "epoch": 501.0948905109489,
      "grad_norm": 21.151199340820312,
      "learning_rate": 2.4945255474452556e-05,
      "loss": 0.8655,
      "step": 68650
    },
    {
      "epoch": 501.1678832116788,
      "grad_norm": 21.964345932006836,
      "learning_rate": 2.494160583941606e-05,
      "loss": 1.0392,
      "step": 68660
    },
    {
      "epoch": 501.24087591240874,
      "grad_norm": 11.293264389038086,
      "learning_rate": 2.4937956204379564e-05,
      "loss": 0.825,
      "step": 68670
    },
    {
      "epoch": 501.3138686131387,
      "grad_norm": 14.419964790344238,
      "learning_rate": 2.4934306569343068e-05,
      "loss": 1.0404,
      "step": 68680
    },
    {
      "epoch": 501.3868613138686,
      "grad_norm": 10.285582542419434,
      "learning_rate": 2.493065693430657e-05,
      "loss": 0.9013,
      "step": 68690
    },
    {
      "epoch": 501.45985401459853,
      "grad_norm": 11.111397743225098,
      "learning_rate": 2.4927007299270075e-05,
      "loss": 0.8066,
      "step": 68700
    },
    {
      "epoch": 501.53284671532845,
      "grad_norm": 7.715066909790039,
      "learning_rate": 2.492335766423358e-05,
      "loss": 0.8002,
      "step": 68710
    },
    {
      "epoch": 501.6058394160584,
      "grad_norm": 9.435546875,
      "learning_rate": 2.491970802919708e-05,
      "loss": 0.8974,
      "step": 68720
    },
    {
      "epoch": 501.6788321167883,
      "grad_norm": 11.576292037963867,
      "learning_rate": 2.4916058394160584e-05,
      "loss": 1.0184,
      "step": 68730
    },
    {
      "epoch": 501.75182481751824,
      "grad_norm": 8.616743087768555,
      "learning_rate": 2.491240875912409e-05,
      "loss": 0.9222,
      "step": 68740
    },
    {
      "epoch": 501.82481751824815,
      "grad_norm": 12.175829887390137,
      "learning_rate": 2.490875912408759e-05,
      "loss": 0.9104,
      "step": 68750
    },
    {
      "epoch": 501.8978102189781,
      "grad_norm": 6.895362377166748,
      "learning_rate": 2.4905109489051095e-05,
      "loss": 0.8085,
      "step": 68760
    },
    {
      "epoch": 501.97080291970804,
      "grad_norm": 8.211466789245605,
      "learning_rate": 2.49014598540146e-05,
      "loss": 0.9198,
      "step": 68770
    },
    {
      "epoch": 502.04379562043795,
      "grad_norm": 13.125489234924316,
      "learning_rate": 2.4897810218978103e-05,
      "loss": 1.0676,
      "step": 68780
    },
    {
      "epoch": 502.11678832116786,
      "grad_norm": 0.08807205408811569,
      "learning_rate": 2.4894160583941607e-05,
      "loss": 0.6345,
      "step": 68790
    },
    {
      "epoch": 502.18978102189783,
      "grad_norm": 0.05380076915025711,
      "learning_rate": 2.489051094890511e-05,
      "loss": 0.4404,
      "step": 68800
    },
    {
      "epoch": 502.26277372262774,
      "grad_norm": 7.503316402435303,
      "learning_rate": 2.4886861313868615e-05,
      "loss": 1.2776,
      "step": 68810
    },
    {
      "epoch": 502.33576642335765,
      "grad_norm": 0.10942559689283371,
      "learning_rate": 2.488321167883212e-05,
      "loss": 1.0592,
      "step": 68820
    },
    {
      "epoch": 502.40875912408757,
      "grad_norm": 7.37326192855835,
      "learning_rate": 2.487956204379562e-05,
      "loss": 0.9283,
      "step": 68830
    },
    {
      "epoch": 502.48175182481754,
      "grad_norm": 7.376672267913818,
      "learning_rate": 2.4875912408759126e-05,
      "loss": 0.9712,
      "step": 68840
    },
    {
      "epoch": 502.55474452554745,
      "grad_norm": 0.2409416139125824,
      "learning_rate": 2.487226277372263e-05,
      "loss": 0.8095,
      "step": 68850
    },
    {
      "epoch": 502.62773722627736,
      "grad_norm": 16.709318161010742,
      "learning_rate": 2.4868613138686134e-05,
      "loss": 1.0442,
      "step": 68860
    },
    {
      "epoch": 502.7007299270073,
      "grad_norm": 6.784618377685547,
      "learning_rate": 2.4864963503649635e-05,
      "loss": 0.711,
      "step": 68870
    },
    {
      "epoch": 502.77372262773724,
      "grad_norm": 4.7045111656188965,
      "learning_rate": 2.486131386861314e-05,
      "loss": 0.8415,
      "step": 68880
    },
    {
      "epoch": 502.84671532846716,
      "grad_norm": 1.5166332721710205,
      "learning_rate": 2.4857664233576642e-05,
      "loss": 1.0534,
      "step": 68890
    },
    {
      "epoch": 502.91970802919707,
      "grad_norm": 8.994124412536621,
      "learning_rate": 2.485401459854015e-05,
      "loss": 0.9742,
      "step": 68900
    },
    {
      "epoch": 502.992700729927,
      "grad_norm": 12.072933197021484,
      "learning_rate": 2.485036496350365e-05,
      "loss": 1.158,
      "step": 68910
    },
    {
      "epoch": 503.06569343065695,
      "grad_norm": 0.09180814772844315,
      "learning_rate": 2.4846715328467154e-05,
      "loss": 0.86,
      "step": 68920
    },
    {
      "epoch": 503.13868613138686,
      "grad_norm": 7.669382095336914,
      "learning_rate": 2.4843065693430658e-05,
      "loss": 0.7832,
      "step": 68930
    },
    {
      "epoch": 503.2116788321168,
      "grad_norm": 11.28527545928955,
      "learning_rate": 2.4839416058394162e-05,
      "loss": 1.0014,
      "step": 68940
    },
    {
      "epoch": 503.2846715328467,
      "grad_norm": 19.253976821899414,
      "learning_rate": 2.4835766423357666e-05,
      "loss": 0.8653,
      "step": 68950
    },
    {
      "epoch": 503.35766423357666,
      "grad_norm": 6.239914417266846,
      "learning_rate": 2.483211678832117e-05,
      "loss": 0.8483,
      "step": 68960
    },
    {
      "epoch": 503.43065693430657,
      "grad_norm": 11.883609771728516,
      "learning_rate": 2.4828467153284674e-05,
      "loss": 0.8718,
      "step": 68970
    },
    {
      "epoch": 503.5036496350365,
      "grad_norm": 8.074337005615234,
      "learning_rate": 2.4824817518248174e-05,
      "loss": 0.7032,
      "step": 68980
    },
    {
      "epoch": 503.57664233576645,
      "grad_norm": 8.515974998474121,
      "learning_rate": 2.4821167883211678e-05,
      "loss": 0.9447,
      "step": 68990
    },
    {
      "epoch": 503.64963503649636,
      "grad_norm": 15.488200187683105,
      "learning_rate": 2.4817518248175185e-05,
      "loss": 1.2119,
      "step": 69000
    },
    {
      "epoch": 503.7226277372263,
      "grad_norm": 11.363275527954102,
      "learning_rate": 2.481386861313869e-05,
      "loss": 0.9375,
      "step": 69010
    },
    {
      "epoch": 503.7956204379562,
      "grad_norm": 13.463887214660645,
      "learning_rate": 2.481021897810219e-05,
      "loss": 1.1985,
      "step": 69020
    },
    {
      "epoch": 503.86861313868616,
      "grad_norm": 11.895268440246582,
      "learning_rate": 2.4806569343065693e-05,
      "loss": 0.8243,
      "step": 69030
    },
    {
      "epoch": 503.94160583941607,
      "grad_norm": 10.2195405960083,
      "learning_rate": 2.4802919708029197e-05,
      "loss": 0.587,
      "step": 69040
    },
    {
      "epoch": 504.014598540146,
      "grad_norm": 6.2914042472839355,
      "learning_rate": 2.4799270072992705e-05,
      "loss": 1.0408,
      "step": 69050
    },
    {
      "epoch": 504.0875912408759,
      "grad_norm": 12.666592597961426,
      "learning_rate": 2.4795620437956205e-05,
      "loss": 0.704,
      "step": 69060
    },
    {
      "epoch": 504.16058394160586,
      "grad_norm": 8.485166549682617,
      "learning_rate": 2.479197080291971e-05,
      "loss": 1.0324,
      "step": 69070
    },
    {
      "epoch": 504.2335766423358,
      "grad_norm": 0.056216444820165634,
      "learning_rate": 2.4788321167883213e-05,
      "loss": 0.888,
      "step": 69080
    },
    {
      "epoch": 504.3065693430657,
      "grad_norm": 11.397047996520996,
      "learning_rate": 2.4784671532846717e-05,
      "loss": 0.8532,
      "step": 69090
    },
    {
      "epoch": 504.3795620437956,
      "grad_norm": 6.281222820281982,
      "learning_rate": 2.478102189781022e-05,
      "loss": 1.0422,
      "step": 69100
    },
    {
      "epoch": 504.45255474452557,
      "grad_norm": 0.029086843132972717,
      "learning_rate": 2.4777372262773724e-05,
      "loss": 0.7396,
      "step": 69110
    },
    {
      "epoch": 504.5255474452555,
      "grad_norm": 14.57008171081543,
      "learning_rate": 2.477372262773723e-05,
      "loss": 1.0606,
      "step": 69120
    },
    {
      "epoch": 504.5985401459854,
      "grad_norm": 0.09992370754480362,
      "learning_rate": 2.4770072992700732e-05,
      "loss": 1.0257,
      "step": 69130
    },
    {
      "epoch": 504.6715328467153,
      "grad_norm": 7.340710163116455,
      "learning_rate": 2.4766423357664233e-05,
      "loss": 0.9682,
      "step": 69140
    },
    {
      "epoch": 504.7445255474453,
      "grad_norm": 16.743749618530273,
      "learning_rate": 2.476277372262774e-05,
      "loss": 0.6335,
      "step": 69150
    },
    {
      "epoch": 504.8175182481752,
      "grad_norm": 0.09526079148054123,
      "learning_rate": 2.4759124087591244e-05,
      "loss": 0.9727,
      "step": 69160
    },
    {
      "epoch": 504.8905109489051,
      "grad_norm": 13.11493968963623,
      "learning_rate": 2.4755474452554744e-05,
      "loss": 0.8328,
      "step": 69170
    },
    {
      "epoch": 504.963503649635,
      "grad_norm": 7.1941680908203125,
      "learning_rate": 2.4751824817518248e-05,
      "loss": 1.1314,
      "step": 69180
    },
    {
      "epoch": 505.036496350365,
      "grad_norm": 0.08018994331359863,
      "learning_rate": 2.4748175182481752e-05,
      "loss": 0.626,
      "step": 69190
    },
    {
      "epoch": 505.1094890510949,
      "grad_norm": 7.779767036437988,
      "learning_rate": 2.4744525547445256e-05,
      "loss": 0.9724,
      "step": 69200
    },
    {
      "epoch": 505.1824817518248,
      "grad_norm": 0.9302229881286621,
      "learning_rate": 2.474087591240876e-05,
      "loss": 0.6269,
      "step": 69210
    },
    {
      "epoch": 505.2554744525547,
      "grad_norm": 7.78871488571167,
      "learning_rate": 2.4737226277372264e-05,
      "loss": 1.0316,
      "step": 69220
    },
    {
      "epoch": 505.3284671532847,
      "grad_norm": 11.933472633361816,
      "learning_rate": 2.4733576642335768e-05,
      "loss": 1.0138,
      "step": 69230
    },
    {
      "epoch": 505.4014598540146,
      "grad_norm": 14.216681480407715,
      "learning_rate": 2.472992700729927e-05,
      "loss": 0.9726,
      "step": 69240
    },
    {
      "epoch": 505.4744525547445,
      "grad_norm": 15.000354766845703,
      "learning_rate": 2.4726277372262775e-05,
      "loss": 0.7452,
      "step": 69250
    },
    {
      "epoch": 505.54744525547443,
      "grad_norm": 0.10691098868846893,
      "learning_rate": 2.472262773722628e-05,
      "loss": 0.9321,
      "step": 69260
    },
    {
      "epoch": 505.6204379562044,
      "grad_norm": 10.43867301940918,
      "learning_rate": 2.4718978102189783e-05,
      "loss": 1.1245,
      "step": 69270
    },
    {
      "epoch": 505.6934306569343,
      "grad_norm": 8.709171295166016,
      "learning_rate": 2.4715328467153287e-05,
      "loss": 0.9439,
      "step": 69280
    },
    {
      "epoch": 505.7664233576642,
      "grad_norm": 11.596863746643066,
      "learning_rate": 2.4711678832116788e-05,
      "loss": 1.0189,
      "step": 69290
    },
    {
      "epoch": 505.83941605839414,
      "grad_norm": 17.139375686645508,
      "learning_rate": 2.470802919708029e-05,
      "loss": 1.2625,
      "step": 69300
    },
    {
      "epoch": 505.9124087591241,
      "grad_norm": 5.579989433288574,
      "learning_rate": 2.47043795620438e-05,
      "loss": 0.6776,
      "step": 69310
    },
    {
      "epoch": 505.985401459854,
      "grad_norm": 0.0768551453948021,
      "learning_rate": 2.4700729927007303e-05,
      "loss": 0.7932,
      "step": 69320
    },
    {
      "epoch": 506.05839416058393,
      "grad_norm": 18.502330780029297,
      "learning_rate": 2.4697080291970803e-05,
      "loss": 0.9076,
      "step": 69330
    },
    {
      "epoch": 506.13138686131384,
      "grad_norm": 15.386717796325684,
      "learning_rate": 2.4693430656934307e-05,
      "loss": 1.0555,
      "step": 69340
    },
    {
      "epoch": 506.2043795620438,
      "grad_norm": 6.650609970092773,
      "learning_rate": 2.468978102189781e-05,
      "loss": 0.6008,
      "step": 69350
    },
    {
      "epoch": 506.2773722627737,
      "grad_norm": 9.407853126525879,
      "learning_rate": 2.4686131386861315e-05,
      "loss": 0.881,
      "step": 69360
    },
    {
      "epoch": 506.35036496350364,
      "grad_norm": 19.352834701538086,
      "learning_rate": 2.468248175182482e-05,
      "loss": 1.3468,
      "step": 69370
    },
    {
      "epoch": 506.42335766423355,
      "grad_norm": 0.06384453177452087,
      "learning_rate": 2.4678832116788323e-05,
      "loss": 1.0306,
      "step": 69380
    },
    {
      "epoch": 506.4963503649635,
      "grad_norm": 6.762497901916504,
      "learning_rate": 2.4675182481751826e-05,
      "loss": 0.9392,
      "step": 69390
    },
    {
      "epoch": 506.56934306569343,
      "grad_norm": 1.7630209922790527,
      "learning_rate": 2.4671532846715327e-05,
      "loss": 1.0812,
      "step": 69400
    },
    {
      "epoch": 506.64233576642334,
      "grad_norm": 14.531561851501465,
      "learning_rate": 2.4667883211678834e-05,
      "loss": 0.4999,
      "step": 69410
    },
    {
      "epoch": 506.7153284671533,
      "grad_norm": 8.7378568649292,
      "learning_rate": 2.4664233576642338e-05,
      "loss": 0.6582,
      "step": 69420
    },
    {
      "epoch": 506.7883211678832,
      "grad_norm": 16.16023826599121,
      "learning_rate": 2.4660583941605842e-05,
      "loss": 0.9859,
      "step": 69430
    },
    {
      "epoch": 506.86131386861314,
      "grad_norm": 14.457514762878418,
      "learning_rate": 2.4656934306569342e-05,
      "loss": 0.5607,
      "step": 69440
    },
    {
      "epoch": 506.93430656934305,
      "grad_norm": 0.045903097838163376,
      "learning_rate": 2.4653284671532846e-05,
      "loss": 1.1854,
      "step": 69450
    },
    {
      "epoch": 507.007299270073,
      "grad_norm": 9.253572463989258,
      "learning_rate": 2.464963503649635e-05,
      "loss": 1.1693,
      "step": 69460
    },
    {
      "epoch": 507.08029197080293,
      "grad_norm": 14.776597023010254,
      "learning_rate": 2.4645985401459857e-05,
      "loss": 1.0678,
      "step": 69470
    },
    {
      "epoch": 507.15328467153284,
      "grad_norm": 7.488952159881592,
      "learning_rate": 2.4642335766423358e-05,
      "loss": 0.7309,
      "step": 69480
    },
    {
      "epoch": 507.22627737226276,
      "grad_norm": 11.334271430969238,
      "learning_rate": 2.4638686131386862e-05,
      "loss": 0.8822,
      "step": 69490
    },
    {
      "epoch": 507.2992700729927,
      "grad_norm": 10.995336532592773,
      "learning_rate": 2.4635036496350366e-05,
      "loss": 0.7267,
      "step": 69500
    },
    {
      "epoch": 507.37226277372264,
      "grad_norm": 5.897200584411621,
      "learning_rate": 2.463138686131387e-05,
      "loss": 0.8778,
      "step": 69510
    },
    {
      "epoch": 507.44525547445255,
      "grad_norm": 10.454538345336914,
      "learning_rate": 2.4627737226277373e-05,
      "loss": 1.1071,
      "step": 69520
    },
    {
      "epoch": 507.51824817518246,
      "grad_norm": 5.606446743011475,
      "learning_rate": 2.4624087591240877e-05,
      "loss": 0.7959,
      "step": 69530
    },
    {
      "epoch": 507.59124087591243,
      "grad_norm": 12.693068504333496,
      "learning_rate": 2.462043795620438e-05,
      "loss": 0.8584,
      "step": 69540
    },
    {
      "epoch": 507.66423357664235,
      "grad_norm": 14.928350448608398,
      "learning_rate": 2.4616788321167885e-05,
      "loss": 0.8112,
      "step": 69550
    },
    {
      "epoch": 507.73722627737226,
      "grad_norm": 18.702392578125,
      "learning_rate": 2.4613138686131386e-05,
      "loss": 1.1413,
      "step": 69560
    },
    {
      "epoch": 507.81021897810217,
      "grad_norm": 10.003793716430664,
      "learning_rate": 2.4609489051094893e-05,
      "loss": 1.0335,
      "step": 69570
    },
    {
      "epoch": 507.88321167883214,
      "grad_norm": 0.06834272295236588,
      "learning_rate": 2.4605839416058397e-05,
      "loss": 0.2859,
      "step": 69580
    },
    {
      "epoch": 507.95620437956205,
      "grad_norm": 0.22373181581497192,
      "learning_rate": 2.4602189781021897e-05,
      "loss": 0.8607,
      "step": 69590
    },
    {
      "epoch": 508.02919708029196,
      "grad_norm": 3.5260298252105713,
      "learning_rate": 2.45985401459854e-05,
      "loss": 1.3453,
      "step": 69600
    },
    {
      "epoch": 508.1021897810219,
      "grad_norm": 5.022700786590576,
      "learning_rate": 2.4594890510948905e-05,
      "loss": 0.543,
      "step": 69610
    },
    {
      "epoch": 508.17518248175185,
      "grad_norm": 13.717342376708984,
      "learning_rate": 2.4591240875912412e-05,
      "loss": 0.8764,
      "step": 69620
    },
    {
      "epoch": 508.24817518248176,
      "grad_norm": 11.917943954467773,
      "learning_rate": 2.4587591240875913e-05,
      "loss": 1.6163,
      "step": 69630
    },
    {
      "epoch": 508.3211678832117,
      "grad_norm": 11.875406265258789,
      "learning_rate": 2.4583941605839417e-05,
      "loss": 1.1922,
      "step": 69640
    },
    {
      "epoch": 508.3941605839416,
      "grad_norm": 11.713884353637695,
      "learning_rate": 2.458029197080292e-05,
      "loss": 1.002,
      "step": 69650
    },
    {
      "epoch": 508.46715328467155,
      "grad_norm": 16.931228637695312,
      "learning_rate": 2.4576642335766424e-05,
      "loss": 0.5293,
      "step": 69660
    },
    {
      "epoch": 508.54014598540147,
      "grad_norm": 14.556649208068848,
      "learning_rate": 2.457299270072993e-05,
      "loss": 1.1299,
      "step": 69670
    },
    {
      "epoch": 508.6131386861314,
      "grad_norm": 12.913277626037598,
      "learning_rate": 2.4569343065693432e-05,
      "loss": 0.6482,
      "step": 69680
    },
    {
      "epoch": 508.6861313868613,
      "grad_norm": 2.940506935119629,
      "learning_rate": 2.4565693430656936e-05,
      "loss": 1.0823,
      "step": 69690
    },
    {
      "epoch": 508.75912408759126,
      "grad_norm": 5.373236179351807,
      "learning_rate": 2.456204379562044e-05,
      "loss": 0.6906,
      "step": 69700
    },
    {
      "epoch": 508.8321167883212,
      "grad_norm": 0.06431825459003448,
      "learning_rate": 2.455839416058394e-05,
      "loss": 1.0237,
      "step": 69710
    },
    {
      "epoch": 508.9051094890511,
      "grad_norm": 10.26545524597168,
      "learning_rate": 2.4554744525547448e-05,
      "loss": 0.7277,
      "step": 69720
    },
    {
      "epoch": 508.978102189781,
      "grad_norm": 9.728145599365234,
      "learning_rate": 2.455109489051095e-05,
      "loss": 0.8771,
      "step": 69730
    },
    {
      "epoch": 509.05109489051097,
      "grad_norm": 15.205607414245605,
      "learning_rate": 2.4547445255474456e-05,
      "loss": 0.8297,
      "step": 69740
    },
    {
      "epoch": 509.1240875912409,
      "grad_norm": 0.45889243483543396,
      "learning_rate": 2.4543795620437956e-05,
      "loss": 0.8369,
      "step": 69750
    },
    {
      "epoch": 509.1970802919708,
      "grad_norm": 0.09930960834026337,
      "learning_rate": 2.454014598540146e-05,
      "loss": 0.9405,
      "step": 69760
    },
    {
      "epoch": 509.2700729927007,
      "grad_norm": 9.499656677246094,
      "learning_rate": 2.4536496350364964e-05,
      "loss": 0.695,
      "step": 69770
    },
    {
      "epoch": 509.3430656934307,
      "grad_norm": 7.9795660972595215,
      "learning_rate": 2.453284671532847e-05,
      "loss": 1.158,
      "step": 69780
    },
    {
      "epoch": 509.4160583941606,
      "grad_norm": 7.950122833251953,
      "learning_rate": 2.452919708029197e-05,
      "loss": 0.7356,
      "step": 69790
    },
    {
      "epoch": 509.4890510948905,
      "grad_norm": 8.12502670288086,
      "learning_rate": 2.4525547445255475e-05,
      "loss": 0.966,
      "step": 69800
    },
    {
      "epoch": 509.5620437956204,
      "grad_norm": 9.955765724182129,
      "learning_rate": 2.452189781021898e-05,
      "loss": 1.0819,
      "step": 69810
    },
    {
      "epoch": 509.6350364963504,
      "grad_norm": 13.707884788513184,
      "learning_rate": 2.4518248175182483e-05,
      "loss": 0.6851,
      "step": 69820
    },
    {
      "epoch": 509.7080291970803,
      "grad_norm": 0.04538393393158913,
      "learning_rate": 2.4514598540145987e-05,
      "loss": 0.8675,
      "step": 69830
    },
    {
      "epoch": 509.7810218978102,
      "grad_norm": 0.030316343531012535,
      "learning_rate": 2.451094890510949e-05,
      "loss": 1.0449,
      "step": 69840
    },
    {
      "epoch": 509.8540145985401,
      "grad_norm": 11.595551490783691,
      "learning_rate": 2.4507299270072995e-05,
      "loss": 0.7958,
      "step": 69850
    },
    {
      "epoch": 509.9270072992701,
      "grad_norm": 6.104777812957764,
      "learning_rate": 2.4503649635036495e-05,
      "loss": 1.0207,
      "step": 69860
    },
    {
      "epoch": 510.0,
      "grad_norm": 31.038768768310547,
      "learning_rate": 2.45e-05,
      "loss": 0.9867,
      "step": 69870
    },
    {
      "epoch": 510.0729927007299,
      "grad_norm": 0.9885632395744324,
      "learning_rate": 2.4496350364963506e-05,
      "loss": 0.6969,
      "step": 69880
    },
    {
      "epoch": 510.1459854014599,
      "grad_norm": 10.384576797485352,
      "learning_rate": 2.449270072992701e-05,
      "loss": 1.1074,
      "step": 69890
    },
    {
      "epoch": 510.2189781021898,
      "grad_norm": 11.41348648071289,
      "learning_rate": 2.448905109489051e-05,
      "loss": 0.9618,
      "step": 69900
    },
    {
      "epoch": 510.2919708029197,
      "grad_norm": 0.08438784629106522,
      "learning_rate": 2.4485401459854015e-05,
      "loss": 0.8832,
      "step": 69910
    },
    {
      "epoch": 510.3649635036496,
      "grad_norm": 7.298375129699707,
      "learning_rate": 2.448175182481752e-05,
      "loss": 0.636,
      "step": 69920
    },
    {
      "epoch": 510.4379562043796,
      "grad_norm": 17.768796920776367,
      "learning_rate": 2.4478102189781026e-05,
      "loss": 1.1085,
      "step": 69930
    },
    {
      "epoch": 510.5109489051095,
      "grad_norm": 9.586926460266113,
      "learning_rate": 2.4474452554744526e-05,
      "loss": 0.8117,
      "step": 69940
    },
    {
      "epoch": 510.5839416058394,
      "grad_norm": 0.3526378870010376,
      "learning_rate": 2.447080291970803e-05,
      "loss": 0.6339,
      "step": 69950
    },
    {
      "epoch": 510.6569343065693,
      "grad_norm": 7.081684589385986,
      "learning_rate": 2.4467153284671534e-05,
      "loss": 1.0223,
      "step": 69960
    },
    {
      "epoch": 510.7299270072993,
      "grad_norm": 3.525949239730835,
      "learning_rate": 2.4463503649635038e-05,
      "loss": 1.007,
      "step": 69970
    },
    {
      "epoch": 510.8029197080292,
      "grad_norm": 7.987232208251953,
      "learning_rate": 2.4459854014598542e-05,
      "loss": 1.1443,
      "step": 69980
    },
    {
      "epoch": 510.8759124087591,
      "grad_norm": 16.427427291870117,
      "learning_rate": 2.4456204379562046e-05,
      "loss": 0.9982,
      "step": 69990
    },
    {
      "epoch": 510.94890510948903,
      "grad_norm": 10.576253890991211,
      "learning_rate": 2.445255474452555e-05,
      "loss": 0.5948,
      "step": 70000
    },
    {
      "epoch": 511.021897810219,
      "grad_norm": 2.9547550678253174,
      "learning_rate": 2.444890510948905e-05,
      "loss": 0.6351,
      "step": 70010
    },
    {
      "epoch": 511.0948905109489,
      "grad_norm": 19.5159854888916,
      "learning_rate": 2.4445255474452554e-05,
      "loss": 1.2063,
      "step": 70020
    },
    {
      "epoch": 511.1678832116788,
      "grad_norm": 14.762917518615723,
      "learning_rate": 2.444160583941606e-05,
      "loss": 0.7932,
      "step": 70030
    },
    {
      "epoch": 511.24087591240874,
      "grad_norm": 11.221776962280273,
      "learning_rate": 2.4437956204379565e-05,
      "loss": 0.8327,
      "step": 70040
    },
    {
      "epoch": 511.3138686131387,
      "grad_norm": 8.752840995788574,
      "learning_rate": 2.4434306569343066e-05,
      "loss": 1.2494,
      "step": 70050
    },
    {
      "epoch": 511.3868613138686,
      "grad_norm": 8.166491508483887,
      "learning_rate": 2.443065693430657e-05,
      "loss": 0.8509,
      "step": 70060
    },
    {
      "epoch": 511.45985401459853,
      "grad_norm": 5.446370601654053,
      "learning_rate": 2.4427007299270073e-05,
      "loss": 0.7209,
      "step": 70070
    },
    {
      "epoch": 511.53284671532845,
      "grad_norm": 1.1202163696289062,
      "learning_rate": 2.4423357664233577e-05,
      "loss": 0.8869,
      "step": 70080
    },
    {
      "epoch": 511.6058394160584,
      "grad_norm": 0.8616184592247009,
      "learning_rate": 2.441970802919708e-05,
      "loss": 1.1518,
      "step": 70090
    },
    {
      "epoch": 511.6788321167883,
      "grad_norm": 0.05659281462430954,
      "learning_rate": 2.4416058394160585e-05,
      "loss": 0.7457,
      "step": 70100
    },
    {
      "epoch": 511.75182481751824,
      "grad_norm": 7.678740978240967,
      "learning_rate": 2.441240875912409e-05,
      "loss": 0.7272,
      "step": 70110
    },
    {
      "epoch": 511.82481751824815,
      "grad_norm": 0.07622293382883072,
      "learning_rate": 2.4408759124087593e-05,
      "loss": 0.9743,
      "step": 70120
    },
    {
      "epoch": 511.8978102189781,
      "grad_norm": 12.001163482666016,
      "learning_rate": 2.4405109489051093e-05,
      "loss": 0.9765,
      "step": 70130
    },
    {
      "epoch": 511.97080291970804,
      "grad_norm": 10.034639358520508,
      "learning_rate": 2.44014598540146e-05,
      "loss": 0.9194,
      "step": 70140
    },
    {
      "epoch": 512.043795620438,
      "grad_norm": 8.438920974731445,
      "learning_rate": 2.4397810218978105e-05,
      "loss": 0.7714,
      "step": 70150
    },
    {
      "epoch": 512.1167883211679,
      "grad_norm": 10.463356971740723,
      "learning_rate": 2.439416058394161e-05,
      "loss": 0.517,
      "step": 70160
    },
    {
      "epoch": 512.1897810218978,
      "grad_norm": 9.309491157531738,
      "learning_rate": 2.439051094890511e-05,
      "loss": 1.0634,
      "step": 70170
    },
    {
      "epoch": 512.2627737226277,
      "grad_norm": 6.424139499664307,
      "learning_rate": 2.4386861313868613e-05,
      "loss": 1.2093,
      "step": 70180
    },
    {
      "epoch": 512.3357664233577,
      "grad_norm": 7.078573703765869,
      "learning_rate": 2.438321167883212e-05,
      "loss": 0.6681,
      "step": 70190
    },
    {
      "epoch": 512.4087591240876,
      "grad_norm": 9.627256393432617,
      "learning_rate": 2.4379562043795624e-05,
      "loss": 1.1442,
      "step": 70200
    },
    {
      "epoch": 512.4817518248175,
      "grad_norm": 10.930912017822266,
      "learning_rate": 2.4375912408759124e-05,
      "loss": 0.8993,
      "step": 70210
    },
    {
      "epoch": 512.5547445255474,
      "grad_norm": 13.10633659362793,
      "learning_rate": 2.437226277372263e-05,
      "loss": 1.1455,
      "step": 70220
    },
    {
      "epoch": 512.6277372262774,
      "grad_norm": 0.0459723025560379,
      "learning_rate": 2.4368613138686132e-05,
      "loss": 0.9579,
      "step": 70230
    },
    {
      "epoch": 512.7007299270073,
      "grad_norm": 12.263072967529297,
      "learning_rate": 2.4364963503649636e-05,
      "loss": 0.7266,
      "step": 70240
    },
    {
      "epoch": 512.7737226277372,
      "grad_norm": 13.897409439086914,
      "learning_rate": 2.436131386861314e-05,
      "loss": 0.5809,
      "step": 70250
    },
    {
      "epoch": 512.8467153284671,
      "grad_norm": 0.16672684252262115,
      "learning_rate": 2.4357664233576644e-05,
      "loss": 0.9703,
      "step": 70260
    },
    {
      "epoch": 512.9197080291971,
      "grad_norm": 14.118885040283203,
      "learning_rate": 2.4354014598540148e-05,
      "loss": 1.0668,
      "step": 70270
    },
    {
      "epoch": 512.992700729927,
      "grad_norm": 12.283180236816406,
      "learning_rate": 2.4350364963503648e-05,
      "loss": 0.5178,
      "step": 70280
    },
    {
      "epoch": 513.0656934306569,
      "grad_norm": 10.470071792602539,
      "learning_rate": 2.4346715328467155e-05,
      "loss": 0.8285,
      "step": 70290
    },
    {
      "epoch": 513.1386861313869,
      "grad_norm": 11.058371543884277,
      "learning_rate": 2.434306569343066e-05,
      "loss": 1.0255,
      "step": 70300
    },
    {
      "epoch": 513.2116788321168,
      "grad_norm": 7.359027862548828,
      "learning_rate": 2.4339416058394163e-05,
      "loss": 1.1376,
      "step": 70310
    },
    {
      "epoch": 513.2846715328467,
      "grad_norm": 6.10758113861084,
      "learning_rate": 2.4335766423357664e-05,
      "loss": 0.8275,
      "step": 70320
    },
    {
      "epoch": 513.3576642335767,
      "grad_norm": 19.81350326538086,
      "learning_rate": 2.4332116788321168e-05,
      "loss": 1.25,
      "step": 70330
    },
    {
      "epoch": 513.4306569343066,
      "grad_norm": 9.558733940124512,
      "learning_rate": 2.432846715328467e-05,
      "loss": 0.6163,
      "step": 70340
    },
    {
      "epoch": 513.5036496350365,
      "grad_norm": 0.055390819907188416,
      "learning_rate": 2.432481751824818e-05,
      "loss": 0.482,
      "step": 70350
    },
    {
      "epoch": 513.5766423357665,
      "grad_norm": 10.6840238571167,
      "learning_rate": 2.432116788321168e-05,
      "loss": 1.0304,
      "step": 70360
    },
    {
      "epoch": 513.6496350364963,
      "grad_norm": 11.177389144897461,
      "learning_rate": 2.4317518248175183e-05,
      "loss": 1.1082,
      "step": 70370
    },
    {
      "epoch": 513.7226277372263,
      "grad_norm": 0.24373140931129456,
      "learning_rate": 2.4313868613138687e-05,
      "loss": 0.8084,
      "step": 70380
    },
    {
      "epoch": 513.7956204379562,
      "grad_norm": 0.10480351001024246,
      "learning_rate": 2.431021897810219e-05,
      "loss": 0.6448,
      "step": 70390
    },
    {
      "epoch": 513.8686131386861,
      "grad_norm": 12.643259048461914,
      "learning_rate": 2.4306569343065695e-05,
      "loss": 1.1286,
      "step": 70400
    },
    {
      "epoch": 513.9416058394161,
      "grad_norm": 15.183465003967285,
      "learning_rate": 2.43029197080292e-05,
      "loss": 0.9896,
      "step": 70410
    },
    {
      "epoch": 514.014598540146,
      "grad_norm": 15.675314903259277,
      "learning_rate": 2.4299270072992703e-05,
      "loss": 0.7478,
      "step": 70420
    },
    {
      "epoch": 514.0875912408759,
      "grad_norm": 0.09536583721637726,
      "learning_rate": 2.4295620437956206e-05,
      "loss": 0.8311,
      "step": 70430
    },
    {
      "epoch": 514.1605839416059,
      "grad_norm": 0.028643634170293808,
      "learning_rate": 2.4291970802919707e-05,
      "loss": 0.6095,
      "step": 70440
    },
    {
      "epoch": 514.2335766423357,
      "grad_norm": 6.516464710235596,
      "learning_rate": 2.4288321167883214e-05,
      "loss": 0.8557,
      "step": 70450
    },
    {
      "epoch": 514.3065693430657,
      "grad_norm": 11.958377838134766,
      "learning_rate": 2.4284671532846718e-05,
      "loss": 1.1811,
      "step": 70460
    },
    {
      "epoch": 514.3795620437957,
      "grad_norm": 10.058683395385742,
      "learning_rate": 2.428102189781022e-05,
      "loss": 1.2114,
      "step": 70470
    },
    {
      "epoch": 514.4525547445255,
      "grad_norm": 13.915803909301758,
      "learning_rate": 2.4277372262773722e-05,
      "loss": 1.2101,
      "step": 70480
    },
    {
      "epoch": 514.5255474452555,
      "grad_norm": 15.727262496948242,
      "learning_rate": 2.4273722627737226e-05,
      "loss": 0.9874,
      "step": 70490
    },
    {
      "epoch": 514.5985401459855,
      "grad_norm": 15.67330265045166,
      "learning_rate": 2.4270072992700734e-05,
      "loss": 0.8733,
      "step": 70500
    },
    {
      "epoch": 514.6715328467153,
      "grad_norm": 15.967047691345215,
      "learning_rate": 2.4266423357664234e-05,
      "loss": 0.9077,
      "step": 70510
    },
    {
      "epoch": 514.7445255474453,
      "grad_norm": 7.394799709320068,
      "learning_rate": 2.4262773722627738e-05,
      "loss": 1.0382,
      "step": 70520
    },
    {
      "epoch": 514.8175182481751,
      "grad_norm": 6.018825531005859,
      "learning_rate": 2.4259124087591242e-05,
      "loss": 0.8042,
      "step": 70530
    },
    {
      "epoch": 514.8905109489051,
      "grad_norm": 6.574833869934082,
      "learning_rate": 2.4255474452554746e-05,
      "loss": 0.6197,
      "step": 70540
    },
    {
      "epoch": 514.9635036496351,
      "grad_norm": 10.2601318359375,
      "learning_rate": 2.425182481751825e-05,
      "loss": 0.8132,
      "step": 70550
    },
    {
      "epoch": 515.0364963503649,
      "grad_norm": 8.290504455566406,
      "learning_rate": 2.4248175182481754e-05,
      "loss": 1.3929,
      "step": 70560
    },
    {
      "epoch": 515.1094890510949,
      "grad_norm": 0.12557491660118103,
      "learning_rate": 2.4244525547445257e-05,
      "loss": 0.7948,
      "step": 70570
    },
    {
      "epoch": 515.1824817518249,
      "grad_norm": 9.072553634643555,
      "learning_rate": 2.424087591240876e-05,
      "loss": 0.8694,
      "step": 70580
    },
    {
      "epoch": 515.2554744525547,
      "grad_norm": 6.148319244384766,
      "learning_rate": 2.4237226277372262e-05,
      "loss": 0.9147,
      "step": 70590
    },
    {
      "epoch": 515.3284671532847,
      "grad_norm": 19.28136444091797,
      "learning_rate": 2.423357664233577e-05,
      "loss": 1.0394,
      "step": 70600
    },
    {
      "epoch": 515.4014598540145,
      "grad_norm": 14.237796783447266,
      "learning_rate": 2.4229927007299273e-05,
      "loss": 1.0868,
      "step": 70610
    },
    {
      "epoch": 515.4744525547445,
      "grad_norm": 7.7107625007629395,
      "learning_rate": 2.4226277372262777e-05,
      "loss": 1.0152,
      "step": 70620
    },
    {
      "epoch": 515.5474452554745,
      "grad_norm": 7.858895778656006,
      "learning_rate": 2.4222627737226277e-05,
      "loss": 0.7517,
      "step": 70630
    },
    {
      "epoch": 515.6204379562043,
      "grad_norm": 7.161059856414795,
      "learning_rate": 2.421897810218978e-05,
      "loss": 0.9279,
      "step": 70640
    },
    {
      "epoch": 515.6934306569343,
      "grad_norm": 9.672748565673828,
      "learning_rate": 2.4215328467153285e-05,
      "loss": 1.159,
      "step": 70650
    },
    {
      "epoch": 515.7664233576643,
      "grad_norm": 15.134065628051758,
      "learning_rate": 2.421167883211679e-05,
      "loss": 0.9943,
      "step": 70660
    },
    {
      "epoch": 515.8394160583941,
      "grad_norm": 10.322091102600098,
      "learning_rate": 2.4208029197080293e-05,
      "loss": 0.6887,
      "step": 70670
    },
    {
      "epoch": 515.9124087591241,
      "grad_norm": 7.8183979988098145,
      "learning_rate": 2.4204379562043797e-05,
      "loss": 0.6184,
      "step": 70680
    },
    {
      "epoch": 515.985401459854,
      "grad_norm": 0.07713629305362701,
      "learning_rate": 2.42007299270073e-05,
      "loss": 0.7955,
      "step": 70690
    },
    {
      "epoch": 516.0583941605839,
      "grad_norm": 0.03812938928604126,
      "learning_rate": 2.4197080291970805e-05,
      "loss": 0.7338,
      "step": 70700
    },
    {
      "epoch": 516.1313868613139,
      "grad_norm": 23.404874801635742,
      "learning_rate": 2.419343065693431e-05,
      "loss": 1.0116,
      "step": 70710
    },
    {
      "epoch": 516.2043795620438,
      "grad_norm": 11.306323051452637,
      "learning_rate": 2.4189781021897812e-05,
      "loss": 0.9473,
      "step": 70720
    },
    {
      "epoch": 516.2773722627737,
      "grad_norm": 10.689292907714844,
      "learning_rate": 2.4186131386861316e-05,
      "loss": 1.0498,
      "step": 70730
    },
    {
      "epoch": 516.3503649635037,
      "grad_norm": 4.338446140289307,
      "learning_rate": 2.4182481751824817e-05,
      "loss": 0.7389,
      "step": 70740
    },
    {
      "epoch": 516.4233576642335,
      "grad_norm": 0.056205183267593384,
      "learning_rate": 2.417883211678832e-05,
      "loss": 0.6176,
      "step": 70750
    },
    {
      "epoch": 516.4963503649635,
      "grad_norm": 15.380284309387207,
      "learning_rate": 2.4175182481751828e-05,
      "loss": 0.7601,
      "step": 70760
    },
    {
      "epoch": 516.5693430656934,
      "grad_norm": 7.748895168304443,
      "learning_rate": 2.417153284671533e-05,
      "loss": 0.475,
      "step": 70770
    },
    {
      "epoch": 516.6423357664233,
      "grad_norm": 1.3495697975158691,
      "learning_rate": 2.4167883211678832e-05,
      "loss": 1.0093,
      "step": 70780
    },
    {
      "epoch": 516.7153284671533,
      "grad_norm": 10.6652250289917,
      "learning_rate": 2.4164233576642336e-05,
      "loss": 1.1035,
      "step": 70790
    },
    {
      "epoch": 516.7883211678832,
      "grad_norm": 0.21871306002140045,
      "learning_rate": 2.416058394160584e-05,
      "loss": 0.8472,
      "step": 70800
    },
    {
      "epoch": 516.8613138686131,
      "grad_norm": 10.391924858093262,
      "learning_rate": 2.4156934306569344e-05,
      "loss": 1.1891,
      "step": 70810
    },
    {
      "epoch": 516.9343065693431,
      "grad_norm": 13.684379577636719,
      "learning_rate": 2.4153284671532848e-05,
      "loss": 1.3983,
      "step": 70820
    },
    {
      "epoch": 517.007299270073,
      "grad_norm": 13.818550109863281,
      "learning_rate": 2.414963503649635e-05,
      "loss": 1.2678,
      "step": 70830
    },
    {
      "epoch": 517.0802919708029,
      "grad_norm": 0.06525950878858566,
      "learning_rate": 2.4145985401459855e-05,
      "loss": 0.9558,
      "step": 70840
    },
    {
      "epoch": 517.1532846715329,
      "grad_norm": 11.3612699508667,
      "learning_rate": 2.414233576642336e-05,
      "loss": 0.7811,
      "step": 70850
    },
    {
      "epoch": 517.2262773722628,
      "grad_norm": 9.605642318725586,
      "learning_rate": 2.4138686131386863e-05,
      "loss": 0.5168,
      "step": 70860
    },
    {
      "epoch": 517.2992700729927,
      "grad_norm": 10.497145652770996,
      "learning_rate": 2.4135036496350367e-05,
      "loss": 1.2103,
      "step": 70870
    },
    {
      "epoch": 517.3722627737226,
      "grad_norm": 11.05117416381836,
      "learning_rate": 2.413138686131387e-05,
      "loss": 0.7576,
      "step": 70880
    },
    {
      "epoch": 517.4452554744526,
      "grad_norm": 0.023549972102046013,
      "learning_rate": 2.412773722627737e-05,
      "loss": 0.7406,
      "step": 70890
    },
    {
      "epoch": 517.5182481751825,
      "grad_norm": 12.719759941101074,
      "learning_rate": 2.4124087591240875e-05,
      "loss": 0.7776,
      "step": 70900
    },
    {
      "epoch": 517.5912408759124,
      "grad_norm": 17.8864803314209,
      "learning_rate": 2.412043795620438e-05,
      "loss": 1.3507,
      "step": 70910
    },
    {
      "epoch": 517.6642335766423,
      "grad_norm": 16.050512313842773,
      "learning_rate": 2.4116788321167887e-05,
      "loss": 1.2609,
      "step": 70920
    },
    {
      "epoch": 517.7372262773723,
      "grad_norm": 9.06839656829834,
      "learning_rate": 2.4113138686131387e-05,
      "loss": 0.7914,
      "step": 70930
    },
    {
      "epoch": 517.8102189781022,
      "grad_norm": 8.383927345275879,
      "learning_rate": 2.410948905109489e-05,
      "loss": 0.8568,
      "step": 70940
    },
    {
      "epoch": 517.8832116788321,
      "grad_norm": 0.020858269184827805,
      "learning_rate": 2.4105839416058395e-05,
      "loss": 1.1051,
      "step": 70950
    },
    {
      "epoch": 517.956204379562,
      "grad_norm": 7.328258991241455,
      "learning_rate": 2.41021897810219e-05,
      "loss": 0.8848,
      "step": 70960
    },
    {
      "epoch": 518.029197080292,
      "grad_norm": 8.13842487335205,
      "learning_rate": 2.4098540145985403e-05,
      "loss": 0.7686,
      "step": 70970
    },
    {
      "epoch": 518.1021897810219,
      "grad_norm": 10.065443992614746,
      "learning_rate": 2.4094890510948906e-05,
      "loss": 0.4797,
      "step": 70980
    },
    {
      "epoch": 518.1751824817518,
      "grad_norm": 6.778240203857422,
      "learning_rate": 2.409124087591241e-05,
      "loss": 0.8051,
      "step": 70990
    },
    {
      "epoch": 518.2481751824818,
      "grad_norm": 11.99566650390625,
      "learning_rate": 2.4087591240875914e-05,
      "loss": 0.4587,
      "step": 71000
    },
    {
      "epoch": 518.3211678832117,
      "grad_norm": 9.000535011291504,
      "learning_rate": 2.4083941605839415e-05,
      "loss": 0.8617,
      "step": 71010
    },
    {
      "epoch": 518.3941605839416,
      "grad_norm": 6.1741156578063965,
      "learning_rate": 2.4080291970802922e-05,
      "loss": 0.766,
      "step": 71020
    },
    {
      "epoch": 518.4671532846716,
      "grad_norm": 9.448956489562988,
      "learning_rate": 2.4076642335766426e-05,
      "loss": 0.8058,
      "step": 71030
    },
    {
      "epoch": 518.5401459854014,
      "grad_norm": 5.813360214233398,
      "learning_rate": 2.407299270072993e-05,
      "loss": 1.3963,
      "step": 71040
    },
    {
      "epoch": 518.6131386861314,
      "grad_norm": 7.724560737609863,
      "learning_rate": 2.406934306569343e-05,
      "loss": 1.0824,
      "step": 71050
    },
    {
      "epoch": 518.6861313868613,
      "grad_norm": 14.657732009887695,
      "learning_rate": 2.4065693430656934e-05,
      "loss": 1.3191,
      "step": 71060
    },
    {
      "epoch": 518.7591240875912,
      "grad_norm": 6.608485698699951,
      "learning_rate": 2.406204379562044e-05,
      "loss": 0.8166,
      "step": 71070
    },
    {
      "epoch": 518.8321167883212,
      "grad_norm": 12.672101020812988,
      "learning_rate": 2.4058394160583945e-05,
      "loss": 0.8201,
      "step": 71080
    },
    {
      "epoch": 518.9051094890511,
      "grad_norm": 11.950024604797363,
      "learning_rate": 2.4054744525547446e-05,
      "loss": 1.049,
      "step": 71090
    },
    {
      "epoch": 518.978102189781,
      "grad_norm": 17.639204025268555,
      "learning_rate": 2.405109489051095e-05,
      "loss": 1.0093,
      "step": 71100
    },
    {
      "epoch": 519.051094890511,
      "grad_norm": 6.749724864959717,
      "learning_rate": 2.4047445255474454e-05,
      "loss": 0.777,
      "step": 71110
    },
    {
      "epoch": 519.1240875912408,
      "grad_norm": 13.861297607421875,
      "learning_rate": 2.4043795620437957e-05,
      "loss": 0.7631,
      "step": 71120
    },
    {
      "epoch": 519.1970802919708,
      "grad_norm": 6.988706111907959,
      "learning_rate": 2.404014598540146e-05,
      "loss": 1.3449,
      "step": 71130
    },
    {
      "epoch": 519.2700729927008,
      "grad_norm": 6.699542045593262,
      "learning_rate": 2.4036496350364965e-05,
      "loss": 1.0324,
      "step": 71140
    },
    {
      "epoch": 519.3430656934306,
      "grad_norm": 13.898686408996582,
      "learning_rate": 2.403284671532847e-05,
      "loss": 1.2074,
      "step": 71150
    },
    {
      "epoch": 519.4160583941606,
      "grad_norm": 13.076040267944336,
      "learning_rate": 2.402919708029197e-05,
      "loss": 1.096,
      "step": 71160
    },
    {
      "epoch": 519.4890510948906,
      "grad_norm": 12.596748352050781,
      "learning_rate": 2.4025547445255477e-05,
      "loss": 0.9934,
      "step": 71170
    },
    {
      "epoch": 519.5620437956204,
      "grad_norm": 14.628595352172852,
      "learning_rate": 2.402189781021898e-05,
      "loss": 0.6327,
      "step": 71180
    },
    {
      "epoch": 519.6350364963504,
      "grad_norm": 0.11855974048376083,
      "learning_rate": 2.4018248175182485e-05,
      "loss": 1.0798,
      "step": 71190
    },
    {
      "epoch": 519.7080291970802,
      "grad_norm": 11.343077659606934,
      "learning_rate": 2.4014598540145985e-05,
      "loss": 0.9086,
      "step": 71200
    },
    {
      "epoch": 519.7810218978102,
      "grad_norm": 14.565141677856445,
      "learning_rate": 2.401094890510949e-05,
      "loss": 0.6969,
      "step": 71210
    },
    {
      "epoch": 519.8540145985402,
      "grad_norm": 0.04141518473625183,
      "learning_rate": 2.4007299270072993e-05,
      "loss": 0.8074,
      "step": 71220
    },
    {
      "epoch": 519.92700729927,
      "grad_norm": 5.875218868255615,
      "learning_rate": 2.40036496350365e-05,
      "loss": 0.8163,
      "step": 71230
    },
    {
      "epoch": 520.0,
      "grad_norm": 0.06389510631561279,
      "learning_rate": 2.4e-05,
      "loss": 0.8214,
      "step": 71240
    },
    {
      "epoch": 520.07299270073,
      "grad_norm": 11.09386920928955,
      "learning_rate": 2.3996350364963504e-05,
      "loss": 0.8162,
      "step": 71250
    },
    {
      "epoch": 520.1459854014598,
      "grad_norm": 12.59123706817627,
      "learning_rate": 2.399270072992701e-05,
      "loss": 1.0187,
      "step": 71260
    },
    {
      "epoch": 520.2189781021898,
      "grad_norm": 12.35336685180664,
      "learning_rate": 2.3989051094890512e-05,
      "loss": 0.6383,
      "step": 71270
    },
    {
      "epoch": 520.2919708029198,
      "grad_norm": 0.06728784739971161,
      "learning_rate": 2.3985401459854016e-05,
      "loss": 0.6137,
      "step": 71280
    },
    {
      "epoch": 520.3649635036496,
      "grad_norm": 0.02818872407078743,
      "learning_rate": 2.398175182481752e-05,
      "loss": 0.9702,
      "step": 71290
    },
    {
      "epoch": 520.4379562043796,
      "grad_norm": 16.237993240356445,
      "learning_rate": 2.3978102189781024e-05,
      "loss": 0.7563,
      "step": 71300
    },
    {
      "epoch": 520.5109489051094,
      "grad_norm": 13.79716968536377,
      "learning_rate": 2.3974452554744524e-05,
      "loss": 0.8466,
      "step": 71310
    },
    {
      "epoch": 520.5839416058394,
      "grad_norm": 12.677274703979492,
      "learning_rate": 2.3970802919708028e-05,
      "loss": 0.8121,
      "step": 71320
    },
    {
      "epoch": 520.6569343065694,
      "grad_norm": 0.10876928269863129,
      "learning_rate": 2.3967153284671536e-05,
      "loss": 0.9195,
      "step": 71330
    },
    {
      "epoch": 520.7299270072992,
      "grad_norm": 5.622483730316162,
      "learning_rate": 2.396350364963504e-05,
      "loss": 0.8549,
      "step": 71340
    },
    {
      "epoch": 520.8029197080292,
      "grad_norm": 0.096013143658638,
      "learning_rate": 2.395985401459854e-05,
      "loss": 0.8491,
      "step": 71350
    },
    {
      "epoch": 520.8759124087592,
      "grad_norm": 9.748800277709961,
      "learning_rate": 2.3956204379562044e-05,
      "loss": 1.1284,
      "step": 71360
    },
    {
      "epoch": 520.948905109489,
      "grad_norm": 13.249361991882324,
      "learning_rate": 2.3952554744525548e-05,
      "loss": 1.0099,
      "step": 71370
    },
    {
      "epoch": 521.021897810219,
      "grad_norm": 9.967409133911133,
      "learning_rate": 2.394890510948905e-05,
      "loss": 1.2537,
      "step": 71380
    },
    {
      "epoch": 521.0948905109489,
      "grad_norm": 9.250531196594238,
      "learning_rate": 2.3945255474452555e-05,
      "loss": 0.677,
      "step": 71390
    },
    {
      "epoch": 521.1678832116788,
      "grad_norm": 9.036042213439941,
      "learning_rate": 2.394160583941606e-05,
      "loss": 0.6761,
      "step": 71400
    },
    {
      "epoch": 521.2408759124088,
      "grad_norm": 0.09165910631418228,
      "learning_rate": 2.3937956204379563e-05,
      "loss": 0.6624,
      "step": 71410
    },
    {
      "epoch": 521.3138686131387,
      "grad_norm": 9.134099960327148,
      "learning_rate": 2.3934306569343067e-05,
      "loss": 0.6895,
      "step": 71420
    },
    {
      "epoch": 521.3868613138686,
      "grad_norm": 0.03872201591730118,
      "learning_rate": 2.393065693430657e-05,
      "loss": 1.1341,
      "step": 71430
    },
    {
      "epoch": 521.4598540145986,
      "grad_norm": 2.278707265853882,
      "learning_rate": 2.3927007299270075e-05,
      "loss": 0.8733,
      "step": 71440
    },
    {
      "epoch": 521.5328467153284,
      "grad_norm": 7.773958683013916,
      "learning_rate": 2.392335766423358e-05,
      "loss": 0.6264,
      "step": 71450
    },
    {
      "epoch": 521.6058394160584,
      "grad_norm": 6.48805570602417,
      "learning_rate": 2.3919708029197083e-05,
      "loss": 1.0193,
      "step": 71460
    },
    {
      "epoch": 521.6788321167883,
      "grad_norm": 15.915918350219727,
      "learning_rate": 2.3916058394160583e-05,
      "loss": 1.2831,
      "step": 71470
    },
    {
      "epoch": 521.7518248175182,
      "grad_norm": 11.422134399414062,
      "learning_rate": 2.3912408759124087e-05,
      "loss": 1.24,
      "step": 71480
    },
    {
      "epoch": 521.8248175182482,
      "grad_norm": 10.630999565124512,
      "learning_rate": 2.3908759124087594e-05,
      "loss": 0.7357,
      "step": 71490
    },
    {
      "epoch": 521.8978102189781,
      "grad_norm": 7.015460968017578,
      "learning_rate": 2.3905109489051098e-05,
      "loss": 1.1652,
      "step": 71500
    },
    {
      "epoch": 521.970802919708,
      "grad_norm": 13.096759796142578,
      "learning_rate": 2.39014598540146e-05,
      "loss": 1.2205,
      "step": 71510
    },
    {
      "epoch": 522.043795620438,
      "grad_norm": 0.13430535793304443,
      "learning_rate": 2.3897810218978103e-05,
      "loss": 0.882,
      "step": 71520
    },
    {
      "epoch": 522.1167883211679,
      "grad_norm": 0.13107171654701233,
      "learning_rate": 2.3894160583941606e-05,
      "loss": 1.2033,
      "step": 71530
    },
    {
      "epoch": 522.1897810218978,
      "grad_norm": 3.3044323921203613,
      "learning_rate": 2.389051094890511e-05,
      "loss": 0.883,
      "step": 71540
    },
    {
      "epoch": 522.2627737226277,
      "grad_norm": 0.13992390036582947,
      "learning_rate": 2.3886861313868614e-05,
      "loss": 1.0447,
      "step": 71550
    },
    {
      "epoch": 522.3357664233577,
      "grad_norm": 0.05007161945104599,
      "learning_rate": 2.3883211678832118e-05,
      "loss": 0.8327,
      "step": 71560
    },
    {
      "epoch": 522.4087591240876,
      "grad_norm": 8.675922393798828,
      "learning_rate": 2.3879562043795622e-05,
      "loss": 0.8578,
      "step": 71570
    },
    {
      "epoch": 522.4817518248175,
      "grad_norm": 6.987083435058594,
      "learning_rate": 2.3875912408759122e-05,
      "loss": 0.8104,
      "step": 71580
    },
    {
      "epoch": 522.5547445255474,
      "grad_norm": 0.12958019971847534,
      "learning_rate": 2.387226277372263e-05,
      "loss": 0.7152,
      "step": 71590
    },
    {
      "epoch": 522.6277372262774,
      "grad_norm": 14.08395004272461,
      "learning_rate": 2.3868613138686134e-05,
      "loss": 0.6579,
      "step": 71600
    },
    {
      "epoch": 522.7007299270073,
      "grad_norm": 5.098075866699219,
      "learning_rate": 2.3864963503649637e-05,
      "loss": 0.8206,
      "step": 71610
    },
    {
      "epoch": 522.7737226277372,
      "grad_norm": 11.373276710510254,
      "learning_rate": 2.3861313868613138e-05,
      "loss": 0.874,
      "step": 71620
    },
    {
      "epoch": 522.8467153284671,
      "grad_norm": 4.371955394744873,
      "learning_rate": 2.3857664233576642e-05,
      "loss": 0.7412,
      "step": 71630
    },
    {
      "epoch": 522.9197080291971,
      "grad_norm": 9.841521263122559,
      "learning_rate": 2.385401459854015e-05,
      "loss": 1.3504,
      "step": 71640
    },
    {
      "epoch": 522.992700729927,
      "grad_norm": 15.315520286560059,
      "learning_rate": 2.3850364963503653e-05,
      "loss": 1.1085,
      "step": 71650
    },
    {
      "epoch": 523.0656934306569,
      "grad_norm": 11.437654495239258,
      "learning_rate": 2.3846715328467154e-05,
      "loss": 0.8617,
      "step": 71660
    },
    {
      "epoch": 523.1386861313869,
      "grad_norm": 8.900411605834961,
      "learning_rate": 2.3843065693430657e-05,
      "loss": 0.6084,
      "step": 71670
    },
    {
      "epoch": 523.2116788321168,
      "grad_norm": 8.213430404663086,
      "learning_rate": 2.383941605839416e-05,
      "loss": 0.5516,
      "step": 71680
    },
    {
      "epoch": 523.2846715328467,
      "grad_norm": 10.327335357666016,
      "learning_rate": 2.3835766423357665e-05,
      "loss": 0.8384,
      "step": 71690
    },
    {
      "epoch": 523.3576642335767,
      "grad_norm": 7.69930362701416,
      "learning_rate": 2.383211678832117e-05,
      "loss": 0.9106,
      "step": 71700
    },
    {
      "epoch": 523.4306569343066,
      "grad_norm": 0.1378287374973297,
      "learning_rate": 2.3828467153284673e-05,
      "loss": 0.7118,
      "step": 71710
    },
    {
      "epoch": 523.5036496350365,
      "grad_norm": 0.04718390107154846,
      "learning_rate": 2.3824817518248177e-05,
      "loss": 0.4789,
      "step": 71720
    },
    {
      "epoch": 523.5766423357665,
      "grad_norm": 12.808826446533203,
      "learning_rate": 2.382116788321168e-05,
      "loss": 1.3654,
      "step": 71730
    },
    {
      "epoch": 523.6496350364963,
      "grad_norm": 7.815513610839844,
      "learning_rate": 2.3817518248175185e-05,
      "loss": 1.1748,
      "step": 71740
    },
    {
      "epoch": 523.7226277372263,
      "grad_norm": 8.188225746154785,
      "learning_rate": 2.381386861313869e-05,
      "loss": 0.8754,
      "step": 71750
    },
    {
      "epoch": 523.7956204379562,
      "grad_norm": 10.395258903503418,
      "learning_rate": 2.3810218978102192e-05,
      "loss": 1.2409,
      "step": 71760
    },
    {
      "epoch": 523.8686131386861,
      "grad_norm": 10.105411529541016,
      "learning_rate": 2.3806569343065693e-05,
      "loss": 1.133,
      "step": 71770
    },
    {
      "epoch": 523.9416058394161,
      "grad_norm": 7.143087863922119,
      "learning_rate": 2.3802919708029197e-05,
      "loss": 0.855,
      "step": 71780
    },
    {
      "epoch": 524.014598540146,
      "grad_norm": 0.12512490153312683,
      "learning_rate": 2.37992700729927e-05,
      "loss": 1.0588,
      "step": 71790
    },
    {
      "epoch": 524.0875912408759,
      "grad_norm": 10.733846664428711,
      "learning_rate": 2.3795620437956208e-05,
      "loss": 0.8263,
      "step": 71800
    },
    {
      "epoch": 524.1605839416059,
      "grad_norm": 15.660109519958496,
      "learning_rate": 2.379197080291971e-05,
      "loss": 0.8288,
      "step": 71810
    },
    {
      "epoch": 524.2335766423357,
      "grad_norm": 11.34835147857666,
      "learning_rate": 2.3788321167883212e-05,
      "loss": 1.1038,
      "step": 71820
    },
    {
      "epoch": 524.3065693430657,
      "grad_norm": 12.030900955200195,
      "learning_rate": 2.3784671532846716e-05,
      "loss": 1.2114,
      "step": 71830
    },
    {
      "epoch": 524.3795620437957,
      "grad_norm": 11.179198265075684,
      "learning_rate": 2.378102189781022e-05,
      "loss": 1.5491,
      "step": 71840
    },
    {
      "epoch": 524.4525547445255,
      "grad_norm": 10.578262329101562,
      "learning_rate": 2.3777372262773724e-05,
      "loss": 0.9228,
      "step": 71850
    },
    {
      "epoch": 524.5255474452555,
      "grad_norm": 7.623622417449951,
      "learning_rate": 2.3773722627737228e-05,
      "loss": 0.5678,
      "step": 71860
    },
    {
      "epoch": 524.5985401459855,
      "grad_norm": 8.870321273803711,
      "learning_rate": 2.377007299270073e-05,
      "loss": 0.7415,
      "step": 71870
    },
    {
      "epoch": 524.6715328467153,
      "grad_norm": 0.03128150478005409,
      "learning_rate": 2.3766423357664236e-05,
      "loss": 0.8644,
      "step": 71880
    },
    {
      "epoch": 524.7445255474453,
      "grad_norm": 8.549886703491211,
      "learning_rate": 2.3762773722627736e-05,
      "loss": 0.8605,
      "step": 71890
    },
    {
      "epoch": 524.8175182481751,
      "grad_norm": 5.6831488609313965,
      "learning_rate": 2.3759124087591243e-05,
      "loss": 0.7606,
      "step": 71900
    },
    {
      "epoch": 524.8905109489051,
      "grad_norm": 5.211680889129639,
      "learning_rate": 2.3755474452554747e-05,
      "loss": 0.8622,
      "step": 71910
    },
    {
      "epoch": 524.9635036496351,
      "grad_norm": 0.3147605061531067,
      "learning_rate": 2.375182481751825e-05,
      "loss": 0.807,
      "step": 71920
    },
    {
      "epoch": 525.0364963503649,
      "grad_norm": 6.042929172515869,
      "learning_rate": 2.374817518248175e-05,
      "loss": 0.6088,
      "step": 71930
    },
    {
      "epoch": 525.1094890510949,
      "grad_norm": 15.911794662475586,
      "learning_rate": 2.3744525547445255e-05,
      "loss": 0.8776,
      "step": 71940
    },
    {
      "epoch": 525.1824817518249,
      "grad_norm": 11.620782852172852,
      "learning_rate": 2.374087591240876e-05,
      "loss": 0.7982,
      "step": 71950
    },
    {
      "epoch": 525.2554744525547,
      "grad_norm": 13.471992492675781,
      "learning_rate": 2.3737226277372263e-05,
      "loss": 1.2109,
      "step": 71960
    },
    {
      "epoch": 525.3284671532847,
      "grad_norm": 16.31178092956543,
      "learning_rate": 2.3733576642335767e-05,
      "loss": 1.0635,
      "step": 71970
    },
    {
      "epoch": 525.4014598540145,
      "grad_norm": 5.961450576782227,
      "learning_rate": 2.372992700729927e-05,
      "loss": 0.8935,
      "step": 71980
    },
    {
      "epoch": 525.4744525547445,
      "grad_norm": 6.284659385681152,
      "learning_rate": 2.3726277372262775e-05,
      "loss": 0.8525,
      "step": 71990
    },
    {
      "epoch": 525.5474452554745,
      "grad_norm": 14.560369491577148,
      "learning_rate": 2.372262773722628e-05,
      "loss": 0.9034,
      "step": 72000
    },
    {
      "epoch": 525.6204379562043,
      "grad_norm": 17.640066146850586,
      "learning_rate": 2.3718978102189783e-05,
      "loss": 1.1454,
      "step": 72010
    },
    {
      "epoch": 525.6934306569343,
      "grad_norm": 0.03785227984189987,
      "learning_rate": 2.3715328467153286e-05,
      "loss": 0.5711,
      "step": 72020
    },
    {
      "epoch": 525.7664233576643,
      "grad_norm": 8.836066246032715,
      "learning_rate": 2.371167883211679e-05,
      "loss": 0.6027,
      "step": 72030
    },
    {
      "epoch": 525.8394160583941,
      "grad_norm": 0.5852436423301697,
      "learning_rate": 2.370802919708029e-05,
      "loss": 1.0107,
      "step": 72040
    },
    {
      "epoch": 525.9124087591241,
      "grad_norm": 0.045668091624975204,
      "learning_rate": 2.3704379562043795e-05,
      "loss": 0.8687,
      "step": 72050
    },
    {
      "epoch": 525.985401459854,
      "grad_norm": 9.954405784606934,
      "learning_rate": 2.3700729927007302e-05,
      "loss": 0.9662,
      "step": 72060
    },
    {
      "epoch": 526.0583941605839,
      "grad_norm": 11.724394798278809,
      "learning_rate": 2.3697080291970806e-05,
      "loss": 1.1455,
      "step": 72070
    },
    {
      "epoch": 526.1313868613139,
      "grad_norm": 0.10511637479066849,
      "learning_rate": 2.3693430656934306e-05,
      "loss": 0.7177,
      "step": 72080
    },
    {
      "epoch": 526.2043795620438,
      "grad_norm": 19.188716888427734,
      "learning_rate": 2.368978102189781e-05,
      "loss": 0.8104,
      "step": 72090
    },
    {
      "epoch": 526.2773722627737,
      "grad_norm": 5.302121639251709,
      "learning_rate": 2.3686131386861314e-05,
      "loss": 0.8456,
      "step": 72100
    },
    {
      "epoch": 526.3503649635037,
      "grad_norm": 15.106898307800293,
      "learning_rate": 2.368248175182482e-05,
      "loss": 0.6814,
      "step": 72110
    },
    {
      "epoch": 526.4233576642335,
      "grad_norm": 13.174490928649902,
      "learning_rate": 2.3678832116788322e-05,
      "loss": 0.8412,
      "step": 72120
    },
    {
      "epoch": 526.4963503649635,
      "grad_norm": 14.168996810913086,
      "learning_rate": 2.3675182481751826e-05,
      "loss": 0.9742,
      "step": 72130
    },
    {
      "epoch": 526.5693430656934,
      "grad_norm": 0.5022420287132263,
      "learning_rate": 2.367153284671533e-05,
      "loss": 0.9204,
      "step": 72140
    },
    {
      "epoch": 526.6423357664233,
      "grad_norm": 23.82340431213379,
      "learning_rate": 2.3667883211678834e-05,
      "loss": 1.186,
      "step": 72150
    },
    {
      "epoch": 526.7153284671533,
      "grad_norm": 9.000441551208496,
      "learning_rate": 2.3664233576642337e-05,
      "loss": 0.9314,
      "step": 72160
    },
    {
      "epoch": 526.7883211678832,
      "grad_norm": 10.545738220214844,
      "learning_rate": 2.366058394160584e-05,
      "loss": 1.1468,
      "step": 72170
    },
    {
      "epoch": 526.8613138686131,
      "grad_norm": 13.330832481384277,
      "learning_rate": 2.3656934306569345e-05,
      "loss": 0.9066,
      "step": 72180
    },
    {
      "epoch": 526.9343065693431,
      "grad_norm": 10.749451637268066,
      "learning_rate": 2.3653284671532846e-05,
      "loss": 0.8619,
      "step": 72190
    },
    {
      "epoch": 527.007299270073,
      "grad_norm": 10.918004035949707,
      "learning_rate": 2.364963503649635e-05,
      "loss": 0.6699,
      "step": 72200
    },
    {
      "epoch": 527.0802919708029,
      "grad_norm": 10.98819637298584,
      "learning_rate": 2.3645985401459857e-05,
      "loss": 1.0487,
      "step": 72210
    },
    {
      "epoch": 527.1532846715329,
      "grad_norm": 7.2692341804504395,
      "learning_rate": 2.364233576642336e-05,
      "loss": 0.8906,
      "step": 72220
    },
    {
      "epoch": 527.2262773722628,
      "grad_norm": 11.144854545593262,
      "learning_rate": 2.363868613138686e-05,
      "loss": 1.0608,
      "step": 72230
    },
    {
      "epoch": 527.2992700729927,
      "grad_norm": 7.584527015686035,
      "learning_rate": 2.3635036496350365e-05,
      "loss": 0.6666,
      "step": 72240
    },
    {
      "epoch": 527.3722627737226,
      "grad_norm": 7.373063564300537,
      "learning_rate": 2.363138686131387e-05,
      "loss": 0.7375,
      "step": 72250
    },
    {
      "epoch": 527.4452554744526,
      "grad_norm": 0.05844654515385628,
      "learning_rate": 2.3627737226277373e-05,
      "loss": 0.7858,
      "step": 72260
    },
    {
      "epoch": 527.5182481751825,
      "grad_norm": 15.131242752075195,
      "learning_rate": 2.3624087591240877e-05,
      "loss": 1.2443,
      "step": 72270
    },
    {
      "epoch": 527.5912408759124,
      "grad_norm": 8.367515563964844,
      "learning_rate": 2.362043795620438e-05,
      "loss": 1.1087,
      "step": 72280
    },
    {
      "epoch": 527.6642335766423,
      "grad_norm": 7.893322467803955,
      "learning_rate": 2.3616788321167885e-05,
      "loss": 0.4746,
      "step": 72290
    },
    {
      "epoch": 527.7372262773723,
      "grad_norm": 0.03495124354958534,
      "learning_rate": 2.361313868613139e-05,
      "loss": 0.6026,
      "step": 72300
    },
    {
      "epoch": 527.8102189781022,
      "grad_norm": 12.080379486083984,
      "learning_rate": 2.3609489051094892e-05,
      "loss": 0.9002,
      "step": 72310
    },
    {
      "epoch": 527.8832116788321,
      "grad_norm": 10.30969524383545,
      "learning_rate": 2.3605839416058396e-05,
      "loss": 1.1042,
      "step": 72320
    },
    {
      "epoch": 527.956204379562,
      "grad_norm": 19.15955352783203,
      "learning_rate": 2.36021897810219e-05,
      "loss": 1.017,
      "step": 72330
    },
    {
      "epoch": 528.029197080292,
      "grad_norm": 11.433974266052246,
      "learning_rate": 2.3598540145985404e-05,
      "loss": 1.1281,
      "step": 72340
    },
    {
      "epoch": 528.1021897810219,
      "grad_norm": 0.2946275472640991,
      "learning_rate": 2.3594890510948904e-05,
      "loss": 0.7746,
      "step": 72350
    },
    {
      "epoch": 528.1751824817518,
      "grad_norm": 8.502306938171387,
      "learning_rate": 2.359124087591241e-05,
      "loss": 1.0178,
      "step": 72360
    },
    {
      "epoch": 528.2481751824818,
      "grad_norm": 17.402820587158203,
      "learning_rate": 2.3587591240875916e-05,
      "loss": 1.2116,
      "step": 72370
    },
    {
      "epoch": 528.3211678832117,
      "grad_norm": 12.257680892944336,
      "learning_rate": 2.3583941605839416e-05,
      "loss": 0.6951,
      "step": 72380
    },
    {
      "epoch": 528.3941605839416,
      "grad_norm": 7.189689636230469,
      "learning_rate": 2.358029197080292e-05,
      "loss": 0.7252,
      "step": 72390
    },
    {
      "epoch": 528.4671532846716,
      "grad_norm": 13.481667518615723,
      "learning_rate": 2.3576642335766424e-05,
      "loss": 1.0815,
      "step": 72400
    },
    {
      "epoch": 528.5401459854014,
      "grad_norm": 18.193675994873047,
      "learning_rate": 2.3572992700729928e-05,
      "loss": 1.1762,
      "step": 72410
    },
    {
      "epoch": 528.6131386861314,
      "grad_norm": 13.870494842529297,
      "learning_rate": 2.356934306569343e-05,
      "loss": 0.601,
      "step": 72420
    },
    {
      "epoch": 528.6861313868613,
      "grad_norm": 8.884928703308105,
      "learning_rate": 2.3565693430656936e-05,
      "loss": 0.9685,
      "step": 72430
    },
    {
      "epoch": 528.7591240875912,
      "grad_norm": 10.619832038879395,
      "learning_rate": 2.356204379562044e-05,
      "loss": 1.0581,
      "step": 72440
    },
    {
      "epoch": 528.8321167883212,
      "grad_norm": 5.829139232635498,
      "learning_rate": 2.3558394160583943e-05,
      "loss": 0.6926,
      "step": 72450
    },
    {
      "epoch": 528.9051094890511,
      "grad_norm": 9.16618537902832,
      "learning_rate": 2.3554744525547444e-05,
      "loss": 0.6793,
      "step": 72460
    },
    {
      "epoch": 528.978102189781,
      "grad_norm": 6.814704895019531,
      "learning_rate": 2.355109489051095e-05,
      "loss": 0.9136,
      "step": 72470
    },
    {
      "epoch": 529.051094890511,
      "grad_norm": 6.484190940856934,
      "learning_rate": 2.3547445255474455e-05,
      "loss": 1.1779,
      "step": 72480
    },
    {
      "epoch": 529.1240875912408,
      "grad_norm": 10.797329902648926,
      "learning_rate": 2.354379562043796e-05,
      "loss": 1.0501,
      "step": 72490
    },
    {
      "epoch": 529.1970802919708,
      "grad_norm": 9.775032997131348,
      "learning_rate": 2.354014598540146e-05,
      "loss": 0.7459,
      "step": 72500
    },
    {
      "epoch": 529.2700729927008,
      "grad_norm": 17.863542556762695,
      "learning_rate": 2.3536496350364963e-05,
      "loss": 1.3117,
      "step": 72510
    },
    {
      "epoch": 529.3430656934306,
      "grad_norm": 8.240469932556152,
      "learning_rate": 2.3532846715328467e-05,
      "loss": 0.7358,
      "step": 72520
    },
    {
      "epoch": 529.4160583941606,
      "grad_norm": 7.683846950531006,
      "learning_rate": 2.3529197080291974e-05,
      "loss": 0.834,
      "step": 72530
    },
    {
      "epoch": 529.4890510948906,
      "grad_norm": 12.25614070892334,
      "learning_rate": 2.3525547445255475e-05,
      "loss": 1.1567,
      "step": 72540
    },
    {
      "epoch": 529.5620437956204,
      "grad_norm": 0.08082875609397888,
      "learning_rate": 2.352189781021898e-05,
      "loss": 0.3511,
      "step": 72550
    },
    {
      "epoch": 529.6350364963504,
      "grad_norm": 0.1367100328207016,
      "learning_rate": 2.3518248175182483e-05,
      "loss": 1.0674,
      "step": 72560
    },
    {
      "epoch": 529.7080291970802,
      "grad_norm": 3.6727938652038574,
      "learning_rate": 2.3514598540145986e-05,
      "loss": 1.0386,
      "step": 72570
    },
    {
      "epoch": 529.7810218978102,
      "grad_norm": 0.11172477155923843,
      "learning_rate": 2.351094890510949e-05,
      "loss": 0.6696,
      "step": 72580
    },
    {
      "epoch": 529.8540145985402,
      "grad_norm": 13.129718780517578,
      "learning_rate": 2.3507299270072994e-05,
      "loss": 1.1758,
      "step": 72590
    },
    {
      "epoch": 529.92700729927,
      "grad_norm": 7.201783657073975,
      "learning_rate": 2.3503649635036498e-05,
      "loss": 0.4774,
      "step": 72600
    },
    {
      "epoch": 530.0,
      "grad_norm": 36.97867202758789,
      "learning_rate": 2.35e-05,
      "loss": 1.1685,
      "step": 72610
    },
    {
      "epoch": 530.07299270073,
      "grad_norm": 12.239948272705078,
      "learning_rate": 2.3496350364963503e-05,
      "loss": 0.8493,
      "step": 72620
    },
    {
      "epoch": 530.1459854014598,
      "grad_norm": 11.624472618103027,
      "learning_rate": 2.349270072992701e-05,
      "loss": 0.721,
      "step": 72630
    },
    {
      "epoch": 530.2189781021898,
      "grad_norm": 0.05710137262940407,
      "learning_rate": 2.3489051094890514e-05,
      "loss": 0.8838,
      "step": 72640
    },
    {
      "epoch": 530.2919708029198,
      "grad_norm": 0.029683245345950127,
      "learning_rate": 2.3485401459854014e-05,
      "loss": 0.9171,
      "step": 72650
    },
    {
      "epoch": 530.3649635036496,
      "grad_norm": 9.359375,
      "learning_rate": 2.3481751824817518e-05,
      "loss": 1.0713,
      "step": 72660
    },
    {
      "epoch": 530.4379562043796,
      "grad_norm": 14.73930549621582,
      "learning_rate": 2.3478102189781022e-05,
      "loss": 0.9369,
      "step": 72670
    },
    {
      "epoch": 530.5109489051094,
      "grad_norm": 4.693166732788086,
      "learning_rate": 2.347445255474453e-05,
      "loss": 0.4505,
      "step": 72680
    },
    {
      "epoch": 530.5839416058394,
      "grad_norm": 4.01107931137085,
      "learning_rate": 2.347080291970803e-05,
      "loss": 0.6644,
      "step": 72690
    },
    {
      "epoch": 530.6569343065694,
      "grad_norm": 10.040907859802246,
      "learning_rate": 2.3467153284671534e-05,
      "loss": 0.9095,
      "step": 72700
    },
    {
      "epoch": 530.7299270072992,
      "grad_norm": 14.954733848571777,
      "learning_rate": 2.3463503649635037e-05,
      "loss": 0.9144,
      "step": 72710
    },
    {
      "epoch": 530.8029197080292,
      "grad_norm": 10.299307823181152,
      "learning_rate": 2.345985401459854e-05,
      "loss": 1.3445,
      "step": 72720
    },
    {
      "epoch": 530.8759124087592,
      "grad_norm": 7.782305717468262,
      "learning_rate": 2.3456204379562045e-05,
      "loss": 0.9481,
      "step": 72730
    },
    {
      "epoch": 530.948905109489,
      "grad_norm": 11.11989688873291,
      "learning_rate": 2.345255474452555e-05,
      "loss": 1.1528,
      "step": 72740
    },
    {
      "epoch": 531.021897810219,
      "grad_norm": 13.42648696899414,
      "learning_rate": 2.3448905109489053e-05,
      "loss": 1.006,
      "step": 72750
    },
    {
      "epoch": 531.0948905109489,
      "grad_norm": 7.690545558929443,
      "learning_rate": 2.3445255474452557e-05,
      "loss": 0.6103,
      "step": 72760
    },
    {
      "epoch": 531.1678832116788,
      "grad_norm": 9.181102752685547,
      "learning_rate": 2.3441605839416057e-05,
      "loss": 1.3792,
      "step": 72770
    },
    {
      "epoch": 531.2408759124088,
      "grad_norm": 20.825075149536133,
      "learning_rate": 2.3437956204379565e-05,
      "loss": 0.8884,
      "step": 72780
    },
    {
      "epoch": 531.3138686131387,
      "grad_norm": 12.224471092224121,
      "learning_rate": 2.343430656934307e-05,
      "loss": 0.5275,
      "step": 72790
    },
    {
      "epoch": 531.3868613138686,
      "grad_norm": 6.520573139190674,
      "learning_rate": 2.3430656934306572e-05,
      "loss": 1.1283,
      "step": 72800
    },
    {
      "epoch": 531.4598540145986,
      "grad_norm": 17.777597427368164,
      "learning_rate": 2.3427007299270073e-05,
      "loss": 1.2125,
      "step": 72810
    },
    {
      "epoch": 531.5328467153284,
      "grad_norm": 8.349893569946289,
      "learning_rate": 2.3423357664233577e-05,
      "loss": 0.8152,
      "step": 72820
    },
    {
      "epoch": 531.6058394160584,
      "grad_norm": 9.511832237243652,
      "learning_rate": 2.341970802919708e-05,
      "loss": 1.0527,
      "step": 72830
    },
    {
      "epoch": 531.6788321167883,
      "grad_norm": 11.032705307006836,
      "learning_rate": 2.3416058394160585e-05,
      "loss": 0.7947,
      "step": 72840
    },
    {
      "epoch": 531.7518248175182,
      "grad_norm": 7.5285725593566895,
      "learning_rate": 2.341240875912409e-05,
      "loss": 0.796,
      "step": 72850
    },
    {
      "epoch": 531.8248175182482,
      "grad_norm": 6.38127326965332,
      "learning_rate": 2.3408759124087592e-05,
      "loss": 0.9864,
      "step": 72860
    },
    {
      "epoch": 531.8978102189781,
      "grad_norm": 0.05626070871949196,
      "learning_rate": 2.3405109489051096e-05,
      "loss": 0.9461,
      "step": 72870
    },
    {
      "epoch": 531.970802919708,
      "grad_norm": 12.119592666625977,
      "learning_rate": 2.34014598540146e-05,
      "loss": 0.8154,
      "step": 72880
    },
    {
      "epoch": 532.043795620438,
      "grad_norm": 8.264153480529785,
      "learning_rate": 2.3397810218978104e-05,
      "loss": 0.8247,
      "step": 72890
    },
    {
      "epoch": 532.1167883211679,
      "grad_norm": 13.05923080444336,
      "learning_rate": 2.3394160583941608e-05,
      "loss": 0.8223,
      "step": 72900
    },
    {
      "epoch": 532.1897810218978,
      "grad_norm": 0.04170461371541023,
      "learning_rate": 2.3390510948905112e-05,
      "loss": 1.0508,
      "step": 72910
    },
    {
      "epoch": 532.2627737226277,
      "grad_norm": 13.610091209411621,
      "learning_rate": 2.3386861313868612e-05,
      "loss": 0.5307,
      "step": 72920
    },
    {
      "epoch": 532.3357664233577,
      "grad_norm": 8.02018928527832,
      "learning_rate": 2.3383211678832116e-05,
      "loss": 0.9619,
      "step": 72930
    },
    {
      "epoch": 532.4087591240876,
      "grad_norm": 0.0674547553062439,
      "learning_rate": 2.3379562043795623e-05,
      "loss": 0.8385,
      "step": 72940
    },
    {
      "epoch": 532.4817518248175,
      "grad_norm": 6.840001106262207,
      "learning_rate": 2.3375912408759127e-05,
      "loss": 0.9749,
      "step": 72950
    },
    {
      "epoch": 532.5547445255474,
      "grad_norm": 13.752915382385254,
      "learning_rate": 2.3372262773722628e-05,
      "loss": 0.7761,
      "step": 72960
    },
    {
      "epoch": 532.6277372262774,
      "grad_norm": 9.795064926147461,
      "learning_rate": 2.336861313868613e-05,
      "loss": 1.0032,
      "step": 72970
    },
    {
      "epoch": 532.7007299270073,
      "grad_norm": 15.561291694641113,
      "learning_rate": 2.3364963503649635e-05,
      "loss": 0.9956,
      "step": 72980
    },
    {
      "epoch": 532.7737226277372,
      "grad_norm": 7.614817142486572,
      "learning_rate": 2.3361313868613143e-05,
      "loss": 1.0258,
      "step": 72990
    },
    {
      "epoch": 532.8467153284671,
      "grad_norm": 4.6707868576049805,
      "learning_rate": 2.3357664233576643e-05,
      "loss": 0.8311,
      "step": 73000
    },
    {
      "epoch": 532.9197080291971,
      "grad_norm": 5.298271656036377,
      "learning_rate": 2.3354014598540147e-05,
      "loss": 0.9367,
      "step": 73010
    },
    {
      "epoch": 532.992700729927,
      "grad_norm": 5.731079578399658,
      "learning_rate": 2.335036496350365e-05,
      "loss": 0.9104,
      "step": 73020
    },
    {
      "epoch": 533.0656934306569,
      "grad_norm": 10.360772132873535,
      "learning_rate": 2.334671532846715e-05,
      "loss": 1.2551,
      "step": 73030
    },
    {
      "epoch": 533.1386861313869,
      "grad_norm": 7.506418704986572,
      "learning_rate": 2.334306569343066e-05,
      "loss": 0.919,
      "step": 73040
    },
    {
      "epoch": 533.2116788321168,
      "grad_norm": 7.304988861083984,
      "learning_rate": 2.3339416058394163e-05,
      "loss": 0.7636,
      "step": 73050
    },
    {
      "epoch": 533.2846715328467,
      "grad_norm": 7.680182933807373,
      "learning_rate": 2.3335766423357667e-05,
      "loss": 1.0472,
      "step": 73060
    },
    {
      "epoch": 533.3576642335767,
      "grad_norm": 11.435808181762695,
      "learning_rate": 2.3332116788321167e-05,
      "loss": 0.5787,
      "step": 73070
    },
    {
      "epoch": 533.4306569343066,
      "grad_norm": 0.09928563237190247,
      "learning_rate": 2.332846715328467e-05,
      "loss": 0.772,
      "step": 73080
    },
    {
      "epoch": 533.5036496350365,
      "grad_norm": 9.887686729431152,
      "learning_rate": 2.3324817518248178e-05,
      "loss": 1.1747,
      "step": 73090
    },
    {
      "epoch": 533.5766423357665,
      "grad_norm": 12.40485954284668,
      "learning_rate": 2.3321167883211682e-05,
      "loss": 0.8654,
      "step": 73100
    },
    {
      "epoch": 533.6496350364963,
      "grad_norm": 0.0727234035730362,
      "learning_rate": 2.3317518248175183e-05,
      "loss": 0.8077,
      "step": 73110
    },
    {
      "epoch": 533.7226277372263,
      "grad_norm": 7.942754745483398,
      "learning_rate": 2.3313868613138686e-05,
      "loss": 0.7444,
      "step": 73120
    },
    {
      "epoch": 533.7956204379562,
      "grad_norm": 19.953353881835938,
      "learning_rate": 2.331021897810219e-05,
      "loss": 1.2672,
      "step": 73130
    },
    {
      "epoch": 533.8686131386861,
      "grad_norm": 0.05487272888422012,
      "learning_rate": 2.3306569343065694e-05,
      "loss": 0.7622,
      "step": 73140
    },
    {
      "epoch": 533.9416058394161,
      "grad_norm": 9.770801544189453,
      "learning_rate": 2.3302919708029198e-05,
      "loss": 0.8179,
      "step": 73150
    },
    {
      "epoch": 534.014598540146,
      "grad_norm": 0.05431682616472244,
      "learning_rate": 2.3299270072992702e-05,
      "loss": 0.7853,
      "step": 73160
    },
    {
      "epoch": 534.0875912408759,
      "grad_norm": 13.81532096862793,
      "learning_rate": 2.3295620437956206e-05,
      "loss": 0.7821,
      "step": 73170
    },
    {
      "epoch": 534.1605839416059,
      "grad_norm": 10.3368558883667,
      "learning_rate": 2.329197080291971e-05,
      "loss": 0.9411,
      "step": 73180
    },
    {
      "epoch": 534.2335766423357,
      "grad_norm": 13.679413795471191,
      "learning_rate": 2.3288321167883214e-05,
      "loss": 0.7859,
      "step": 73190
    },
    {
      "epoch": 534.3065693430657,
      "grad_norm": 8.384293556213379,
      "learning_rate": 2.3284671532846718e-05,
      "loss": 0.9683,
      "step": 73200
    },
    {
      "epoch": 534.3795620437957,
      "grad_norm": 12.216080665588379,
      "learning_rate": 2.328102189781022e-05,
      "loss": 0.9824,
      "step": 73210
    },
    {
      "epoch": 534.4525547445255,
      "grad_norm": 11.549029350280762,
      "learning_rate": 2.3277372262773725e-05,
      "loss": 1.0729,
      "step": 73220
    },
    {
      "epoch": 534.5255474452555,
      "grad_norm": 1.9995592832565308,
      "learning_rate": 2.3273722627737226e-05,
      "loss": 1.0178,
      "step": 73230
    },
    {
      "epoch": 534.5985401459855,
      "grad_norm": 8.499357223510742,
      "learning_rate": 2.327007299270073e-05,
      "loss": 0.7273,
      "step": 73240
    },
    {
      "epoch": 534.6715328467153,
      "grad_norm": 5.774085998535156,
      "learning_rate": 2.3266423357664237e-05,
      "loss": 0.6726,
      "step": 73250
    },
    {
      "epoch": 534.7445255474453,
      "grad_norm": 10.83448600769043,
      "learning_rate": 2.3262773722627737e-05,
      "loss": 0.9562,
      "step": 73260
    },
    {
      "epoch": 534.8175182481751,
      "grad_norm": 7.661673545837402,
      "learning_rate": 2.325912408759124e-05,
      "loss": 0.8304,
      "step": 73270
    },
    {
      "epoch": 534.8905109489051,
      "grad_norm": 0.10275661945343018,
      "learning_rate": 2.3255474452554745e-05,
      "loss": 0.9967,
      "step": 73280
    },
    {
      "epoch": 534.9635036496351,
      "grad_norm": 13.453197479248047,
      "learning_rate": 2.325182481751825e-05,
      "loss": 1.0339,
      "step": 73290
    },
    {
      "epoch": 535.0364963503649,
      "grad_norm": 5.250988960266113,
      "learning_rate": 2.3248175182481753e-05,
      "loss": 1.0575,
      "step": 73300
    },
    {
      "epoch": 535.1094890510949,
      "grad_norm": 5.909421920776367,
      "learning_rate": 2.3244525547445257e-05,
      "loss": 0.6277,
      "step": 73310
    },
    {
      "epoch": 535.1824817518249,
      "grad_norm": 18.87265396118164,
      "learning_rate": 2.324087591240876e-05,
      "loss": 1.3017,
      "step": 73320
    },
    {
      "epoch": 535.2554744525547,
      "grad_norm": 7.537862300872803,
      "learning_rate": 2.3237226277372265e-05,
      "loss": 0.5731,
      "step": 73330
    },
    {
      "epoch": 535.3284671532847,
      "grad_norm": 18.656587600708008,
      "learning_rate": 2.3233576642335765e-05,
      "loss": 1.4361,
      "step": 73340
    },
    {
      "epoch": 535.4014598540145,
      "grad_norm": 0.12530405819416046,
      "learning_rate": 2.3229927007299272e-05,
      "loss": 0.9628,
      "step": 73350
    },
    {
      "epoch": 535.4744525547445,
      "grad_norm": 0.049195919185876846,
      "learning_rate": 2.3226277372262776e-05,
      "loss": 0.612,
      "step": 73360
    },
    {
      "epoch": 535.5474452554745,
      "grad_norm": 0.07527711987495422,
      "learning_rate": 2.322262773722628e-05,
      "loss": 0.6224,
      "step": 73370
    },
    {
      "epoch": 535.6204379562043,
      "grad_norm": 7.117202281951904,
      "learning_rate": 2.321897810218978e-05,
      "loss": 0.7376,
      "step": 73380
    },
    {
      "epoch": 535.6934306569343,
      "grad_norm": 7.628276824951172,
      "learning_rate": 2.3215328467153285e-05,
      "loss": 0.8012,
      "step": 73390
    },
    {
      "epoch": 535.7664233576643,
      "grad_norm": 6.004244327545166,
      "learning_rate": 2.321167883211679e-05,
      "loss": 0.9409,
      "step": 73400
    },
    {
      "epoch": 535.8394160583941,
      "grad_norm": 7.255771636962891,
      "learning_rate": 2.3208029197080296e-05,
      "loss": 1.0148,
      "step": 73410
    },
    {
      "epoch": 535.9124087591241,
      "grad_norm": 11.442602157592773,
      "learning_rate": 2.3204379562043796e-05,
      "loss": 0.9452,
      "step": 73420
    },
    {
      "epoch": 535.985401459854,
      "grad_norm": 16.650833129882812,
      "learning_rate": 2.32007299270073e-05,
      "loss": 1.0561,
      "step": 73430
    },
    {
      "epoch": 536.0583941605839,
      "grad_norm": 0.15382125973701477,
      "learning_rate": 2.3197080291970804e-05,
      "loss": 0.9832,
      "step": 73440
    },
    {
      "epoch": 536.1313868613139,
      "grad_norm": 12.61153507232666,
      "learning_rate": 2.3193430656934308e-05,
      "loss": 1.0039,
      "step": 73450
    },
    {
      "epoch": 536.2043795620438,
      "grad_norm": 9.683966636657715,
      "learning_rate": 2.318978102189781e-05,
      "loss": 0.8043,
      "step": 73460
    },
    {
      "epoch": 536.2773722627737,
      "grad_norm": 7.634881019592285,
      "learning_rate": 2.3186131386861316e-05,
      "loss": 0.7496,
      "step": 73470
    },
    {
      "epoch": 536.3503649635037,
      "grad_norm": 12.806840896606445,
      "learning_rate": 2.318248175182482e-05,
      "loss": 1.0311,
      "step": 73480
    },
    {
      "epoch": 536.4233576642335,
      "grad_norm": 7.184874057769775,
      "learning_rate": 2.317883211678832e-05,
      "loss": 0.8762,
      "step": 73490
    },
    {
      "epoch": 536.4963503649635,
      "grad_norm": 0.08597195893526077,
      "learning_rate": 2.3175182481751824e-05,
      "loss": 0.6557,
      "step": 73500
    },
    {
      "epoch": 536.5693430656934,
      "grad_norm": 9.159985542297363,
      "learning_rate": 2.317153284671533e-05,
      "loss": 0.9436,
      "step": 73510
    },
    {
      "epoch": 536.6423357664233,
      "grad_norm": 5.470837116241455,
      "learning_rate": 2.3167883211678835e-05,
      "loss": 1.2523,
      "step": 73520
    },
    {
      "epoch": 536.7153284671533,
      "grad_norm": 0.03883111849427223,
      "learning_rate": 2.3164233576642335e-05,
      "loss": 0.9479,
      "step": 73530
    },
    {
      "epoch": 536.7883211678832,
      "grad_norm": 7.476873397827148,
      "learning_rate": 2.316058394160584e-05,
      "loss": 0.7739,
      "step": 73540
    },
    {
      "epoch": 536.8613138686131,
      "grad_norm": 6.909102439880371,
      "learning_rate": 2.3156934306569343e-05,
      "loss": 1.092,
      "step": 73550
    },
    {
      "epoch": 536.9343065693431,
      "grad_norm": 5.373532772064209,
      "learning_rate": 2.315328467153285e-05,
      "loss": 0.9473,
      "step": 73560
    },
    {
      "epoch": 537.007299270073,
      "grad_norm": 6.759096622467041,
      "learning_rate": 2.314963503649635e-05,
      "loss": 0.8943,
      "step": 73570
    },
    {
      "epoch": 537.0802919708029,
      "grad_norm": 10.686454772949219,
      "learning_rate": 2.3145985401459855e-05,
      "loss": 0.7041,
      "step": 73580
    },
    {
      "epoch": 537.1532846715329,
      "grad_norm": 5.549527645111084,
      "learning_rate": 2.314233576642336e-05,
      "loss": 1.0141,
      "step": 73590
    },
    {
      "epoch": 537.2262773722628,
      "grad_norm": 13.967535018920898,
      "learning_rate": 2.3138686131386863e-05,
      "loss": 0.6702,
      "step": 73600
    },
    {
      "epoch": 537.2992700729927,
      "grad_norm": 8.180344581604004,
      "learning_rate": 2.3135036496350367e-05,
      "loss": 1.1215,
      "step": 73610
    },
    {
      "epoch": 537.3722627737226,
      "grad_norm": 4.771588325500488,
      "learning_rate": 2.313138686131387e-05,
      "loss": 0.7863,
      "step": 73620
    },
    {
      "epoch": 537.4452554744526,
      "grad_norm": 0.029867535457015038,
      "learning_rate": 2.3127737226277374e-05,
      "loss": 0.9257,
      "step": 73630
    },
    {
      "epoch": 537.5182481751825,
      "grad_norm": 12.013813018798828,
      "learning_rate": 2.3124087591240878e-05,
      "loss": 1.1363,
      "step": 73640
    },
    {
      "epoch": 537.5912408759124,
      "grad_norm": 16.590839385986328,
      "learning_rate": 2.312043795620438e-05,
      "loss": 0.9547,
      "step": 73650
    },
    {
      "epoch": 537.6642335766423,
      "grad_norm": 10.44417953491211,
      "learning_rate": 2.3116788321167886e-05,
      "loss": 0.9105,
      "step": 73660
    },
    {
      "epoch": 537.7372262773723,
      "grad_norm": 10.497529983520508,
      "learning_rate": 2.311313868613139e-05,
      "loss": 0.8739,
      "step": 73670
    },
    {
      "epoch": 537.8102189781022,
      "grad_norm": 8.258048057556152,
      "learning_rate": 2.310948905109489e-05,
      "loss": 0.9029,
      "step": 73680
    },
    {
      "epoch": 537.8832116788321,
      "grad_norm": 9.413956642150879,
      "learning_rate": 2.3105839416058394e-05,
      "loss": 0.8365,
      "step": 73690
    },
    {
      "epoch": 537.956204379562,
      "grad_norm": 0.1646154820919037,
      "learning_rate": 2.3102189781021898e-05,
      "loss": 0.9696,
      "step": 73700
    },
    {
      "epoch": 538.029197080292,
      "grad_norm": 4.1764302253723145,
      "learning_rate": 2.3098540145985402e-05,
      "loss": 0.7625,
      "step": 73710
    },
    {
      "epoch": 538.1021897810219,
      "grad_norm": 15.038145065307617,
      "learning_rate": 2.3094890510948906e-05,
      "loss": 1.1113,
      "step": 73720
    },
    {
      "epoch": 538.1751824817518,
      "grad_norm": 10.763876914978027,
      "learning_rate": 2.309124087591241e-05,
      "loss": 1.1431,
      "step": 73730
    },
    {
      "epoch": 538.2481751824818,
      "grad_norm": 9.193193435668945,
      "learning_rate": 2.3087591240875914e-05,
      "loss": 0.6465,
      "step": 73740
    },
    {
      "epoch": 538.3211678832117,
      "grad_norm": 9.481351852416992,
      "learning_rate": 2.3083941605839417e-05,
      "loss": 1.1771,
      "step": 73750
    },
    {
      "epoch": 538.3941605839416,
      "grad_norm": 20.585676193237305,
      "learning_rate": 2.308029197080292e-05,
      "loss": 1.0263,
      "step": 73760
    },
    {
      "epoch": 538.4671532846716,
      "grad_norm": 6.432122230529785,
      "learning_rate": 2.3076642335766425e-05,
      "loss": 0.6201,
      "step": 73770
    },
    {
      "epoch": 538.5401459854014,
      "grad_norm": 7.013211727142334,
      "learning_rate": 2.307299270072993e-05,
      "loss": 0.5227,
      "step": 73780
    },
    {
      "epoch": 538.6131386861314,
      "grad_norm": 10.576530456542969,
      "learning_rate": 2.3069343065693433e-05,
      "loss": 1.3121,
      "step": 73790
    },
    {
      "epoch": 538.6861313868613,
      "grad_norm": 0.08661682158708572,
      "learning_rate": 2.3065693430656934e-05,
      "loss": 0.7121,
      "step": 73800
    },
    {
      "epoch": 538.7591240875912,
      "grad_norm": 0.06549079716205597,
      "learning_rate": 2.3062043795620437e-05,
      "loss": 1.0827,
      "step": 73810
    },
    {
      "epoch": 538.8321167883212,
      "grad_norm": 14.017053604125977,
      "learning_rate": 2.3058394160583945e-05,
      "loss": 0.5729,
      "step": 73820
    },
    {
      "epoch": 538.9051094890511,
      "grad_norm": 7.383330821990967,
      "learning_rate": 2.305474452554745e-05,
      "loss": 1.0269,
      "step": 73830
    },
    {
      "epoch": 538.978102189781,
      "grad_norm": 5.582099914550781,
      "learning_rate": 2.305109489051095e-05,
      "loss": 1.1562,
      "step": 73840
    },
    {
      "epoch": 539.051094890511,
      "grad_norm": 7.711668491363525,
      "learning_rate": 2.3047445255474453e-05,
      "loss": 0.9379,
      "step": 73850
    },
    {
      "epoch": 539.1240875912408,
      "grad_norm": 13.629592895507812,
      "learning_rate": 2.3043795620437957e-05,
      "loss": 1.1738,
      "step": 73860
    },
    {
      "epoch": 539.1970802919708,
      "grad_norm": 7.71968936920166,
      "learning_rate": 2.304014598540146e-05,
      "loss": 0.9096,
      "step": 73870
    },
    {
      "epoch": 539.2700729927008,
      "grad_norm": 0.08809421956539154,
      "learning_rate": 2.3036496350364965e-05,
      "loss": 0.9188,
      "step": 73880
    },
    {
      "epoch": 539.3430656934306,
      "grad_norm": 5.006694316864014,
      "learning_rate": 2.303284671532847e-05,
      "loss": 0.9536,
      "step": 73890
    },
    {
      "epoch": 539.4160583941606,
      "grad_norm": 18.723430633544922,
      "learning_rate": 2.3029197080291972e-05,
      "loss": 0.993,
      "step": 73900
    },
    {
      "epoch": 539.4890510948906,
      "grad_norm": 0.5199089050292969,
      "learning_rate": 2.3025547445255473e-05,
      "loss": 0.8369,
      "step": 73910
    },
    {
      "epoch": 539.5620437956204,
      "grad_norm": 0.5195624232292175,
      "learning_rate": 2.302189781021898e-05,
      "loss": 1.0003,
      "step": 73920
    },
    {
      "epoch": 539.6350364963504,
      "grad_norm": 0.0718241035938263,
      "learning_rate": 2.3018248175182484e-05,
      "loss": 0.5909,
      "step": 73930
    },
    {
      "epoch": 539.7080291970802,
      "grad_norm": 7.965206623077393,
      "learning_rate": 2.3014598540145988e-05,
      "loss": 0.8244,
      "step": 73940
    },
    {
      "epoch": 539.7810218978102,
      "grad_norm": 8.805458068847656,
      "learning_rate": 2.301094890510949e-05,
      "loss": 0.9174,
      "step": 73950
    },
    {
      "epoch": 539.8540145985402,
      "grad_norm": 0.02899979054927826,
      "learning_rate": 2.3007299270072992e-05,
      "loss": 1.0064,
      "step": 73960
    },
    {
      "epoch": 539.92700729927,
      "grad_norm": 5.524941444396973,
      "learning_rate": 2.3003649635036496e-05,
      "loss": 0.5903,
      "step": 73970
    },
    {
      "epoch": 540.0,
      "grad_norm": 0.17172059416770935,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.9359,
      "step": 73980
    },
    {
      "epoch": 540.07299270073,
      "grad_norm": 17.27676773071289,
      "learning_rate": 2.2996350364963504e-05,
      "loss": 0.7521,
      "step": 73990
    },
    {
      "epoch": 540.1459854014598,
      "grad_norm": 16.913372039794922,
      "learning_rate": 2.2992700729927008e-05,
      "loss": 0.7378,
      "step": 74000
    },
    {
      "epoch": 540.2189781021898,
      "grad_norm": 9.868246078491211,
      "learning_rate": 2.298905109489051e-05,
      "loss": 1.1591,
      "step": 74010
    },
    {
      "epoch": 540.2919708029198,
      "grad_norm": 10.494525909423828,
      "learning_rate": 2.2985401459854016e-05,
      "loss": 0.7962,
      "step": 74020
    },
    {
      "epoch": 540.3649635036496,
      "grad_norm": 7.198845386505127,
      "learning_rate": 2.298175182481752e-05,
      "loss": 1.042,
      "step": 74030
    },
    {
      "epoch": 540.4379562043796,
      "grad_norm": 12.652178764343262,
      "learning_rate": 2.2978102189781023e-05,
      "loss": 0.8581,
      "step": 74040
    },
    {
      "epoch": 540.5109489051094,
      "grad_norm": 7.885515213012695,
      "learning_rate": 2.2974452554744527e-05,
      "loss": 1.0212,
      "step": 74050
    },
    {
      "epoch": 540.5839416058394,
      "grad_norm": 0.7133910655975342,
      "learning_rate": 2.297080291970803e-05,
      "loss": 0.8257,
      "step": 74060
    },
    {
      "epoch": 540.6569343065694,
      "grad_norm": 7.002625465393066,
      "learning_rate": 2.296715328467153e-05,
      "loss": 1.0991,
      "step": 74070
    },
    {
      "epoch": 540.7299270072992,
      "grad_norm": 5.42224645614624,
      "learning_rate": 2.296350364963504e-05,
      "loss": 0.9401,
      "step": 74080
    },
    {
      "epoch": 540.8029197080292,
      "grad_norm": 9.834915161132812,
      "learning_rate": 2.2959854014598543e-05,
      "loss": 0.7332,
      "step": 74090
    },
    {
      "epoch": 540.8759124087592,
      "grad_norm": 9.881388664245605,
      "learning_rate": 2.2956204379562047e-05,
      "loss": 1.1298,
      "step": 74100
    },
    {
      "epoch": 540.948905109489,
      "grad_norm": 0.07794387638568878,
      "learning_rate": 2.2952554744525547e-05,
      "loss": 0.7303,
      "step": 74110
    },
    {
      "epoch": 541.021897810219,
      "grad_norm": 14.605913162231445,
      "learning_rate": 2.294890510948905e-05,
      "loss": 0.9331,
      "step": 74120
    },
    {
      "epoch": 541.0948905109489,
      "grad_norm": 11.564663887023926,
      "learning_rate": 2.2945255474452558e-05,
      "loss": 1.1261,
      "step": 74130
    },
    {
      "epoch": 541.1678832116788,
      "grad_norm": 19.770343780517578,
      "learning_rate": 2.294160583941606e-05,
      "loss": 0.9846,
      "step": 74140
    },
    {
      "epoch": 541.2408759124088,
      "grad_norm": 10.069591522216797,
      "learning_rate": 2.2937956204379563e-05,
      "loss": 0.9666,
      "step": 74150
    },
    {
      "epoch": 541.3138686131387,
      "grad_norm": 8.835832595825195,
      "learning_rate": 2.2934306569343067e-05,
      "loss": 1.0162,
      "step": 74160
    },
    {
      "epoch": 541.3868613138686,
      "grad_norm": 6.833882808685303,
      "learning_rate": 2.293065693430657e-05,
      "loss": 0.656,
      "step": 74170
    },
    {
      "epoch": 541.4598540145986,
      "grad_norm": 11.807639122009277,
      "learning_rate": 2.2927007299270074e-05,
      "loss": 0.7418,
      "step": 74180
    },
    {
      "epoch": 541.5328467153284,
      "grad_norm": 2.522160768508911,
      "learning_rate": 2.2923357664233578e-05,
      "loss": 0.8301,
      "step": 74190
    },
    {
      "epoch": 541.6058394160584,
      "grad_norm": 17.645967483520508,
      "learning_rate": 2.2919708029197082e-05,
      "loss": 1.0704,
      "step": 74200
    },
    {
      "epoch": 541.6788321167883,
      "grad_norm": 16.939199447631836,
      "learning_rate": 2.2916058394160586e-05,
      "loss": 1.0315,
      "step": 74210
    },
    {
      "epoch": 541.7518248175182,
      "grad_norm": 11.286853790283203,
      "learning_rate": 2.2912408759124086e-05,
      "loss": 0.7681,
      "step": 74220
    },
    {
      "epoch": 541.8248175182482,
      "grad_norm": 9.940200805664062,
      "learning_rate": 2.2908759124087594e-05,
      "loss": 1.0573,
      "step": 74230
    },
    {
      "epoch": 541.8978102189781,
      "grad_norm": 0.08981010317802429,
      "learning_rate": 2.2905109489051098e-05,
      "loss": 0.928,
      "step": 74240
    },
    {
      "epoch": 541.970802919708,
      "grad_norm": 11.364899635314941,
      "learning_rate": 2.29014598540146e-05,
      "loss": 0.6364,
      "step": 74250
    },
    {
      "epoch": 542.043795620438,
      "grad_norm": 14.29438591003418,
      "learning_rate": 2.2897810218978102e-05,
      "loss": 1.0237,
      "step": 74260
    },
    {
      "epoch": 542.1167883211679,
      "grad_norm": 12.864529609680176,
      "learning_rate": 2.2894160583941606e-05,
      "loss": 1.2666,
      "step": 74270
    },
    {
      "epoch": 542.1897810218978,
      "grad_norm": 13.080215454101562,
      "learning_rate": 2.289051094890511e-05,
      "loss": 1.1172,
      "step": 74280
    },
    {
      "epoch": 542.2627737226277,
      "grad_norm": 13.619638442993164,
      "learning_rate": 2.2886861313868617e-05,
      "loss": 1.0559,
      "step": 74290
    },
    {
      "epoch": 542.3357664233577,
      "grad_norm": 6.525155067443848,
      "learning_rate": 2.2883211678832117e-05,
      "loss": 0.6331,
      "step": 74300
    },
    {
      "epoch": 542.4087591240876,
      "grad_norm": 10.295135498046875,
      "learning_rate": 2.287956204379562e-05,
      "loss": 1.0528,
      "step": 74310
    },
    {
      "epoch": 542.4817518248175,
      "grad_norm": 15.021711349487305,
      "learning_rate": 2.2875912408759125e-05,
      "loss": 0.9139,
      "step": 74320
    },
    {
      "epoch": 542.5547445255474,
      "grad_norm": 9.732954978942871,
      "learning_rate": 2.287226277372263e-05,
      "loss": 0.8821,
      "step": 74330
    },
    {
      "epoch": 542.6277372262774,
      "grad_norm": 9.312994956970215,
      "learning_rate": 2.2868613138686133e-05,
      "loss": 0.9013,
      "step": 74340
    },
    {
      "epoch": 542.7007299270073,
      "grad_norm": 12.394150733947754,
      "learning_rate": 2.2864963503649637e-05,
      "loss": 0.8004,
      "step": 74350
    },
    {
      "epoch": 542.7737226277372,
      "grad_norm": 12.907817840576172,
      "learning_rate": 2.286131386861314e-05,
      "loss": 0.7323,
      "step": 74360
    },
    {
      "epoch": 542.8467153284671,
      "grad_norm": 0.0742378979921341,
      "learning_rate": 2.285766423357664e-05,
      "loss": 1.2044,
      "step": 74370
    },
    {
      "epoch": 542.9197080291971,
      "grad_norm": 2.779392957687378,
      "learning_rate": 2.2854014598540145e-05,
      "loss": 0.6183,
      "step": 74380
    },
    {
      "epoch": 542.992700729927,
      "grad_norm": 15.508357048034668,
      "learning_rate": 2.2850364963503652e-05,
      "loss": 0.6641,
      "step": 74390
    },
    {
      "epoch": 543.0656934306569,
      "grad_norm": 12.370160102844238,
      "learning_rate": 2.2846715328467156e-05,
      "loss": 0.6138,
      "step": 74400
    },
    {
      "epoch": 543.1386861313869,
      "grad_norm": 10.27769947052002,
      "learning_rate": 2.2843065693430657e-05,
      "loss": 0.8672,
      "step": 74410
    },
    {
      "epoch": 543.2116788321168,
      "grad_norm": 3.0402679443359375,
      "learning_rate": 2.283941605839416e-05,
      "loss": 1.2491,
      "step": 74420
    },
    {
      "epoch": 543.2846715328467,
      "grad_norm": 10.832863807678223,
      "learning_rate": 2.2835766423357665e-05,
      "loss": 0.6882,
      "step": 74430
    },
    {
      "epoch": 543.3576642335767,
      "grad_norm": 15.138501167297363,
      "learning_rate": 2.283211678832117e-05,
      "loss": 0.9024,
      "step": 74440
    },
    {
      "epoch": 543.4306569343066,
      "grad_norm": 5.793277740478516,
      "learning_rate": 2.2828467153284672e-05,
      "loss": 0.844,
      "step": 74450
    },
    {
      "epoch": 543.5036496350365,
      "grad_norm": 9.530014991760254,
      "learning_rate": 2.2824817518248176e-05,
      "loss": 0.7901,
      "step": 74460
    },
    {
      "epoch": 543.5766423357665,
      "grad_norm": 7.283578872680664,
      "learning_rate": 2.282116788321168e-05,
      "loss": 0.6812,
      "step": 74470
    },
    {
      "epoch": 543.6496350364963,
      "grad_norm": 13.284475326538086,
      "learning_rate": 2.2817518248175184e-05,
      "loss": 1.3514,
      "step": 74480
    },
    {
      "epoch": 543.7226277372263,
      "grad_norm": 12.346772193908691,
      "learning_rate": 2.2813868613138688e-05,
      "loss": 1.2095,
      "step": 74490
    },
    {
      "epoch": 543.7956204379562,
      "grad_norm": 6.374217987060547,
      "learning_rate": 2.2810218978102192e-05,
      "loss": 0.6601,
      "step": 74500
    },
    {
      "epoch": 543.8686131386861,
      "grad_norm": 7.977912425994873,
      "learning_rate": 2.2806569343065696e-05,
      "loss": 1.2496,
      "step": 74510
    },
    {
      "epoch": 543.9416058394161,
      "grad_norm": 15.77871322631836,
      "learning_rate": 2.28029197080292e-05,
      "loss": 0.9108,
      "step": 74520
    },
    {
      "epoch": 544.014598540146,
      "grad_norm": 17.162965774536133,
      "learning_rate": 2.27992700729927e-05,
      "loss": 1.1252,
      "step": 74530
    },
    {
      "epoch": 544.0875912408759,
      "grad_norm": 11.790789604187012,
      "learning_rate": 2.2795620437956204e-05,
      "loss": 0.7868,
      "step": 74540
    },
    {
      "epoch": 544.1605839416059,
      "grad_norm": 11.455639839172363,
      "learning_rate": 2.279197080291971e-05,
      "loss": 0.7993,
      "step": 74550
    },
    {
      "epoch": 544.2335766423357,
      "grad_norm": 7.004872798919678,
      "learning_rate": 2.278832116788321e-05,
      "loss": 1.0924,
      "step": 74560
    },
    {
      "epoch": 544.3065693430657,
      "grad_norm": 6.506970405578613,
      "learning_rate": 2.2784671532846716e-05,
      "loss": 0.5738,
      "step": 74570
    },
    {
      "epoch": 544.3795620437957,
      "grad_norm": 14.007493019104004,
      "learning_rate": 2.278102189781022e-05,
      "loss": 0.8921,
      "step": 74580
    },
    {
      "epoch": 544.4525547445255,
      "grad_norm": 12.978992462158203,
      "learning_rate": 2.2777372262773723e-05,
      "loss": 0.9043,
      "step": 74590
    },
    {
      "epoch": 544.5255474452555,
      "grad_norm": 7.586548805236816,
      "learning_rate": 2.2773722627737227e-05,
      "loss": 1.1182,
      "step": 74600
    },
    {
      "epoch": 544.5985401459855,
      "grad_norm": 11.32745361328125,
      "learning_rate": 2.277007299270073e-05,
      "loss": 0.703,
      "step": 74610
    },
    {
      "epoch": 544.6715328467153,
      "grad_norm": 7.760513782501221,
      "learning_rate": 2.2766423357664235e-05,
      "loss": 0.6249,
      "step": 74620
    },
    {
      "epoch": 544.7445255474453,
      "grad_norm": 15.273425102233887,
      "learning_rate": 2.276277372262774e-05,
      "loss": 1.246,
      "step": 74630
    },
    {
      "epoch": 544.8175182481751,
      "grad_norm": 7.1763715744018555,
      "learning_rate": 2.275912408759124e-05,
      "loss": 0.9638,
      "step": 74640
    },
    {
      "epoch": 544.8905109489051,
      "grad_norm": 8.09361457824707,
      "learning_rate": 2.2755474452554747e-05,
      "loss": 0.8532,
      "step": 74650
    },
    {
      "epoch": 544.9635036496351,
      "grad_norm": 19.875028610229492,
      "learning_rate": 2.275182481751825e-05,
      "loss": 0.9035,
      "step": 74660
    },
    {
      "epoch": 545.0364963503649,
      "grad_norm": 8.892542839050293,
      "learning_rate": 2.2748175182481754e-05,
      "loss": 0.8562,
      "step": 74670
    },
    {
      "epoch": 545.1094890510949,
      "grad_norm": 6.012356281280518,
      "learning_rate": 2.2744525547445255e-05,
      "loss": 0.8714,
      "step": 74680
    },
    {
      "epoch": 545.1824817518249,
      "grad_norm": 12.202622413635254,
      "learning_rate": 2.274087591240876e-05,
      "loss": 0.7563,
      "step": 74690
    },
    {
      "epoch": 545.2554744525547,
      "grad_norm": 8.654878616333008,
      "learning_rate": 2.2737226277372266e-05,
      "loss": 0.4661,
      "step": 74700
    },
    {
      "epoch": 545.3284671532847,
      "grad_norm": 9.72990894317627,
      "learning_rate": 2.273357664233577e-05,
      "loss": 1.0596,
      "step": 74710
    },
    {
      "epoch": 545.4014598540145,
      "grad_norm": 8.428438186645508,
      "learning_rate": 2.272992700729927e-05,
      "loss": 0.914,
      "step": 74720
    },
    {
      "epoch": 545.4744525547445,
      "grad_norm": 7.596436500549316,
      "learning_rate": 2.2726277372262774e-05,
      "loss": 0.9205,
      "step": 74730
    },
    {
      "epoch": 545.5474452554745,
      "grad_norm": 6.3465800285339355,
      "learning_rate": 2.2722627737226278e-05,
      "loss": 0.6784,
      "step": 74740
    },
    {
      "epoch": 545.6204379562043,
      "grad_norm": 9.822013854980469,
      "learning_rate": 2.2718978102189782e-05,
      "loss": 0.9202,
      "step": 74750
    },
    {
      "epoch": 545.6934306569343,
      "grad_norm": 13.251330375671387,
      "learning_rate": 2.2715328467153286e-05,
      "loss": 0.8952,
      "step": 74760
    },
    {
      "epoch": 545.7664233576643,
      "grad_norm": 7.0358476638793945,
      "learning_rate": 2.271167883211679e-05,
      "loss": 1.0364,
      "step": 74770
    },
    {
      "epoch": 545.8394160583941,
      "grad_norm": 0.07094478607177734,
      "learning_rate": 2.2708029197080294e-05,
      "loss": 1.3801,
      "step": 74780
    },
    {
      "epoch": 545.9124087591241,
      "grad_norm": 12.614720344543457,
      "learning_rate": 2.2704379562043794e-05,
      "loss": 0.7032,
      "step": 74790
    },
    {
      "epoch": 545.985401459854,
      "grad_norm": 0.07732158899307251,
      "learning_rate": 2.27007299270073e-05,
      "loss": 1.293,
      "step": 74800
    },
    {
      "epoch": 546.0583941605839,
      "grad_norm": 5.199324607849121,
      "learning_rate": 2.2697080291970805e-05,
      "loss": 0.992,
      "step": 74810
    },
    {
      "epoch": 546.1313868613139,
      "grad_norm": 0.06707894057035446,
      "learning_rate": 2.269343065693431e-05,
      "loss": 0.8617,
      "step": 74820
    },
    {
      "epoch": 546.2043795620438,
      "grad_norm": 8.790858268737793,
      "learning_rate": 2.268978102189781e-05,
      "loss": 0.7982,
      "step": 74830
    },
    {
      "epoch": 546.2773722627737,
      "grad_norm": 12.574578285217285,
      "learning_rate": 2.2686131386861314e-05,
      "loss": 0.7565,
      "step": 74840
    },
    {
      "epoch": 546.3503649635037,
      "grad_norm": 4.950042247772217,
      "learning_rate": 2.2682481751824817e-05,
      "loss": 0.8155,
      "step": 74850
    },
    {
      "epoch": 546.4233576642335,
      "grad_norm": 7.227537631988525,
      "learning_rate": 2.2678832116788325e-05,
      "loss": 0.6326,
      "step": 74860
    },
    {
      "epoch": 546.4963503649635,
      "grad_norm": 7.506039142608643,
      "learning_rate": 2.2675182481751825e-05,
      "loss": 0.8492,
      "step": 74870
    },
    {
      "epoch": 546.5693430656934,
      "grad_norm": 16.524063110351562,
      "learning_rate": 2.267153284671533e-05,
      "loss": 1.0353,
      "step": 74880
    },
    {
      "epoch": 546.6423357664233,
      "grad_norm": 0.12542706727981567,
      "learning_rate": 2.2667883211678833e-05,
      "loss": 0.7944,
      "step": 74890
    },
    {
      "epoch": 546.7153284671533,
      "grad_norm": 11.633907318115234,
      "learning_rate": 2.2664233576642337e-05,
      "loss": 1.1739,
      "step": 74900
    },
    {
      "epoch": 546.7883211678832,
      "grad_norm": 9.724303245544434,
      "learning_rate": 2.266058394160584e-05,
      "loss": 1.0927,
      "step": 74910
    },
    {
      "epoch": 546.8613138686131,
      "grad_norm": 9.517210960388184,
      "learning_rate": 2.2656934306569345e-05,
      "loss": 0.5048,
      "step": 74920
    },
    {
      "epoch": 546.9343065693431,
      "grad_norm": 1.2215160131454468,
      "learning_rate": 2.265328467153285e-05,
      "loss": 1.1004,
      "step": 74930
    },
    {
      "epoch": 547.007299270073,
      "grad_norm": 0.04696078598499298,
      "learning_rate": 2.2649635036496352e-05,
      "loss": 1.1434,
      "step": 74940
    },
    {
      "epoch": 547.0802919708029,
      "grad_norm": 2.3538355827331543,
      "learning_rate": 2.2645985401459853e-05,
      "loss": 1.261,
      "step": 74950
    },
    {
      "epoch": 547.1532846715329,
      "grad_norm": 7.409821510314941,
      "learning_rate": 2.264233576642336e-05,
      "loss": 1.0986,
      "step": 74960
    },
    {
      "epoch": 547.2262773722628,
      "grad_norm": 9.170613288879395,
      "learning_rate": 2.2638686131386864e-05,
      "loss": 1.1122,
      "step": 74970
    },
    {
      "epoch": 547.2992700729927,
      "grad_norm": 9.424135208129883,
      "learning_rate": 2.2635036496350365e-05,
      "loss": 0.2723,
      "step": 74980
    },
    {
      "epoch": 547.3722627737226,
      "grad_norm": 6.442610740661621,
      "learning_rate": 2.263138686131387e-05,
      "loss": 0.9087,
      "step": 74990
    },
    {
      "epoch": 547.4452554744526,
      "grad_norm": 0.052021950483322144,
      "learning_rate": 2.2627737226277372e-05,
      "loss": 0.7869,
      "step": 75000
    },
    {
      "epoch": 547.5182481751825,
      "grad_norm": 13.473616600036621,
      "learning_rate": 2.2624087591240876e-05,
      "loss": 1.2157,
      "step": 75010
    },
    {
      "epoch": 547.5912408759124,
      "grad_norm": 9.18411922454834,
      "learning_rate": 2.262043795620438e-05,
      "loss": 1.0464,
      "step": 75020
    },
    {
      "epoch": 547.6642335766423,
      "grad_norm": 7.560300350189209,
      "learning_rate": 2.2616788321167884e-05,
      "loss": 0.9951,
      "step": 75030
    },
    {
      "epoch": 547.7372262773723,
      "grad_norm": 11.655035972595215,
      "learning_rate": 2.2613138686131388e-05,
      "loss": 0.9739,
      "step": 75040
    },
    {
      "epoch": 547.8102189781022,
      "grad_norm": 17.611614227294922,
      "learning_rate": 2.2609489051094892e-05,
      "loss": 0.8559,
      "step": 75050
    },
    {
      "epoch": 547.8832116788321,
      "grad_norm": 9.144054412841797,
      "learning_rate": 2.2605839416058396e-05,
      "loss": 0.5942,
      "step": 75060
    },
    {
      "epoch": 547.956204379562,
      "grad_norm": 5.806997776031494,
      "learning_rate": 2.26021897810219e-05,
      "loss": 0.8258,
      "step": 75070
    },
    {
      "epoch": 548.029197080292,
      "grad_norm": 12.581192970275879,
      "learning_rate": 2.2598540145985403e-05,
      "loss": 0.762,
      "step": 75080
    },
    {
      "epoch": 548.1021897810219,
      "grad_norm": 13.889857292175293,
      "learning_rate": 2.2594890510948907e-05,
      "loss": 1.1989,
      "step": 75090
    },
    {
      "epoch": 548.1751824817518,
      "grad_norm": 2.8786733150482178,
      "learning_rate": 2.2591240875912408e-05,
      "loss": 0.7561,
      "step": 75100
    },
    {
      "epoch": 548.2481751824818,
      "grad_norm": 0.0879644975066185,
      "learning_rate": 2.258759124087591e-05,
      "loss": 0.8444,
      "step": 75110
    },
    {
      "epoch": 548.3211678832117,
      "grad_norm": 10.615577697753906,
      "learning_rate": 2.258394160583942e-05,
      "loss": 0.6574,
      "step": 75120
    },
    {
      "epoch": 548.3941605839416,
      "grad_norm": 0.16093824803829193,
      "learning_rate": 2.2580291970802923e-05,
      "loss": 0.9524,
      "step": 75130
    },
    {
      "epoch": 548.4671532846716,
      "grad_norm": 14.802838325500488,
      "learning_rate": 2.2576642335766423e-05,
      "loss": 0.7593,
      "step": 75140
    },
    {
      "epoch": 548.5401459854014,
      "grad_norm": 11.415602684020996,
      "learning_rate": 2.2572992700729927e-05,
      "loss": 0.9934,
      "step": 75150
    },
    {
      "epoch": 548.6131386861314,
      "grad_norm": 13.854560852050781,
      "learning_rate": 2.256934306569343e-05,
      "loss": 0.836,
      "step": 75160
    },
    {
      "epoch": 548.6861313868613,
      "grad_norm": 7.833519458770752,
      "learning_rate": 2.256569343065694e-05,
      "loss": 0.9522,
      "step": 75170
    },
    {
      "epoch": 548.7591240875912,
      "grad_norm": 4.227386951446533,
      "learning_rate": 2.256204379562044e-05,
      "loss": 1.082,
      "step": 75180
    },
    {
      "epoch": 548.8321167883212,
      "grad_norm": 12.447542190551758,
      "learning_rate": 2.2558394160583943e-05,
      "loss": 0.9425,
      "step": 75190
    },
    {
      "epoch": 548.9051094890511,
      "grad_norm": 8.654986381530762,
      "learning_rate": 2.2554744525547447e-05,
      "loss": 1.3184,
      "step": 75200
    },
    {
      "epoch": 548.978102189781,
      "grad_norm": 16.47640609741211,
      "learning_rate": 2.2551094890510947e-05,
      "loss": 1.1749,
      "step": 75210
    },
    {
      "epoch": 549.051094890511,
      "grad_norm": 8.46159839630127,
      "learning_rate": 2.2547445255474454e-05,
      "loss": 0.7598,
      "step": 75220
    },
    {
      "epoch": 549.1240875912408,
      "grad_norm": 19.39093780517578,
      "learning_rate": 2.2543795620437958e-05,
      "loss": 0.9019,
      "step": 75230
    },
    {
      "epoch": 549.1970802919708,
      "grad_norm": 9.212764739990234,
      "learning_rate": 2.2540145985401462e-05,
      "loss": 1.0311,
      "step": 75240
    },
    {
      "epoch": 549.2700729927008,
      "grad_norm": 6.755619049072266,
      "learning_rate": 2.2536496350364963e-05,
      "loss": 0.9859,
      "step": 75250
    },
    {
      "epoch": 549.3430656934306,
      "grad_norm": 0.05116669461131096,
      "learning_rate": 2.2532846715328466e-05,
      "loss": 0.8604,
      "step": 75260
    },
    {
      "epoch": 549.4160583941606,
      "grad_norm": 9.196919441223145,
      "learning_rate": 2.2529197080291974e-05,
      "loss": 0.844,
      "step": 75270
    },
    {
      "epoch": 549.4890510948906,
      "grad_norm": 0.0392841175198555,
      "learning_rate": 2.2525547445255478e-05,
      "loss": 0.8605,
      "step": 75280
    },
    {
      "epoch": 549.5620437956204,
      "grad_norm": 9.08603286743164,
      "learning_rate": 2.2521897810218978e-05,
      "loss": 0.8741,
      "step": 75290
    },
    {
      "epoch": 549.6350364963504,
      "grad_norm": 8.165464401245117,
      "learning_rate": 2.2518248175182482e-05,
      "loss": 0.8361,
      "step": 75300
    },
    {
      "epoch": 549.7080291970802,
      "grad_norm": 11.772246360778809,
      "learning_rate": 2.2514598540145986e-05,
      "loss": 0.9148,
      "step": 75310
    },
    {
      "epoch": 549.7810218978102,
      "grad_norm": 7.215641498565674,
      "learning_rate": 2.251094890510949e-05,
      "loss": 0.8581,
      "step": 75320
    },
    {
      "epoch": 549.8540145985402,
      "grad_norm": 7.717775821685791,
      "learning_rate": 2.2507299270072994e-05,
      "loss": 1.0101,
      "step": 75330
    },
    {
      "epoch": 549.92700729927,
      "grad_norm": 14.248523712158203,
      "learning_rate": 2.2503649635036498e-05,
      "loss": 1.1289,
      "step": 75340
    },
    {
      "epoch": 550.0,
      "grad_norm": 28.710418701171875,
      "learning_rate": 2.25e-05,
      "loss": 0.685,
      "step": 75350
    },
    {
      "epoch": 550.07299270073,
      "grad_norm": 7.342403888702393,
      "learning_rate": 2.2496350364963505e-05,
      "loss": 0.8915,
      "step": 75360
    },
    {
      "epoch": 550.1459854014598,
      "grad_norm": 13.34550666809082,
      "learning_rate": 2.249270072992701e-05,
      "loss": 0.7035,
      "step": 75370
    },
    {
      "epoch": 550.2189781021898,
      "grad_norm": 7.987117767333984,
      "learning_rate": 2.2489051094890513e-05,
      "loss": 0.7796,
      "step": 75380
    },
    {
      "epoch": 550.2919708029198,
      "grad_norm": 12.210602760314941,
      "learning_rate": 2.2485401459854017e-05,
      "loss": 0.7127,
      "step": 75390
    },
    {
      "epoch": 550.3649635036496,
      "grad_norm": 6.5044474601745605,
      "learning_rate": 2.2481751824817517e-05,
      "loss": 0.881,
      "step": 75400
    },
    {
      "epoch": 550.4379562043796,
      "grad_norm": 4.979891777038574,
      "learning_rate": 2.247810218978102e-05,
      "loss": 1.2509,
      "step": 75410
    },
    {
      "epoch": 550.5109489051094,
      "grad_norm": 7.735804080963135,
      "learning_rate": 2.2474452554744525e-05,
      "loss": 1.1199,
      "step": 75420
    },
    {
      "epoch": 550.5839416058394,
      "grad_norm": 7.000060558319092,
      "learning_rate": 2.2470802919708032e-05,
      "loss": 0.9796,
      "step": 75430
    },
    {
      "epoch": 550.6569343065694,
      "grad_norm": 0.07529187947511673,
      "learning_rate": 2.2467153284671533e-05,
      "loss": 0.9102,
      "step": 75440
    },
    {
      "epoch": 550.7299270072992,
      "grad_norm": 7.491832733154297,
      "learning_rate": 2.2463503649635037e-05,
      "loss": 1.1358,
      "step": 75450
    },
    {
      "epoch": 550.8029197080292,
      "grad_norm": 9.53045654296875,
      "learning_rate": 2.245985401459854e-05,
      "loss": 0.7872,
      "step": 75460
    },
    {
      "epoch": 550.8759124087592,
      "grad_norm": 7.503964424133301,
      "learning_rate": 2.2456204379562045e-05,
      "loss": 0.6529,
      "step": 75470
    },
    {
      "epoch": 550.948905109489,
      "grad_norm": 13.68281364440918,
      "learning_rate": 2.245255474452555e-05,
      "loss": 1.2728,
      "step": 75480
    },
    {
      "epoch": 551.021897810219,
      "grad_norm": 12.32262897491455,
      "learning_rate": 2.2448905109489052e-05,
      "loss": 0.7758,
      "step": 75490
    },
    {
      "epoch": 551.0948905109489,
      "grad_norm": 13.575295448303223,
      "learning_rate": 2.2445255474452556e-05,
      "loss": 0.7583,
      "step": 75500
    },
    {
      "epoch": 551.1678832116788,
      "grad_norm": 15.203285217285156,
      "learning_rate": 2.244160583941606e-05,
      "loss": 1.3648,
      "step": 75510
    },
    {
      "epoch": 551.2408759124088,
      "grad_norm": 12.32955551147461,
      "learning_rate": 2.243795620437956e-05,
      "loss": 1.0943,
      "step": 75520
    },
    {
      "epoch": 551.3138686131387,
      "grad_norm": 5.266175746917725,
      "learning_rate": 2.2434306569343068e-05,
      "loss": 0.648,
      "step": 75530
    },
    {
      "epoch": 551.3868613138686,
      "grad_norm": 5.851998805999756,
      "learning_rate": 2.2430656934306572e-05,
      "loss": 0.9451,
      "step": 75540
    },
    {
      "epoch": 551.4598540145986,
      "grad_norm": 17.451385498046875,
      "learning_rate": 2.2427007299270076e-05,
      "loss": 0.5607,
      "step": 75550
    },
    {
      "epoch": 551.5328467153284,
      "grad_norm": 8.379569053649902,
      "learning_rate": 2.2423357664233576e-05,
      "loss": 0.8573,
      "step": 75560
    },
    {
      "epoch": 551.6058394160584,
      "grad_norm": 0.07579172402620316,
      "learning_rate": 2.241970802919708e-05,
      "loss": 0.901,
      "step": 75570
    },
    {
      "epoch": 551.6788321167883,
      "grad_norm": 7.291210651397705,
      "learning_rate": 2.2416058394160584e-05,
      "loss": 0.9447,
      "step": 75580
    },
    {
      "epoch": 551.7518248175182,
      "grad_norm": 0.09214670211076736,
      "learning_rate": 2.241240875912409e-05,
      "loss": 0.6183,
      "step": 75590
    },
    {
      "epoch": 551.8248175182482,
      "grad_norm": 9.509090423583984,
      "learning_rate": 2.2408759124087592e-05,
      "loss": 0.8682,
      "step": 75600
    },
    {
      "epoch": 551.8978102189781,
      "grad_norm": 7.985975742340088,
      "learning_rate": 2.2405109489051096e-05,
      "loss": 0.852,
      "step": 75610
    },
    {
      "epoch": 551.970802919708,
      "grad_norm": 8.348657608032227,
      "learning_rate": 2.24014598540146e-05,
      "loss": 1.2436,
      "step": 75620
    },
    {
      "epoch": 552.043795620438,
      "grad_norm": 9.931791305541992,
      "learning_rate": 2.2397810218978103e-05,
      "loss": 0.6398,
      "step": 75630
    },
    {
      "epoch": 552.1167883211679,
      "grad_norm": 6.910834312438965,
      "learning_rate": 2.2394160583941607e-05,
      "loss": 0.6339,
      "step": 75640
    },
    {
      "epoch": 552.1897810218978,
      "grad_norm": 14.24610424041748,
      "learning_rate": 2.239051094890511e-05,
      "loss": 0.7749,
      "step": 75650
    },
    {
      "epoch": 552.2627737226277,
      "grad_norm": 0.05577339231967926,
      "learning_rate": 2.2386861313868615e-05,
      "loss": 0.8243,
      "step": 75660
    },
    {
      "epoch": 552.3357664233577,
      "grad_norm": 8.988158226013184,
      "learning_rate": 2.2383211678832115e-05,
      "loss": 1.4821,
      "step": 75670
    },
    {
      "epoch": 552.4087591240876,
      "grad_norm": 8.17131519317627,
      "learning_rate": 2.237956204379562e-05,
      "loss": 0.7682,
      "step": 75680
    },
    {
      "epoch": 552.4817518248175,
      "grad_norm": 5.913969039916992,
      "learning_rate": 2.2375912408759127e-05,
      "loss": 0.8577,
      "step": 75690
    },
    {
      "epoch": 552.5547445255474,
      "grad_norm": 0.13815389573574066,
      "learning_rate": 2.237226277372263e-05,
      "loss": 0.8813,
      "step": 75700
    },
    {
      "epoch": 552.6277372262774,
      "grad_norm": 0.06673914939165115,
      "learning_rate": 2.236861313868613e-05,
      "loss": 1.2313,
      "step": 75710
    },
    {
      "epoch": 552.7007299270073,
      "grad_norm": 12.26990032196045,
      "learning_rate": 2.2364963503649635e-05,
      "loss": 0.9313,
      "step": 75720
    },
    {
      "epoch": 552.7737226277372,
      "grad_norm": 6.455047607421875,
      "learning_rate": 2.236131386861314e-05,
      "loss": 0.8891,
      "step": 75730
    },
    {
      "epoch": 552.8467153284671,
      "grad_norm": 8.832345962524414,
      "learning_rate": 2.2357664233576646e-05,
      "loss": 0.856,
      "step": 75740
    },
    {
      "epoch": 552.9197080291971,
      "grad_norm": 9.578619956970215,
      "learning_rate": 2.2354014598540147e-05,
      "loss": 1.087,
      "step": 75750
    },
    {
      "epoch": 552.992700729927,
      "grad_norm": 9.614829063415527,
      "learning_rate": 2.235036496350365e-05,
      "loss": 0.7356,
      "step": 75760
    },
    {
      "epoch": 553.0656934306569,
      "grad_norm": 15.84993839263916,
      "learning_rate": 2.2346715328467154e-05,
      "loss": 0.9099,
      "step": 75770
    },
    {
      "epoch": 553.1386861313869,
      "grad_norm": 5.205669403076172,
      "learning_rate": 2.2343065693430658e-05,
      "loss": 0.6447,
      "step": 75780
    },
    {
      "epoch": 553.2116788321168,
      "grad_norm": 3.6284005641937256,
      "learning_rate": 2.2339416058394162e-05,
      "loss": 0.9055,
      "step": 75790
    },
    {
      "epoch": 553.2846715328467,
      "grad_norm": 11.453646659851074,
      "learning_rate": 2.2335766423357666e-05,
      "loss": 0.8469,
      "step": 75800
    },
    {
      "epoch": 553.3576642335767,
      "grad_norm": 0.07843896746635437,
      "learning_rate": 2.233211678832117e-05,
      "loss": 0.7551,
      "step": 75810
    },
    {
      "epoch": 553.4306569343066,
      "grad_norm": 7.642819404602051,
      "learning_rate": 2.2328467153284674e-05,
      "loss": 0.8422,
      "step": 75820
    },
    {
      "epoch": 553.5036496350365,
      "grad_norm": 0.02527492120862007,
      "learning_rate": 2.2324817518248174e-05,
      "loss": 0.8784,
      "step": 75830
    },
    {
      "epoch": 553.5766423357665,
      "grad_norm": 4.500210762023926,
      "learning_rate": 2.232116788321168e-05,
      "loss": 1.0374,
      "step": 75840
    },
    {
      "epoch": 553.6496350364963,
      "grad_norm": 8.594595909118652,
      "learning_rate": 2.2317518248175185e-05,
      "loss": 0.6292,
      "step": 75850
    },
    {
      "epoch": 553.7226277372263,
      "grad_norm": 11.458367347717285,
      "learning_rate": 2.2313868613138686e-05,
      "loss": 1.1544,
      "step": 75860
    },
    {
      "epoch": 553.7956204379562,
      "grad_norm": 3.792332649230957,
      "learning_rate": 2.231021897810219e-05,
      "loss": 0.9797,
      "step": 75870
    },
    {
      "epoch": 553.8686131386861,
      "grad_norm": 0.12256547808647156,
      "learning_rate": 2.2306569343065694e-05,
      "loss": 1.2882,
      "step": 75880
    },
    {
      "epoch": 553.9416058394161,
      "grad_norm": 0.151341512799263,
      "learning_rate": 2.2302919708029198e-05,
      "loss": 0.8594,
      "step": 75890
    },
    {
      "epoch": 554.014598540146,
      "grad_norm": 6.473875999450684,
      "learning_rate": 2.22992700729927e-05,
      "loss": 0.6123,
      "step": 75900
    },
    {
      "epoch": 554.0875912408759,
      "grad_norm": 7.1854071617126465,
      "learning_rate": 2.2295620437956205e-05,
      "loss": 1.0735,
      "step": 75910
    },
    {
      "epoch": 554.1605839416059,
      "grad_norm": 7.92489767074585,
      "learning_rate": 2.229197080291971e-05,
      "loss": 1.0918,
      "step": 75920
    },
    {
      "epoch": 554.2335766423357,
      "grad_norm": 12.178573608398438,
      "learning_rate": 2.2288321167883213e-05,
      "loss": 0.9999,
      "step": 75930
    },
    {
      "epoch": 554.3065693430657,
      "grad_norm": 9.650848388671875,
      "learning_rate": 2.2284671532846717e-05,
      "loss": 0.7616,
      "step": 75940
    },
    {
      "epoch": 554.3795620437957,
      "grad_norm": 10.04256534576416,
      "learning_rate": 2.228102189781022e-05,
      "loss": 0.6762,
      "step": 75950
    },
    {
      "epoch": 554.4525547445255,
      "grad_norm": 6.353184223175049,
      "learning_rate": 2.2277372262773725e-05,
      "loss": 0.7307,
      "step": 75960
    },
    {
      "epoch": 554.5255474452555,
      "grad_norm": 9.638513565063477,
      "learning_rate": 2.227372262773723e-05,
      "loss": 0.8183,
      "step": 75970
    },
    {
      "epoch": 554.5985401459855,
      "grad_norm": 11.235428810119629,
      "learning_rate": 2.227007299270073e-05,
      "loss": 0.9407,
      "step": 75980
    },
    {
      "epoch": 554.6715328467153,
      "grad_norm": 10.135854721069336,
      "learning_rate": 2.2266423357664233e-05,
      "loss": 1.1737,
      "step": 75990
    },
    {
      "epoch": 554.7445255474453,
      "grad_norm": 0.1338496059179306,
      "learning_rate": 2.226277372262774e-05,
      "loss": 1.3732,
      "step": 76000
    },
    {
      "epoch": 554.8175182481751,
      "grad_norm": 14.51779556274414,
      "learning_rate": 2.2259124087591244e-05,
      "loss": 0.7532,
      "step": 76010
    },
    {
      "epoch": 554.8905109489051,
      "grad_norm": 0.08765781670808792,
      "learning_rate": 2.2255474452554745e-05,
      "loss": 0.9075,
      "step": 76020
    },
    {
      "epoch": 554.9635036496351,
      "grad_norm": 11.865943908691406,
      "learning_rate": 2.225182481751825e-05,
      "loss": 0.8833,
      "step": 76030
    },
    {
      "epoch": 555.0364963503649,
      "grad_norm": 10.241716384887695,
      "learning_rate": 2.2248175182481752e-05,
      "loss": 0.583,
      "step": 76040
    },
    {
      "epoch": 555.1094890510949,
      "grad_norm": 0.21504615247249603,
      "learning_rate": 2.2244525547445256e-05,
      "loss": 0.9791,
      "step": 76050
    },
    {
      "epoch": 555.1824817518249,
      "grad_norm": 14.945270538330078,
      "learning_rate": 2.224087591240876e-05,
      "loss": 0.6116,
      "step": 76060
    },
    {
      "epoch": 555.2554744525547,
      "grad_norm": 15.867920875549316,
      "learning_rate": 2.2237226277372264e-05,
      "loss": 0.9237,
      "step": 76070
    },
    {
      "epoch": 555.3284671532847,
      "grad_norm": 6.742900848388672,
      "learning_rate": 2.2233576642335768e-05,
      "loss": 0.8649,
      "step": 76080
    },
    {
      "epoch": 555.4014598540145,
      "grad_norm": 10.10892105102539,
      "learning_rate": 2.222992700729927e-05,
      "loss": 0.9497,
      "step": 76090
    },
    {
      "epoch": 555.4744525547445,
      "grad_norm": 10.307903289794922,
      "learning_rate": 2.2226277372262776e-05,
      "loss": 0.5704,
      "step": 76100
    },
    {
      "epoch": 555.5474452554745,
      "grad_norm": 0.07216514647006989,
      "learning_rate": 2.222262773722628e-05,
      "loss": 0.7468,
      "step": 76110
    },
    {
      "epoch": 555.6204379562043,
      "grad_norm": 0.0247249286621809,
      "learning_rate": 2.2218978102189783e-05,
      "loss": 0.7884,
      "step": 76120
    },
    {
      "epoch": 555.6934306569343,
      "grad_norm": 4.092880725860596,
      "learning_rate": 2.2215328467153284e-05,
      "loss": 1.3145,
      "step": 76130
    },
    {
      "epoch": 555.7664233576643,
      "grad_norm": 15.965487480163574,
      "learning_rate": 2.2211678832116788e-05,
      "loss": 0.9264,
      "step": 76140
    },
    {
      "epoch": 555.8394160583941,
      "grad_norm": 6.991044521331787,
      "learning_rate": 2.2208029197080295e-05,
      "loss": 1.0552,
      "step": 76150
    },
    {
      "epoch": 555.9124087591241,
      "grad_norm": 8.121017456054688,
      "learning_rate": 2.22043795620438e-05,
      "loss": 1.3466,
      "step": 76160
    },
    {
      "epoch": 555.985401459854,
      "grad_norm": 5.714542865753174,
      "learning_rate": 2.22007299270073e-05,
      "loss": 0.816,
      "step": 76170
    },
    {
      "epoch": 556.0583941605839,
      "grad_norm": 0.05498344451189041,
      "learning_rate": 2.2197080291970803e-05,
      "loss": 0.4553,
      "step": 76180
    },
    {
      "epoch": 556.1313868613139,
      "grad_norm": 12.2890625,
      "learning_rate": 2.2193430656934307e-05,
      "loss": 1.094,
      "step": 76190
    },
    {
      "epoch": 556.2043795620438,
      "grad_norm": 11.32938003540039,
      "learning_rate": 2.218978102189781e-05,
      "loss": 0.8016,
      "step": 76200
    },
    {
      "epoch": 556.2773722627737,
      "grad_norm": 10.332903861999512,
      "learning_rate": 2.2186131386861315e-05,
      "loss": 1.163,
      "step": 76210
    },
    {
      "epoch": 556.3503649635037,
      "grad_norm": 3.1369826793670654,
      "learning_rate": 2.218248175182482e-05,
      "loss": 0.4127,
      "step": 76220
    },
    {
      "epoch": 556.4233576642335,
      "grad_norm": 9.520304679870605,
      "learning_rate": 2.2178832116788323e-05,
      "loss": 0.7774,
      "step": 76230
    },
    {
      "epoch": 556.4963503649635,
      "grad_norm": 10.212902069091797,
      "learning_rate": 2.2175182481751827e-05,
      "loss": 1.5255,
      "step": 76240
    },
    {
      "epoch": 556.5693430656934,
      "grad_norm": 8.257328033447266,
      "learning_rate": 2.217153284671533e-05,
      "loss": 0.7694,
      "step": 76250
    },
    {
      "epoch": 556.6423357664233,
      "grad_norm": 0.7443780899047852,
      "learning_rate": 2.2167883211678834e-05,
      "loss": 1.0272,
      "step": 76260
    },
    {
      "epoch": 556.7153284671533,
      "grad_norm": 0.13124041259288788,
      "learning_rate": 2.2164233576642338e-05,
      "loss": 0.7416,
      "step": 76270
    },
    {
      "epoch": 556.7883211678832,
      "grad_norm": 10.88877010345459,
      "learning_rate": 2.216058394160584e-05,
      "loss": 0.4675,
      "step": 76280
    },
    {
      "epoch": 556.8613138686131,
      "grad_norm": 10.713139533996582,
      "learning_rate": 2.2156934306569343e-05,
      "loss": 1.1095,
      "step": 76290
    },
    {
      "epoch": 556.9343065693431,
      "grad_norm": 13.304015159606934,
      "learning_rate": 2.2153284671532847e-05,
      "loss": 1.0961,
      "step": 76300
    },
    {
      "epoch": 557.007299270073,
      "grad_norm": 8.918082237243652,
      "learning_rate": 2.2149635036496354e-05,
      "loss": 1.0449,
      "step": 76310
    },
    {
      "epoch": 557.0802919708029,
      "grad_norm": 16.72464942932129,
      "learning_rate": 2.2145985401459854e-05,
      "loss": 1.4058,
      "step": 76320
    },
    {
      "epoch": 557.1532846715329,
      "grad_norm": 0.024200255051255226,
      "learning_rate": 2.2142335766423358e-05,
      "loss": 0.6524,
      "step": 76330
    },
    {
      "epoch": 557.2262773722628,
      "grad_norm": 4.093901634216309,
      "learning_rate": 2.2138686131386862e-05,
      "loss": 1.1141,
      "step": 76340
    },
    {
      "epoch": 557.2992700729927,
      "grad_norm": 11.798715591430664,
      "learning_rate": 2.2135036496350366e-05,
      "loss": 1.1713,
      "step": 76350
    },
    {
      "epoch": 557.3722627737226,
      "grad_norm": 8.360686302185059,
      "learning_rate": 2.213138686131387e-05,
      "loss": 0.7313,
      "step": 76360
    },
    {
      "epoch": 557.4452554744526,
      "grad_norm": 17.177772521972656,
      "learning_rate": 2.2127737226277374e-05,
      "loss": 0.7972,
      "step": 76370
    },
    {
      "epoch": 557.5182481751825,
      "grad_norm": 10.895007133483887,
      "learning_rate": 2.2124087591240878e-05,
      "loss": 0.839,
      "step": 76380
    },
    {
      "epoch": 557.5912408759124,
      "grad_norm": 5.232411861419678,
      "learning_rate": 2.212043795620438e-05,
      "loss": 0.8212,
      "step": 76390
    },
    {
      "epoch": 557.6642335766423,
      "grad_norm": 1.425092339515686,
      "learning_rate": 2.2116788321167882e-05,
      "loss": 0.4073,
      "step": 76400
    },
    {
      "epoch": 557.7372262773723,
      "grad_norm": 16.757930755615234,
      "learning_rate": 2.211313868613139e-05,
      "loss": 1.1761,
      "step": 76410
    },
    {
      "epoch": 557.8102189781022,
      "grad_norm": 12.32577896118164,
      "learning_rate": 2.2109489051094893e-05,
      "loss": 0.5875,
      "step": 76420
    },
    {
      "epoch": 557.8832116788321,
      "grad_norm": 14.747036933898926,
      "learning_rate": 2.2105839416058397e-05,
      "loss": 1.2894,
      "step": 76430
    },
    {
      "epoch": 557.956204379562,
      "grad_norm": 11.624929428100586,
      "learning_rate": 2.2102189781021897e-05,
      "loss": 0.876,
      "step": 76440
    },
    {
      "epoch": 558.029197080292,
      "grad_norm": 9.359111785888672,
      "learning_rate": 2.20985401459854e-05,
      "loss": 0.7785,
      "step": 76450
    },
    {
      "epoch": 558.1021897810219,
      "grad_norm": 14.292109489440918,
      "learning_rate": 2.2094890510948905e-05,
      "loss": 1.1677,
      "step": 76460
    },
    {
      "epoch": 558.1751824817518,
      "grad_norm": 17.801572799682617,
      "learning_rate": 2.2091240875912413e-05,
      "loss": 0.8806,
      "step": 76470
    },
    {
      "epoch": 558.2481751824818,
      "grad_norm": 12.507474899291992,
      "learning_rate": 2.2087591240875913e-05,
      "loss": 1.2108,
      "step": 76480
    },
    {
      "epoch": 558.3211678832117,
      "grad_norm": 13.52499771118164,
      "learning_rate": 2.2083941605839417e-05,
      "loss": 1.176,
      "step": 76490
    },
    {
      "epoch": 558.3941605839416,
      "grad_norm": 7.513223648071289,
      "learning_rate": 2.208029197080292e-05,
      "loss": 1.1442,
      "step": 76500
    },
    {
      "epoch": 558.4671532846716,
      "grad_norm": 7.280715465545654,
      "learning_rate": 2.2076642335766425e-05,
      "loss": 0.9351,
      "step": 76510
    },
    {
      "epoch": 558.5401459854014,
      "grad_norm": 6.456920623779297,
      "learning_rate": 2.207299270072993e-05,
      "loss": 0.8402,
      "step": 76520
    },
    {
      "epoch": 558.6131386861314,
      "grad_norm": 14.796956062316895,
      "learning_rate": 2.2069343065693432e-05,
      "loss": 1.4759,
      "step": 76530
    },
    {
      "epoch": 558.6861313868613,
      "grad_norm": 8.080224990844727,
      "learning_rate": 2.2065693430656936e-05,
      "loss": 0.8301,
      "step": 76540
    },
    {
      "epoch": 558.7591240875912,
      "grad_norm": 11.626959800720215,
      "learning_rate": 2.2062043795620437e-05,
      "loss": 0.5095,
      "step": 76550
    },
    {
      "epoch": 558.8321167883212,
      "grad_norm": 6.067122936248779,
      "learning_rate": 2.205839416058394e-05,
      "loss": 0.6284,
      "step": 76560
    },
    {
      "epoch": 558.9051094890511,
      "grad_norm": 10.343663215637207,
      "learning_rate": 2.2054744525547448e-05,
      "loss": 0.7994,
      "step": 76570
    },
    {
      "epoch": 558.978102189781,
      "grad_norm": 7.496370792388916,
      "learning_rate": 2.2051094890510952e-05,
      "loss": 0.8674,
      "step": 76580
    },
    {
      "epoch": 559.051094890511,
      "grad_norm": 1.7631100416183472,
      "learning_rate": 2.2047445255474452e-05,
      "loss": 0.7263,
      "step": 76590
    },
    {
      "epoch": 559.1240875912408,
      "grad_norm": 7.281322956085205,
      "learning_rate": 2.2043795620437956e-05,
      "loss": 0.6832,
      "step": 76600
    },
    {
      "epoch": 559.1970802919708,
      "grad_norm": 12.616504669189453,
      "learning_rate": 2.204014598540146e-05,
      "loss": 1.0931,
      "step": 76610
    },
    {
      "epoch": 559.2700729927008,
      "grad_norm": 0.1187700554728508,
      "learning_rate": 2.2036496350364967e-05,
      "loss": 0.7786,
      "step": 76620
    },
    {
      "epoch": 559.3430656934306,
      "grad_norm": 10.323409080505371,
      "learning_rate": 2.2032846715328468e-05,
      "loss": 1.2621,
      "step": 76630
    },
    {
      "epoch": 559.4160583941606,
      "grad_norm": 16.241878509521484,
      "learning_rate": 2.2029197080291972e-05,
      "loss": 0.8848,
      "step": 76640
    },
    {
      "epoch": 559.4890510948906,
      "grad_norm": 7.450185775756836,
      "learning_rate": 2.2025547445255476e-05,
      "loss": 0.7015,
      "step": 76650
    },
    {
      "epoch": 559.5620437956204,
      "grad_norm": 4.580923080444336,
      "learning_rate": 2.202189781021898e-05,
      "loss": 0.7081,
      "step": 76660
    },
    {
      "epoch": 559.6350364963504,
      "grad_norm": 0.07417623698711395,
      "learning_rate": 2.2018248175182483e-05,
      "loss": 0.8717,
      "step": 76670
    },
    {
      "epoch": 559.7080291970802,
      "grad_norm": 9.164690017700195,
      "learning_rate": 2.2014598540145987e-05,
      "loss": 1.0721,
      "step": 76680
    },
    {
      "epoch": 559.7810218978102,
      "grad_norm": 6.885008811950684,
      "learning_rate": 2.201094890510949e-05,
      "loss": 0.9139,
      "step": 76690
    },
    {
      "epoch": 559.8540145985402,
      "grad_norm": 14.005269050598145,
      "learning_rate": 2.200729927007299e-05,
      "loss": 0.5421,
      "step": 76700
    },
    {
      "epoch": 559.92700729927,
      "grad_norm": 7.538712501525879,
      "learning_rate": 2.2003649635036496e-05,
      "loss": 1.2023,
      "step": 76710
    },
    {
      "epoch": 560.0,
      "grad_norm": 0.22610284388065338,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.769,
      "step": 76720
    },
    {
      "epoch": 560.07299270073,
      "grad_norm": 7.689879894256592,
      "learning_rate": 2.1996350364963507e-05,
      "loss": 0.4962,
      "step": 76730
    },
    {
      "epoch": 560.1459854014598,
      "grad_norm": 0.05261383205652237,
      "learning_rate": 2.1992700729927007e-05,
      "loss": 1.1573,
      "step": 76740
    },
    {
      "epoch": 560.2189781021898,
      "grad_norm": 16.21249771118164,
      "learning_rate": 2.198905109489051e-05,
      "loss": 0.9636,
      "step": 76750
    },
    {
      "epoch": 560.2919708029198,
      "grad_norm": 8.22449779510498,
      "learning_rate": 2.1985401459854015e-05,
      "loss": 0.9186,
      "step": 76760
    },
    {
      "epoch": 560.3649635036496,
      "grad_norm": 7.260616779327393,
      "learning_rate": 2.198175182481752e-05,
      "loss": 0.8564,
      "step": 76770
    },
    {
      "epoch": 560.4379562043796,
      "grad_norm": 0.07557620108127594,
      "learning_rate": 2.1978102189781023e-05,
      "loss": 0.7653,
      "step": 76780
    },
    {
      "epoch": 560.5109489051094,
      "grad_norm": 8.990179061889648,
      "learning_rate": 2.1974452554744527e-05,
      "loss": 1.0692,
      "step": 76790
    },
    {
      "epoch": 560.5839416058394,
      "grad_norm": 0.061307478696107864,
      "learning_rate": 2.197080291970803e-05,
      "loss": 0.7417,
      "step": 76800
    },
    {
      "epoch": 560.6569343065694,
      "grad_norm": 9.976174354553223,
      "learning_rate": 2.1967153284671534e-05,
      "loss": 0.7439,
      "step": 76810
    },
    {
      "epoch": 560.7299270072992,
      "grad_norm": 11.79081916809082,
      "learning_rate": 2.1963503649635038e-05,
      "loss": 0.6145,
      "step": 76820
    },
    {
      "epoch": 560.8029197080292,
      "grad_norm": 15.634687423706055,
      "learning_rate": 2.1959854014598542e-05,
      "loss": 0.7517,
      "step": 76830
    },
    {
      "epoch": 560.8759124087592,
      "grad_norm": 0.06404794752597809,
      "learning_rate": 2.1956204379562046e-05,
      "loss": 1.1405,
      "step": 76840
    },
    {
      "epoch": 560.948905109489,
      "grad_norm": 7.344428539276123,
      "learning_rate": 2.195255474452555e-05,
      "loss": 1.1731,
      "step": 76850
    },
    {
      "epoch": 561.021897810219,
      "grad_norm": 11.63964557647705,
      "learning_rate": 2.194890510948905e-05,
      "loss": 1.1862,
      "step": 76860
    },
    {
      "epoch": 561.0948905109489,
      "grad_norm": 0.019752034917473793,
      "learning_rate": 2.1945255474452554e-05,
      "loss": 0.9599,
      "step": 76870
    },
    {
      "epoch": 561.1678832116788,
      "grad_norm": 11.349154472351074,
      "learning_rate": 2.194160583941606e-05,
      "loss": 0.9155,
      "step": 76880
    },
    {
      "epoch": 561.2408759124088,
      "grad_norm": 7.814881801605225,
      "learning_rate": 2.1937956204379565e-05,
      "loss": 0.729,
      "step": 76890
    },
    {
      "epoch": 561.3138686131387,
      "grad_norm": 12.337360382080078,
      "learning_rate": 2.1934306569343066e-05,
      "loss": 1.0499,
      "step": 76900
    },
    {
      "epoch": 561.3868613138686,
      "grad_norm": 8.485966682434082,
      "learning_rate": 2.193065693430657e-05,
      "loss": 0.9983,
      "step": 76910
    },
    {
      "epoch": 561.4598540145986,
      "grad_norm": 8.754199981689453,
      "learning_rate": 2.1927007299270074e-05,
      "loss": 0.938,
      "step": 76920
    },
    {
      "epoch": 561.5328467153284,
      "grad_norm": 19.332693099975586,
      "learning_rate": 2.1923357664233578e-05,
      "loss": 0.9199,
      "step": 76930
    },
    {
      "epoch": 561.6058394160584,
      "grad_norm": 0.05582816153764725,
      "learning_rate": 2.191970802919708e-05,
      "loss": 1.0756,
      "step": 76940
    },
    {
      "epoch": 561.6788321167883,
      "grad_norm": 10.191370964050293,
      "learning_rate": 2.1916058394160585e-05,
      "loss": 0.9536,
      "step": 76950
    },
    {
      "epoch": 561.7518248175182,
      "grad_norm": 7.690389633178711,
      "learning_rate": 2.191240875912409e-05,
      "loss": 0.8922,
      "step": 76960
    },
    {
      "epoch": 561.8248175182482,
      "grad_norm": 12.881620407104492,
      "learning_rate": 2.190875912408759e-05,
      "loss": 0.4427,
      "step": 76970
    },
    {
      "epoch": 561.8978102189781,
      "grad_norm": 7.350528240203857,
      "learning_rate": 2.1905109489051097e-05,
      "loss": 0.8955,
      "step": 76980
    },
    {
      "epoch": 561.970802919708,
      "grad_norm": 7.899584770202637,
      "learning_rate": 2.19014598540146e-05,
      "loss": 0.8076,
      "step": 76990
    },
    {
      "epoch": 562.043795620438,
      "grad_norm": 13.394895553588867,
      "learning_rate": 2.1897810218978105e-05,
      "loss": 0.9786,
      "step": 77000
    },
    {
      "epoch": 562.1167883211679,
      "grad_norm": 8.929964065551758,
      "learning_rate": 2.1894160583941605e-05,
      "loss": 1.2854,
      "step": 77010
    },
    {
      "epoch": 562.1897810218978,
      "grad_norm": 9.715499877929688,
      "learning_rate": 2.189051094890511e-05,
      "loss": 0.7095,
      "step": 77020
    },
    {
      "epoch": 562.2627737226277,
      "grad_norm": 14.751238822937012,
      "learning_rate": 2.1886861313868613e-05,
      "loss": 0.6792,
      "step": 77030
    },
    {
      "epoch": 562.3357664233577,
      "grad_norm": 11.315445899963379,
      "learning_rate": 2.188321167883212e-05,
      "loss": 0.831,
      "step": 77040
    },
    {
      "epoch": 562.4087591240876,
      "grad_norm": 7.212388515472412,
      "learning_rate": 2.187956204379562e-05,
      "loss": 0.8791,
      "step": 77050
    },
    {
      "epoch": 562.4817518248175,
      "grad_norm": 14.891155242919922,
      "learning_rate": 2.1875912408759125e-05,
      "loss": 1.3536,
      "step": 77060
    },
    {
      "epoch": 562.5547445255474,
      "grad_norm": 9.848703384399414,
      "learning_rate": 2.187226277372263e-05,
      "loss": 0.7519,
      "step": 77070
    },
    {
      "epoch": 562.6277372262774,
      "grad_norm": 11.244731903076172,
      "learning_rate": 2.1868613138686132e-05,
      "loss": 1.0524,
      "step": 77080
    },
    {
      "epoch": 562.7007299270073,
      "grad_norm": 7.601675510406494,
      "learning_rate": 2.1864963503649636e-05,
      "loss": 0.8926,
      "step": 77090
    },
    {
      "epoch": 562.7737226277372,
      "grad_norm": 9.82901382446289,
      "learning_rate": 2.186131386861314e-05,
      "loss": 0.8527,
      "step": 77100
    },
    {
      "epoch": 562.8467153284671,
      "grad_norm": 5.106551647186279,
      "learning_rate": 2.1857664233576644e-05,
      "loss": 0.7091,
      "step": 77110
    },
    {
      "epoch": 562.9197080291971,
      "grad_norm": 1.584324598312378,
      "learning_rate": 2.1854014598540145e-05,
      "loss": 0.9762,
      "step": 77120
    },
    {
      "epoch": 562.992700729927,
      "grad_norm": 21.13395881652832,
      "learning_rate": 2.185036496350365e-05,
      "loss": 1.061,
      "step": 77130
    },
    {
      "epoch": 563.0656934306569,
      "grad_norm": 0.025885263457894325,
      "learning_rate": 2.1846715328467156e-05,
      "loss": 0.7699,
      "step": 77140
    },
    {
      "epoch": 563.1386861313869,
      "grad_norm": 21.007715225219727,
      "learning_rate": 2.184306569343066e-05,
      "loss": 0.9345,
      "step": 77150
    },
    {
      "epoch": 563.2116788321168,
      "grad_norm": 6.00018835067749,
      "learning_rate": 2.183941605839416e-05,
      "loss": 0.6998,
      "step": 77160
    },
    {
      "epoch": 563.2846715328467,
      "grad_norm": 5.774420261383057,
      "learning_rate": 2.1835766423357664e-05,
      "loss": 0.793,
      "step": 77170
    },
    {
      "epoch": 563.3576642335767,
      "grad_norm": 12.256668090820312,
      "learning_rate": 2.1832116788321168e-05,
      "loss": 0.7566,
      "step": 77180
    },
    {
      "epoch": 563.4306569343066,
      "grad_norm": 9.43624210357666,
      "learning_rate": 2.1828467153284675e-05,
      "loss": 0.951,
      "step": 77190
    },
    {
      "epoch": 563.5036496350365,
      "grad_norm": 14.771576881408691,
      "learning_rate": 2.1824817518248176e-05,
      "loss": 0.9296,
      "step": 77200
    },
    {
      "epoch": 563.5766423357665,
      "grad_norm": 0.09735982865095139,
      "learning_rate": 2.182116788321168e-05,
      "loss": 1.022,
      "step": 77210
    },
    {
      "epoch": 563.6496350364963,
      "grad_norm": 11.01033878326416,
      "learning_rate": 2.1817518248175183e-05,
      "loss": 1.1746,
      "step": 77220
    },
    {
      "epoch": 563.7226277372263,
      "grad_norm": 8.22690200805664,
      "learning_rate": 2.1813868613138687e-05,
      "loss": 1.0183,
      "step": 77230
    },
    {
      "epoch": 563.7956204379562,
      "grad_norm": 14.373231887817383,
      "learning_rate": 2.181021897810219e-05,
      "loss": 0.923,
      "step": 77240
    },
    {
      "epoch": 563.8686131386861,
      "grad_norm": 7.802647113800049,
      "learning_rate": 2.1806569343065695e-05,
      "loss": 0.893,
      "step": 77250
    },
    {
      "epoch": 563.9416058394161,
      "grad_norm": 6.074705123901367,
      "learning_rate": 2.18029197080292e-05,
      "loss": 0.8706,
      "step": 77260
    },
    {
      "epoch": 564.014598540146,
      "grad_norm": 11.897201538085938,
      "learning_rate": 2.1799270072992703e-05,
      "loss": 0.7685,
      "step": 77270
    },
    {
      "epoch": 564.0875912408759,
      "grad_norm": 8.707975387573242,
      "learning_rate": 2.1795620437956203e-05,
      "loss": 0.9622,
      "step": 77280
    },
    {
      "epoch": 564.1605839416059,
      "grad_norm": 14.314557075500488,
      "learning_rate": 2.179197080291971e-05,
      "loss": 1.1303,
      "step": 77290
    },
    {
      "epoch": 564.2335766423357,
      "grad_norm": 11.20707893371582,
      "learning_rate": 2.1788321167883214e-05,
      "loss": 0.9445,
      "step": 77300
    },
    {
      "epoch": 564.3065693430657,
      "grad_norm": 14.508424758911133,
      "learning_rate": 2.178467153284672e-05,
      "loss": 0.7931,
      "step": 77310
    },
    {
      "epoch": 564.3795620437957,
      "grad_norm": 12.435616493225098,
      "learning_rate": 2.178102189781022e-05,
      "loss": 0.6569,
      "step": 77320
    },
    {
      "epoch": 564.4525547445255,
      "grad_norm": 6.17288875579834,
      "learning_rate": 2.1777372262773723e-05,
      "loss": 0.8694,
      "step": 77330
    },
    {
      "epoch": 564.5255474452555,
      "grad_norm": 14.206741333007812,
      "learning_rate": 2.1773722627737227e-05,
      "loss": 1.0241,
      "step": 77340
    },
    {
      "epoch": 564.5985401459855,
      "grad_norm": 15.27454948425293,
      "learning_rate": 2.177007299270073e-05,
      "loss": 0.667,
      "step": 77350
    },
    {
      "epoch": 564.6715328467153,
      "grad_norm": 14.66794204711914,
      "learning_rate": 2.1766423357664234e-05,
      "loss": 1.1377,
      "step": 77360
    },
    {
      "epoch": 564.7445255474453,
      "grad_norm": 20.07864761352539,
      "learning_rate": 2.1762773722627738e-05,
      "loss": 0.9606,
      "step": 77370
    },
    {
      "epoch": 564.8175182481751,
      "grad_norm": 8.499404907226562,
      "learning_rate": 2.1759124087591242e-05,
      "loss": 1.061,
      "step": 77380
    },
    {
      "epoch": 564.8905109489051,
      "grad_norm": 8.097588539123535,
      "learning_rate": 2.1755474452554746e-05,
      "loss": 0.9639,
      "step": 77390
    },
    {
      "epoch": 564.9635036496351,
      "grad_norm": 0.04774805158376694,
      "learning_rate": 2.175182481751825e-05,
      "loss": 0.7003,
      "step": 77400
    },
    {
      "epoch": 565.0364963503649,
      "grad_norm": 6.2935075759887695,
      "learning_rate": 2.1748175182481754e-05,
      "loss": 0.8099,
      "step": 77410
    },
    {
      "epoch": 565.1094890510949,
      "grad_norm": 11.929450035095215,
      "learning_rate": 2.1744525547445258e-05,
      "loss": 0.8098,
      "step": 77420
    },
    {
      "epoch": 565.1824817518249,
      "grad_norm": 18.801959991455078,
      "learning_rate": 2.1740875912408758e-05,
      "loss": 1.1293,
      "step": 77430
    },
    {
      "epoch": 565.2554744525547,
      "grad_norm": 12.577444076538086,
      "learning_rate": 2.1737226277372262e-05,
      "loss": 1.0177,
      "step": 77440
    },
    {
      "epoch": 565.3284671532847,
      "grad_norm": 7.199697494506836,
      "learning_rate": 2.173357664233577e-05,
      "loss": 0.7485,
      "step": 77450
    },
    {
      "epoch": 565.4014598540145,
      "grad_norm": 11.364593505859375,
      "learning_rate": 2.1729927007299273e-05,
      "loss": 0.8734,
      "step": 77460
    },
    {
      "epoch": 565.4744525547445,
      "grad_norm": 9.955315589904785,
      "learning_rate": 2.1726277372262774e-05,
      "loss": 0.8368,
      "step": 77470
    },
    {
      "epoch": 565.5474452554745,
      "grad_norm": 0.580971896648407,
      "learning_rate": 2.1722627737226278e-05,
      "loss": 0.9402,
      "step": 77480
    },
    {
      "epoch": 565.6204379562043,
      "grad_norm": 9.970999717712402,
      "learning_rate": 2.171897810218978e-05,
      "loss": 0.6817,
      "step": 77490
    },
    {
      "epoch": 565.6934306569343,
      "grad_norm": 12.02462100982666,
      "learning_rate": 2.1715328467153285e-05,
      "loss": 1.0746,
      "step": 77500
    },
    {
      "epoch": 565.7664233576643,
      "grad_norm": 15.02933120727539,
      "learning_rate": 2.171167883211679e-05,
      "loss": 0.8692,
      "step": 77510
    },
    {
      "epoch": 565.8394160583941,
      "grad_norm": 15.107105255126953,
      "learning_rate": 2.1708029197080293e-05,
      "loss": 1.3014,
      "step": 77520
    },
    {
      "epoch": 565.9124087591241,
      "grad_norm": 12.950355529785156,
      "learning_rate": 2.1704379562043797e-05,
      "loss": 0.9244,
      "step": 77530
    },
    {
      "epoch": 565.985401459854,
      "grad_norm": 8.802029609680176,
      "learning_rate": 2.17007299270073e-05,
      "loss": 0.8906,
      "step": 77540
    },
    {
      "epoch": 566.0583941605839,
      "grad_norm": 22.52789306640625,
      "learning_rate": 2.1697080291970805e-05,
      "loss": 1.1583,
      "step": 77550
    },
    {
      "epoch": 566.1313868613139,
      "grad_norm": 8.372682571411133,
      "learning_rate": 2.169343065693431e-05,
      "loss": 1.2666,
      "step": 77560
    },
    {
      "epoch": 566.2043795620438,
      "grad_norm": 11.73800277709961,
      "learning_rate": 2.1689781021897812e-05,
      "loss": 0.8038,
      "step": 77570
    },
    {
      "epoch": 566.2773722627737,
      "grad_norm": 7.864951133728027,
      "learning_rate": 2.1686131386861313e-05,
      "loss": 0.7747,
      "step": 77580
    },
    {
      "epoch": 566.3503649635037,
      "grad_norm": 0.023674804717302322,
      "learning_rate": 2.1682481751824817e-05,
      "loss": 0.6201,
      "step": 77590
    },
    {
      "epoch": 566.4233576642335,
      "grad_norm": 0.06395658850669861,
      "learning_rate": 2.167883211678832e-05,
      "loss": 0.8308,
      "step": 77600
    },
    {
      "epoch": 566.4963503649635,
      "grad_norm": 8.699870109558105,
      "learning_rate": 2.1675182481751828e-05,
      "loss": 1.1423,
      "step": 77610
    },
    {
      "epoch": 566.5693430656934,
      "grad_norm": 16.744718551635742,
      "learning_rate": 2.167153284671533e-05,
      "loss": 1.0847,
      "step": 77620
    },
    {
      "epoch": 566.6423357664233,
      "grad_norm": 1.5304559469223022,
      "learning_rate": 2.1667883211678832e-05,
      "loss": 0.8332,
      "step": 77630
    },
    {
      "epoch": 566.7153284671533,
      "grad_norm": 8.777820587158203,
      "learning_rate": 2.1664233576642336e-05,
      "loss": 0.8238,
      "step": 77640
    },
    {
      "epoch": 566.7883211678832,
      "grad_norm": 13.367461204528809,
      "learning_rate": 2.166058394160584e-05,
      "loss": 1.3203,
      "step": 77650
    },
    {
      "epoch": 566.8613138686131,
      "grad_norm": 0.04526488482952118,
      "learning_rate": 2.1656934306569344e-05,
      "loss": 0.5249,
      "step": 77660
    },
    {
      "epoch": 566.9343065693431,
      "grad_norm": 0.4053646922111511,
      "learning_rate": 2.1653284671532848e-05,
      "loss": 0.718,
      "step": 77670
    },
    {
      "epoch": 567.007299270073,
      "grad_norm": 11.96422290802002,
      "learning_rate": 2.1649635036496352e-05,
      "loss": 0.6857,
      "step": 77680
    },
    {
      "epoch": 567.0802919708029,
      "grad_norm": 0.039306461811065674,
      "learning_rate": 2.1645985401459856e-05,
      "loss": 1.0074,
      "step": 77690
    },
    {
      "epoch": 567.1532846715329,
      "grad_norm": 10.455230712890625,
      "learning_rate": 2.1642335766423356e-05,
      "loss": 1.0679,
      "step": 77700
    },
    {
      "epoch": 567.2262773722628,
      "grad_norm": 8.66539192199707,
      "learning_rate": 2.1638686131386863e-05,
      "loss": 1.1313,
      "step": 77710
    },
    {
      "epoch": 567.2992700729927,
      "grad_norm": 11.201091766357422,
      "learning_rate": 2.1635036496350367e-05,
      "loss": 1.0192,
      "step": 77720
    },
    {
      "epoch": 567.3722627737226,
      "grad_norm": 13.651605606079102,
      "learning_rate": 2.163138686131387e-05,
      "loss": 1.0724,
      "step": 77730
    },
    {
      "epoch": 567.4452554744526,
      "grad_norm": 0.06982666254043579,
      "learning_rate": 2.1627737226277372e-05,
      "loss": 0.6696,
      "step": 77740
    },
    {
      "epoch": 567.5182481751825,
      "grad_norm": 8.020638465881348,
      "learning_rate": 2.1624087591240876e-05,
      "loss": 0.5671,
      "step": 77750
    },
    {
      "epoch": 567.5912408759124,
      "grad_norm": 11.532297134399414,
      "learning_rate": 2.1620437956204383e-05,
      "loss": 0.4929,
      "step": 77760
    },
    {
      "epoch": 567.6642335766423,
      "grad_norm": 10.071911811828613,
      "learning_rate": 2.1616788321167883e-05,
      "loss": 1.2791,
      "step": 77770
    },
    {
      "epoch": 567.7372262773723,
      "grad_norm": 4.500804424285889,
      "learning_rate": 2.1613138686131387e-05,
      "loss": 0.444,
      "step": 77780
    },
    {
      "epoch": 567.8102189781022,
      "grad_norm": 8.746644020080566,
      "learning_rate": 2.160948905109489e-05,
      "loss": 0.8653,
      "step": 77790
    },
    {
      "epoch": 567.8832116788321,
      "grad_norm": 6.582787990570068,
      "learning_rate": 2.1605839416058395e-05,
      "loss": 1.0533,
      "step": 77800
    },
    {
      "epoch": 567.956204379562,
      "grad_norm": 9.221685409545898,
      "learning_rate": 2.16021897810219e-05,
      "loss": 1.0667,
      "step": 77810
    },
    {
      "epoch": 568.029197080292,
      "grad_norm": 7.918512344360352,
      "learning_rate": 2.1598540145985403e-05,
      "loss": 0.7211,
      "step": 77820
    },
    {
      "epoch": 568.1021897810219,
      "grad_norm": 7.5717597007751465,
      "learning_rate": 2.1594890510948907e-05,
      "loss": 1.0812,
      "step": 77830
    },
    {
      "epoch": 568.1751824817518,
      "grad_norm": 6.725597858428955,
      "learning_rate": 2.159124087591241e-05,
      "loss": 0.8218,
      "step": 77840
    },
    {
      "epoch": 568.2481751824818,
      "grad_norm": 10.109350204467773,
      "learning_rate": 2.158759124087591e-05,
      "loss": 1.0261,
      "step": 77850
    },
    {
      "epoch": 568.3211678832117,
      "grad_norm": 10.464198112487793,
      "learning_rate": 2.158394160583942e-05,
      "loss": 0.9975,
      "step": 77860
    },
    {
      "epoch": 568.3941605839416,
      "grad_norm": 7.568404197692871,
      "learning_rate": 2.1580291970802922e-05,
      "loss": 0.945,
      "step": 77870
    },
    {
      "epoch": 568.4671532846716,
      "grad_norm": 15.911526679992676,
      "learning_rate": 2.1576642335766426e-05,
      "loss": 0.5973,
      "step": 77880
    },
    {
      "epoch": 568.5401459854014,
      "grad_norm": 10.001441955566406,
      "learning_rate": 2.1572992700729927e-05,
      "loss": 0.8701,
      "step": 77890
    },
    {
      "epoch": 568.6131386861314,
      "grad_norm": 13.55194091796875,
      "learning_rate": 2.156934306569343e-05,
      "loss": 1.2854,
      "step": 77900
    },
    {
      "epoch": 568.6861313868613,
      "grad_norm": 11.615389823913574,
      "learning_rate": 2.1565693430656934e-05,
      "loss": 1.2621,
      "step": 77910
    },
    {
      "epoch": 568.7591240875912,
      "grad_norm": 6.943127632141113,
      "learning_rate": 2.156204379562044e-05,
      "loss": 0.7251,
      "step": 77920
    },
    {
      "epoch": 568.8321167883212,
      "grad_norm": 0.043032657355070114,
      "learning_rate": 2.1558394160583942e-05,
      "loss": 0.475,
      "step": 77930
    },
    {
      "epoch": 568.9051094890511,
      "grad_norm": 8.440302848815918,
      "learning_rate": 2.1554744525547446e-05,
      "loss": 0.7163,
      "step": 77940
    },
    {
      "epoch": 568.978102189781,
      "grad_norm": 7.099628925323486,
      "learning_rate": 2.155109489051095e-05,
      "loss": 0.7797,
      "step": 77950
    },
    {
      "epoch": 569.051094890511,
      "grad_norm": 13.074490547180176,
      "learning_rate": 2.1547445255474454e-05,
      "loss": 0.9628,
      "step": 77960
    },
    {
      "epoch": 569.1240875912408,
      "grad_norm": 8.024057388305664,
      "learning_rate": 2.1543795620437958e-05,
      "loss": 0.9168,
      "step": 77970
    },
    {
      "epoch": 569.1970802919708,
      "grad_norm": 10.710904121398926,
      "learning_rate": 2.154014598540146e-05,
      "loss": 1.0871,
      "step": 77980
    },
    {
      "epoch": 569.2700729927008,
      "grad_norm": 12.2738618850708,
      "learning_rate": 2.1536496350364965e-05,
      "loss": 0.8882,
      "step": 77990
    },
    {
      "epoch": 569.3430656934306,
      "grad_norm": 9.727716445922852,
      "learning_rate": 2.1532846715328466e-05,
      "loss": 0.7788,
      "step": 78000
    },
    {
      "epoch": 569.4160583941606,
      "grad_norm": 7.385907173156738,
      "learning_rate": 2.152919708029197e-05,
      "loss": 0.8424,
      "step": 78010
    },
    {
      "epoch": 569.4890510948906,
      "grad_norm": 10.20259952545166,
      "learning_rate": 2.1525547445255477e-05,
      "loss": 0.9169,
      "step": 78020
    },
    {
      "epoch": 569.5620437956204,
      "grad_norm": 0.20177948474884033,
      "learning_rate": 2.152189781021898e-05,
      "loss": 0.8775,
      "step": 78030
    },
    {
      "epoch": 569.6350364963504,
      "grad_norm": 13.685873031616211,
      "learning_rate": 2.151824817518248e-05,
      "loss": 1.3173,
      "step": 78040
    },
    {
      "epoch": 569.7080291970802,
      "grad_norm": 6.3884406089782715,
      "learning_rate": 2.1514598540145985e-05,
      "loss": 0.7387,
      "step": 78050
    },
    {
      "epoch": 569.7810218978102,
      "grad_norm": 0.025651490315794945,
      "learning_rate": 2.151094890510949e-05,
      "loss": 0.8576,
      "step": 78060
    },
    {
      "epoch": 569.8540145985402,
      "grad_norm": 0.07269410789012909,
      "learning_rate": 2.1507299270072993e-05,
      "loss": 0.8827,
      "step": 78070
    },
    {
      "epoch": 569.92700729927,
      "grad_norm": 7.025879383087158,
      "learning_rate": 2.1503649635036497e-05,
      "loss": 0.6623,
      "step": 78080
    },
    {
      "epoch": 570.0,
      "grad_norm": 0.055506471544504166,
      "learning_rate": 2.15e-05,
      "loss": 1.0183,
      "step": 78090
    },
    {
      "epoch": 570.07299270073,
      "grad_norm": 11.559856414794922,
      "learning_rate": 2.1496350364963505e-05,
      "loss": 0.7974,
      "step": 78100
    },
    {
      "epoch": 570.1459854014598,
      "grad_norm": 14.904645919799805,
      "learning_rate": 2.149270072992701e-05,
      "loss": 1.4116,
      "step": 78110
    },
    {
      "epoch": 570.2189781021898,
      "grad_norm": 10.2061185836792,
      "learning_rate": 2.1489051094890512e-05,
      "loss": 0.7215,
      "step": 78120
    },
    {
      "epoch": 570.2919708029198,
      "grad_norm": 9.773183822631836,
      "learning_rate": 2.1485401459854016e-05,
      "loss": 0.7084,
      "step": 78130
    },
    {
      "epoch": 570.3649635036496,
      "grad_norm": 9.059290885925293,
      "learning_rate": 2.148175182481752e-05,
      "loss": 1.2311,
      "step": 78140
    },
    {
      "epoch": 570.4379562043796,
      "grad_norm": 0.05420570448040962,
      "learning_rate": 2.1478102189781024e-05,
      "loss": 0.8841,
      "step": 78150
    },
    {
      "epoch": 570.5109489051094,
      "grad_norm": 0.04100031405687332,
      "learning_rate": 2.1474452554744525e-05,
      "loss": 0.7384,
      "step": 78160
    },
    {
      "epoch": 570.5839416058394,
      "grad_norm": 21.57137680053711,
      "learning_rate": 2.147080291970803e-05,
      "loss": 0.9117,
      "step": 78170
    },
    {
      "epoch": 570.6569343065694,
      "grad_norm": 13.633234977722168,
      "learning_rate": 2.1467153284671536e-05,
      "loss": 0.7753,
      "step": 78180
    },
    {
      "epoch": 570.7299270072992,
      "grad_norm": 11.41696548461914,
      "learning_rate": 2.146350364963504e-05,
      "loss": 0.9656,
      "step": 78190
    },
    {
      "epoch": 570.8029197080292,
      "grad_norm": 10.050056457519531,
      "learning_rate": 2.145985401459854e-05,
      "loss": 0.8076,
      "step": 78200
    },
    {
      "epoch": 570.8759124087592,
      "grad_norm": 10.92578125,
      "learning_rate": 2.1456204379562044e-05,
      "loss": 1.095,
      "step": 78210
    },
    {
      "epoch": 570.948905109489,
      "grad_norm": 15.371091842651367,
      "learning_rate": 2.1452554744525548e-05,
      "loss": 0.8106,
      "step": 78220
    },
    {
      "epoch": 571.021897810219,
      "grad_norm": 7.146913051605225,
      "learning_rate": 2.1448905109489052e-05,
      "loss": 0.6963,
      "step": 78230
    },
    {
      "epoch": 571.0948905109489,
      "grad_norm": 0.694913387298584,
      "learning_rate": 2.1445255474452556e-05,
      "loss": 0.9357,
      "step": 78240
    },
    {
      "epoch": 571.1678832116788,
      "grad_norm": 0.020681405439972878,
      "learning_rate": 2.144160583941606e-05,
      "loss": 0.79,
      "step": 78250
    },
    {
      "epoch": 571.2408759124088,
      "grad_norm": 10.63021469116211,
      "learning_rate": 2.1437956204379563e-05,
      "loss": 0.8141,
      "step": 78260
    },
    {
      "epoch": 571.3138686131387,
      "grad_norm": 17.743242263793945,
      "learning_rate": 2.1434306569343064e-05,
      "loss": 0.9708,
      "step": 78270
    },
    {
      "epoch": 571.3868613138686,
      "grad_norm": 0.02173076756298542,
      "learning_rate": 2.143065693430657e-05,
      "loss": 1.0104,
      "step": 78280
    },
    {
      "epoch": 571.4598540145986,
      "grad_norm": 0.0759764015674591,
      "learning_rate": 2.1427007299270075e-05,
      "loss": 1.0187,
      "step": 78290
    },
    {
      "epoch": 571.5328467153284,
      "grad_norm": 11.577879905700684,
      "learning_rate": 2.142335766423358e-05,
      "loss": 0.9211,
      "step": 78300
    },
    {
      "epoch": 571.6058394160584,
      "grad_norm": 13.675383567810059,
      "learning_rate": 2.141970802919708e-05,
      "loss": 1.0176,
      "step": 78310
    },
    {
      "epoch": 571.6788321167883,
      "grad_norm": 17.55974769592285,
      "learning_rate": 2.1416058394160583e-05,
      "loss": 1.0571,
      "step": 78320
    },
    {
      "epoch": 571.7518248175182,
      "grad_norm": 0.06359665095806122,
      "learning_rate": 2.141240875912409e-05,
      "loss": 0.8918,
      "step": 78330
    },
    {
      "epoch": 571.8248175182482,
      "grad_norm": 8.133763313293457,
      "learning_rate": 2.1408759124087594e-05,
      "loss": 0.8409,
      "step": 78340
    },
    {
      "epoch": 571.8978102189781,
      "grad_norm": 19.424043655395508,
      "learning_rate": 2.1405109489051095e-05,
      "loss": 1.0137,
      "step": 78350
    },
    {
      "epoch": 571.970802919708,
      "grad_norm": 9.390863418579102,
      "learning_rate": 2.14014598540146e-05,
      "loss": 0.801,
      "step": 78360
    },
    {
      "epoch": 572.043795620438,
      "grad_norm": 16.16261863708496,
      "learning_rate": 2.1397810218978103e-05,
      "loss": 0.973,
      "step": 78370
    },
    {
      "epoch": 572.1167883211679,
      "grad_norm": 6.960090637207031,
      "learning_rate": 2.1394160583941607e-05,
      "loss": 0.9486,
      "step": 78380
    },
    {
      "epoch": 572.1897810218978,
      "grad_norm": 16.895648956298828,
      "learning_rate": 2.139051094890511e-05,
      "loss": 0.9769,
      "step": 78390
    },
    {
      "epoch": 572.2627737226277,
      "grad_norm": 7.277209281921387,
      "learning_rate": 2.1386861313868614e-05,
      "loss": 0.8597,
      "step": 78400
    },
    {
      "epoch": 572.3357664233577,
      "grad_norm": 12.468050003051758,
      "learning_rate": 2.1383211678832118e-05,
      "loss": 1.0109,
      "step": 78410
    },
    {
      "epoch": 572.4087591240876,
      "grad_norm": 9.643752098083496,
      "learning_rate": 2.137956204379562e-05,
      "loss": 0.7412,
      "step": 78420
    },
    {
      "epoch": 572.4817518248175,
      "grad_norm": 12.245331764221191,
      "learning_rate": 2.1375912408759126e-05,
      "loss": 1.0248,
      "step": 78430
    },
    {
      "epoch": 572.5547445255474,
      "grad_norm": 10.035126686096191,
      "learning_rate": 2.137226277372263e-05,
      "loss": 1.0379,
      "step": 78440
    },
    {
      "epoch": 572.6277372262774,
      "grad_norm": 13.11586856842041,
      "learning_rate": 2.1368613138686134e-05,
      "loss": 1.0429,
      "step": 78450
    },
    {
      "epoch": 572.7007299270073,
      "grad_norm": 12.146943092346191,
      "learning_rate": 2.1364963503649634e-05,
      "loss": 0.9087,
      "step": 78460
    },
    {
      "epoch": 572.7737226277372,
      "grad_norm": 12.222406387329102,
      "learning_rate": 2.1361313868613138e-05,
      "loss": 0.7946,
      "step": 78470
    },
    {
      "epoch": 572.8467153284671,
      "grad_norm": 12.499898910522461,
      "learning_rate": 2.1357664233576642e-05,
      "loss": 0.9154,
      "step": 78480
    },
    {
      "epoch": 572.9197080291971,
      "grad_norm": 15.486830711364746,
      "learning_rate": 2.135401459854015e-05,
      "loss": 0.7643,
      "step": 78490
    },
    {
      "epoch": 572.992700729927,
      "grad_norm": 7.5699286460876465,
      "learning_rate": 2.135036496350365e-05,
      "loss": 0.7637,
      "step": 78500
    },
    {
      "epoch": 573.0656934306569,
      "grad_norm": 10.895914077758789,
      "learning_rate": 2.1346715328467154e-05,
      "loss": 1.0185,
      "step": 78510
    },
    {
      "epoch": 573.1386861313869,
      "grad_norm": 17.439727783203125,
      "learning_rate": 2.1343065693430658e-05,
      "loss": 0.942,
      "step": 78520
    },
    {
      "epoch": 573.2116788321168,
      "grad_norm": 4.974097728729248,
      "learning_rate": 2.133941605839416e-05,
      "loss": 0.8024,
      "step": 78530
    },
    {
      "epoch": 573.2846715328467,
      "grad_norm": 14.420304298400879,
      "learning_rate": 2.1335766423357665e-05,
      "loss": 0.8747,
      "step": 78540
    },
    {
      "epoch": 573.3576642335767,
      "grad_norm": 8.421948432922363,
      "learning_rate": 2.133211678832117e-05,
      "loss": 1.0622,
      "step": 78550
    },
    {
      "epoch": 573.4306569343066,
      "grad_norm": 9.684920310974121,
      "learning_rate": 2.1328467153284673e-05,
      "loss": 0.6651,
      "step": 78560
    },
    {
      "epoch": 573.5036496350365,
      "grad_norm": 9.5957612991333,
      "learning_rate": 2.1324817518248177e-05,
      "loss": 1.0842,
      "step": 78570
    },
    {
      "epoch": 573.5766423357665,
      "grad_norm": 10.02369213104248,
      "learning_rate": 2.1321167883211678e-05,
      "loss": 0.7951,
      "step": 78580
    },
    {
      "epoch": 573.6496350364963,
      "grad_norm": 0.12554506957530975,
      "learning_rate": 2.1317518248175185e-05,
      "loss": 0.6782,
      "step": 78590
    },
    {
      "epoch": 573.7226277372263,
      "grad_norm": 10.366057395935059,
      "learning_rate": 2.131386861313869e-05,
      "loss": 0.6971,
      "step": 78600
    },
    {
      "epoch": 573.7956204379562,
      "grad_norm": 0.05463521555066109,
      "learning_rate": 2.1310218978102193e-05,
      "loss": 0.9909,
      "step": 78610
    },
    {
      "epoch": 573.8686131386861,
      "grad_norm": 8.2103910446167,
      "learning_rate": 2.1306569343065693e-05,
      "loss": 1.1631,
      "step": 78620
    },
    {
      "epoch": 573.9416058394161,
      "grad_norm": 9.157025337219238,
      "learning_rate": 2.1302919708029197e-05,
      "loss": 0.6507,
      "step": 78630
    },
    {
      "epoch": 574.014598540146,
      "grad_norm": 0.02583390660583973,
      "learning_rate": 2.12992700729927e-05,
      "loss": 0.9108,
      "step": 78640
    },
    {
      "epoch": 574.0875912408759,
      "grad_norm": 5.073747634887695,
      "learning_rate": 2.1295620437956205e-05,
      "loss": 0.5587,
      "step": 78650
    },
    {
      "epoch": 574.1605839416059,
      "grad_norm": 6.620268821716309,
      "learning_rate": 2.129197080291971e-05,
      "loss": 0.6318,
      "step": 78660
    },
    {
      "epoch": 574.2335766423357,
      "grad_norm": 16.388267517089844,
      "learning_rate": 2.1288321167883212e-05,
      "loss": 0.7082,
      "step": 78670
    },
    {
      "epoch": 574.3065693430657,
      "grad_norm": 0.11882452666759491,
      "learning_rate": 2.1284671532846716e-05,
      "loss": 0.8142,
      "step": 78680
    },
    {
      "epoch": 574.3795620437957,
      "grad_norm": 0.09608903527259827,
      "learning_rate": 2.128102189781022e-05,
      "loss": 0.7054,
      "step": 78690
    },
    {
      "epoch": 574.4525547445255,
      "grad_norm": 0.05160000920295715,
      "learning_rate": 2.1277372262773724e-05,
      "loss": 0.7173,
      "step": 78700
    },
    {
      "epoch": 574.5255474452555,
      "grad_norm": 0.029314594343304634,
      "learning_rate": 2.1273722627737228e-05,
      "loss": 1.1075,
      "step": 78710
    },
    {
      "epoch": 574.5985401459855,
      "grad_norm": 13.310049057006836,
      "learning_rate": 2.1270072992700732e-05,
      "loss": 1.0394,
      "step": 78720
    },
    {
      "epoch": 574.6715328467153,
      "grad_norm": 6.424357891082764,
      "learning_rate": 2.1266423357664232e-05,
      "loss": 1.0767,
      "step": 78730
    },
    {
      "epoch": 574.7445255474453,
      "grad_norm": 8.128424644470215,
      "learning_rate": 2.1262773722627736e-05,
      "loss": 1.1482,
      "step": 78740
    },
    {
      "epoch": 574.8175182481751,
      "grad_norm": 11.9396390914917,
      "learning_rate": 2.1259124087591244e-05,
      "loss": 0.9811,
      "step": 78750
    },
    {
      "epoch": 574.8905109489051,
      "grad_norm": 0.14146168529987335,
      "learning_rate": 2.1255474452554747e-05,
      "loss": 0.9343,
      "step": 78760
    },
    {
      "epoch": 574.9635036496351,
      "grad_norm": 19.990692138671875,
      "learning_rate": 2.1251824817518248e-05,
      "loss": 1.3177,
      "step": 78770
    },
    {
      "epoch": 575.0364963503649,
      "grad_norm": 7.938167572021484,
      "learning_rate": 2.1248175182481752e-05,
      "loss": 1.0321,
      "step": 78780
    },
    {
      "epoch": 575.1094890510949,
      "grad_norm": 11.177144050598145,
      "learning_rate": 2.1244525547445256e-05,
      "loss": 0.6801,
      "step": 78790
    },
    {
      "epoch": 575.1824817518249,
      "grad_norm": 0.03230160102248192,
      "learning_rate": 2.1240875912408763e-05,
      "loss": 1.3439,
      "step": 78800
    },
    {
      "epoch": 575.2554744525547,
      "grad_norm": 0.23979546129703522,
      "learning_rate": 2.1237226277372263e-05,
      "loss": 1.1045,
      "step": 78810
    },
    {
      "epoch": 575.3284671532847,
      "grad_norm": 0.05144031345844269,
      "learning_rate": 2.1233576642335767e-05,
      "loss": 0.9425,
      "step": 78820
    },
    {
      "epoch": 575.4014598540145,
      "grad_norm": 9.054763793945312,
      "learning_rate": 2.122992700729927e-05,
      "loss": 0.6024,
      "step": 78830
    },
    {
      "epoch": 575.4744525547445,
      "grad_norm": 7.973454475402832,
      "learning_rate": 2.122627737226277e-05,
      "loss": 0.8211,
      "step": 78840
    },
    {
      "epoch": 575.5474452554745,
      "grad_norm": 9.313944816589355,
      "learning_rate": 2.122262773722628e-05,
      "loss": 0.8853,
      "step": 78850
    },
    {
      "epoch": 575.6204379562043,
      "grad_norm": 13.160387992858887,
      "learning_rate": 2.1218978102189783e-05,
      "loss": 0.7438,
      "step": 78860
    },
    {
      "epoch": 575.6934306569343,
      "grad_norm": 10.388657569885254,
      "learning_rate": 2.1215328467153287e-05,
      "loss": 0.5266,
      "step": 78870
    },
    {
      "epoch": 575.7664233576643,
      "grad_norm": 14.960179328918457,
      "learning_rate": 2.1211678832116787e-05,
      "loss": 1.184,
      "step": 78880
    },
    {
      "epoch": 575.8394160583941,
      "grad_norm": 9.268648147583008,
      "learning_rate": 2.120802919708029e-05,
      "loss": 0.8926,
      "step": 78890
    },
    {
      "epoch": 575.9124087591241,
      "grad_norm": 10.0440673828125,
      "learning_rate": 2.12043795620438e-05,
      "loss": 1.1807,
      "step": 78900
    },
    {
      "epoch": 575.985401459854,
      "grad_norm": 10.553583145141602,
      "learning_rate": 2.1200729927007302e-05,
      "loss": 0.6556,
      "step": 78910
    },
    {
      "epoch": 576.0583941605839,
      "grad_norm": 11.325859069824219,
      "learning_rate": 2.1197080291970803e-05,
      "loss": 1.0811,
      "step": 78920
    },
    {
      "epoch": 576.1313868613139,
      "grad_norm": 6.599400997161865,
      "learning_rate": 2.1193430656934307e-05,
      "loss": 0.8467,
      "step": 78930
    },
    {
      "epoch": 576.2043795620438,
      "grad_norm": 5.262166976928711,
      "learning_rate": 2.118978102189781e-05,
      "loss": 0.8497,
      "step": 78940
    },
    {
      "epoch": 576.2773722627737,
      "grad_norm": 7.740496635437012,
      "learning_rate": 2.1186131386861314e-05,
      "loss": 0.9245,
      "step": 78950
    },
    {
      "epoch": 576.3503649635037,
      "grad_norm": 20.355575561523438,
      "learning_rate": 2.1182481751824818e-05,
      "loss": 1.1348,
      "step": 78960
    },
    {
      "epoch": 576.4233576642335,
      "grad_norm": 5.606220245361328,
      "learning_rate": 2.1178832116788322e-05,
      "loss": 0.9776,
      "step": 78970
    },
    {
      "epoch": 576.4963503649635,
      "grad_norm": 10.568367004394531,
      "learning_rate": 2.1175182481751826e-05,
      "loss": 1.0559,
      "step": 78980
    },
    {
      "epoch": 576.5693430656934,
      "grad_norm": 5.010844707489014,
      "learning_rate": 2.117153284671533e-05,
      "loss": 0.5226,
      "step": 78990
    },
    {
      "epoch": 576.6423357664233,
      "grad_norm": 12.66388988494873,
      "learning_rate": 2.1167883211678834e-05,
      "loss": 0.6285,
      "step": 79000
    },
    {
      "epoch": 576.7153284671533,
      "grad_norm": 8.616581916809082,
      "learning_rate": 2.1164233576642338e-05,
      "loss": 0.9801,
      "step": 79010
    },
    {
      "epoch": 576.7883211678832,
      "grad_norm": 0.04633411020040512,
      "learning_rate": 2.116058394160584e-05,
      "loss": 0.9157,
      "step": 79020
    },
    {
      "epoch": 576.8613138686131,
      "grad_norm": 0.1055518090724945,
      "learning_rate": 2.1156934306569345e-05,
      "loss": 0.8149,
      "step": 79030
    },
    {
      "epoch": 576.9343065693431,
      "grad_norm": 11.324797630310059,
      "learning_rate": 2.1153284671532846e-05,
      "loss": 0.7539,
      "step": 79040
    },
    {
      "epoch": 577.007299270073,
      "grad_norm": 10.487528800964355,
      "learning_rate": 2.114963503649635e-05,
      "loss": 0.7616,
      "step": 79050
    },
    {
      "epoch": 577.0802919708029,
      "grad_norm": 0.029568243771791458,
      "learning_rate": 2.1145985401459857e-05,
      "loss": 1.0513,
      "step": 79060
    },
    {
      "epoch": 577.1532846715329,
      "grad_norm": 5.066650867462158,
      "learning_rate": 2.1142335766423358e-05,
      "loss": 0.9431,
      "step": 79070
    },
    {
      "epoch": 577.2262773722628,
      "grad_norm": 0.0416877306997776,
      "learning_rate": 2.113868613138686e-05,
      "loss": 0.8034,
      "step": 79080
    },
    {
      "epoch": 577.2992700729927,
      "grad_norm": 18.989761352539062,
      "learning_rate": 2.1135036496350365e-05,
      "loss": 0.7372,
      "step": 79090
    },
    {
      "epoch": 577.3722627737226,
      "grad_norm": 8.387713432312012,
      "learning_rate": 2.113138686131387e-05,
      "loss": 0.6273,
      "step": 79100
    },
    {
      "epoch": 577.4452554744526,
      "grad_norm": 5.71636962890625,
      "learning_rate": 2.1127737226277373e-05,
      "loss": 0.8949,
      "step": 79110
    },
    {
      "epoch": 577.5182481751825,
      "grad_norm": 11.156489372253418,
      "learning_rate": 2.1124087591240877e-05,
      "loss": 0.9548,
      "step": 79120
    },
    {
      "epoch": 577.5912408759124,
      "grad_norm": 18.575443267822266,
      "learning_rate": 2.112043795620438e-05,
      "loss": 0.9371,
      "step": 79130
    },
    {
      "epoch": 577.6642335766423,
      "grad_norm": 11.923321723937988,
      "learning_rate": 2.1116788321167885e-05,
      "loss": 0.7476,
      "step": 79140
    },
    {
      "epoch": 577.7372262773723,
      "grad_norm": 14.935722351074219,
      "learning_rate": 2.1113138686131385e-05,
      "loss": 0.9418,
      "step": 79150
    },
    {
      "epoch": 577.8102189781022,
      "grad_norm": 19.775760650634766,
      "learning_rate": 2.1109489051094893e-05,
      "loss": 1.4196,
      "step": 79160
    },
    {
      "epoch": 577.8832116788321,
      "grad_norm": 9.384742736816406,
      "learning_rate": 2.1105839416058396e-05,
      "loss": 0.7616,
      "step": 79170
    },
    {
      "epoch": 577.956204379562,
      "grad_norm": 8.248434066772461,
      "learning_rate": 2.11021897810219e-05,
      "loss": 0.8631,
      "step": 79180
    },
    {
      "epoch": 578.029197080292,
      "grad_norm": 0.0561814047396183,
      "learning_rate": 2.10985401459854e-05,
      "loss": 0.8222,
      "step": 79190
    },
    {
      "epoch": 578.1021897810219,
      "grad_norm": 11.030728340148926,
      "learning_rate": 2.1094890510948905e-05,
      "loss": 0.9378,
      "step": 79200
    },
    {
      "epoch": 578.1751824817518,
      "grad_norm": 11.780082702636719,
      "learning_rate": 2.1091240875912412e-05,
      "loss": 0.697,
      "step": 79210
    },
    {
      "epoch": 578.2481751824818,
      "grad_norm": 0.8479506969451904,
      "learning_rate": 2.1087591240875916e-05,
      "loss": 0.6804,
      "step": 79220
    },
    {
      "epoch": 578.3211678832117,
      "grad_norm": 14.199773788452148,
      "learning_rate": 2.1083941605839416e-05,
      "loss": 1.0116,
      "step": 79230
    },
    {
      "epoch": 578.3941605839416,
      "grad_norm": 9.761757850646973,
      "learning_rate": 2.108029197080292e-05,
      "loss": 0.8684,
      "step": 79240
    },
    {
      "epoch": 578.4671532846716,
      "grad_norm": 12.061929702758789,
      "learning_rate": 2.1076642335766424e-05,
      "loss": 0.6329,
      "step": 79250
    },
    {
      "epoch": 578.5401459854014,
      "grad_norm": 16.20299530029297,
      "learning_rate": 2.1072992700729928e-05,
      "loss": 1.0644,
      "step": 79260
    },
    {
      "epoch": 578.6131386861314,
      "grad_norm": 13.34732437133789,
      "learning_rate": 2.1069343065693432e-05,
      "loss": 0.909,
      "step": 79270
    },
    {
      "epoch": 578.6861313868613,
      "grad_norm": 7.203158855438232,
      "learning_rate": 2.1065693430656936e-05,
      "loss": 1.2322,
      "step": 79280
    },
    {
      "epoch": 578.7591240875912,
      "grad_norm": 7.923095703125,
      "learning_rate": 2.106204379562044e-05,
      "loss": 0.8296,
      "step": 79290
    },
    {
      "epoch": 578.8321167883212,
      "grad_norm": 16.288740158081055,
      "learning_rate": 2.105839416058394e-05,
      "loss": 1.1097,
      "step": 79300
    },
    {
      "epoch": 578.9051094890511,
      "grad_norm": 0.44839221239089966,
      "learning_rate": 2.1054744525547447e-05,
      "loss": 0.8932,
      "step": 79310
    },
    {
      "epoch": 578.978102189781,
      "grad_norm": 11.771587371826172,
      "learning_rate": 2.105109489051095e-05,
      "loss": 0.682,
      "step": 79320
    },
    {
      "epoch": 579.051094890511,
      "grad_norm": 18.219589233398438,
      "learning_rate": 2.1047445255474455e-05,
      "loss": 1.2928,
      "step": 79330
    },
    {
      "epoch": 579.1240875912408,
      "grad_norm": 0.06104932352900505,
      "learning_rate": 2.1043795620437956e-05,
      "loss": 0.9179,
      "step": 79340
    },
    {
      "epoch": 579.1970802919708,
      "grad_norm": 6.370360374450684,
      "learning_rate": 2.104014598540146e-05,
      "loss": 0.7739,
      "step": 79350
    },
    {
      "epoch": 579.2700729927008,
      "grad_norm": 11.372373580932617,
      "learning_rate": 2.1036496350364963e-05,
      "loss": 0.8008,
      "step": 79360
    },
    {
      "epoch": 579.3430656934306,
      "grad_norm": 14.445929527282715,
      "learning_rate": 2.103284671532847e-05,
      "loss": 0.8932,
      "step": 79370
    },
    {
      "epoch": 579.4160583941606,
      "grad_norm": 7.437005043029785,
      "learning_rate": 2.102919708029197e-05,
      "loss": 0.8635,
      "step": 79380
    },
    {
      "epoch": 579.4890510948906,
      "grad_norm": 15.595540046691895,
      "learning_rate": 2.1025547445255475e-05,
      "loss": 0.862,
      "step": 79390
    },
    {
      "epoch": 579.5620437956204,
      "grad_norm": 15.449691772460938,
      "learning_rate": 2.102189781021898e-05,
      "loss": 0.8515,
      "step": 79400
    },
    {
      "epoch": 579.6350364963504,
      "grad_norm": 9.644954681396484,
      "learning_rate": 2.1018248175182483e-05,
      "loss": 0.9816,
      "step": 79410
    },
    {
      "epoch": 579.7080291970802,
      "grad_norm": 5.62483549118042,
      "learning_rate": 2.1014598540145987e-05,
      "loss": 0.6763,
      "step": 79420
    },
    {
      "epoch": 579.7810218978102,
      "grad_norm": 12.71097469329834,
      "learning_rate": 2.101094890510949e-05,
      "loss": 1.3025,
      "step": 79430
    },
    {
      "epoch": 579.8540145985402,
      "grad_norm": 14.143797874450684,
      "learning_rate": 2.1007299270072994e-05,
      "loss": 0.9556,
      "step": 79440
    },
    {
      "epoch": 579.92700729927,
      "grad_norm": 13.848001480102539,
      "learning_rate": 2.10036496350365e-05,
      "loss": 0.806,
      "step": 79450
    },
    {
      "epoch": 580.0,
      "grad_norm": 0.12695182859897614,
      "learning_rate": 2.1e-05,
      "loss": 0.6682,
      "step": 79460
    },
    {
      "epoch": 580.07299270073,
      "grad_norm": 11.757164001464844,
      "learning_rate": 2.0996350364963506e-05,
      "loss": 0.7124,
      "step": 79470
    },
    {
      "epoch": 580.1459854014598,
      "grad_norm": 11.882996559143066,
      "learning_rate": 2.099270072992701e-05,
      "loss": 1.1763,
      "step": 79480
    },
    {
      "epoch": 580.2189781021898,
      "grad_norm": 10.32214069366455,
      "learning_rate": 2.098905109489051e-05,
      "loss": 0.7235,
      "step": 79490
    },
    {
      "epoch": 580.2919708029198,
      "grad_norm": 9.005826950073242,
      "learning_rate": 2.0985401459854014e-05,
      "loss": 1.0541,
      "step": 79500
    },
    {
      "epoch": 580.3649635036496,
      "grad_norm": 10.651808738708496,
      "learning_rate": 2.0981751824817518e-05,
      "loss": 1.0352,
      "step": 79510
    },
    {
      "epoch": 580.4379562043796,
      "grad_norm": 16.819185256958008,
      "learning_rate": 2.0978102189781022e-05,
      "loss": 0.7878,
      "step": 79520
    },
    {
      "epoch": 580.5109489051094,
      "grad_norm": 0.0894765630364418,
      "learning_rate": 2.0974452554744526e-05,
      "loss": 0.6767,
      "step": 79530
    },
    {
      "epoch": 580.5839416058394,
      "grad_norm": 9.558581352233887,
      "learning_rate": 2.097080291970803e-05,
      "loss": 1.0891,
      "step": 79540
    },
    {
      "epoch": 580.6569343065694,
      "grad_norm": 9.270968437194824,
      "learning_rate": 2.0967153284671534e-05,
      "loss": 1.0512,
      "step": 79550
    },
    {
      "epoch": 580.7299270072992,
      "grad_norm": 7.847524642944336,
      "learning_rate": 2.0963503649635038e-05,
      "loss": 0.905,
      "step": 79560
    },
    {
      "epoch": 580.8029197080292,
      "grad_norm": 10.808516502380371,
      "learning_rate": 2.095985401459854e-05,
      "loss": 0.9136,
      "step": 79570
    },
    {
      "epoch": 580.8759124087592,
      "grad_norm": 0.1771836131811142,
      "learning_rate": 2.0956204379562045e-05,
      "loss": 0.5255,
      "step": 79580
    },
    {
      "epoch": 580.948905109489,
      "grad_norm": 0.09398985654115677,
      "learning_rate": 2.095255474452555e-05,
      "loss": 0.7302,
      "step": 79590
    },
    {
      "epoch": 581.021897810219,
      "grad_norm": 8.512316703796387,
      "learning_rate": 2.0948905109489053e-05,
      "loss": 1.1106,
      "step": 79600
    },
    {
      "epoch": 581.0948905109489,
      "grad_norm": 7.564980983734131,
      "learning_rate": 2.0945255474452554e-05,
      "loss": 0.7937,
      "step": 79610
    },
    {
      "epoch": 581.1678832116788,
      "grad_norm": 10.982656478881836,
      "learning_rate": 2.0941605839416058e-05,
      "loss": 1.0553,
      "step": 79620
    },
    {
      "epoch": 581.2408759124088,
      "grad_norm": 10.41712760925293,
      "learning_rate": 2.0937956204379565e-05,
      "loss": 0.9457,
      "step": 79630
    },
    {
      "epoch": 581.3138686131387,
      "grad_norm": 10.048834800720215,
      "learning_rate": 2.093430656934307e-05,
      "loss": 1.0721,
      "step": 79640
    },
    {
      "epoch": 581.3868613138686,
      "grad_norm": 13.607089042663574,
      "learning_rate": 2.093065693430657e-05,
      "loss": 0.9143,
      "step": 79650
    },
    {
      "epoch": 581.4598540145986,
      "grad_norm": 7.496973514556885,
      "learning_rate": 2.0927007299270073e-05,
      "loss": 1.2517,
      "step": 79660
    },
    {
      "epoch": 581.5328467153284,
      "grad_norm": 0.05407783016562462,
      "learning_rate": 2.0923357664233577e-05,
      "loss": 0.8885,
      "step": 79670
    },
    {
      "epoch": 581.6058394160584,
      "grad_norm": 0.061884671449661255,
      "learning_rate": 2.0919708029197084e-05,
      "loss": 0.708,
      "step": 79680
    },
    {
      "epoch": 581.6788321167883,
      "grad_norm": 6.2691969871521,
      "learning_rate": 2.0916058394160585e-05,
      "loss": 0.9549,
      "step": 79690
    },
    {
      "epoch": 581.7518248175182,
      "grad_norm": 0.05087919905781746,
      "learning_rate": 2.091240875912409e-05,
      "loss": 0.7506,
      "step": 79700
    },
    {
      "epoch": 581.8248175182482,
      "grad_norm": 13.557530403137207,
      "learning_rate": 2.0908759124087593e-05,
      "loss": 0.6063,
      "step": 79710
    },
    {
      "epoch": 581.8978102189781,
      "grad_norm": 14.019875526428223,
      "learning_rate": 2.0905109489051093e-05,
      "loss": 0.9511,
      "step": 79720
    },
    {
      "epoch": 581.970802919708,
      "grad_norm": 0.0308846402913332,
      "learning_rate": 2.09014598540146e-05,
      "loss": 1.0955,
      "step": 79730
    },
    {
      "epoch": 582.043795620438,
      "grad_norm": 13.661590576171875,
      "learning_rate": 2.0897810218978104e-05,
      "loss": 0.7254,
      "step": 79740
    },
    {
      "epoch": 582.1167883211679,
      "grad_norm": 15.632692337036133,
      "learning_rate": 2.0894160583941608e-05,
      "loss": 1.0844,
      "step": 79750
    },
    {
      "epoch": 582.1897810218978,
      "grad_norm": 6.698800563812256,
      "learning_rate": 2.089051094890511e-05,
      "loss": 0.902,
      "step": 79760
    },
    {
      "epoch": 582.2627737226277,
      "grad_norm": 10.712432861328125,
      "learning_rate": 2.0886861313868612e-05,
      "loss": 0.994,
      "step": 79770
    },
    {
      "epoch": 582.3357664233577,
      "grad_norm": 16.53957176208496,
      "learning_rate": 2.088321167883212e-05,
      "loss": 1.1323,
      "step": 79780
    },
    {
      "epoch": 582.4087591240876,
      "grad_norm": 10.800646781921387,
      "learning_rate": 2.0879562043795624e-05,
      "loss": 0.7207,
      "step": 79790
    },
    {
      "epoch": 582.4817518248175,
      "grad_norm": 12.089911460876465,
      "learning_rate": 2.0875912408759124e-05,
      "loss": 1.2084,
      "step": 79800
    },
    {
      "epoch": 582.5547445255474,
      "grad_norm": 5.847259521484375,
      "learning_rate": 2.0872262773722628e-05,
      "loss": 0.6214,
      "step": 79810
    },
    {
      "epoch": 582.6277372262774,
      "grad_norm": 5.779404640197754,
      "learning_rate": 2.0868613138686132e-05,
      "loss": 0.946,
      "step": 79820
    },
    {
      "epoch": 582.7007299270073,
      "grad_norm": 14.538844108581543,
      "learning_rate": 2.0864963503649636e-05,
      "loss": 0.9931,
      "step": 79830
    },
    {
      "epoch": 582.7737226277372,
      "grad_norm": 0.09189112484455109,
      "learning_rate": 2.086131386861314e-05,
      "loss": 1.1371,
      "step": 79840
    },
    {
      "epoch": 582.8467153284671,
      "grad_norm": 9.227947235107422,
      "learning_rate": 2.0857664233576643e-05,
      "loss": 1.2194,
      "step": 79850
    },
    {
      "epoch": 582.9197080291971,
      "grad_norm": 4.861805438995361,
      "learning_rate": 2.0854014598540147e-05,
      "loss": 0.7173,
      "step": 79860
    },
    {
      "epoch": 582.992700729927,
      "grad_norm": 9.52160358428955,
      "learning_rate": 2.085036496350365e-05,
      "loss": 0.6375,
      "step": 79870
    },
    {
      "epoch": 583.0656934306569,
      "grad_norm": 11.950241088867188,
      "learning_rate": 2.0846715328467155e-05,
      "loss": 0.8194,
      "step": 79880
    },
    {
      "epoch": 583.1386861313869,
      "grad_norm": 7.958868503570557,
      "learning_rate": 2.084306569343066e-05,
      "loss": 0.9311,
      "step": 79890
    },
    {
      "epoch": 583.2116788321168,
      "grad_norm": 0.039646074175834656,
      "learning_rate": 2.0839416058394163e-05,
      "loss": 0.8065,
      "step": 79900
    },
    {
      "epoch": 583.2846715328467,
      "grad_norm": 7.883755207061768,
      "learning_rate": 2.0835766423357667e-05,
      "loss": 0.9199,
      "step": 79910
    },
    {
      "epoch": 583.3576642335767,
      "grad_norm": 7.60719633102417,
      "learning_rate": 2.0832116788321167e-05,
      "loss": 0.9723,
      "step": 79920
    },
    {
      "epoch": 583.4306569343066,
      "grad_norm": 12.662187576293945,
      "learning_rate": 2.082846715328467e-05,
      "loss": 1.0574,
      "step": 79930
    },
    {
      "epoch": 583.5036496350365,
      "grad_norm": 7.611775875091553,
      "learning_rate": 2.082481751824818e-05,
      "loss": 1.0085,
      "step": 79940
    },
    {
      "epoch": 583.5766423357665,
      "grad_norm": 10.87818717956543,
      "learning_rate": 2.082116788321168e-05,
      "loss": 0.8558,
      "step": 79950
    },
    {
      "epoch": 583.6496350364963,
      "grad_norm": 12.802186012268066,
      "learning_rate": 2.0817518248175183e-05,
      "loss": 0.7911,
      "step": 79960
    },
    {
      "epoch": 583.7226277372263,
      "grad_norm": 6.971522808074951,
      "learning_rate": 2.0813868613138687e-05,
      "loss": 0.85,
      "step": 79970
    },
    {
      "epoch": 583.7956204379562,
      "grad_norm": 7.451981067657471,
      "learning_rate": 2.081021897810219e-05,
      "loss": 1.0076,
      "step": 79980
    },
    {
      "epoch": 583.8686131386861,
      "grad_norm": 0.023606540635228157,
      "learning_rate": 2.0806569343065694e-05,
      "loss": 0.8617,
      "step": 79990
    },
    {
      "epoch": 583.9416058394161,
      "grad_norm": 10.18302059173584,
      "learning_rate": 2.08029197080292e-05,
      "loss": 1.1088,
      "step": 80000
    },
    {
      "epoch": 584.014598540146,
      "grad_norm": 13.161175727844238,
      "learning_rate": 2.0799270072992702e-05,
      "loss": 0.7685,
      "step": 80010
    },
    {
      "epoch": 584.0875912408759,
      "grad_norm": 14.249297142028809,
      "learning_rate": 2.0795620437956206e-05,
      "loss": 0.9747,
      "step": 80020
    },
    {
      "epoch": 584.1605839416059,
      "grad_norm": 12.341048240661621,
      "learning_rate": 2.0791970802919707e-05,
      "loss": 0.8803,
      "step": 80030
    },
    {
      "epoch": 584.2335766423357,
      "grad_norm": 7.944994926452637,
      "learning_rate": 2.0788321167883214e-05,
      "loss": 0.449,
      "step": 80040
    },
    {
      "epoch": 584.3065693430657,
      "grad_norm": 11.801260948181152,
      "learning_rate": 2.0784671532846718e-05,
      "loss": 1.0055,
      "step": 80050
    },
    {
      "epoch": 584.3795620437957,
      "grad_norm": 6.541499614715576,
      "learning_rate": 2.078102189781022e-05,
      "loss": 0.9133,
      "step": 80060
    },
    {
      "epoch": 584.4525547445255,
      "grad_norm": 7.853071212768555,
      "learning_rate": 2.0777372262773722e-05,
      "loss": 1.1732,
      "step": 80070
    },
    {
      "epoch": 584.5255474452555,
      "grad_norm": 0.1036067083477974,
      "learning_rate": 2.0773722627737226e-05,
      "loss": 0.6029,
      "step": 80080
    },
    {
      "epoch": 584.5985401459855,
      "grad_norm": 9.32875919342041,
      "learning_rate": 2.077007299270073e-05,
      "loss": 1.1533,
      "step": 80090
    },
    {
      "epoch": 584.6715328467153,
      "grad_norm": 7.58872127532959,
      "learning_rate": 2.0766423357664237e-05,
      "loss": 1.2288,
      "step": 80100
    },
    {
      "epoch": 584.7445255474453,
      "grad_norm": 7.707119941711426,
      "learning_rate": 2.0762773722627738e-05,
      "loss": 0.8554,
      "step": 80110
    },
    {
      "epoch": 584.8175182481751,
      "grad_norm": 6.964879512786865,
      "learning_rate": 2.075912408759124e-05,
      "loss": 0.7124,
      "step": 80120
    },
    {
      "epoch": 584.8905109489051,
      "grad_norm": 0.054722100496292114,
      "learning_rate": 2.0755474452554745e-05,
      "loss": 0.5305,
      "step": 80130
    },
    {
      "epoch": 584.9635036496351,
      "grad_norm": 10.491582870483398,
      "learning_rate": 2.075182481751825e-05,
      "loss": 1.0419,
      "step": 80140
    },
    {
      "epoch": 585.0364963503649,
      "grad_norm": 9.694205284118652,
      "learning_rate": 2.0748175182481753e-05,
      "loss": 1.2135,
      "step": 80150
    },
    {
      "epoch": 585.1094890510949,
      "grad_norm": 17.313648223876953,
      "learning_rate": 2.0744525547445257e-05,
      "loss": 0.7765,
      "step": 80160
    },
    {
      "epoch": 585.1824817518249,
      "grad_norm": 7.813305377960205,
      "learning_rate": 2.074087591240876e-05,
      "loss": 1.1398,
      "step": 80170
    },
    {
      "epoch": 585.2554744525547,
      "grad_norm": 10.117823600769043,
      "learning_rate": 2.073722627737226e-05,
      "loss": 0.7419,
      "step": 80180
    },
    {
      "epoch": 585.3284671532847,
      "grad_norm": 12.967239379882812,
      "learning_rate": 2.0733576642335765e-05,
      "loss": 1.0564,
      "step": 80190
    },
    {
      "epoch": 585.4014598540145,
      "grad_norm": 10.863923072814941,
      "learning_rate": 2.0729927007299273e-05,
      "loss": 0.6819,
      "step": 80200
    },
    {
      "epoch": 585.4744525547445,
      "grad_norm": 7.531614780426025,
      "learning_rate": 2.0726277372262776e-05,
      "loss": 0.923,
      "step": 80210
    },
    {
      "epoch": 585.5474452554745,
      "grad_norm": 14.326959609985352,
      "learning_rate": 2.0722627737226277e-05,
      "loss": 1.1533,
      "step": 80220
    },
    {
      "epoch": 585.6204379562043,
      "grad_norm": 5.686578273773193,
      "learning_rate": 2.071897810218978e-05,
      "loss": 0.7375,
      "step": 80230
    },
    {
      "epoch": 585.6934306569343,
      "grad_norm": 0.13258832693099976,
      "learning_rate": 2.0715328467153285e-05,
      "loss": 0.723,
      "step": 80240
    },
    {
      "epoch": 585.7664233576643,
      "grad_norm": 7.113861083984375,
      "learning_rate": 2.0711678832116792e-05,
      "loss": 0.9994,
      "step": 80250
    },
    {
      "epoch": 585.8394160583941,
      "grad_norm": 9.411581039428711,
      "learning_rate": 2.0708029197080292e-05,
      "loss": 0.8615,
      "step": 80260
    },
    {
      "epoch": 585.9124087591241,
      "grad_norm": 14.98556137084961,
      "learning_rate": 2.0704379562043796e-05,
      "loss": 1.5629,
      "step": 80270
    },
    {
      "epoch": 585.985401459854,
      "grad_norm": 12.787055015563965,
      "learning_rate": 2.07007299270073e-05,
      "loss": 0.8444,
      "step": 80280
    },
    {
      "epoch": 586.0583941605839,
      "grad_norm": 0.035583775490522385,
      "learning_rate": 2.0697080291970804e-05,
      "loss": 1.0858,
      "step": 80290
    },
    {
      "epoch": 586.1313868613139,
      "grad_norm": 0.033263351768255234,
      "learning_rate": 2.0693430656934308e-05,
      "loss": 0.6301,
      "step": 80300
    },
    {
      "epoch": 586.2043795620438,
      "grad_norm": 7.253903865814209,
      "learning_rate": 2.0689781021897812e-05,
      "loss": 0.8398,
      "step": 80310
    },
    {
      "epoch": 586.2773722627737,
      "grad_norm": 11.414865493774414,
      "learning_rate": 2.0686131386861316e-05,
      "loss": 1.1735,
      "step": 80320
    },
    {
      "epoch": 586.3503649635037,
      "grad_norm": 6.557438373565674,
      "learning_rate": 2.068248175182482e-05,
      "loss": 0.7054,
      "step": 80330
    },
    {
      "epoch": 586.4233576642335,
      "grad_norm": 0.08024381101131439,
      "learning_rate": 2.067883211678832e-05,
      "loss": 0.5867,
      "step": 80340
    },
    {
      "epoch": 586.4963503649635,
      "grad_norm": 13.443764686584473,
      "learning_rate": 2.0675182481751827e-05,
      "loss": 1.2728,
      "step": 80350
    },
    {
      "epoch": 586.5693430656934,
      "grad_norm": 7.647920608520508,
      "learning_rate": 2.067153284671533e-05,
      "loss": 0.736,
      "step": 80360
    },
    {
      "epoch": 586.6423357664233,
      "grad_norm": 10.458708763122559,
      "learning_rate": 2.0667883211678832e-05,
      "loss": 1.1497,
      "step": 80370
    },
    {
      "epoch": 586.7153284671533,
      "grad_norm": 5.271579742431641,
      "learning_rate": 2.0664233576642336e-05,
      "loss": 0.5324,
      "step": 80380
    },
    {
      "epoch": 586.7883211678832,
      "grad_norm": 7.673997402191162,
      "learning_rate": 2.066058394160584e-05,
      "loss": 0.7944,
      "step": 80390
    },
    {
      "epoch": 586.8613138686131,
      "grad_norm": 15.414772987365723,
      "learning_rate": 2.0656934306569343e-05,
      "loss": 0.9414,
      "step": 80400
    },
    {
      "epoch": 586.9343065693431,
      "grad_norm": 7.364735126495361,
      "learning_rate": 2.0653284671532847e-05,
      "loss": 0.924,
      "step": 80410
    },
    {
      "epoch": 587.007299270073,
      "grad_norm": 7.614131927490234,
      "learning_rate": 2.064963503649635e-05,
      "loss": 1.3611,
      "step": 80420
    },
    {
      "epoch": 587.0802919708029,
      "grad_norm": 13.952831268310547,
      "learning_rate": 2.0645985401459855e-05,
      "loss": 0.8656,
      "step": 80430
    },
    {
      "epoch": 587.1532846715329,
      "grad_norm": 9.195480346679688,
      "learning_rate": 2.064233576642336e-05,
      "loss": 0.8375,
      "step": 80440
    },
    {
      "epoch": 587.2262773722628,
      "grad_norm": 13.164617538452148,
      "learning_rate": 2.0638686131386863e-05,
      "loss": 0.8011,
      "step": 80450
    },
    {
      "epoch": 587.2992700729927,
      "grad_norm": 9.18403148651123,
      "learning_rate": 2.0635036496350367e-05,
      "loss": 0.6083,
      "step": 80460
    },
    {
      "epoch": 587.3722627737226,
      "grad_norm": 7.159942150115967,
      "learning_rate": 2.063138686131387e-05,
      "loss": 0.979,
      "step": 80470
    },
    {
      "epoch": 587.4452554744526,
      "grad_norm": 9.161795616149902,
      "learning_rate": 2.0627737226277375e-05,
      "loss": 0.7767,
      "step": 80480
    },
    {
      "epoch": 587.5182481751825,
      "grad_norm": 7.250945091247559,
      "learning_rate": 2.0624087591240875e-05,
      "loss": 0.9885,
      "step": 80490
    },
    {
      "epoch": 587.5912408759124,
      "grad_norm": 7.184325218200684,
      "learning_rate": 2.062043795620438e-05,
      "loss": 0.9141,
      "step": 80500
    },
    {
      "epoch": 587.6642335766423,
      "grad_norm": 20.90001678466797,
      "learning_rate": 2.0616788321167886e-05,
      "loss": 0.9396,
      "step": 80510
    },
    {
      "epoch": 587.7372262773723,
      "grad_norm": 13.925883293151855,
      "learning_rate": 2.061313868613139e-05,
      "loss": 1.1369,
      "step": 80520
    },
    {
      "epoch": 587.8102189781022,
      "grad_norm": 0.032412540167570114,
      "learning_rate": 2.060948905109489e-05,
      "loss": 0.7622,
      "step": 80530
    },
    {
      "epoch": 587.8832116788321,
      "grad_norm": 0.03433915972709656,
      "learning_rate": 2.0605839416058394e-05,
      "loss": 0.7191,
      "step": 80540
    },
    {
      "epoch": 587.956204379562,
      "grad_norm": 9.202123641967773,
      "learning_rate": 2.0602189781021898e-05,
      "loss": 1.0501,
      "step": 80550
    },
    {
      "epoch": 588.029197080292,
      "grad_norm": 8.831279754638672,
      "learning_rate": 2.0598540145985402e-05,
      "loss": 0.9763,
      "step": 80560
    },
    {
      "epoch": 588.1021897810219,
      "grad_norm": 12.903059005737305,
      "learning_rate": 2.0594890510948906e-05,
      "loss": 0.6196,
      "step": 80570
    },
    {
      "epoch": 588.1751824817518,
      "grad_norm": 8.318758010864258,
      "learning_rate": 2.059124087591241e-05,
      "loss": 1.1673,
      "step": 80580
    },
    {
      "epoch": 588.2481751824818,
      "grad_norm": 8.877131462097168,
      "learning_rate": 2.0587591240875914e-05,
      "loss": 0.7946,
      "step": 80590
    },
    {
      "epoch": 588.3211678832117,
      "grad_norm": 14.630013465881348,
      "learning_rate": 2.0583941605839414e-05,
      "loss": 1.4802,
      "step": 80600
    },
    {
      "epoch": 588.3941605839416,
      "grad_norm": 8.762319564819336,
      "learning_rate": 2.058029197080292e-05,
      "loss": 0.7013,
      "step": 80610
    },
    {
      "epoch": 588.4671532846716,
      "grad_norm": 8.738951683044434,
      "learning_rate": 2.0576642335766425e-05,
      "loss": 0.8002,
      "step": 80620
    },
    {
      "epoch": 588.5401459854014,
      "grad_norm": 8.250031471252441,
      "learning_rate": 2.057299270072993e-05,
      "loss": 0.7413,
      "step": 80630
    },
    {
      "epoch": 588.6131386861314,
      "grad_norm": 8.354479789733887,
      "learning_rate": 2.056934306569343e-05,
      "loss": 0.5727,
      "step": 80640
    },
    {
      "epoch": 588.6861313868613,
      "grad_norm": 14.65962028503418,
      "learning_rate": 2.0565693430656934e-05,
      "loss": 1.0571,
      "step": 80650
    },
    {
      "epoch": 588.7591240875912,
      "grad_norm": 15.675423622131348,
      "learning_rate": 2.0562043795620438e-05,
      "loss": 1.0661,
      "step": 80660
    },
    {
      "epoch": 588.8321167883212,
      "grad_norm": 10.441667556762695,
      "learning_rate": 2.0558394160583945e-05,
      "loss": 0.9633,
      "step": 80670
    },
    {
      "epoch": 588.9051094890511,
      "grad_norm": 11.7101469039917,
      "learning_rate": 2.0554744525547445e-05,
      "loss": 1.131,
      "step": 80680
    },
    {
      "epoch": 588.978102189781,
      "grad_norm": 10.022135734558105,
      "learning_rate": 2.055109489051095e-05,
      "loss": 0.6971,
      "step": 80690
    },
    {
      "epoch": 589.051094890511,
      "grad_norm": 10.314480781555176,
      "learning_rate": 2.0547445255474453e-05,
      "loss": 1.2067,
      "step": 80700
    },
    {
      "epoch": 589.1240875912408,
      "grad_norm": 12.875609397888184,
      "learning_rate": 2.0543795620437957e-05,
      "loss": 1.3825,
      "step": 80710
    },
    {
      "epoch": 589.1970802919708,
      "grad_norm": 0.030528247356414795,
      "learning_rate": 2.054014598540146e-05,
      "loss": 0.7511,
      "step": 80720
    },
    {
      "epoch": 589.2700729927008,
      "grad_norm": 7.429754257202148,
      "learning_rate": 2.0536496350364965e-05,
      "loss": 0.888,
      "step": 80730
    },
    {
      "epoch": 589.3430656934306,
      "grad_norm": 0.11141043901443481,
      "learning_rate": 2.053284671532847e-05,
      "loss": 0.6435,
      "step": 80740
    },
    {
      "epoch": 589.4160583941606,
      "grad_norm": 7.147293567657471,
      "learning_rate": 2.0529197080291973e-05,
      "loss": 0.683,
      "step": 80750
    },
    {
      "epoch": 589.4890510948906,
      "grad_norm": 10.054240226745605,
      "learning_rate": 2.0525547445255473e-05,
      "loss": 0.8291,
      "step": 80760
    },
    {
      "epoch": 589.5620437956204,
      "grad_norm": 3.990407943725586,
      "learning_rate": 2.052189781021898e-05,
      "loss": 0.8224,
      "step": 80770
    },
    {
      "epoch": 589.6350364963504,
      "grad_norm": 9.76610279083252,
      "learning_rate": 2.0518248175182484e-05,
      "loss": 0.7041,
      "step": 80780
    },
    {
      "epoch": 589.7080291970802,
      "grad_norm": 17.819984436035156,
      "learning_rate": 2.0514598540145985e-05,
      "loss": 0.8572,
      "step": 80790
    },
    {
      "epoch": 589.7810218978102,
      "grad_norm": 9.288152694702148,
      "learning_rate": 2.051094890510949e-05,
      "loss": 1.1397,
      "step": 80800
    },
    {
      "epoch": 589.8540145985402,
      "grad_norm": 4.9015421867370605,
      "learning_rate": 2.0507299270072992e-05,
      "loss": 1.0371,
      "step": 80810
    },
    {
      "epoch": 589.92700729927,
      "grad_norm": 12.600017547607422,
      "learning_rate": 2.05036496350365e-05,
      "loss": 1.0085,
      "step": 80820
    },
    {
      "epoch": 590.0,
      "grad_norm": 0.03878113999962807,
      "learning_rate": 2.05e-05,
      "loss": 0.7421,
      "step": 80830
    },
    {
      "epoch": 590.07299270073,
      "grad_norm": 3.460782766342163,
      "learning_rate": 2.0496350364963504e-05,
      "loss": 0.6353,
      "step": 80840
    },
    {
      "epoch": 590.1459854014598,
      "grad_norm": 0.05015847459435463,
      "learning_rate": 2.0492700729927008e-05,
      "loss": 0.9243,
      "step": 80850
    },
    {
      "epoch": 590.2189781021898,
      "grad_norm": 5.4813690185546875,
      "learning_rate": 2.0489051094890512e-05,
      "loss": 0.8032,
      "step": 80860
    },
    {
      "epoch": 590.2919708029198,
      "grad_norm": 10.127166748046875,
      "learning_rate": 2.0485401459854016e-05,
      "loss": 1.1189,
      "step": 80870
    },
    {
      "epoch": 590.3649635036496,
      "grad_norm": 9.48726749420166,
      "learning_rate": 2.048175182481752e-05,
      "loss": 1.0367,
      "step": 80880
    },
    {
      "epoch": 590.4379562043796,
      "grad_norm": 0.022951122373342514,
      "learning_rate": 2.0478102189781024e-05,
      "loss": 0.976,
      "step": 80890
    },
    {
      "epoch": 590.5109489051094,
      "grad_norm": 6.770526885986328,
      "learning_rate": 2.0474452554744527e-05,
      "loss": 0.5581,
      "step": 80900
    },
    {
      "epoch": 590.5839416058394,
      "grad_norm": 7.330738067626953,
      "learning_rate": 2.0470802919708028e-05,
      "loss": 1.0311,
      "step": 80910
    },
    {
      "epoch": 590.6569343065694,
      "grad_norm": 5.536131858825684,
      "learning_rate": 2.0467153284671535e-05,
      "loss": 1.056,
      "step": 80920
    },
    {
      "epoch": 590.7299270072992,
      "grad_norm": 7.4915971755981445,
      "learning_rate": 2.046350364963504e-05,
      "loss": 0.7432,
      "step": 80930
    },
    {
      "epoch": 590.8029197080292,
      "grad_norm": 12.091763496398926,
      "learning_rate": 2.0459854014598543e-05,
      "loss": 0.8687,
      "step": 80940
    },
    {
      "epoch": 590.8759124087592,
      "grad_norm": 17.55762481689453,
      "learning_rate": 2.0456204379562043e-05,
      "loss": 0.9283,
      "step": 80950
    },
    {
      "epoch": 590.948905109489,
      "grad_norm": 13.635428428649902,
      "learning_rate": 2.0452554744525547e-05,
      "loss": 0.8577,
      "step": 80960
    },
    {
      "epoch": 591.021897810219,
      "grad_norm": 3.9748711585998535,
      "learning_rate": 2.044890510948905e-05,
      "loss": 0.8996,
      "step": 80970
    },
    {
      "epoch": 591.0948905109489,
      "grad_norm": 10.554996490478516,
      "learning_rate": 2.044525547445256e-05,
      "loss": 0.3032,
      "step": 80980
    },
    {
      "epoch": 591.1678832116788,
      "grad_norm": 8.161588668823242,
      "learning_rate": 2.044160583941606e-05,
      "loss": 0.7298,
      "step": 80990
    },
    {
      "epoch": 591.2408759124088,
      "grad_norm": 12.797181129455566,
      "learning_rate": 2.0437956204379563e-05,
      "loss": 1.1482,
      "step": 81000
    },
    {
      "epoch": 591.3138686131387,
      "grad_norm": 14.9509916305542,
      "learning_rate": 2.0434306569343067e-05,
      "loss": 1.2617,
      "step": 81010
    },
    {
      "epoch": 591.3868613138686,
      "grad_norm": 10.580316543579102,
      "learning_rate": 2.043065693430657e-05,
      "loss": 0.6883,
      "step": 81020
    },
    {
      "epoch": 591.4598540145986,
      "grad_norm": 19.632850646972656,
      "learning_rate": 2.0427007299270074e-05,
      "loss": 0.991,
      "step": 81030
    },
    {
      "epoch": 591.5328467153284,
      "grad_norm": 6.307230472564697,
      "learning_rate": 2.042335766423358e-05,
      "loss": 0.7119,
      "step": 81040
    },
    {
      "epoch": 591.6058394160584,
      "grad_norm": 12.416662216186523,
      "learning_rate": 2.0419708029197082e-05,
      "loss": 1.3184,
      "step": 81050
    },
    {
      "epoch": 591.6788321167883,
      "grad_norm": 0.2061779946088791,
      "learning_rate": 2.0416058394160583e-05,
      "loss": 0.8441,
      "step": 81060
    },
    {
      "epoch": 591.7518248175182,
      "grad_norm": 9.224417686462402,
      "learning_rate": 2.0412408759124087e-05,
      "loss": 0.9085,
      "step": 81070
    },
    {
      "epoch": 591.8248175182482,
      "grad_norm": 18.444984436035156,
      "learning_rate": 2.0408759124087594e-05,
      "loss": 1.0761,
      "step": 81080
    },
    {
      "epoch": 591.8978102189781,
      "grad_norm": 12.653834342956543,
      "learning_rate": 2.0405109489051098e-05,
      "loss": 1.0662,
      "step": 81090
    },
    {
      "epoch": 591.970802919708,
      "grad_norm": 0.034309886395931244,
      "learning_rate": 2.0401459854014598e-05,
      "loss": 0.8714,
      "step": 81100
    },
    {
      "epoch": 592.043795620438,
      "grad_norm": 0.04912471026182175,
      "learning_rate": 2.0397810218978102e-05,
      "loss": 1.0848,
      "step": 81110
    },
    {
      "epoch": 592.1167883211679,
      "grad_norm": 0.043743815273046494,
      "learning_rate": 2.0394160583941606e-05,
      "loss": 1.0212,
      "step": 81120
    },
    {
      "epoch": 592.1897810218978,
      "grad_norm": 5.591742038726807,
      "learning_rate": 2.039051094890511e-05,
      "loss": 0.7985,
      "step": 81130
    },
    {
      "epoch": 592.2627737226277,
      "grad_norm": 13.607751846313477,
      "learning_rate": 2.0386861313868614e-05,
      "loss": 0.9983,
      "step": 81140
    },
    {
      "epoch": 592.3357664233577,
      "grad_norm": 6.406525611877441,
      "learning_rate": 2.0383211678832118e-05,
      "loss": 1.1283,
      "step": 81150
    },
    {
      "epoch": 592.4087591240876,
      "grad_norm": 13.475332260131836,
      "learning_rate": 2.037956204379562e-05,
      "loss": 0.9994,
      "step": 81160
    },
    {
      "epoch": 592.4817518248175,
      "grad_norm": 7.67202615737915,
      "learning_rate": 2.0375912408759125e-05,
      "loss": 0.53,
      "step": 81170
    },
    {
      "epoch": 592.5547445255474,
      "grad_norm": 13.018706321716309,
      "learning_rate": 2.037226277372263e-05,
      "loss": 1.044,
      "step": 81180
    },
    {
      "epoch": 592.6277372262774,
      "grad_norm": 5.366607189178467,
      "learning_rate": 2.0368613138686133e-05,
      "loss": 0.7,
      "step": 81190
    },
    {
      "epoch": 592.7007299270073,
      "grad_norm": 2.3883113861083984,
      "learning_rate": 2.0364963503649637e-05,
      "loss": 0.6897,
      "step": 81200
    },
    {
      "epoch": 592.7737226277372,
      "grad_norm": 15.053560256958008,
      "learning_rate": 2.0361313868613138e-05,
      "loss": 1.0609,
      "step": 81210
    },
    {
      "epoch": 592.8467153284671,
      "grad_norm": 13.79166316986084,
      "learning_rate": 2.035766423357664e-05,
      "loss": 0.9908,
      "step": 81220
    },
    {
      "epoch": 592.9197080291971,
      "grad_norm": 12.208420753479004,
      "learning_rate": 2.0354014598540145e-05,
      "loss": 0.8801,
      "step": 81230
    },
    {
      "epoch": 592.992700729927,
      "grad_norm": 11.154475212097168,
      "learning_rate": 2.0350364963503653e-05,
      "loss": 0.8491,
      "step": 81240
    },
    {
      "epoch": 593.0656934306569,
      "grad_norm": 0.10471080988645554,
      "learning_rate": 2.0346715328467153e-05,
      "loss": 0.9842,
      "step": 81250
    },
    {
      "epoch": 593.1386861313869,
      "grad_norm": 0.09554906189441681,
      "learning_rate": 2.0343065693430657e-05,
      "loss": 0.9224,
      "step": 81260
    },
    {
      "epoch": 593.2116788321168,
      "grad_norm": 11.956412315368652,
      "learning_rate": 2.033941605839416e-05,
      "loss": 1.2418,
      "step": 81270
    },
    {
      "epoch": 593.2846715328467,
      "grad_norm": 15.46446704864502,
      "learning_rate": 2.0335766423357665e-05,
      "loss": 1.1055,
      "step": 81280
    },
    {
      "epoch": 593.3576642335767,
      "grad_norm": 0.019263233989477158,
      "learning_rate": 2.033211678832117e-05,
      "loss": 0.9737,
      "step": 81290
    },
    {
      "epoch": 593.4306569343066,
      "grad_norm": 12.317691802978516,
      "learning_rate": 2.0328467153284673e-05,
      "loss": 0.9243,
      "step": 81300
    },
    {
      "epoch": 593.5036496350365,
      "grad_norm": 6.803791046142578,
      "learning_rate": 2.0324817518248176e-05,
      "loss": 0.8112,
      "step": 81310
    },
    {
      "epoch": 593.5766423357665,
      "grad_norm": 8.579222679138184,
      "learning_rate": 2.032116788321168e-05,
      "loss": 0.5555,
      "step": 81320
    },
    {
      "epoch": 593.6496350364963,
      "grad_norm": 13.429017066955566,
      "learning_rate": 2.031751824817518e-05,
      "loss": 0.7707,
      "step": 81330
    },
    {
      "epoch": 593.7226277372263,
      "grad_norm": 6.873630046844482,
      "learning_rate": 2.0313868613138688e-05,
      "loss": 0.8737,
      "step": 81340
    },
    {
      "epoch": 593.7956204379562,
      "grad_norm": 9.6530179977417,
      "learning_rate": 2.0310218978102192e-05,
      "loss": 0.6559,
      "step": 81350
    },
    {
      "epoch": 593.8686131386861,
      "grad_norm": 7.6793694496154785,
      "learning_rate": 2.0306569343065696e-05,
      "loss": 1.3943,
      "step": 81360
    },
    {
      "epoch": 593.9416058394161,
      "grad_norm": 14.50386905670166,
      "learning_rate": 2.0302919708029196e-05,
      "loss": 0.6363,
      "step": 81370
    },
    {
      "epoch": 594.014598540146,
      "grad_norm": 18.933502197265625,
      "learning_rate": 2.02992700729927e-05,
      "loss": 1.0948,
      "step": 81380
    },
    {
      "epoch": 594.0875912408759,
      "grad_norm": 4.210506439208984,
      "learning_rate": 2.0295620437956207e-05,
      "loss": 0.6885,
      "step": 81390
    },
    {
      "epoch": 594.1605839416059,
      "grad_norm": 0.023208390921354294,
      "learning_rate": 2.029197080291971e-05,
      "loss": 1.0081,
      "step": 81400
    },
    {
      "epoch": 594.2335766423357,
      "grad_norm": 14.1274995803833,
      "learning_rate": 2.0288321167883212e-05,
      "loss": 0.9786,
      "step": 81410
    },
    {
      "epoch": 594.3065693430657,
      "grad_norm": 12.80433464050293,
      "learning_rate": 2.0284671532846716e-05,
      "loss": 0.7513,
      "step": 81420
    },
    {
      "epoch": 594.3795620437957,
      "grad_norm": 9.907898902893066,
      "learning_rate": 2.028102189781022e-05,
      "loss": 0.5227,
      "step": 81430
    },
    {
      "epoch": 594.4525547445255,
      "grad_norm": 13.515937805175781,
      "learning_rate": 2.0277372262773724e-05,
      "loss": 0.7023,
      "step": 81440
    },
    {
      "epoch": 594.5255474452555,
      "grad_norm": 0.8304605484008789,
      "learning_rate": 2.0273722627737227e-05,
      "loss": 0.7303,
      "step": 81450
    },
    {
      "epoch": 594.5985401459855,
      "grad_norm": 18.499778747558594,
      "learning_rate": 2.027007299270073e-05,
      "loss": 1.4492,
      "step": 81460
    },
    {
      "epoch": 594.6715328467153,
      "grad_norm": 0.21258169412612915,
      "learning_rate": 2.0266423357664235e-05,
      "loss": 1.0092,
      "step": 81470
    },
    {
      "epoch": 594.7445255474453,
      "grad_norm": 6.724307060241699,
      "learning_rate": 2.0262773722627736e-05,
      "loss": 0.9564,
      "step": 81480
    },
    {
      "epoch": 594.8175182481751,
      "grad_norm": 0.033680275082588196,
      "learning_rate": 2.0259124087591243e-05,
      "loss": 0.8966,
      "step": 81490
    },
    {
      "epoch": 594.8905109489051,
      "grad_norm": 0.030724208801984787,
      "learning_rate": 2.0255474452554747e-05,
      "loss": 1.0404,
      "step": 81500
    },
    {
      "epoch": 594.9635036496351,
      "grad_norm": 15.248441696166992,
      "learning_rate": 2.025182481751825e-05,
      "loss": 0.8637,
      "step": 81510
    },
    {
      "epoch": 595.0364963503649,
      "grad_norm": 6.309360980987549,
      "learning_rate": 2.024817518248175e-05,
      "loss": 0.6943,
      "step": 81520
    },
    {
      "epoch": 595.1094890510949,
      "grad_norm": 13.6616792678833,
      "learning_rate": 2.0244525547445255e-05,
      "loss": 1.1784,
      "step": 81530
    },
    {
      "epoch": 595.1824817518249,
      "grad_norm": 10.141061782836914,
      "learning_rate": 2.024087591240876e-05,
      "loss": 0.6701,
      "step": 81540
    },
    {
      "epoch": 595.2554744525547,
      "grad_norm": 11.212444305419922,
      "learning_rate": 2.0237226277372266e-05,
      "loss": 0.9474,
      "step": 81550
    },
    {
      "epoch": 595.3284671532847,
      "grad_norm": 11.972257614135742,
      "learning_rate": 2.0233576642335767e-05,
      "loss": 0.6853,
      "step": 81560
    },
    {
      "epoch": 595.4014598540145,
      "grad_norm": 13.666441917419434,
      "learning_rate": 2.022992700729927e-05,
      "loss": 1.0834,
      "step": 81570
    },
    {
      "epoch": 595.4744525547445,
      "grad_norm": 7.8754191398620605,
      "learning_rate": 2.0226277372262774e-05,
      "loss": 0.9207,
      "step": 81580
    },
    {
      "epoch": 595.5474452554745,
      "grad_norm": 0.09545652568340302,
      "learning_rate": 2.022262773722628e-05,
      "loss": 0.8234,
      "step": 81590
    },
    {
      "epoch": 595.6204379562043,
      "grad_norm": 8.608199119567871,
      "learning_rate": 2.0218978102189782e-05,
      "loss": 0.7692,
      "step": 81600
    },
    {
      "epoch": 595.6934306569343,
      "grad_norm": 0.06398461014032364,
      "learning_rate": 2.0215328467153286e-05,
      "loss": 0.8511,
      "step": 81610
    },
    {
      "epoch": 595.7664233576643,
      "grad_norm": 6.512141704559326,
      "learning_rate": 2.021167883211679e-05,
      "loss": 0.8656,
      "step": 81620
    },
    {
      "epoch": 595.8394160583941,
      "grad_norm": 12.09179401397705,
      "learning_rate": 2.0208029197080294e-05,
      "loss": 0.6449,
      "step": 81630
    },
    {
      "epoch": 595.9124087591241,
      "grad_norm": 9.764886856079102,
      "learning_rate": 2.0204379562043794e-05,
      "loss": 0.9738,
      "step": 81640
    },
    {
      "epoch": 595.985401459854,
      "grad_norm": 11.818697929382324,
      "learning_rate": 2.02007299270073e-05,
      "loss": 1.3179,
      "step": 81650
    },
    {
      "epoch": 596.0583941605839,
      "grad_norm": 11.043685913085938,
      "learning_rate": 2.0197080291970806e-05,
      "loss": 0.6712,
      "step": 81660
    },
    {
      "epoch": 596.1313868613139,
      "grad_norm": 7.556684494018555,
      "learning_rate": 2.0193430656934306e-05,
      "loss": 0.6294,
      "step": 81670
    },
    {
      "epoch": 596.2043795620438,
      "grad_norm": 12.065001487731934,
      "learning_rate": 2.018978102189781e-05,
      "loss": 0.8123,
      "step": 81680
    },
    {
      "epoch": 596.2773722627737,
      "grad_norm": 7.4363694190979,
      "learning_rate": 2.0186131386861314e-05,
      "loss": 0.9737,
      "step": 81690
    },
    {
      "epoch": 596.3503649635037,
      "grad_norm": 13.802595138549805,
      "learning_rate": 2.0182481751824818e-05,
      "loss": 0.9399,
      "step": 81700
    },
    {
      "epoch": 596.4233576642335,
      "grad_norm": 8.183755874633789,
      "learning_rate": 2.017883211678832e-05,
      "loss": 0.9399,
      "step": 81710
    },
    {
      "epoch": 596.4963503649635,
      "grad_norm": 5.623455047607422,
      "learning_rate": 2.0175182481751825e-05,
      "loss": 0.9386,
      "step": 81720
    },
    {
      "epoch": 596.5693430656934,
      "grad_norm": 17.037765502929688,
      "learning_rate": 2.017153284671533e-05,
      "loss": 1.3152,
      "step": 81730
    },
    {
      "epoch": 596.6423357664233,
      "grad_norm": 7.876954555511475,
      "learning_rate": 2.0167883211678833e-05,
      "loss": 0.9839,
      "step": 81740
    },
    {
      "epoch": 596.7153284671533,
      "grad_norm": 9.371947288513184,
      "learning_rate": 2.0164233576642337e-05,
      "loss": 1.2521,
      "step": 81750
    },
    {
      "epoch": 596.7883211678832,
      "grad_norm": 10.55223274230957,
      "learning_rate": 2.016058394160584e-05,
      "loss": 1.122,
      "step": 81760
    },
    {
      "epoch": 596.8613138686131,
      "grad_norm": 5.28587532043457,
      "learning_rate": 2.0156934306569345e-05,
      "loss": 0.5523,
      "step": 81770
    },
    {
      "epoch": 596.9343065693431,
      "grad_norm": 10.738189697265625,
      "learning_rate": 2.015328467153285e-05,
      "loss": 0.8514,
      "step": 81780
    },
    {
      "epoch": 597.007299270073,
      "grad_norm": 0.03220418468117714,
      "learning_rate": 2.014963503649635e-05,
      "loss": 0.816,
      "step": 81790
    },
    {
      "epoch": 597.0802919708029,
      "grad_norm": 0.03404301777482033,
      "learning_rate": 2.0145985401459853e-05,
      "loss": 1.0626,
      "step": 81800
    },
    {
      "epoch": 597.1532846715329,
      "grad_norm": 8.959123611450195,
      "learning_rate": 2.014233576642336e-05,
      "loss": 1.0206,
      "step": 81810
    },
    {
      "epoch": 597.2262773722628,
      "grad_norm": 0.05609053745865822,
      "learning_rate": 2.0138686131386864e-05,
      "loss": 0.6768,
      "step": 81820
    },
    {
      "epoch": 597.2992700729927,
      "grad_norm": 6.779876232147217,
      "learning_rate": 2.0135036496350365e-05,
      "loss": 0.9708,
      "step": 81830
    },
    {
      "epoch": 597.3722627737226,
      "grad_norm": 10.262933731079102,
      "learning_rate": 2.013138686131387e-05,
      "loss": 0.8936,
      "step": 81840
    },
    {
      "epoch": 597.4452554744526,
      "grad_norm": 9.415911674499512,
      "learning_rate": 2.0127737226277373e-05,
      "loss": 0.8846,
      "step": 81850
    },
    {
      "epoch": 597.5182481751825,
      "grad_norm": 11.688448905944824,
      "learning_rate": 2.0124087591240876e-05,
      "loss": 1.0281,
      "step": 81860
    },
    {
      "epoch": 597.5912408759124,
      "grad_norm": 16.895536422729492,
      "learning_rate": 2.012043795620438e-05,
      "loss": 0.6722,
      "step": 81870
    },
    {
      "epoch": 597.6642335766423,
      "grad_norm": 6.982899188995361,
      "learning_rate": 2.0116788321167884e-05,
      "loss": 1.02,
      "step": 81880
    },
    {
      "epoch": 597.7372262773723,
      "grad_norm": 8.225127220153809,
      "learning_rate": 2.0113138686131388e-05,
      "loss": 0.5141,
      "step": 81890
    },
    {
      "epoch": 597.8102189781022,
      "grad_norm": 0.16940464079380035,
      "learning_rate": 2.010948905109489e-05,
      "loss": 0.7647,
      "step": 81900
    },
    {
      "epoch": 597.8832116788321,
      "grad_norm": 15.382705688476562,
      "learning_rate": 2.0105839416058396e-05,
      "loss": 1.3087,
      "step": 81910
    },
    {
      "epoch": 597.956204379562,
      "grad_norm": 15.233465194702148,
      "learning_rate": 2.01021897810219e-05,
      "loss": 1.0156,
      "step": 81920
    },
    {
      "epoch": 598.029197080292,
      "grad_norm": 8.532644271850586,
      "learning_rate": 2.0098540145985404e-05,
      "loss": 1.0966,
      "step": 81930
    },
    {
      "epoch": 598.1021897810219,
      "grad_norm": 7.662110328674316,
      "learning_rate": 2.0094890510948904e-05,
      "loss": 0.7926,
      "step": 81940
    },
    {
      "epoch": 598.1751824817518,
      "grad_norm": 11.38395881652832,
      "learning_rate": 2.0091240875912408e-05,
      "loss": 0.7848,
      "step": 81950
    },
    {
      "epoch": 598.2481751824818,
      "grad_norm": 0.04391105845570564,
      "learning_rate": 2.0087591240875915e-05,
      "loss": 0.9257,
      "step": 81960
    },
    {
      "epoch": 598.3211678832117,
      "grad_norm": 0.04125472530722618,
      "learning_rate": 2.008394160583942e-05,
      "loss": 0.9622,
      "step": 81970
    },
    {
      "epoch": 598.3941605839416,
      "grad_norm": 6.359255790710449,
      "learning_rate": 2.008029197080292e-05,
      "loss": 0.8275,
      "step": 81980
    },
    {
      "epoch": 598.4671532846716,
      "grad_norm": 8.297218322753906,
      "learning_rate": 2.0076642335766423e-05,
      "loss": 0.6295,
      "step": 81990
    },
    {
      "epoch": 598.5401459854014,
      "grad_norm": 12.072022438049316,
      "learning_rate": 2.0072992700729927e-05,
      "loss": 0.6513,
      "step": 82000
    },
    {
      "epoch": 598.6131386861314,
      "grad_norm": 0.01633765920996666,
      "learning_rate": 2.006934306569343e-05,
      "loss": 1.2965,
      "step": 82010
    },
    {
      "epoch": 598.6861313868613,
      "grad_norm": 8.351332664489746,
      "learning_rate": 2.0065693430656935e-05,
      "loss": 0.7378,
      "step": 82020
    },
    {
      "epoch": 598.7591240875912,
      "grad_norm": 9.739072799682617,
      "learning_rate": 2.006204379562044e-05,
      "loss": 0.6706,
      "step": 82030
    },
    {
      "epoch": 598.8321167883212,
      "grad_norm": 21.290449142456055,
      "learning_rate": 2.0058394160583943e-05,
      "loss": 1.425,
      "step": 82040
    },
    {
      "epoch": 598.9051094890511,
      "grad_norm": 17.779630661010742,
      "learning_rate": 2.0054744525547447e-05,
      "loss": 1.1548,
      "step": 82050
    },
    {
      "epoch": 598.978102189781,
      "grad_norm": 18.953277587890625,
      "learning_rate": 2.005109489051095e-05,
      "loss": 0.8622,
      "step": 82060
    },
    {
      "epoch": 599.051094890511,
      "grad_norm": 19.21369171142578,
      "learning_rate": 2.0047445255474455e-05,
      "loss": 1.1269,
      "step": 82070
    },
    {
      "epoch": 599.1240875912408,
      "grad_norm": 0.03483465686440468,
      "learning_rate": 2.004379562043796e-05,
      "loss": 0.9979,
      "step": 82080
    },
    {
      "epoch": 599.1970802919708,
      "grad_norm": 8.814669609069824,
      "learning_rate": 2.004014598540146e-05,
      "loss": 0.7718,
      "step": 82090
    },
    {
      "epoch": 599.2700729927008,
      "grad_norm": 10.044398307800293,
      "learning_rate": 2.0036496350364963e-05,
      "loss": 0.95,
      "step": 82100
    },
    {
      "epoch": 599.3430656934306,
      "grad_norm": 10.558900833129883,
      "learning_rate": 2.0032846715328467e-05,
      "loss": 0.6885,
      "step": 82110
    },
    {
      "epoch": 599.4160583941606,
      "grad_norm": 7.810436248779297,
      "learning_rate": 2.0029197080291974e-05,
      "loss": 0.8146,
      "step": 82120
    },
    {
      "epoch": 599.4890510948906,
      "grad_norm": 1.4943004846572876,
      "learning_rate": 2.0025547445255474e-05,
      "loss": 0.7631,
      "step": 82130
    },
    {
      "epoch": 599.5620437956204,
      "grad_norm": 0.1287803202867508,
      "learning_rate": 2.002189781021898e-05,
      "loss": 0.6949,
      "step": 82140
    },
    {
      "epoch": 599.6350364963504,
      "grad_norm": 8.66390609741211,
      "learning_rate": 2.0018248175182482e-05,
      "loss": 0.6479,
      "step": 82150
    },
    {
      "epoch": 599.7080291970802,
      "grad_norm": 9.519883155822754,
      "learning_rate": 2.0014598540145986e-05,
      "loss": 1.0362,
      "step": 82160
    },
    {
      "epoch": 599.7810218978102,
      "grad_norm": 0.1468566209077835,
      "learning_rate": 2.001094890510949e-05,
      "loss": 1.2203,
      "step": 82170
    },
    {
      "epoch": 599.8540145985402,
      "grad_norm": 14.082542419433594,
      "learning_rate": 2.0007299270072994e-05,
      "loss": 0.9969,
      "step": 82180
    },
    {
      "epoch": 599.92700729927,
      "grad_norm": 1.8624749183654785,
      "learning_rate": 2.0003649635036498e-05,
      "loss": 0.8045,
      "step": 82190
    },
    {
      "epoch": 600.0,
      "grad_norm": 0.01843021996319294,
      "learning_rate": 2e-05,
      "loss": 1.1208,
      "step": 82200
    },
    {
      "epoch": 600.07299270073,
      "grad_norm": 6.814246654510498,
      "learning_rate": 1.9996350364963502e-05,
      "loss": 0.4489,
      "step": 82210
    },
    {
      "epoch": 600.1459854014598,
      "grad_norm": 13.88853931427002,
      "learning_rate": 1.999270072992701e-05,
      "loss": 1.3776,
      "step": 82220
    },
    {
      "epoch": 600.2189781021898,
      "grad_norm": 9.471370697021484,
      "learning_rate": 1.9989051094890513e-05,
      "loss": 1.0724,
      "step": 82230
    },
    {
      "epoch": 600.2919708029198,
      "grad_norm": 10.420907020568848,
      "learning_rate": 1.9985401459854017e-05,
      "loss": 0.9564,
      "step": 82240
    },
    {
      "epoch": 600.3649635036496,
      "grad_norm": 13.244751930236816,
      "learning_rate": 1.9981751824817518e-05,
      "loss": 1.305,
      "step": 82250
    },
    {
      "epoch": 600.4379562043796,
      "grad_norm": 9.54396915435791,
      "learning_rate": 1.997810218978102e-05,
      "loss": 1.0243,
      "step": 82260
    },
    {
      "epoch": 600.5109489051094,
      "grad_norm": 13.038115501403809,
      "learning_rate": 1.997445255474453e-05,
      "loss": 0.92,
      "step": 82270
    },
    {
      "epoch": 600.5839416058394,
      "grad_norm": 7.621487617492676,
      "learning_rate": 1.9970802919708033e-05,
      "loss": 0.8478,
      "step": 82280
    },
    {
      "epoch": 600.6569343065694,
      "grad_norm": 11.108241081237793,
      "learning_rate": 1.9967153284671533e-05,
      "loss": 0.5626,
      "step": 82290
    },
    {
      "epoch": 600.7299270072992,
      "grad_norm": 14.190844535827637,
      "learning_rate": 1.9963503649635037e-05,
      "loss": 0.9499,
      "step": 82300
    },
    {
      "epoch": 600.8029197080292,
      "grad_norm": 10.525758743286133,
      "learning_rate": 1.995985401459854e-05,
      "loss": 1.0894,
      "step": 82310
    },
    {
      "epoch": 600.8759124087592,
      "grad_norm": 0.023219013586640358,
      "learning_rate": 1.9956204379562045e-05,
      "loss": 0.5176,
      "step": 82320
    },
    {
      "epoch": 600.948905109489,
      "grad_norm": 8.111492156982422,
      "learning_rate": 1.995255474452555e-05,
      "loss": 1.0462,
      "step": 82330
    },
    {
      "epoch": 601.021897810219,
      "grad_norm": 5.332322120666504,
      "learning_rate": 1.9948905109489053e-05,
      "loss": 0.7439,
      "step": 82340
    },
    {
      "epoch": 601.0948905109489,
      "grad_norm": 9.773974418640137,
      "learning_rate": 1.9945255474452556e-05,
      "loss": 0.978,
      "step": 82350
    },
    {
      "epoch": 601.1678832116788,
      "grad_norm": 6.207062721252441,
      "learning_rate": 1.9941605839416057e-05,
      "loss": 0.9601,
      "step": 82360
    },
    {
      "epoch": 601.2408759124088,
      "grad_norm": 7.271883010864258,
      "learning_rate": 1.9937956204379564e-05,
      "loss": 0.5316,
      "step": 82370
    },
    {
      "epoch": 601.3138686131387,
      "grad_norm": 9.53986930847168,
      "learning_rate": 1.9934306569343068e-05,
      "loss": 0.9097,
      "step": 82380
    },
    {
      "epoch": 601.3868613138686,
      "grad_norm": 8.422003746032715,
      "learning_rate": 1.9930656934306572e-05,
      "loss": 0.9649,
      "step": 82390
    },
    {
      "epoch": 601.4598540145986,
      "grad_norm": 0.08128751814365387,
      "learning_rate": 1.9927007299270073e-05,
      "loss": 0.9042,
      "step": 82400
    },
    {
      "epoch": 601.5328467153284,
      "grad_norm": 5.389983654022217,
      "learning_rate": 1.9923357664233576e-05,
      "loss": 0.7561,
      "step": 82410
    },
    {
      "epoch": 601.6058394160584,
      "grad_norm": 12.709338188171387,
      "learning_rate": 1.991970802919708e-05,
      "loss": 1.0764,
      "step": 82420
    },
    {
      "epoch": 601.6788321167883,
      "grad_norm": 6.168675422668457,
      "learning_rate": 1.9916058394160588e-05,
      "loss": 0.8325,
      "step": 82430
    },
    {
      "epoch": 601.7518248175182,
      "grad_norm": 0.041796546429395676,
      "learning_rate": 1.9912408759124088e-05,
      "loss": 0.9366,
      "step": 82440
    },
    {
      "epoch": 601.8248175182482,
      "grad_norm": 8.728047370910645,
      "learning_rate": 1.9908759124087592e-05,
      "loss": 1.2803,
      "step": 82450
    },
    {
      "epoch": 601.8978102189781,
      "grad_norm": 15.822785377502441,
      "learning_rate": 1.9905109489051096e-05,
      "loss": 1.0163,
      "step": 82460
    },
    {
      "epoch": 601.970802919708,
      "grad_norm": 11.630677223205566,
      "learning_rate": 1.99014598540146e-05,
      "loss": 0.8634,
      "step": 82470
    },
    {
      "epoch": 602.043795620438,
      "grad_norm": 0.05754418671131134,
      "learning_rate": 1.9897810218978104e-05,
      "loss": 1.0854,
      "step": 82480
    },
    {
      "epoch": 602.1167883211679,
      "grad_norm": 1.9875041246414185,
      "learning_rate": 1.9894160583941607e-05,
      "loss": 0.7636,
      "step": 82490
    },
    {
      "epoch": 602.1897810218978,
      "grad_norm": 6.76588249206543,
      "learning_rate": 1.989051094890511e-05,
      "loss": 0.9987,
      "step": 82500
    },
    {
      "epoch": 602.2627737226277,
      "grad_norm": 7.535438060760498,
      "learning_rate": 1.9886861313868612e-05,
      "loss": 0.8076,
      "step": 82510
    },
    {
      "epoch": 602.3357664233577,
      "grad_norm": 9.292125701904297,
      "learning_rate": 1.9883211678832116e-05,
      "loss": 0.5768,
      "step": 82520
    },
    {
      "epoch": 602.4087591240876,
      "grad_norm": 16.217086791992188,
      "learning_rate": 1.9879562043795623e-05,
      "loss": 0.6081,
      "step": 82530
    },
    {
      "epoch": 602.4817518248175,
      "grad_norm": 0.027343247085809708,
      "learning_rate": 1.9875912408759127e-05,
      "loss": 0.8963,
      "step": 82540
    },
    {
      "epoch": 602.5547445255474,
      "grad_norm": 6.280416488647461,
      "learning_rate": 1.9872262773722627e-05,
      "loss": 0.7383,
      "step": 82550
    },
    {
      "epoch": 602.6277372262774,
      "grad_norm": 0.04711107909679413,
      "learning_rate": 1.986861313868613e-05,
      "loss": 0.9554,
      "step": 82560
    },
    {
      "epoch": 602.7007299270073,
      "grad_norm": 0.06873208284378052,
      "learning_rate": 1.9864963503649635e-05,
      "loss": 0.5621,
      "step": 82570
    },
    {
      "epoch": 602.7737226277372,
      "grad_norm": 8.672259330749512,
      "learning_rate": 1.986131386861314e-05,
      "loss": 0.7188,
      "step": 82580
    },
    {
      "epoch": 602.8467153284671,
      "grad_norm": 11.19338321685791,
      "learning_rate": 1.9857664233576643e-05,
      "loss": 0.9438,
      "step": 82590
    },
    {
      "epoch": 602.9197080291971,
      "grad_norm": 17.47275733947754,
      "learning_rate": 1.9854014598540147e-05,
      "loss": 1.4093,
      "step": 82600
    },
    {
      "epoch": 602.992700729927,
      "grad_norm": 16.54915428161621,
      "learning_rate": 1.985036496350365e-05,
      "loss": 1.325,
      "step": 82610
    },
    {
      "epoch": 603.0656934306569,
      "grad_norm": 0.028034048154950142,
      "learning_rate": 1.9846715328467155e-05,
      "loss": 0.9,
      "step": 82620
    },
    {
      "epoch": 603.1386861313869,
      "grad_norm": 20.61276626586914,
      "learning_rate": 1.984306569343066e-05,
      "loss": 1.5073,
      "step": 82630
    },
    {
      "epoch": 603.2116788321168,
      "grad_norm": 0.043525900691747665,
      "learning_rate": 1.9839416058394162e-05,
      "loss": 0.878,
      "step": 82640
    },
    {
      "epoch": 603.2846715328467,
      "grad_norm": 7.306567192077637,
      "learning_rate": 1.9835766423357666e-05,
      "loss": 0.5906,
      "step": 82650
    },
    {
      "epoch": 603.3576642335767,
      "grad_norm": 2.6752545833587646,
      "learning_rate": 1.983211678832117e-05,
      "loss": 0.6806,
      "step": 82660
    },
    {
      "epoch": 603.4306569343066,
      "grad_norm": 7.222276210784912,
      "learning_rate": 1.982846715328467e-05,
      "loss": 0.9032,
      "step": 82670
    },
    {
      "epoch": 603.5036496350365,
      "grad_norm": 14.615731239318848,
      "learning_rate": 1.9824817518248174e-05,
      "loss": 1.0774,
      "step": 82680
    },
    {
      "epoch": 603.5766423357665,
      "grad_norm": 6.212535858154297,
      "learning_rate": 1.9821167883211682e-05,
      "loss": 1.4336,
      "step": 82690
    },
    {
      "epoch": 603.6496350364963,
      "grad_norm": 8.52468490600586,
      "learning_rate": 1.9817518248175186e-05,
      "loss": 0.7966,
      "step": 82700
    },
    {
      "epoch": 603.7226277372263,
      "grad_norm": 10.46938419342041,
      "learning_rate": 1.9813868613138686e-05,
      "loss": 0.9553,
      "step": 82710
    },
    {
      "epoch": 603.7956204379562,
      "grad_norm": 6.707752227783203,
      "learning_rate": 1.981021897810219e-05,
      "loss": 0.8373,
      "step": 82720
    },
    {
      "epoch": 603.8686131386861,
      "grad_norm": 12.606647491455078,
      "learning_rate": 1.9806569343065694e-05,
      "loss": 0.9294,
      "step": 82730
    },
    {
      "epoch": 603.9416058394161,
      "grad_norm": 6.592535972595215,
      "learning_rate": 1.9802919708029198e-05,
      "loss": 0.8771,
      "step": 82740
    },
    {
      "epoch": 604.014598540146,
      "grad_norm": 9.231032371520996,
      "learning_rate": 1.97992700729927e-05,
      "loss": 0.5653,
      "step": 82750
    },
    {
      "epoch": 604.0875912408759,
      "grad_norm": 8.715062141418457,
      "learning_rate": 1.9795620437956205e-05,
      "loss": 0.8561,
      "step": 82760
    },
    {
      "epoch": 604.1605839416059,
      "grad_norm": 17.65418815612793,
      "learning_rate": 1.979197080291971e-05,
      "loss": 0.7318,
      "step": 82770
    },
    {
      "epoch": 604.2335766423357,
      "grad_norm": 7.703891754150391,
      "learning_rate": 1.978832116788321e-05,
      "loss": 0.5924,
      "step": 82780
    },
    {
      "epoch": 604.3065693430657,
      "grad_norm": 15.954357147216797,
      "learning_rate": 1.9784671532846717e-05,
      "loss": 1.292,
      "step": 82790
    },
    {
      "epoch": 604.3795620437957,
      "grad_norm": 0.048758793622255325,
      "learning_rate": 1.978102189781022e-05,
      "loss": 0.778,
      "step": 82800
    },
    {
      "epoch": 604.4525547445255,
      "grad_norm": 11.600692749023438,
      "learning_rate": 1.9777372262773725e-05,
      "loss": 1.3001,
      "step": 82810
    },
    {
      "epoch": 604.5255474452555,
      "grad_norm": 8.20522689819336,
      "learning_rate": 1.9773722627737225e-05,
      "loss": 0.7498,
      "step": 82820
    },
    {
      "epoch": 604.5985401459855,
      "grad_norm": 10.305717468261719,
      "learning_rate": 1.977007299270073e-05,
      "loss": 1.0178,
      "step": 82830
    },
    {
      "epoch": 604.6715328467153,
      "grad_norm": 7.549118518829346,
      "learning_rate": 1.9766423357664237e-05,
      "loss": 0.9479,
      "step": 82840
    },
    {
      "epoch": 604.7445255474453,
      "grad_norm": 14.59423542022705,
      "learning_rate": 1.976277372262774e-05,
      "loss": 1.0348,
      "step": 82850
    },
    {
      "epoch": 604.8175182481751,
      "grad_norm": 15.503596305847168,
      "learning_rate": 1.975912408759124e-05,
      "loss": 0.7951,
      "step": 82860
    },
    {
      "epoch": 604.8905109489051,
      "grad_norm": 7.261826038360596,
      "learning_rate": 1.9755474452554745e-05,
      "loss": 0.9019,
      "step": 82870
    },
    {
      "epoch": 604.9635036496351,
      "grad_norm": 0.0938943475484848,
      "learning_rate": 1.975182481751825e-05,
      "loss": 0.6087,
      "step": 82880
    },
    {
      "epoch": 605.0364963503649,
      "grad_norm": 0.037346210330724716,
      "learning_rate": 1.9748175182481753e-05,
      "loss": 0.8441,
      "step": 82890
    },
    {
      "epoch": 605.1094890510949,
      "grad_norm": 0.716251015663147,
      "learning_rate": 1.9744525547445256e-05,
      "loss": 0.7586,
      "step": 82900
    },
    {
      "epoch": 605.1824817518249,
      "grad_norm": 8.640764236450195,
      "learning_rate": 1.974087591240876e-05,
      "loss": 0.69,
      "step": 82910
    },
    {
      "epoch": 605.2554744525547,
      "grad_norm": 13.790071487426758,
      "learning_rate": 1.9737226277372264e-05,
      "loss": 0.6136,
      "step": 82920
    },
    {
      "epoch": 605.3284671532847,
      "grad_norm": 9.507972717285156,
      "learning_rate": 1.9733576642335768e-05,
      "loss": 1.1329,
      "step": 82930
    },
    {
      "epoch": 605.4014598540145,
      "grad_norm": 13.559621810913086,
      "learning_rate": 1.9729927007299272e-05,
      "loss": 0.8814,
      "step": 82940
    },
    {
      "epoch": 605.4744525547445,
      "grad_norm": 9.1651611328125,
      "learning_rate": 1.9726277372262776e-05,
      "loss": 0.9579,
      "step": 82950
    },
    {
      "epoch": 605.5474452554745,
      "grad_norm": 8.11544132232666,
      "learning_rate": 1.972262773722628e-05,
      "loss": 0.695,
      "step": 82960
    },
    {
      "epoch": 605.6204379562043,
      "grad_norm": 13.149779319763184,
      "learning_rate": 1.971897810218978e-05,
      "loss": 1.2739,
      "step": 82970
    },
    {
      "epoch": 605.6934306569343,
      "grad_norm": 0.0491255559027195,
      "learning_rate": 1.9715328467153284e-05,
      "loss": 0.2481,
      "step": 82980
    },
    {
      "epoch": 605.7664233576643,
      "grad_norm": 3.367550849914551,
      "learning_rate": 1.9711678832116788e-05,
      "loss": 1.1595,
      "step": 82990
    },
    {
      "epoch": 605.8394160583941,
      "grad_norm": 16.08568572998047,
      "learning_rate": 1.9708029197080295e-05,
      "loss": 1.1048,
      "step": 83000
    },
    {
      "epoch": 605.9124087591241,
      "grad_norm": 11.625563621520996,
      "learning_rate": 1.9704379562043796e-05,
      "loss": 0.9598,
      "step": 83010
    },
    {
      "epoch": 605.985401459854,
      "grad_norm": 8.918022155761719,
      "learning_rate": 1.97007299270073e-05,
      "loss": 1.1011,
      "step": 83020
    },
    {
      "epoch": 606.0583941605839,
      "grad_norm": 11.714347839355469,
      "learning_rate": 1.9697080291970804e-05,
      "loss": 0.9614,
      "step": 83030
    },
    {
      "epoch": 606.1313868613139,
      "grad_norm": 0.05326849967241287,
      "learning_rate": 1.9693430656934307e-05,
      "loss": 0.7696,
      "step": 83040
    },
    {
      "epoch": 606.2043795620438,
      "grad_norm": 0.11094944179058075,
      "learning_rate": 1.968978102189781e-05,
      "loss": 1.038,
      "step": 83050
    },
    {
      "epoch": 606.2773722627737,
      "grad_norm": 9.072736740112305,
      "learning_rate": 1.9686131386861315e-05,
      "loss": 0.7747,
      "step": 83060
    },
    {
      "epoch": 606.3503649635037,
      "grad_norm": 10.714740753173828,
      "learning_rate": 1.968248175182482e-05,
      "loss": 0.8163,
      "step": 83070
    },
    {
      "epoch": 606.4233576642335,
      "grad_norm": 6.302770614624023,
      "learning_rate": 1.9678832116788323e-05,
      "loss": 0.3914,
      "step": 83080
    },
    {
      "epoch": 606.4963503649635,
      "grad_norm": 14.092796325683594,
      "learning_rate": 1.9675182481751823e-05,
      "loss": 0.8217,
      "step": 83090
    },
    {
      "epoch": 606.5693430656934,
      "grad_norm": 5.314008712768555,
      "learning_rate": 1.967153284671533e-05,
      "loss": 0.6879,
      "step": 83100
    },
    {
      "epoch": 606.6423357664233,
      "grad_norm": 6.1112565994262695,
      "learning_rate": 1.9667883211678835e-05,
      "loss": 1.0158,
      "step": 83110
    },
    {
      "epoch": 606.7153284671533,
      "grad_norm": 12.46415901184082,
      "learning_rate": 1.966423357664234e-05,
      "loss": 1.0529,
      "step": 83120
    },
    {
      "epoch": 606.7883211678832,
      "grad_norm": 6.243082523345947,
      "learning_rate": 1.966058394160584e-05,
      "loss": 0.982,
      "step": 83130
    },
    {
      "epoch": 606.8613138686131,
      "grad_norm": 17.692705154418945,
      "learning_rate": 1.9656934306569343e-05,
      "loss": 0.9562,
      "step": 83140
    },
    {
      "epoch": 606.9343065693431,
      "grad_norm": 8.264443397521973,
      "learning_rate": 1.9653284671532847e-05,
      "loss": 1.2607,
      "step": 83150
    },
    {
      "epoch": 607.007299270073,
      "grad_norm": 13.316083908081055,
      "learning_rate": 1.964963503649635e-05,
      "loss": 1.0958,
      "step": 83160
    },
    {
      "epoch": 607.0802919708029,
      "grad_norm": 7.510903358459473,
      "learning_rate": 1.9645985401459855e-05,
      "loss": 0.8929,
      "step": 83170
    },
    {
      "epoch": 607.1532846715329,
      "grad_norm": 11.800074577331543,
      "learning_rate": 1.964233576642336e-05,
      "loss": 0.617,
      "step": 83180
    },
    {
      "epoch": 607.2262773722628,
      "grad_norm": 15.278915405273438,
      "learning_rate": 1.9638686131386862e-05,
      "loss": 0.7173,
      "step": 83190
    },
    {
      "epoch": 607.2992700729927,
      "grad_norm": 7.600043773651123,
      "learning_rate": 1.9635036496350366e-05,
      "loss": 0.8568,
      "step": 83200
    },
    {
      "epoch": 607.3722627737226,
      "grad_norm": 9.60647201538086,
      "learning_rate": 1.963138686131387e-05,
      "loss": 0.9929,
      "step": 83210
    },
    {
      "epoch": 607.4452554744526,
      "grad_norm": 24.621212005615234,
      "learning_rate": 1.9627737226277374e-05,
      "loss": 1.1185,
      "step": 83220
    },
    {
      "epoch": 607.5182481751825,
      "grad_norm": 16.758296966552734,
      "learning_rate": 1.9624087591240878e-05,
      "loss": 1.0209,
      "step": 83230
    },
    {
      "epoch": 607.5912408759124,
      "grad_norm": 5.253245830535889,
      "learning_rate": 1.9620437956204378e-05,
      "loss": 1.2068,
      "step": 83240
    },
    {
      "epoch": 607.6642335766423,
      "grad_norm": 12.134361267089844,
      "learning_rate": 1.9616788321167882e-05,
      "loss": 1.0237,
      "step": 83250
    },
    {
      "epoch": 607.7372262773723,
      "grad_norm": 10.2225341796875,
      "learning_rate": 1.961313868613139e-05,
      "loss": 1.29,
      "step": 83260
    },
    {
      "epoch": 607.8102189781022,
      "grad_norm": 0.026079783216118813,
      "learning_rate": 1.9609489051094893e-05,
      "loss": 0.9444,
      "step": 83270
    },
    {
      "epoch": 607.8832116788321,
      "grad_norm": 1.2888529300689697,
      "learning_rate": 1.9605839416058394e-05,
      "loss": 0.8344,
      "step": 83280
    },
    {
      "epoch": 607.956204379562,
      "grad_norm": 13.977499961853027,
      "learning_rate": 1.9602189781021898e-05,
      "loss": 0.7715,
      "step": 83290
    },
    {
      "epoch": 608.029197080292,
      "grad_norm": 5.137868404388428,
      "learning_rate": 1.95985401459854e-05,
      "loss": 0.6332,
      "step": 83300
    },
    {
      "epoch": 608.1021897810219,
      "grad_norm": 7.624394416809082,
      "learning_rate": 1.959489051094891e-05,
      "loss": 1.0527,
      "step": 83310
    },
    {
      "epoch": 608.1751824817518,
      "grad_norm": 0.4639236629009247,
      "learning_rate": 1.959124087591241e-05,
      "loss": 1.0713,
      "step": 83320
    },
    {
      "epoch": 608.2481751824818,
      "grad_norm": 0.26307442784309387,
      "learning_rate": 1.9587591240875913e-05,
      "loss": 0.5119,
      "step": 83330
    },
    {
      "epoch": 608.3211678832117,
      "grad_norm": 6.154417991638184,
      "learning_rate": 1.9583941605839417e-05,
      "loss": 0.6329,
      "step": 83340
    },
    {
      "epoch": 608.3941605839416,
      "grad_norm": 7.084640026092529,
      "learning_rate": 1.958029197080292e-05,
      "loss": 0.9827,
      "step": 83350
    },
    {
      "epoch": 608.4671532846716,
      "grad_norm": 19.65755844116211,
      "learning_rate": 1.9576642335766425e-05,
      "loss": 0.7812,
      "step": 83360
    },
    {
      "epoch": 608.5401459854014,
      "grad_norm": 7.005793571472168,
      "learning_rate": 1.957299270072993e-05,
      "loss": 0.8915,
      "step": 83370
    },
    {
      "epoch": 608.6131386861314,
      "grad_norm": 6.789045810699463,
      "learning_rate": 1.9569343065693433e-05,
      "loss": 0.694,
      "step": 83380
    },
    {
      "epoch": 608.6861313868613,
      "grad_norm": 15.219128608703613,
      "learning_rate": 1.9565693430656933e-05,
      "loss": 0.817,
      "step": 83390
    },
    {
      "epoch": 608.7591240875912,
      "grad_norm": 10.707130432128906,
      "learning_rate": 1.9562043795620437e-05,
      "loss": 1.267,
      "step": 83400
    },
    {
      "epoch": 608.8321167883212,
      "grad_norm": 5.9053215980529785,
      "learning_rate": 1.9558394160583944e-05,
      "loss": 1.0432,
      "step": 83410
    },
    {
      "epoch": 608.9051094890511,
      "grad_norm": 11.825209617614746,
      "learning_rate": 1.9554744525547448e-05,
      "loss": 1.2202,
      "step": 83420
    },
    {
      "epoch": 608.978102189781,
      "grad_norm": 10.456304550170898,
      "learning_rate": 1.955109489051095e-05,
      "loss": 0.6309,
      "step": 83430
    },
    {
      "epoch": 609.051094890511,
      "grad_norm": 9.706228256225586,
      "learning_rate": 1.9547445255474453e-05,
      "loss": 0.5739,
      "step": 83440
    },
    {
      "epoch": 609.1240875912408,
      "grad_norm": 0.1623072773218155,
      "learning_rate": 1.9543795620437956e-05,
      "loss": 0.7361,
      "step": 83450
    },
    {
      "epoch": 609.1970802919708,
      "grad_norm": 5.305001258850098,
      "learning_rate": 1.954014598540146e-05,
      "loss": 0.5756,
      "step": 83460
    },
    {
      "epoch": 609.2700729927008,
      "grad_norm": 0.1395520716905594,
      "learning_rate": 1.9536496350364964e-05,
      "loss": 1.0193,
      "step": 83470
    },
    {
      "epoch": 609.3430656934306,
      "grad_norm": 0.10155802220106125,
      "learning_rate": 1.9532846715328468e-05,
      "loss": 1.2616,
      "step": 83480
    },
    {
      "epoch": 609.4160583941606,
      "grad_norm": 9.095338821411133,
      "learning_rate": 1.9529197080291972e-05,
      "loss": 0.6303,
      "step": 83490
    },
    {
      "epoch": 609.4890510948906,
      "grad_norm": 2.237945318222046,
      "learning_rate": 1.9525547445255476e-05,
      "loss": 0.9187,
      "step": 83500
    },
    {
      "epoch": 609.5620437956204,
      "grad_norm": 15.34969425201416,
      "learning_rate": 1.952189781021898e-05,
      "loss": 1.0074,
      "step": 83510
    },
    {
      "epoch": 609.6350364963504,
      "grad_norm": 12.641863822937012,
      "learning_rate": 1.9518248175182484e-05,
      "loss": 0.8396,
      "step": 83520
    },
    {
      "epoch": 609.7080291970802,
      "grad_norm": 8.755088806152344,
      "learning_rate": 1.9514598540145988e-05,
      "loss": 0.9909,
      "step": 83530
    },
    {
      "epoch": 609.7810218978102,
      "grad_norm": 9.271589279174805,
      "learning_rate": 1.951094890510949e-05,
      "loss": 1.0642,
      "step": 83540
    },
    {
      "epoch": 609.8540145985402,
      "grad_norm": 6.654469013214111,
      "learning_rate": 1.9507299270072992e-05,
      "loss": 0.7375,
      "step": 83550
    },
    {
      "epoch": 609.92700729927,
      "grad_norm": 8.720001220703125,
      "learning_rate": 1.9503649635036496e-05,
      "loss": 0.9482,
      "step": 83560
    },
    {
      "epoch": 610.0,
      "grad_norm": 0.04812236130237579,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.992,
      "step": 83570
    },
    {
      "epoch": 610.07299270073,
      "grad_norm": 14.3085298538208,
      "learning_rate": 1.9496350364963504e-05,
      "loss": 0.6414,
      "step": 83580
    },
    {
      "epoch": 610.1459854014598,
      "grad_norm": 4.796775817871094,
      "learning_rate": 1.9492700729927007e-05,
      "loss": 1.0881,
      "step": 83590
    },
    {
      "epoch": 610.2189781021898,
      "grad_norm": 12.529831886291504,
      "learning_rate": 1.948905109489051e-05,
      "loss": 0.8156,
      "step": 83600
    },
    {
      "epoch": 610.2919708029198,
      "grad_norm": 7.715768814086914,
      "learning_rate": 1.9485401459854015e-05,
      "loss": 0.9991,
      "step": 83610
    },
    {
      "epoch": 610.3649635036496,
      "grad_norm": 5.711723327636719,
      "learning_rate": 1.948175182481752e-05,
      "loss": 0.9305,
      "step": 83620
    },
    {
      "epoch": 610.4379562043796,
      "grad_norm": 7.479133605957031,
      "learning_rate": 1.9478102189781023e-05,
      "loss": 0.6746,
      "step": 83630
    },
    {
      "epoch": 610.5109489051094,
      "grad_norm": 19.78911781311035,
      "learning_rate": 1.9474452554744527e-05,
      "loss": 1.0051,
      "step": 83640
    },
    {
      "epoch": 610.5839416058394,
      "grad_norm": 0.09529384225606918,
      "learning_rate": 1.947080291970803e-05,
      "loss": 0.6466,
      "step": 83650
    },
    {
      "epoch": 610.6569343065694,
      "grad_norm": 15.713154792785645,
      "learning_rate": 1.946715328467153e-05,
      "loss": 1.0636,
      "step": 83660
    },
    {
      "epoch": 610.7299270072992,
      "grad_norm": 11.607976913452148,
      "learning_rate": 1.946350364963504e-05,
      "loss": 0.6952,
      "step": 83670
    },
    {
      "epoch": 610.8029197080292,
      "grad_norm": 13.76955795288086,
      "learning_rate": 1.9459854014598542e-05,
      "loss": 1.2546,
      "step": 83680
    },
    {
      "epoch": 610.8759124087592,
      "grad_norm": 8.255989074707031,
      "learning_rate": 1.9456204379562046e-05,
      "loss": 1.0709,
      "step": 83690
    },
    {
      "epoch": 610.948905109489,
      "grad_norm": 8.145423889160156,
      "learning_rate": 1.9452554744525547e-05,
      "loss": 0.8969,
      "step": 83700
    },
    {
      "epoch": 611.021897810219,
      "grad_norm": 13.46668529510498,
      "learning_rate": 1.944890510948905e-05,
      "loss": 1.0016,
      "step": 83710
    },
    {
      "epoch": 611.0948905109489,
      "grad_norm": 5.859368801116943,
      "learning_rate": 1.9445255474452554e-05,
      "loss": 0.9715,
      "step": 83720
    },
    {
      "epoch": 611.1678832116788,
      "grad_norm": 7.316417694091797,
      "learning_rate": 1.9441605839416062e-05,
      "loss": 1.0264,
      "step": 83730
    },
    {
      "epoch": 611.2408759124088,
      "grad_norm": 12.504258155822754,
      "learning_rate": 1.9437956204379562e-05,
      "loss": 0.6999,
      "step": 83740
    },
    {
      "epoch": 611.3138686131387,
      "grad_norm": 6.813915252685547,
      "learning_rate": 1.9434306569343066e-05,
      "loss": 0.7567,
      "step": 83750
    },
    {
      "epoch": 611.3868613138686,
      "grad_norm": 16.695781707763672,
      "learning_rate": 1.943065693430657e-05,
      "loss": 0.9857,
      "step": 83760
    },
    {
      "epoch": 611.4598540145986,
      "grad_norm": 10.0111665725708,
      "learning_rate": 1.9427007299270074e-05,
      "loss": 0.6316,
      "step": 83770
    },
    {
      "epoch": 611.5328467153284,
      "grad_norm": 4.803679943084717,
      "learning_rate": 1.9423357664233578e-05,
      "loss": 0.831,
      "step": 83780
    },
    {
      "epoch": 611.6058394160584,
      "grad_norm": 7.841739654541016,
      "learning_rate": 1.941970802919708e-05,
      "loss": 1.1774,
      "step": 83790
    },
    {
      "epoch": 611.6788321167883,
      "grad_norm": 0.030193675309419632,
      "learning_rate": 1.9416058394160586e-05,
      "loss": 0.7529,
      "step": 83800
    },
    {
      "epoch": 611.7518248175182,
      "grad_norm": 10.804557800292969,
      "learning_rate": 1.9412408759124086e-05,
      "loss": 1.0175,
      "step": 83810
    },
    {
      "epoch": 611.8248175182482,
      "grad_norm": 7.0365681648254395,
      "learning_rate": 1.940875912408759e-05,
      "loss": 0.7548,
      "step": 83820
    },
    {
      "epoch": 611.8978102189781,
      "grad_norm": 10.625993728637695,
      "learning_rate": 1.9405109489051097e-05,
      "loss": 1.1471,
      "step": 83830
    },
    {
      "epoch": 611.970802919708,
      "grad_norm": 0.028900621458888054,
      "learning_rate": 1.94014598540146e-05,
      "loss": 0.9664,
      "step": 83840
    },
    {
      "epoch": 612.043795620438,
      "grad_norm": 8.344809532165527,
      "learning_rate": 1.93978102189781e-05,
      "loss": 0.5591,
      "step": 83850
    },
    {
      "epoch": 612.1167883211679,
      "grad_norm": 12.369845390319824,
      "learning_rate": 1.9394160583941605e-05,
      "loss": 0.7424,
      "step": 83860
    },
    {
      "epoch": 612.1897810218978,
      "grad_norm": 9.581133842468262,
      "learning_rate": 1.939051094890511e-05,
      "loss": 0.7599,
      "step": 83870
    },
    {
      "epoch": 612.2627737226277,
      "grad_norm": 14.517777442932129,
      "learning_rate": 1.9386861313868617e-05,
      "loss": 1.1612,
      "step": 83880
    },
    {
      "epoch": 612.3357664233577,
      "grad_norm": 13.85739803314209,
      "learning_rate": 1.9383211678832117e-05,
      "loss": 0.7582,
      "step": 83890
    },
    {
      "epoch": 612.4087591240876,
      "grad_norm": 11.728217124938965,
      "learning_rate": 1.937956204379562e-05,
      "loss": 0.981,
      "step": 83900
    },
    {
      "epoch": 612.4817518248175,
      "grad_norm": 12.017138481140137,
      "learning_rate": 1.9375912408759125e-05,
      "loss": 1.0802,
      "step": 83910
    },
    {
      "epoch": 612.5547445255474,
      "grad_norm": 9.559134483337402,
      "learning_rate": 1.937226277372263e-05,
      "loss": 1.2294,
      "step": 83920
    },
    {
      "epoch": 612.6277372262774,
      "grad_norm": 19.638959884643555,
      "learning_rate": 1.9368613138686133e-05,
      "loss": 1.0522,
      "step": 83930
    },
    {
      "epoch": 612.7007299270073,
      "grad_norm": 18.19182586669922,
      "learning_rate": 1.9364963503649637e-05,
      "loss": 1.0442,
      "step": 83940
    },
    {
      "epoch": 612.7737226277372,
      "grad_norm": 8.023698806762695,
      "learning_rate": 1.936131386861314e-05,
      "loss": 0.6693,
      "step": 83950
    },
    {
      "epoch": 612.8467153284671,
      "grad_norm": 8.224815368652344,
      "learning_rate": 1.9357664233576644e-05,
      "loss": 0.7451,
      "step": 83960
    },
    {
      "epoch": 612.9197080291971,
      "grad_norm": 15.17625617980957,
      "learning_rate": 1.9354014598540145e-05,
      "loss": 0.9698,
      "step": 83970
    },
    {
      "epoch": 612.992700729927,
      "grad_norm": 9.801141738891602,
      "learning_rate": 1.9350364963503652e-05,
      "loss": 0.8161,
      "step": 83980
    },
    {
      "epoch": 613.0656934306569,
      "grad_norm": 0.03791995346546173,
      "learning_rate": 1.9346715328467156e-05,
      "loss": 0.6033,
      "step": 83990
    },
    {
      "epoch": 613.1386861313869,
      "grad_norm": 12.250128746032715,
      "learning_rate": 1.934306569343066e-05,
      "loss": 1.187,
      "step": 84000
    },
    {
      "epoch": 613.2116788321168,
      "grad_norm": 12.730584144592285,
      "learning_rate": 1.933941605839416e-05,
      "loss": 1.1938,
      "step": 84010
    },
    {
      "epoch": 613.2846715328467,
      "grad_norm": 10.035244941711426,
      "learning_rate": 1.9335766423357664e-05,
      "loss": 0.8952,
      "step": 84020
    },
    {
      "epoch": 613.3576642335767,
      "grad_norm": 11.609574317932129,
      "learning_rate": 1.9332116788321168e-05,
      "loss": 0.9935,
      "step": 84030
    },
    {
      "epoch": 613.4306569343066,
      "grad_norm": 8.213991165161133,
      "learning_rate": 1.9328467153284672e-05,
      "loss": 0.8453,
      "step": 84040
    },
    {
      "epoch": 613.5036496350365,
      "grad_norm": 7.86778450012207,
      "learning_rate": 1.9324817518248176e-05,
      "loss": 1.0354,
      "step": 84050
    },
    {
      "epoch": 613.5766423357665,
      "grad_norm": 7.33701229095459,
      "learning_rate": 1.932116788321168e-05,
      "loss": 0.7556,
      "step": 84060
    },
    {
      "epoch": 613.6496350364963,
      "grad_norm": 5.803525924682617,
      "learning_rate": 1.9317518248175184e-05,
      "loss": 1.058,
      "step": 84070
    },
    {
      "epoch": 613.7226277372263,
      "grad_norm": 0.08091124147176743,
      "learning_rate": 1.9313868613138687e-05,
      "loss": 0.7014,
      "step": 84080
    },
    {
      "epoch": 613.7956204379562,
      "grad_norm": 8.19012451171875,
      "learning_rate": 1.931021897810219e-05,
      "loss": 0.7027,
      "step": 84090
    },
    {
      "epoch": 613.8686131386861,
      "grad_norm": 5.523245334625244,
      "learning_rate": 1.9306569343065695e-05,
      "loss": 0.865,
      "step": 84100
    },
    {
      "epoch": 613.9416058394161,
      "grad_norm": 16.587583541870117,
      "learning_rate": 1.93029197080292e-05,
      "loss": 0.4498,
      "step": 84110
    },
    {
      "epoch": 614.014598540146,
      "grad_norm": 12.655739784240723,
      "learning_rate": 1.92992700729927e-05,
      "loss": 0.7517,
      "step": 84120
    },
    {
      "epoch": 614.0875912408759,
      "grad_norm": 9.536251068115234,
      "learning_rate": 1.9295620437956204e-05,
      "loss": 1.1122,
      "step": 84130
    },
    {
      "epoch": 614.1605839416059,
      "grad_norm": 3.9914374351501465,
      "learning_rate": 1.929197080291971e-05,
      "loss": 1.1299,
      "step": 84140
    },
    {
      "epoch": 614.2335766423357,
      "grad_norm": 7.921984672546387,
      "learning_rate": 1.9288321167883215e-05,
      "loss": 0.7228,
      "step": 84150
    },
    {
      "epoch": 614.3065693430657,
      "grad_norm": 4.2430315017700195,
      "learning_rate": 1.9284671532846715e-05,
      "loss": 1.0917,
      "step": 84160
    },
    {
      "epoch": 614.3795620437957,
      "grad_norm": 8.079483985900879,
      "learning_rate": 1.928102189781022e-05,
      "loss": 0.8461,
      "step": 84170
    },
    {
      "epoch": 614.4525547445255,
      "grad_norm": 0.20845746994018555,
      "learning_rate": 1.9277372262773723e-05,
      "loss": 0.6599,
      "step": 84180
    },
    {
      "epoch": 614.5255474452555,
      "grad_norm": 9.957351684570312,
      "learning_rate": 1.9273722627737227e-05,
      "loss": 0.996,
      "step": 84190
    },
    {
      "epoch": 614.5985401459855,
      "grad_norm": 10.698229789733887,
      "learning_rate": 1.927007299270073e-05,
      "loss": 0.8042,
      "step": 84200
    },
    {
      "epoch": 614.6715328467153,
      "grad_norm": 15.061575889587402,
      "learning_rate": 1.9266423357664235e-05,
      "loss": 0.7975,
      "step": 84210
    },
    {
      "epoch": 614.7445255474453,
      "grad_norm": 7.2924957275390625,
      "learning_rate": 1.926277372262774e-05,
      "loss": 0.5663,
      "step": 84220
    },
    {
      "epoch": 614.8175182481751,
      "grad_norm": 11.72805404663086,
      "learning_rate": 1.925912408759124e-05,
      "loss": 0.9028,
      "step": 84230
    },
    {
      "epoch": 614.8905109489051,
      "grad_norm": 10.982967376708984,
      "learning_rate": 1.9255474452554746e-05,
      "loss": 0.9444,
      "step": 84240
    },
    {
      "epoch": 614.9635036496351,
      "grad_norm": 11.832317352294922,
      "learning_rate": 1.925182481751825e-05,
      "loss": 1.271,
      "step": 84250
    },
    {
      "epoch": 615.0364963503649,
      "grad_norm": 10.774343490600586,
      "learning_rate": 1.9248175182481754e-05,
      "loss": 0.9978,
      "step": 84260
    },
    {
      "epoch": 615.1094890510949,
      "grad_norm": 7.62460470199585,
      "learning_rate": 1.9244525547445254e-05,
      "loss": 0.3392,
      "step": 84270
    },
    {
      "epoch": 615.1824817518249,
      "grad_norm": 13.135973930358887,
      "learning_rate": 1.924087591240876e-05,
      "loss": 0.7271,
      "step": 84280
    },
    {
      "epoch": 615.2554744525547,
      "grad_norm": 0.027085889130830765,
      "learning_rate": 1.9237226277372262e-05,
      "loss": 0.7916,
      "step": 84290
    },
    {
      "epoch": 615.3284671532847,
      "grad_norm": 8.128365516662598,
      "learning_rate": 1.923357664233577e-05,
      "loss": 1.2035,
      "step": 84300
    },
    {
      "epoch": 615.4014598540145,
      "grad_norm": 10.173542976379395,
      "learning_rate": 1.922992700729927e-05,
      "loss": 0.6329,
      "step": 84310
    },
    {
      "epoch": 615.4744525547445,
      "grad_norm": 12.924809455871582,
      "learning_rate": 1.9226277372262774e-05,
      "loss": 0.5642,
      "step": 84320
    },
    {
      "epoch": 615.5474452554745,
      "grad_norm": 10.750406265258789,
      "learning_rate": 1.9222627737226278e-05,
      "loss": 1.2172,
      "step": 84330
    },
    {
      "epoch": 615.6204379562043,
      "grad_norm": 6.980185031890869,
      "learning_rate": 1.921897810218978e-05,
      "loss": 0.8408,
      "step": 84340
    },
    {
      "epoch": 615.6934306569343,
      "grad_norm": 22.170581817626953,
      "learning_rate": 1.9215328467153286e-05,
      "loss": 1.1735,
      "step": 84350
    },
    {
      "epoch": 615.7664233576643,
      "grad_norm": 8.72590160369873,
      "learning_rate": 1.921167883211679e-05,
      "loss": 1.2089,
      "step": 84360
    },
    {
      "epoch": 615.8394160583941,
      "grad_norm": 0.039615534245967865,
      "learning_rate": 1.9208029197080293e-05,
      "loss": 1.0754,
      "step": 84370
    },
    {
      "epoch": 615.9124087591241,
      "grad_norm": 0.039848484098911285,
      "learning_rate": 1.9204379562043797e-05,
      "loss": 0.924,
      "step": 84380
    },
    {
      "epoch": 615.985401459854,
      "grad_norm": 10.90666389465332,
      "learning_rate": 1.9200729927007298e-05,
      "loss": 1.1601,
      "step": 84390
    },
    {
      "epoch": 616.0583941605839,
      "grad_norm": 0.022417500615119934,
      "learning_rate": 1.9197080291970805e-05,
      "loss": 0.9725,
      "step": 84400
    },
    {
      "epoch": 616.1313868613139,
      "grad_norm": 0.021952906623482704,
      "learning_rate": 1.919343065693431e-05,
      "loss": 0.9997,
      "step": 84410
    },
    {
      "epoch": 616.2043795620438,
      "grad_norm": 17.39593505859375,
      "learning_rate": 1.9189781021897813e-05,
      "loss": 1.1164,
      "step": 84420
    },
    {
      "epoch": 616.2773722627737,
      "grad_norm": 12.918807983398438,
      "learning_rate": 1.9186131386861313e-05,
      "loss": 1.0278,
      "step": 84430
    },
    {
      "epoch": 616.3503649635037,
      "grad_norm": 8.29970645904541,
      "learning_rate": 1.9182481751824817e-05,
      "loss": 1.0264,
      "step": 84440
    },
    {
      "epoch": 616.4233576642335,
      "grad_norm": 11.966064453125,
      "learning_rate": 1.9178832116788324e-05,
      "loss": 0.7695,
      "step": 84450
    },
    {
      "epoch": 616.4963503649635,
      "grad_norm": 7.836285591125488,
      "learning_rate": 1.9175182481751825e-05,
      "loss": 1.047,
      "step": 84460
    },
    {
      "epoch": 616.5693430656934,
      "grad_norm": 0.052339740097522736,
      "learning_rate": 1.917153284671533e-05,
      "loss": 0.6347,
      "step": 84470
    },
    {
      "epoch": 616.6423357664233,
      "grad_norm": 12.084983825683594,
      "learning_rate": 1.9167883211678833e-05,
      "loss": 0.8289,
      "step": 84480
    },
    {
      "epoch": 616.7153284671533,
      "grad_norm": 11.509906768798828,
      "learning_rate": 1.9164233576642337e-05,
      "loss": 0.6596,
      "step": 84490
    },
    {
      "epoch": 616.7883211678832,
      "grad_norm": 0.021851547062397003,
      "learning_rate": 1.916058394160584e-05,
      "loss": 1.4182,
      "step": 84500
    },
    {
      "epoch": 616.8613138686131,
      "grad_norm": 7.6435627937316895,
      "learning_rate": 1.9156934306569344e-05,
      "loss": 0.6328,
      "step": 84510
    },
    {
      "epoch": 616.9343065693431,
      "grad_norm": 0.07826334983110428,
      "learning_rate": 1.9153284671532848e-05,
      "loss": 1.0,
      "step": 84520
    },
    {
      "epoch": 617.007299270073,
      "grad_norm": 1.2533942461013794,
      "learning_rate": 1.9149635036496352e-05,
      "loss": 0.8145,
      "step": 84530
    },
    {
      "epoch": 617.0802919708029,
      "grad_norm": 8.226791381835938,
      "learning_rate": 1.9145985401459853e-05,
      "loss": 1.1714,
      "step": 84540
    },
    {
      "epoch": 617.1532846715329,
      "grad_norm": 6.908121109008789,
      "learning_rate": 1.914233576642336e-05,
      "loss": 1.308,
      "step": 84550
    },
    {
      "epoch": 617.2262773722628,
      "grad_norm": 9.77337646484375,
      "learning_rate": 1.9138686131386864e-05,
      "loss": 0.9963,
      "step": 84560
    },
    {
      "epoch": 617.2992700729927,
      "grad_norm": 19.57164192199707,
      "learning_rate": 1.9135036496350368e-05,
      "loss": 0.8191,
      "step": 84570
    },
    {
      "epoch": 617.3722627737226,
      "grad_norm": 8.656645774841309,
      "learning_rate": 1.9131386861313868e-05,
      "loss": 0.6796,
      "step": 84580
    },
    {
      "epoch": 617.4452554744526,
      "grad_norm": 6.324681282043457,
      "learning_rate": 1.9127737226277372e-05,
      "loss": 0.8061,
      "step": 84590
    },
    {
      "epoch": 617.5182481751825,
      "grad_norm": 0.11128687113523483,
      "learning_rate": 1.9124087591240876e-05,
      "loss": 0.9294,
      "step": 84600
    },
    {
      "epoch": 617.5912408759124,
      "grad_norm": 6.360477447509766,
      "learning_rate": 1.9120437956204383e-05,
      "loss": 0.7225,
      "step": 84610
    },
    {
      "epoch": 617.6642335766423,
      "grad_norm": 13.170202255249023,
      "learning_rate": 1.9116788321167884e-05,
      "loss": 0.8602,
      "step": 84620
    },
    {
      "epoch": 617.7372262773723,
      "grad_norm": 8.611120223999023,
      "learning_rate": 1.9113138686131387e-05,
      "loss": 0.8794,
      "step": 84630
    },
    {
      "epoch": 617.8102189781022,
      "grad_norm": 0.06408324837684631,
      "learning_rate": 1.910948905109489e-05,
      "loss": 0.8911,
      "step": 84640
    },
    {
      "epoch": 617.8832116788321,
      "grad_norm": 9.567368507385254,
      "learning_rate": 1.9105839416058395e-05,
      "loss": 0.6876,
      "step": 84650
    },
    {
      "epoch": 617.956204379562,
      "grad_norm": 6.6765923500061035,
      "learning_rate": 1.91021897810219e-05,
      "loss": 0.9789,
      "step": 84660
    },
    {
      "epoch": 618.029197080292,
      "grad_norm": 7.739912033081055,
      "learning_rate": 1.9098540145985403e-05,
      "loss": 0.7279,
      "step": 84670
    },
    {
      "epoch": 618.1021897810219,
      "grad_norm": 7.49851655960083,
      "learning_rate": 1.9094890510948907e-05,
      "loss": 1.0301,
      "step": 84680
    },
    {
      "epoch": 618.1751824817518,
      "grad_norm": 9.344945907592773,
      "learning_rate": 1.9091240875912407e-05,
      "loss": 0.8602,
      "step": 84690
    },
    {
      "epoch": 618.2481751824818,
      "grad_norm": 14.536776542663574,
      "learning_rate": 1.908759124087591e-05,
      "loss": 1.336,
      "step": 84700
    },
    {
      "epoch": 618.3211678832117,
      "grad_norm": 5.8054680824279785,
      "learning_rate": 1.908394160583942e-05,
      "loss": 0.6961,
      "step": 84710
    },
    {
      "epoch": 618.3941605839416,
      "grad_norm": 11.895283699035645,
      "learning_rate": 1.9080291970802922e-05,
      "loss": 0.8402,
      "step": 84720
    },
    {
      "epoch": 618.4671532846716,
      "grad_norm": 5.906752586364746,
      "learning_rate": 1.9076642335766423e-05,
      "loss": 0.6523,
      "step": 84730
    },
    {
      "epoch": 618.5401459854014,
      "grad_norm": 10.611409187316895,
      "learning_rate": 1.9072992700729927e-05,
      "loss": 0.9387,
      "step": 84740
    },
    {
      "epoch": 618.6131386861314,
      "grad_norm": 9.568877220153809,
      "learning_rate": 1.906934306569343e-05,
      "loss": 1.124,
      "step": 84750
    },
    {
      "epoch": 618.6861313868613,
      "grad_norm": 19.255508422851562,
      "learning_rate": 1.9065693430656938e-05,
      "loss": 0.793,
      "step": 84760
    },
    {
      "epoch": 618.7591240875912,
      "grad_norm": 11.171089172363281,
      "learning_rate": 1.906204379562044e-05,
      "loss": 0.7066,
      "step": 84770
    },
    {
      "epoch": 618.8321167883212,
      "grad_norm": 13.158171653747559,
      "learning_rate": 1.9058394160583942e-05,
      "loss": 1.1562,
      "step": 84780
    },
    {
      "epoch": 618.9051094890511,
      "grad_norm": 1.2212369441986084,
      "learning_rate": 1.9054744525547446e-05,
      "loss": 0.5413,
      "step": 84790
    },
    {
      "epoch": 618.978102189781,
      "grad_norm": 14.41857624053955,
      "learning_rate": 1.905109489051095e-05,
      "loss": 0.9292,
      "step": 84800
    },
    {
      "epoch": 619.051094890511,
      "grad_norm": 8.259400367736816,
      "learning_rate": 1.9047445255474454e-05,
      "loss": 1.131,
      "step": 84810
    },
    {
      "epoch": 619.1240875912408,
      "grad_norm": 0.1567145437002182,
      "learning_rate": 1.9043795620437958e-05,
      "loss": 0.7025,
      "step": 84820
    },
    {
      "epoch": 619.1970802919708,
      "grad_norm": 18.315462112426758,
      "learning_rate": 1.9040145985401462e-05,
      "loss": 0.6373,
      "step": 84830
    },
    {
      "epoch": 619.2700729927008,
      "grad_norm": 9.588177680969238,
      "learning_rate": 1.9036496350364966e-05,
      "loss": 0.9683,
      "step": 84840
    },
    {
      "epoch": 619.3430656934306,
      "grad_norm": 5.80528450012207,
      "learning_rate": 1.9032846715328466e-05,
      "loss": 0.6208,
      "step": 84850
    },
    {
      "epoch": 619.4160583941606,
      "grad_norm": 11.62118911743164,
      "learning_rate": 1.902919708029197e-05,
      "loss": 1.1869,
      "step": 84860
    },
    {
      "epoch": 619.4890510948906,
      "grad_norm": 12.947549819946289,
      "learning_rate": 1.9025547445255477e-05,
      "loss": 0.6272,
      "step": 84870
    },
    {
      "epoch": 619.5620437956204,
      "grad_norm": 11.57280158996582,
      "learning_rate": 1.9021897810218978e-05,
      "loss": 0.8064,
      "step": 84880
    },
    {
      "epoch": 619.6350364963504,
      "grad_norm": 13.71440601348877,
      "learning_rate": 1.901824817518248e-05,
      "loss": 1.2279,
      "step": 84890
    },
    {
      "epoch": 619.7080291970802,
      "grad_norm": 10.13444995880127,
      "learning_rate": 1.9014598540145986e-05,
      "loss": 1.2386,
      "step": 84900
    },
    {
      "epoch": 619.7810218978102,
      "grad_norm": 11.296076774597168,
      "learning_rate": 1.901094890510949e-05,
      "loss": 1.154,
      "step": 84910
    },
    {
      "epoch": 619.8540145985402,
      "grad_norm": 18.323997497558594,
      "learning_rate": 1.9007299270072993e-05,
      "loss": 0.8676,
      "step": 84920
    },
    {
      "epoch": 619.92700729927,
      "grad_norm": 6.690735340118408,
      "learning_rate": 1.9003649635036497e-05,
      "loss": 0.5965,
      "step": 84930
    },
    {
      "epoch": 620.0,
      "grad_norm": 15.013321876525879,
      "learning_rate": 1.9e-05,
      "loss": 1.016,
      "step": 84940
    },
    {
      "epoch": 620.07299270073,
      "grad_norm": 13.264569282531738,
      "learning_rate": 1.8996350364963505e-05,
      "loss": 0.7299,
      "step": 84950
    },
    {
      "epoch": 620.1459854014598,
      "grad_norm": 8.88059139251709,
      "learning_rate": 1.8992700729927005e-05,
      "loss": 0.9948,
      "step": 84960
    },
    {
      "epoch": 620.2189781021898,
      "grad_norm": 6.551542282104492,
      "learning_rate": 1.8989051094890513e-05,
      "loss": 0.8622,
      "step": 84970
    },
    {
      "epoch": 620.2919708029198,
      "grad_norm": 14.55842399597168,
      "learning_rate": 1.8985401459854017e-05,
      "loss": 0.8462,
      "step": 84980
    },
    {
      "epoch": 620.3649635036496,
      "grad_norm": 21.3409481048584,
      "learning_rate": 1.898175182481752e-05,
      "loss": 1.1659,
      "step": 84990
    },
    {
      "epoch": 620.4379562043796,
      "grad_norm": 5.945128917694092,
      "learning_rate": 1.897810218978102e-05,
      "loss": 0.8385,
      "step": 85000
    },
    {
      "epoch": 620.5109489051094,
      "grad_norm": 8.449162483215332,
      "learning_rate": 1.8974452554744525e-05,
      "loss": 0.7033,
      "step": 85010
    },
    {
      "epoch": 620.5839416058394,
      "grad_norm": 13.867788314819336,
      "learning_rate": 1.8970802919708032e-05,
      "loss": 0.5231,
      "step": 85020
    },
    {
      "epoch": 620.6569343065694,
      "grad_norm": 12.307271957397461,
      "learning_rate": 1.8967153284671536e-05,
      "loss": 1.426,
      "step": 85030
    },
    {
      "epoch": 620.7299270072992,
      "grad_norm": 14.341915130615234,
      "learning_rate": 1.8963503649635036e-05,
      "loss": 1.0828,
      "step": 85040
    },
    {
      "epoch": 620.8029197080292,
      "grad_norm": 3.2705836296081543,
      "learning_rate": 1.895985401459854e-05,
      "loss": 0.5212,
      "step": 85050
    },
    {
      "epoch": 620.8759124087592,
      "grad_norm": 13.152135848999023,
      "learning_rate": 1.8956204379562044e-05,
      "loss": 0.9317,
      "step": 85060
    },
    {
      "epoch": 620.948905109489,
      "grad_norm": 10.040068626403809,
      "learning_rate": 1.8952554744525548e-05,
      "loss": 1.1424,
      "step": 85070
    },
    {
      "epoch": 621.021897810219,
      "grad_norm": 10.490463256835938,
      "learning_rate": 1.8948905109489052e-05,
      "loss": 1.0118,
      "step": 85080
    },
    {
      "epoch": 621.0948905109489,
      "grad_norm": 14.345102310180664,
      "learning_rate": 1.8945255474452556e-05,
      "loss": 0.6728,
      "step": 85090
    },
    {
      "epoch": 621.1678832116788,
      "grad_norm": 0.06887275725603104,
      "learning_rate": 1.894160583941606e-05,
      "loss": 0.536,
      "step": 85100
    },
    {
      "epoch": 621.2408759124088,
      "grad_norm": 13.126644134521484,
      "learning_rate": 1.893795620437956e-05,
      "loss": 1.0922,
      "step": 85110
    },
    {
      "epoch": 621.3138686131387,
      "grad_norm": 7.286797046661377,
      "learning_rate": 1.8934306569343068e-05,
      "loss": 1.1581,
      "step": 85120
    },
    {
      "epoch": 621.3868613138686,
      "grad_norm": 8.386458396911621,
      "learning_rate": 1.893065693430657e-05,
      "loss": 0.8791,
      "step": 85130
    },
    {
      "epoch": 621.4598540145986,
      "grad_norm": 8.215764999389648,
      "learning_rate": 1.8927007299270075e-05,
      "loss": 1.0775,
      "step": 85140
    },
    {
      "epoch": 621.5328467153284,
      "grad_norm": 11.865748405456543,
      "learning_rate": 1.8923357664233576e-05,
      "loss": 1.0238,
      "step": 85150
    },
    {
      "epoch": 621.6058394160584,
      "grad_norm": 5.829418659210205,
      "learning_rate": 1.891970802919708e-05,
      "loss": 0.654,
      "step": 85160
    },
    {
      "epoch": 621.6788321167883,
      "grad_norm": 23.868804931640625,
      "learning_rate": 1.8916058394160584e-05,
      "loss": 1.3944,
      "step": 85170
    },
    {
      "epoch": 621.7518248175182,
      "grad_norm": 8.567939758300781,
      "learning_rate": 1.891240875912409e-05,
      "loss": 0.7071,
      "step": 85180
    },
    {
      "epoch": 621.8248175182482,
      "grad_norm": 9.48895263671875,
      "learning_rate": 1.890875912408759e-05,
      "loss": 1.1295,
      "step": 85190
    },
    {
      "epoch": 621.8978102189781,
      "grad_norm": 7.721057891845703,
      "learning_rate": 1.8905109489051095e-05,
      "loss": 0.707,
      "step": 85200
    },
    {
      "epoch": 621.970802919708,
      "grad_norm": 7.691699028015137,
      "learning_rate": 1.89014598540146e-05,
      "loss": 1.103,
      "step": 85210
    },
    {
      "epoch": 622.043795620438,
      "grad_norm": 8.486342430114746,
      "learning_rate": 1.8897810218978103e-05,
      "loss": 0.9707,
      "step": 85220
    },
    {
      "epoch": 622.1167883211679,
      "grad_norm": 9.158790588378906,
      "learning_rate": 1.8894160583941607e-05,
      "loss": 1.0847,
      "step": 85230
    },
    {
      "epoch": 622.1897810218978,
      "grad_norm": 12.701909065246582,
      "learning_rate": 1.889051094890511e-05,
      "loss": 0.9241,
      "step": 85240
    },
    {
      "epoch": 622.2627737226277,
      "grad_norm": 7.960766792297363,
      "learning_rate": 1.8886861313868615e-05,
      "loss": 0.873,
      "step": 85250
    },
    {
      "epoch": 622.3357664233577,
      "grad_norm": 14.308032035827637,
      "learning_rate": 1.888321167883212e-05,
      "loss": 0.6167,
      "step": 85260
    },
    {
      "epoch": 622.4087591240876,
      "grad_norm": 12.154535293579102,
      "learning_rate": 1.887956204379562e-05,
      "loss": 1.1782,
      "step": 85270
    },
    {
      "epoch": 622.4817518248175,
      "grad_norm": 0.04207003861665726,
      "learning_rate": 1.8875912408759126e-05,
      "loss": 0.6754,
      "step": 85280
    },
    {
      "epoch": 622.5547445255474,
      "grad_norm": 21.345190048217773,
      "learning_rate": 1.887226277372263e-05,
      "loss": 0.9356,
      "step": 85290
    },
    {
      "epoch": 622.6277372262774,
      "grad_norm": 13.548136711120605,
      "learning_rate": 1.8868613138686134e-05,
      "loss": 1.0073,
      "step": 85300
    },
    {
      "epoch": 622.7007299270073,
      "grad_norm": 12.093628883361816,
      "learning_rate": 1.8864963503649635e-05,
      "loss": 0.9129,
      "step": 85310
    },
    {
      "epoch": 622.7737226277372,
      "grad_norm": 0.05349888280034065,
      "learning_rate": 1.886131386861314e-05,
      "loss": 1.2645,
      "step": 85320
    },
    {
      "epoch": 622.8467153284671,
      "grad_norm": 12.300254821777344,
      "learning_rate": 1.8857664233576646e-05,
      "loss": 0.4781,
      "step": 85330
    },
    {
      "epoch": 622.9197080291971,
      "grad_norm": 9.773172378540039,
      "learning_rate": 1.8854014598540146e-05,
      "loss": 0.9745,
      "step": 85340
    },
    {
      "epoch": 622.992700729927,
      "grad_norm": 12.465115547180176,
      "learning_rate": 1.885036496350365e-05,
      "loss": 0.8242,
      "step": 85350
    },
    {
      "epoch": 623.0656934306569,
      "grad_norm": 0.05678819119930267,
      "learning_rate": 1.8846715328467154e-05,
      "loss": 0.7605,
      "step": 85360
    },
    {
      "epoch": 623.1386861313869,
      "grad_norm": 0.06055170297622681,
      "learning_rate": 1.8843065693430658e-05,
      "loss": 0.7273,
      "step": 85370
    },
    {
      "epoch": 623.2116788321168,
      "grad_norm": 17.00545883178711,
      "learning_rate": 1.8839416058394162e-05,
      "loss": 0.9525,
      "step": 85380
    },
    {
      "epoch": 623.2846715328467,
      "grad_norm": 14.1287260055542,
      "learning_rate": 1.8835766423357666e-05,
      "loss": 0.7798,
      "step": 85390
    },
    {
      "epoch": 623.3576642335767,
      "grad_norm": 6.32738733291626,
      "learning_rate": 1.883211678832117e-05,
      "loss": 0.83,
      "step": 85400
    },
    {
      "epoch": 623.4306569343066,
      "grad_norm": 10.555778503417969,
      "learning_rate": 1.8828467153284673e-05,
      "loss": 1.1851,
      "step": 85410
    },
    {
      "epoch": 623.5036496350365,
      "grad_norm": 18.893190383911133,
      "learning_rate": 1.8824817518248174e-05,
      "loss": 1.1669,
      "step": 85420
    },
    {
      "epoch": 623.5766423357665,
      "grad_norm": 4.231283187866211,
      "learning_rate": 1.882116788321168e-05,
      "loss": 0.8902,
      "step": 85430
    },
    {
      "epoch": 623.6496350364963,
      "grad_norm": 0.7386015057563782,
      "learning_rate": 1.8817518248175185e-05,
      "loss": 0.704,
      "step": 85440
    },
    {
      "epoch": 623.7226277372263,
      "grad_norm": 10.40375804901123,
      "learning_rate": 1.881386861313869e-05,
      "loss": 1.2482,
      "step": 85450
    },
    {
      "epoch": 623.7956204379562,
      "grad_norm": 11.853658676147461,
      "learning_rate": 1.881021897810219e-05,
      "loss": 0.8083,
      "step": 85460
    },
    {
      "epoch": 623.8686131386861,
      "grad_norm": 8.672810554504395,
      "learning_rate": 1.8806569343065693e-05,
      "loss": 1.0255,
      "step": 85470
    },
    {
      "epoch": 623.9416058394161,
      "grad_norm": 7.546658992767334,
      "learning_rate": 1.8802919708029197e-05,
      "loss": 0.9195,
      "step": 85480
    },
    {
      "epoch": 624.014598540146,
      "grad_norm": 11.905498504638672,
      "learning_rate": 1.8799270072992704e-05,
      "loss": 0.7352,
      "step": 85490
    },
    {
      "epoch": 624.0875912408759,
      "grad_norm": 8.223613739013672,
      "learning_rate": 1.8795620437956205e-05,
      "loss": 1.0723,
      "step": 85500
    },
    {
      "epoch": 624.1605839416059,
      "grad_norm": 8.039196968078613,
      "learning_rate": 1.879197080291971e-05,
      "loss": 1.0766,
      "step": 85510
    },
    {
      "epoch": 624.2335766423357,
      "grad_norm": 9.006904602050781,
      "learning_rate": 1.8788321167883213e-05,
      "loss": 1.3187,
      "step": 85520
    },
    {
      "epoch": 624.3065693430657,
      "grad_norm": 10.636524200439453,
      "learning_rate": 1.8784671532846717e-05,
      "loss": 0.9797,
      "step": 85530
    },
    {
      "epoch": 624.3795620437957,
      "grad_norm": 0.04688004031777382,
      "learning_rate": 1.878102189781022e-05,
      "loss": 0.5422,
      "step": 85540
    },
    {
      "epoch": 624.4525547445255,
      "grad_norm": 11.539549827575684,
      "learning_rate": 1.8777372262773724e-05,
      "loss": 1.1164,
      "step": 85550
    },
    {
      "epoch": 624.5255474452555,
      "grad_norm": 11.110722541809082,
      "learning_rate": 1.8773722627737228e-05,
      "loss": 0.4755,
      "step": 85560
    },
    {
      "epoch": 624.5985401459855,
      "grad_norm": 9.781225204467773,
      "learning_rate": 1.877007299270073e-05,
      "loss": 1.3872,
      "step": 85570
    },
    {
      "epoch": 624.6715328467153,
      "grad_norm": 14.168364524841309,
      "learning_rate": 1.8766423357664233e-05,
      "loss": 0.7829,
      "step": 85580
    },
    {
      "epoch": 624.7445255474453,
      "grad_norm": 11.702452659606934,
      "learning_rate": 1.876277372262774e-05,
      "loss": 0.8755,
      "step": 85590
    },
    {
      "epoch": 624.8175182481751,
      "grad_norm": 13.330794334411621,
      "learning_rate": 1.8759124087591244e-05,
      "loss": 0.8274,
      "step": 85600
    },
    {
      "epoch": 624.8905109489051,
      "grad_norm": 17.201457977294922,
      "learning_rate": 1.8755474452554744e-05,
      "loss": 0.7667,
      "step": 85610
    },
    {
      "epoch": 624.9635036496351,
      "grad_norm": 4.869319438934326,
      "learning_rate": 1.8751824817518248e-05,
      "loss": 0.8715,
      "step": 85620
    },
    {
      "epoch": 625.0364963503649,
      "grad_norm": 17.088186264038086,
      "learning_rate": 1.8748175182481752e-05,
      "loss": 1.1018,
      "step": 85630
    },
    {
      "epoch": 625.1094890510949,
      "grad_norm": 0.040511760860681534,
      "learning_rate": 1.8744525547445256e-05,
      "loss": 0.5296,
      "step": 85640
    },
    {
      "epoch": 625.1824817518249,
      "grad_norm": 19.709407806396484,
      "learning_rate": 1.874087591240876e-05,
      "loss": 0.5931,
      "step": 85650
    },
    {
      "epoch": 625.2554744525547,
      "grad_norm": 17.650487899780273,
      "learning_rate": 1.8737226277372264e-05,
      "loss": 0.8572,
      "step": 85660
    },
    {
      "epoch": 625.3284671532847,
      "grad_norm": 5.843925952911377,
      "learning_rate": 1.8733576642335768e-05,
      "loss": 0.8899,
      "step": 85670
    },
    {
      "epoch": 625.4014598540145,
      "grad_norm": 7.686685562133789,
      "learning_rate": 1.872992700729927e-05,
      "loss": 0.6049,
      "step": 85680
    },
    {
      "epoch": 625.4744525547445,
      "grad_norm": 13.217862129211426,
      "learning_rate": 1.8726277372262775e-05,
      "loss": 1.0959,
      "step": 85690
    },
    {
      "epoch": 625.5474452554745,
      "grad_norm": 11.276105880737305,
      "learning_rate": 1.872262773722628e-05,
      "loss": 1.1874,
      "step": 85700
    },
    {
      "epoch": 625.6204379562043,
      "grad_norm": 6.756892204284668,
      "learning_rate": 1.8718978102189783e-05,
      "loss": 1.0588,
      "step": 85710
    },
    {
      "epoch": 625.6934306569343,
      "grad_norm": 6.285338401794434,
      "learning_rate": 1.8715328467153287e-05,
      "loss": 0.7368,
      "step": 85720
    },
    {
      "epoch": 625.7664233576643,
      "grad_norm": 7.897600173950195,
      "learning_rate": 1.8711678832116787e-05,
      "loss": 0.8862,
      "step": 85730
    },
    {
      "epoch": 625.8394160583941,
      "grad_norm": 8.976232528686523,
      "learning_rate": 1.870802919708029e-05,
      "loss": 1.3385,
      "step": 85740
    },
    {
      "epoch": 625.9124087591241,
      "grad_norm": 6.034680366516113,
      "learning_rate": 1.87043795620438e-05,
      "loss": 0.6465,
      "step": 85750
    },
    {
      "epoch": 625.985401459854,
      "grad_norm": 11.7746000289917,
      "learning_rate": 1.87007299270073e-05,
      "loss": 0.7659,
      "step": 85760
    },
    {
      "epoch": 626.0583941605839,
      "grad_norm": 11.826842308044434,
      "learning_rate": 1.8697080291970803e-05,
      "loss": 0.6624,
      "step": 85770
    },
    {
      "epoch": 626.1313868613139,
      "grad_norm": 14.709413528442383,
      "learning_rate": 1.8693430656934307e-05,
      "loss": 0.8841,
      "step": 85780
    },
    {
      "epoch": 626.2043795620438,
      "grad_norm": 5.059703350067139,
      "learning_rate": 1.868978102189781e-05,
      "loss": 1.0443,
      "step": 85790
    },
    {
      "epoch": 626.2773722627737,
      "grad_norm": 12.4739990234375,
      "learning_rate": 1.8686131386861315e-05,
      "loss": 0.8399,
      "step": 85800
    },
    {
      "epoch": 626.3503649635037,
      "grad_norm": 9.911704063415527,
      "learning_rate": 1.868248175182482e-05,
      "loss": 1.0649,
      "step": 85810
    },
    {
      "epoch": 626.4233576642335,
      "grad_norm": 0.15403838455677032,
      "learning_rate": 1.8678832116788322e-05,
      "loss": 0.9566,
      "step": 85820
    },
    {
      "epoch": 626.4963503649635,
      "grad_norm": 6.960309982299805,
      "learning_rate": 1.8675182481751826e-05,
      "loss": 0.8108,
      "step": 85830
    },
    {
      "epoch": 626.5693430656934,
      "grad_norm": 6.574078559875488,
      "learning_rate": 1.8671532846715327e-05,
      "loss": 0.8863,
      "step": 85840
    },
    {
      "epoch": 626.6423357664233,
      "grad_norm": 21.75324058532715,
      "learning_rate": 1.8667883211678834e-05,
      "loss": 0.8694,
      "step": 85850
    },
    {
      "epoch": 626.7153284671533,
      "grad_norm": 21.888046264648438,
      "learning_rate": 1.8664233576642338e-05,
      "loss": 0.3808,
      "step": 85860
    },
    {
      "epoch": 626.7883211678832,
      "grad_norm": 13.885127067565918,
      "learning_rate": 1.8660583941605842e-05,
      "loss": 1.0123,
      "step": 85870
    },
    {
      "epoch": 626.8613138686131,
      "grad_norm": 14.353280067443848,
      "learning_rate": 1.8656934306569342e-05,
      "loss": 1.1575,
      "step": 85880
    },
    {
      "epoch": 626.9343065693431,
      "grad_norm": 6.64040994644165,
      "learning_rate": 1.8653284671532846e-05,
      "loss": 1.0026,
      "step": 85890
    },
    {
      "epoch": 627.007299270073,
      "grad_norm": 14.170757293701172,
      "learning_rate": 1.8649635036496353e-05,
      "loss": 0.9772,
      "step": 85900
    },
    {
      "epoch": 627.0802919708029,
      "grad_norm": 10.742223739624023,
      "learning_rate": 1.8645985401459857e-05,
      "loss": 0.681,
      "step": 85910
    },
    {
      "epoch": 627.1532846715329,
      "grad_norm": 6.3761820793151855,
      "learning_rate": 1.8642335766423358e-05,
      "loss": 0.8892,
      "step": 85920
    },
    {
      "epoch": 627.2262773722628,
      "grad_norm": 7.90833854675293,
      "learning_rate": 1.863868613138686e-05,
      "loss": 0.8634,
      "step": 85930
    },
    {
      "epoch": 627.2992700729927,
      "grad_norm": 21.758838653564453,
      "learning_rate": 1.8635036496350366e-05,
      "loss": 1.1925,
      "step": 85940
    },
    {
      "epoch": 627.3722627737226,
      "grad_norm": 0.037445537745952606,
      "learning_rate": 1.863138686131387e-05,
      "loss": 0.5399,
      "step": 85950
    },
    {
      "epoch": 627.4452554744526,
      "grad_norm": 9.032102584838867,
      "learning_rate": 1.8627737226277373e-05,
      "loss": 0.7786,
      "step": 85960
    },
    {
      "epoch": 627.5182481751825,
      "grad_norm": 11.357101440429688,
      "learning_rate": 1.8624087591240877e-05,
      "loss": 1.2283,
      "step": 85970
    },
    {
      "epoch": 627.5912408759124,
      "grad_norm": 11.55102252960205,
      "learning_rate": 1.862043795620438e-05,
      "loss": 0.7572,
      "step": 85980
    },
    {
      "epoch": 627.6642335766423,
      "grad_norm": 9.964071273803711,
      "learning_rate": 1.861678832116788e-05,
      "loss": 1.0544,
      "step": 85990
    },
    {
      "epoch": 627.7372262773723,
      "grad_norm": 8.252002716064453,
      "learning_rate": 1.861313868613139e-05,
      "loss": 0.6936,
      "step": 86000
    },
    {
      "epoch": 627.8102189781022,
      "grad_norm": 8.270931243896484,
      "learning_rate": 1.8609489051094893e-05,
      "loss": 0.8342,
      "step": 86010
    },
    {
      "epoch": 627.8832116788321,
      "grad_norm": 16.870193481445312,
      "learning_rate": 1.8605839416058397e-05,
      "loss": 1.0987,
      "step": 86020
    },
    {
      "epoch": 627.956204379562,
      "grad_norm": 7.630460262298584,
      "learning_rate": 1.8602189781021897e-05,
      "loss": 1.0579,
      "step": 86030
    },
    {
      "epoch": 628.029197080292,
      "grad_norm": 12.2733793258667,
      "learning_rate": 1.85985401459854e-05,
      "loss": 0.9349,
      "step": 86040
    },
    {
      "epoch": 628.1021897810219,
      "grad_norm": 15.93727970123291,
      "learning_rate": 1.8594890510948905e-05,
      "loss": 1.1757,
      "step": 86050
    },
    {
      "epoch": 628.1751824817518,
      "grad_norm": 7.979156017303467,
      "learning_rate": 1.8591240875912412e-05,
      "loss": 0.8805,
      "step": 86060
    },
    {
      "epoch": 628.2481751824818,
      "grad_norm": 11.316513061523438,
      "learning_rate": 1.8587591240875913e-05,
      "loss": 1.2834,
      "step": 86070
    },
    {
      "epoch": 628.3211678832117,
      "grad_norm": 15.941920280456543,
      "learning_rate": 1.8583941605839417e-05,
      "loss": 1.064,
      "step": 86080
    },
    {
      "epoch": 628.3941605839416,
      "grad_norm": 0.03056350164115429,
      "learning_rate": 1.858029197080292e-05,
      "loss": 0.8364,
      "step": 86090
    },
    {
      "epoch": 628.4671532846716,
      "grad_norm": 0.0813421756029129,
      "learning_rate": 1.8576642335766424e-05,
      "loss": 0.6406,
      "step": 86100
    },
    {
      "epoch": 628.5401459854014,
      "grad_norm": 6.639750003814697,
      "learning_rate": 1.8572992700729928e-05,
      "loss": 0.4846,
      "step": 86110
    },
    {
      "epoch": 628.6131386861314,
      "grad_norm": 10.7535400390625,
      "learning_rate": 1.8569343065693432e-05,
      "loss": 0.4027,
      "step": 86120
    },
    {
      "epoch": 628.6861313868613,
      "grad_norm": 0.09592308104038239,
      "learning_rate": 1.8565693430656936e-05,
      "loss": 0.7189,
      "step": 86130
    },
    {
      "epoch": 628.7591240875912,
      "grad_norm": 9.529484748840332,
      "learning_rate": 1.856204379562044e-05,
      "loss": 0.9625,
      "step": 86140
    },
    {
      "epoch": 628.8321167883212,
      "grad_norm": 7.294052600860596,
      "learning_rate": 1.855839416058394e-05,
      "loss": 0.6918,
      "step": 86150
    },
    {
      "epoch": 628.9051094890511,
      "grad_norm": 12.211043357849121,
      "learning_rate": 1.8554744525547448e-05,
      "loss": 1.1802,
      "step": 86160
    },
    {
      "epoch": 628.978102189781,
      "grad_norm": 9.787303924560547,
      "learning_rate": 1.855109489051095e-05,
      "loss": 0.7492,
      "step": 86170
    },
    {
      "epoch": 629.051094890511,
      "grad_norm": 11.613630294799805,
      "learning_rate": 1.8547445255474452e-05,
      "loss": 1.4032,
      "step": 86180
    },
    {
      "epoch": 629.1240875912408,
      "grad_norm": 6.9192214012146,
      "learning_rate": 1.8543795620437956e-05,
      "loss": 0.6938,
      "step": 86190
    },
    {
      "epoch": 629.1970802919708,
      "grad_norm": 15.437935829162598,
      "learning_rate": 1.854014598540146e-05,
      "loss": 1.0233,
      "step": 86200
    },
    {
      "epoch": 629.2700729927008,
      "grad_norm": 8.130254745483398,
      "learning_rate": 1.8536496350364964e-05,
      "loss": 0.8866,
      "step": 86210
    },
    {
      "epoch": 629.3430656934306,
      "grad_norm": 7.632058620452881,
      "learning_rate": 1.8532846715328468e-05,
      "loss": 0.9153,
      "step": 86220
    },
    {
      "epoch": 629.4160583941606,
      "grad_norm": 13.353960037231445,
      "learning_rate": 1.852919708029197e-05,
      "loss": 1.5006,
      "step": 86230
    },
    {
      "epoch": 629.4890510948906,
      "grad_norm": 9.604175567626953,
      "learning_rate": 1.8525547445255475e-05,
      "loss": 0.8123,
      "step": 86240
    },
    {
      "epoch": 629.5620437956204,
      "grad_norm": 6.61851692199707,
      "learning_rate": 1.852189781021898e-05,
      "loss": 1.0389,
      "step": 86250
    },
    {
      "epoch": 629.6350364963504,
      "grad_norm": 7.240190505981445,
      "learning_rate": 1.8518248175182483e-05,
      "loss": 0.7155,
      "step": 86260
    },
    {
      "epoch": 629.7080291970802,
      "grad_norm": 7.728484630584717,
      "learning_rate": 1.8514598540145987e-05,
      "loss": 0.7347,
      "step": 86270
    },
    {
      "epoch": 629.7810218978102,
      "grad_norm": 12.012900352478027,
      "learning_rate": 1.851094890510949e-05,
      "loss": 0.6959,
      "step": 86280
    },
    {
      "epoch": 629.8540145985402,
      "grad_norm": 15.714361190795898,
      "learning_rate": 1.8507299270072995e-05,
      "loss": 0.7953,
      "step": 86290
    },
    {
      "epoch": 629.92700729927,
      "grad_norm": 18.070018768310547,
      "learning_rate": 1.8503649635036495e-05,
      "loss": 0.7497,
      "step": 86300
    },
    {
      "epoch": 630.0,
      "grad_norm": 30.29099464416504,
      "learning_rate": 1.85e-05,
      "loss": 0.9619,
      "step": 86310
    },
    {
      "epoch": 630.07299270073,
      "grad_norm": 9.702901840209961,
      "learning_rate": 1.8496350364963506e-05,
      "loss": 0.779,
      "step": 86320
    },
    {
      "epoch": 630.1459854014598,
      "grad_norm": 6.441865921020508,
      "learning_rate": 1.849270072992701e-05,
      "loss": 0.7897,
      "step": 86330
    },
    {
      "epoch": 630.2189781021898,
      "grad_norm": 13.177878379821777,
      "learning_rate": 1.848905109489051e-05,
      "loss": 1.0647,
      "step": 86340
    },
    {
      "epoch": 630.2919708029198,
      "grad_norm": 7.3729166984558105,
      "learning_rate": 1.8485401459854015e-05,
      "loss": 1.027,
      "step": 86350
    },
    {
      "epoch": 630.3649635036496,
      "grad_norm": 0.0956035777926445,
      "learning_rate": 1.848175182481752e-05,
      "loss": 0.979,
      "step": 86360
    },
    {
      "epoch": 630.4379562043796,
      "grad_norm": 12.875360488891602,
      "learning_rate": 1.8478102189781026e-05,
      "loss": 0.569,
      "step": 86370
    },
    {
      "epoch": 630.5109489051094,
      "grad_norm": 0.03891107067465782,
      "learning_rate": 1.8474452554744526e-05,
      "loss": 0.7621,
      "step": 86380
    },
    {
      "epoch": 630.5839416058394,
      "grad_norm": 15.657735824584961,
      "learning_rate": 1.847080291970803e-05,
      "loss": 0.6469,
      "step": 86390
    },
    {
      "epoch": 630.6569343065694,
      "grad_norm": 8.058265686035156,
      "learning_rate": 1.8467153284671534e-05,
      "loss": 0.8539,
      "step": 86400
    },
    {
      "epoch": 630.7299270072992,
      "grad_norm": 18.387939453125,
      "learning_rate": 1.8463503649635034e-05,
      "loss": 1.085,
      "step": 86410
    },
    {
      "epoch": 630.8029197080292,
      "grad_norm": 9.03187370300293,
      "learning_rate": 1.8459854014598542e-05,
      "loss": 1.0689,
      "step": 86420
    },
    {
      "epoch": 630.8759124087592,
      "grad_norm": 10.131352424621582,
      "learning_rate": 1.8456204379562046e-05,
      "loss": 1.3952,
      "step": 86430
    },
    {
      "epoch": 630.948905109489,
      "grad_norm": 10.698175430297852,
      "learning_rate": 1.845255474452555e-05,
      "loss": 0.7319,
      "step": 86440
    },
    {
      "epoch": 631.021897810219,
      "grad_norm": 6.158005714416504,
      "learning_rate": 1.844890510948905e-05,
      "loss": 0.7964,
      "step": 86450
    },
    {
      "epoch": 631.0948905109489,
      "grad_norm": 9.33654499053955,
      "learning_rate": 1.8445255474452554e-05,
      "loss": 0.7785,
      "step": 86460
    },
    {
      "epoch": 631.1678832116788,
      "grad_norm": 7.679080963134766,
      "learning_rate": 1.844160583941606e-05,
      "loss": 0.7075,
      "step": 86470
    },
    {
      "epoch": 631.2408759124088,
      "grad_norm": 8.21673583984375,
      "learning_rate": 1.8437956204379565e-05,
      "loss": 0.7147,
      "step": 86480
    },
    {
      "epoch": 631.3138686131387,
      "grad_norm": 7.718308448791504,
      "learning_rate": 1.8434306569343066e-05,
      "loss": 1.0343,
      "step": 86490
    },
    {
      "epoch": 631.3868613138686,
      "grad_norm": 10.772730827331543,
      "learning_rate": 1.843065693430657e-05,
      "loss": 1.1236,
      "step": 86500
    },
    {
      "epoch": 631.4598540145986,
      "grad_norm": 7.451783180236816,
      "learning_rate": 1.8427007299270073e-05,
      "loss": 0.6198,
      "step": 86510
    },
    {
      "epoch": 631.5328467153284,
      "grad_norm": 12.264670372009277,
      "learning_rate": 1.8423357664233577e-05,
      "loss": 0.5687,
      "step": 86520
    },
    {
      "epoch": 631.6058394160584,
      "grad_norm": 7.292670249938965,
      "learning_rate": 1.841970802919708e-05,
      "loss": 1.378,
      "step": 86530
    },
    {
      "epoch": 631.6788321167883,
      "grad_norm": 12.734254837036133,
      "learning_rate": 1.8416058394160585e-05,
      "loss": 1.1093,
      "step": 86540
    },
    {
      "epoch": 631.7518248175182,
      "grad_norm": 9.731155395507812,
      "learning_rate": 1.841240875912409e-05,
      "loss": 0.8416,
      "step": 86550
    },
    {
      "epoch": 631.8248175182482,
      "grad_norm": 11.111648559570312,
      "learning_rate": 1.8408759124087593e-05,
      "loss": 1.0371,
      "step": 86560
    },
    {
      "epoch": 631.8978102189781,
      "grad_norm": 11.0921630859375,
      "learning_rate": 1.8405109489051097e-05,
      "loss": 0.9254,
      "step": 86570
    },
    {
      "epoch": 631.970802919708,
      "grad_norm": 4.779082775115967,
      "learning_rate": 1.84014598540146e-05,
      "loss": 1.0734,
      "step": 86580
    },
    {
      "epoch": 632.043795620438,
      "grad_norm": 12.276156425476074,
      "learning_rate": 1.8397810218978104e-05,
      "loss": 0.957,
      "step": 86590
    },
    {
      "epoch": 632.1167883211679,
      "grad_norm": 18.17441177368164,
      "learning_rate": 1.8394160583941605e-05,
      "loss": 1.05,
      "step": 86600
    },
    {
      "epoch": 632.1897810218978,
      "grad_norm": 13.569934844970703,
      "learning_rate": 1.839051094890511e-05,
      "loss": 0.7934,
      "step": 86610
    },
    {
      "epoch": 632.2627737226277,
      "grad_norm": 12.064961433410645,
      "learning_rate": 1.8386861313868613e-05,
      "loss": 0.9066,
      "step": 86620
    },
    {
      "epoch": 632.3357664233577,
      "grad_norm": 11.799826622009277,
      "learning_rate": 1.838321167883212e-05,
      "loss": 0.8578,
      "step": 86630
    },
    {
      "epoch": 632.4087591240876,
      "grad_norm": 6.746786594390869,
      "learning_rate": 1.837956204379562e-05,
      "loss": 1.0593,
      "step": 86640
    },
    {
      "epoch": 632.4817518248175,
      "grad_norm": 9.91928482055664,
      "learning_rate": 1.8375912408759124e-05,
      "loss": 0.7652,
      "step": 86650
    },
    {
      "epoch": 632.5547445255474,
      "grad_norm": 13.606797218322754,
      "learning_rate": 1.8372262773722628e-05,
      "loss": 1.0568,
      "step": 86660
    },
    {
      "epoch": 632.6277372262774,
      "grad_norm": 8.605432510375977,
      "learning_rate": 1.8368613138686132e-05,
      "loss": 0.9026,
      "step": 86670
    },
    {
      "epoch": 632.7007299270073,
      "grad_norm": 7.78264045715332,
      "learning_rate": 1.8364963503649636e-05,
      "loss": 0.9578,
      "step": 86680
    },
    {
      "epoch": 632.7737226277372,
      "grad_norm": 6.457708835601807,
      "learning_rate": 1.836131386861314e-05,
      "loss": 0.7726,
      "step": 86690
    },
    {
      "epoch": 632.8467153284671,
      "grad_norm": 6.872453212738037,
      "learning_rate": 1.8357664233576644e-05,
      "loss": 0.639,
      "step": 86700
    },
    {
      "epoch": 632.9197080291971,
      "grad_norm": 6.017731666564941,
      "learning_rate": 1.8354014598540148e-05,
      "loss": 1.1749,
      "step": 86710
    },
    {
      "epoch": 632.992700729927,
      "grad_norm": 9.497393608093262,
      "learning_rate": 1.8350364963503648e-05,
      "loss": 0.8124,
      "step": 86720
    },
    {
      "epoch": 633.0656934306569,
      "grad_norm": 13.074649810791016,
      "learning_rate": 1.8346715328467155e-05,
      "loss": 0.6754,
      "step": 86730
    },
    {
      "epoch": 633.1386861313869,
      "grad_norm": 0.2114015817642212,
      "learning_rate": 1.834306569343066e-05,
      "loss": 1.0161,
      "step": 86740
    },
    {
      "epoch": 633.2116788321168,
      "grad_norm": 9.836393356323242,
      "learning_rate": 1.8339416058394163e-05,
      "loss": 1.0781,
      "step": 86750
    },
    {
      "epoch": 633.2846715328467,
      "grad_norm": 11.382646560668945,
      "learning_rate": 1.8335766423357664e-05,
      "loss": 0.6382,
      "step": 86760
    },
    {
      "epoch": 633.3576642335767,
      "grad_norm": 14.657540321350098,
      "learning_rate": 1.8332116788321167e-05,
      "loss": 0.7399,
      "step": 86770
    },
    {
      "epoch": 633.4306569343066,
      "grad_norm": 11.517824172973633,
      "learning_rate": 1.832846715328467e-05,
      "loss": 0.9484,
      "step": 86780
    },
    {
      "epoch": 633.5036496350365,
      "grad_norm": 9.46773910522461,
      "learning_rate": 1.832481751824818e-05,
      "loss": 0.6018,
      "step": 86790
    },
    {
      "epoch": 633.5766423357665,
      "grad_norm": 11.003154754638672,
      "learning_rate": 1.832116788321168e-05,
      "loss": 0.8752,
      "step": 86800
    },
    {
      "epoch": 633.6496350364963,
      "grad_norm": 7.267205238342285,
      "learning_rate": 1.8317518248175183e-05,
      "loss": 0.774,
      "step": 86810
    },
    {
      "epoch": 633.7226277372263,
      "grad_norm": 7.97523307800293,
      "learning_rate": 1.8313868613138687e-05,
      "loss": 1.0579,
      "step": 86820
    },
    {
      "epoch": 633.7956204379562,
      "grad_norm": 9.898640632629395,
      "learning_rate": 1.831021897810219e-05,
      "loss": 0.9964,
      "step": 86830
    },
    {
      "epoch": 633.8686131386861,
      "grad_norm": 0.039151761680841446,
      "learning_rate": 1.8306569343065695e-05,
      "loss": 1.2865,
      "step": 86840
    },
    {
      "epoch": 633.9416058394161,
      "grad_norm": 13.19396686553955,
      "learning_rate": 1.83029197080292e-05,
      "loss": 0.8341,
      "step": 86850
    },
    {
      "epoch": 634.014598540146,
      "grad_norm": 4.75296688079834,
      "learning_rate": 1.8299270072992702e-05,
      "loss": 0.7079,
      "step": 86860
    },
    {
      "epoch": 634.0875912408759,
      "grad_norm": 6.536191463470459,
      "learning_rate": 1.8295620437956203e-05,
      "loss": 0.8981,
      "step": 86870
    },
    {
      "epoch": 634.1605839416059,
      "grad_norm": 12.669768333435059,
      "learning_rate": 1.8291970802919707e-05,
      "loss": 0.6549,
      "step": 86880
    },
    {
      "epoch": 634.2335766423357,
      "grad_norm": 0.04309981316328049,
      "learning_rate": 1.8288321167883214e-05,
      "loss": 0.7899,
      "step": 86890
    },
    {
      "epoch": 634.3065693430657,
      "grad_norm": 0.05407528579235077,
      "learning_rate": 1.8284671532846718e-05,
      "loss": 0.723,
      "step": 86900
    },
    {
      "epoch": 634.3795620437957,
      "grad_norm": 9.691540718078613,
      "learning_rate": 1.828102189781022e-05,
      "loss": 0.7348,
      "step": 86910
    },
    {
      "epoch": 634.4525547445255,
      "grad_norm": 10.949254035949707,
      "learning_rate": 1.8277372262773722e-05,
      "loss": 0.4201,
      "step": 86920
    },
    {
      "epoch": 634.5255474452555,
      "grad_norm": 1.0584427118301392,
      "learning_rate": 1.8273722627737226e-05,
      "loss": 0.6501,
      "step": 86930
    },
    {
      "epoch": 634.5985401459855,
      "grad_norm": 7.838311672210693,
      "learning_rate": 1.8270072992700733e-05,
      "loss": 1.2878,
      "step": 86940
    },
    {
      "epoch": 634.6715328467153,
      "grad_norm": 11.582878112792969,
      "learning_rate": 1.8266423357664234e-05,
      "loss": 0.9762,
      "step": 86950
    },
    {
      "epoch": 634.7445255474453,
      "grad_norm": 13.192144393920898,
      "learning_rate": 1.8262773722627738e-05,
      "loss": 1.4017,
      "step": 86960
    },
    {
      "epoch": 634.8175182481751,
      "grad_norm": 24.520851135253906,
      "learning_rate": 1.8259124087591242e-05,
      "loss": 0.9423,
      "step": 86970
    },
    {
      "epoch": 634.8905109489051,
      "grad_norm": 6.943400859832764,
      "learning_rate": 1.8255474452554746e-05,
      "loss": 0.8023,
      "step": 86980
    },
    {
      "epoch": 634.9635036496351,
      "grad_norm": 18.850072860717773,
      "learning_rate": 1.825182481751825e-05,
      "loss": 1.3902,
      "step": 86990
    },
    {
      "epoch": 635.0364963503649,
      "grad_norm": 11.79709529876709,
      "learning_rate": 1.8248175182481753e-05,
      "loss": 0.9275,
      "step": 87000
    },
    {
      "epoch": 635.1094890510949,
      "grad_norm": 9.690361022949219,
      "learning_rate": 1.8244525547445257e-05,
      "loss": 1.2466,
      "step": 87010
    },
    {
      "epoch": 635.1824817518249,
      "grad_norm": 0.025835983455181122,
      "learning_rate": 1.824087591240876e-05,
      "loss": 0.7956,
      "step": 87020
    },
    {
      "epoch": 635.2554744525547,
      "grad_norm": 14.093756675720215,
      "learning_rate": 1.823722627737226e-05,
      "loss": 0.5725,
      "step": 87030
    },
    {
      "epoch": 635.3284671532847,
      "grad_norm": 7.018664360046387,
      "learning_rate": 1.823357664233577e-05,
      "loss": 0.9533,
      "step": 87040
    },
    {
      "epoch": 635.4014598540145,
      "grad_norm": 0.07393012940883636,
      "learning_rate": 1.8229927007299273e-05,
      "loss": 0.7128,
      "step": 87050
    },
    {
      "epoch": 635.4744525547445,
      "grad_norm": 12.58485221862793,
      "learning_rate": 1.8226277372262773e-05,
      "loss": 0.9796,
      "step": 87060
    },
    {
      "epoch": 635.5474452554745,
      "grad_norm": 16.917394638061523,
      "learning_rate": 1.8222627737226277e-05,
      "loss": 1.0159,
      "step": 87070
    },
    {
      "epoch": 635.6204379562043,
      "grad_norm": 0.5265589952468872,
      "learning_rate": 1.821897810218978e-05,
      "loss": 0.5074,
      "step": 87080
    },
    {
      "epoch": 635.6934306569343,
      "grad_norm": 7.670341491699219,
      "learning_rate": 1.8215328467153285e-05,
      "loss": 1.2046,
      "step": 87090
    },
    {
      "epoch": 635.7664233576643,
      "grad_norm": 5.969288349151611,
      "learning_rate": 1.821167883211679e-05,
      "loss": 0.7599,
      "step": 87100
    },
    {
      "epoch": 635.8394160583941,
      "grad_norm": 5.987823009490967,
      "learning_rate": 1.8208029197080293e-05,
      "loss": 1.0049,
      "step": 87110
    },
    {
      "epoch": 635.9124087591241,
      "grad_norm": 9.006049156188965,
      "learning_rate": 1.8204379562043797e-05,
      "loss": 0.753,
      "step": 87120
    },
    {
      "epoch": 635.985401459854,
      "grad_norm": 16.491865158081055,
      "learning_rate": 1.82007299270073e-05,
      "loss": 1.2466,
      "step": 87130
    },
    {
      "epoch": 636.0583941605839,
      "grad_norm": 0.6956243515014648,
      "learning_rate": 1.8197080291970804e-05,
      "loss": 1.0065,
      "step": 87140
    },
    {
      "epoch": 636.1313868613139,
      "grad_norm": 10.182180404663086,
      "learning_rate": 1.8193430656934308e-05,
      "loss": 1.3045,
      "step": 87150
    },
    {
      "epoch": 636.2043795620438,
      "grad_norm": 10.085038185119629,
      "learning_rate": 1.8189781021897812e-05,
      "loss": 0.6977,
      "step": 87160
    },
    {
      "epoch": 636.2773722627737,
      "grad_norm": 8.64234733581543,
      "learning_rate": 1.8186131386861316e-05,
      "loss": 0.8026,
      "step": 87170
    },
    {
      "epoch": 636.3503649635037,
      "grad_norm": 10.466801643371582,
      "learning_rate": 1.8182481751824816e-05,
      "loss": 0.9492,
      "step": 87180
    },
    {
      "epoch": 636.4233576642335,
      "grad_norm": 9.359878540039062,
      "learning_rate": 1.817883211678832e-05,
      "loss": 0.6996,
      "step": 87190
    },
    {
      "epoch": 636.4963503649635,
      "grad_norm": 7.84163236618042,
      "learning_rate": 1.8175182481751828e-05,
      "loss": 0.6437,
      "step": 87200
    },
    {
      "epoch": 636.5693430656934,
      "grad_norm": 0.08404282480478287,
      "learning_rate": 1.817153284671533e-05,
      "loss": 1.1024,
      "step": 87210
    },
    {
      "epoch": 636.6423357664233,
      "grad_norm": 11.630313873291016,
      "learning_rate": 1.8167883211678832e-05,
      "loss": 0.9722,
      "step": 87220
    },
    {
      "epoch": 636.7153284671533,
      "grad_norm": 3.223385810852051,
      "learning_rate": 1.8164233576642336e-05,
      "loss": 0.868,
      "step": 87230
    },
    {
      "epoch": 636.7883211678832,
      "grad_norm": 8.476601600646973,
      "learning_rate": 1.816058394160584e-05,
      "loss": 1.0803,
      "step": 87240
    },
    {
      "epoch": 636.8613138686131,
      "grad_norm": 11.047658920288086,
      "learning_rate": 1.8156934306569344e-05,
      "loss": 1.1644,
      "step": 87250
    },
    {
      "epoch": 636.9343065693431,
      "grad_norm": 13.83834171295166,
      "learning_rate": 1.8153284671532848e-05,
      "loss": 0.6367,
      "step": 87260
    },
    {
      "epoch": 637.007299270073,
      "grad_norm": 0.039658837020397186,
      "learning_rate": 1.814963503649635e-05,
      "loss": 0.9958,
      "step": 87270
    },
    {
      "epoch": 637.0802919708029,
      "grad_norm": 15.615608215332031,
      "learning_rate": 1.8145985401459855e-05,
      "loss": 1.4225,
      "step": 87280
    },
    {
      "epoch": 637.1532846715329,
      "grad_norm": 15.577475547790527,
      "learning_rate": 1.8142335766423356e-05,
      "loss": 0.8252,
      "step": 87290
    },
    {
      "epoch": 637.2262773722628,
      "grad_norm": 14.272645950317383,
      "learning_rate": 1.8138686131386863e-05,
      "loss": 0.9906,
      "step": 87300
    },
    {
      "epoch": 637.2992700729927,
      "grad_norm": 5.438063621520996,
      "learning_rate": 1.8135036496350367e-05,
      "loss": 0.9092,
      "step": 87310
    },
    {
      "epoch": 637.3722627737226,
      "grad_norm": 9.382558822631836,
      "learning_rate": 1.813138686131387e-05,
      "loss": 0.6836,
      "step": 87320
    },
    {
      "epoch": 637.4452554744526,
      "grad_norm": 4.713458061218262,
      "learning_rate": 1.812773722627737e-05,
      "loss": 0.6296,
      "step": 87330
    },
    {
      "epoch": 637.5182481751825,
      "grad_norm": 10.506664276123047,
      "learning_rate": 1.8124087591240875e-05,
      "loss": 0.8267,
      "step": 87340
    },
    {
      "epoch": 637.5912408759124,
      "grad_norm": 11.910120964050293,
      "learning_rate": 1.812043795620438e-05,
      "loss": 0.9644,
      "step": 87350
    },
    {
      "epoch": 637.6642335766423,
      "grad_norm": 10.700922966003418,
      "learning_rate": 1.8116788321167886e-05,
      "loss": 0.9254,
      "step": 87360
    },
    {
      "epoch": 637.7372262773723,
      "grad_norm": 8.204063415527344,
      "learning_rate": 1.8113138686131387e-05,
      "loss": 0.5167,
      "step": 87370
    },
    {
      "epoch": 637.8102189781022,
      "grad_norm": 10.398926734924316,
      "learning_rate": 1.810948905109489e-05,
      "loss": 1.1683,
      "step": 87380
    },
    {
      "epoch": 637.8832116788321,
      "grad_norm": 5.04658317565918,
      "learning_rate": 1.8105839416058395e-05,
      "loss": 1.3261,
      "step": 87390
    },
    {
      "epoch": 637.956204379562,
      "grad_norm": 9.285966873168945,
      "learning_rate": 1.81021897810219e-05,
      "loss": 0.6703,
      "step": 87400
    },
    {
      "epoch": 638.029197080292,
      "grad_norm": 6.369239807128906,
      "learning_rate": 1.8098540145985402e-05,
      "loss": 0.8119,
      "step": 87410
    },
    {
      "epoch": 638.1021897810219,
      "grad_norm": 11.933534622192383,
      "learning_rate": 1.8094890510948906e-05,
      "loss": 0.6697,
      "step": 87420
    },
    {
      "epoch": 638.1751824817518,
      "grad_norm": 8.133991241455078,
      "learning_rate": 1.809124087591241e-05,
      "loss": 1.0588,
      "step": 87430
    },
    {
      "epoch": 638.2481751824818,
      "grad_norm": 11.177901268005371,
      "learning_rate": 1.8087591240875914e-05,
      "loss": 0.7827,
      "step": 87440
    },
    {
      "epoch": 638.3211678832117,
      "grad_norm": 0.06375902146100998,
      "learning_rate": 1.8083941605839415e-05,
      "loss": 0.6815,
      "step": 87450
    },
    {
      "epoch": 638.3941605839416,
      "grad_norm": 5.445826530456543,
      "learning_rate": 1.8080291970802922e-05,
      "loss": 0.8978,
      "step": 87460
    },
    {
      "epoch": 638.4671532846716,
      "grad_norm": 0.08589167892932892,
      "learning_rate": 1.8076642335766426e-05,
      "loss": 0.8148,
      "step": 87470
    },
    {
      "epoch": 638.5401459854014,
      "grad_norm": 5.681158542633057,
      "learning_rate": 1.8072992700729926e-05,
      "loss": 0.9305,
      "step": 87480
    },
    {
      "epoch": 638.6131386861314,
      "grad_norm": 13.651947021484375,
      "learning_rate": 1.806934306569343e-05,
      "loss": 0.8224,
      "step": 87490
    },
    {
      "epoch": 638.6861313868613,
      "grad_norm": 10.2501220703125,
      "learning_rate": 1.8065693430656934e-05,
      "loss": 1.0601,
      "step": 87500
    },
    {
      "epoch": 638.7591240875912,
      "grad_norm": 8.465415954589844,
      "learning_rate": 1.806204379562044e-05,
      "loss": 1.1672,
      "step": 87510
    },
    {
      "epoch": 638.8321167883212,
      "grad_norm": 9.087540626525879,
      "learning_rate": 1.8058394160583942e-05,
      "loss": 1.1468,
      "step": 87520
    },
    {
      "epoch": 638.9051094890511,
      "grad_norm": 9.324207305908203,
      "learning_rate": 1.8054744525547446e-05,
      "loss": 0.9411,
      "step": 87530
    },
    {
      "epoch": 638.978102189781,
      "grad_norm": 16.42682456970215,
      "learning_rate": 1.805109489051095e-05,
      "loss": 1.0386,
      "step": 87540
    },
    {
      "epoch": 639.051094890511,
      "grad_norm": 8.569493293762207,
      "learning_rate": 1.8047445255474453e-05,
      "loss": 0.9589,
      "step": 87550
    },
    {
      "epoch": 639.1240875912408,
      "grad_norm": 3.3367726802825928,
      "learning_rate": 1.8043795620437957e-05,
      "loss": 1.1429,
      "step": 87560
    },
    {
      "epoch": 639.1970802919708,
      "grad_norm": 10.86173152923584,
      "learning_rate": 1.804014598540146e-05,
      "loss": 0.8324,
      "step": 87570
    },
    {
      "epoch": 639.2700729927008,
      "grad_norm": 13.90648078918457,
      "learning_rate": 1.8036496350364965e-05,
      "loss": 0.799,
      "step": 87580
    },
    {
      "epoch": 639.3430656934306,
      "grad_norm": 6.610496997833252,
      "learning_rate": 1.803284671532847e-05,
      "loss": 0.7885,
      "step": 87590
    },
    {
      "epoch": 639.4160583941606,
      "grad_norm": 7.977221965789795,
      "learning_rate": 1.802919708029197e-05,
      "loss": 1.2806,
      "step": 87600
    },
    {
      "epoch": 639.4890510948906,
      "grad_norm": 8.03642749786377,
      "learning_rate": 1.8025547445255477e-05,
      "loss": 1.0447,
      "step": 87610
    },
    {
      "epoch": 639.5620437956204,
      "grad_norm": 7.136366367340088,
      "learning_rate": 1.802189781021898e-05,
      "loss": 0.7121,
      "step": 87620
    },
    {
      "epoch": 639.6350364963504,
      "grad_norm": 5.814723968505859,
      "learning_rate": 1.8018248175182484e-05,
      "loss": 0.3538,
      "step": 87630
    },
    {
      "epoch": 639.7080291970802,
      "grad_norm": 10.54707145690918,
      "learning_rate": 1.8014598540145985e-05,
      "loss": 0.8532,
      "step": 87640
    },
    {
      "epoch": 639.7810218978102,
      "grad_norm": 9.358790397644043,
      "learning_rate": 1.801094890510949e-05,
      "loss": 1.1685,
      "step": 87650
    },
    {
      "epoch": 639.8540145985402,
      "grad_norm": 0.05390952527523041,
      "learning_rate": 1.8007299270072993e-05,
      "loss": 0.8691,
      "step": 87660
    },
    {
      "epoch": 639.92700729927,
      "grad_norm": 5.9191131591796875,
      "learning_rate": 1.80036496350365e-05,
      "loss": 0.6653,
      "step": 87670
    },
    {
      "epoch": 640.0,
      "grad_norm": 16.8259220123291,
      "learning_rate": 1.8e-05,
      "loss": 1.1496,
      "step": 87680
    },
    {
      "epoch": 640.07299270073,
      "grad_norm": 7.061201095581055,
      "learning_rate": 1.7996350364963504e-05,
      "loss": 0.5925,
      "step": 87690
    },
    {
      "epoch": 640.1459854014598,
      "grad_norm": 0.13783662021160126,
      "learning_rate": 1.7992700729927008e-05,
      "loss": 0.9208,
      "step": 87700
    },
    {
      "epoch": 640.2189781021898,
      "grad_norm": 11.92199420928955,
      "learning_rate": 1.7989051094890512e-05,
      "loss": 1.3462,
      "step": 87710
    },
    {
      "epoch": 640.2919708029198,
      "grad_norm": 12.467001914978027,
      "learning_rate": 1.7985401459854016e-05,
      "loss": 0.9929,
      "step": 87720
    },
    {
      "epoch": 640.3649635036496,
      "grad_norm": 5.84785270690918,
      "learning_rate": 1.798175182481752e-05,
      "loss": 0.6496,
      "step": 87730
    },
    {
      "epoch": 640.4379562043796,
      "grad_norm": 14.652507781982422,
      "learning_rate": 1.7978102189781024e-05,
      "loss": 1.0161,
      "step": 87740
    },
    {
      "epoch": 640.5109489051094,
      "grad_norm": 15.007624626159668,
      "learning_rate": 1.7974452554744524e-05,
      "loss": 0.9718,
      "step": 87750
    },
    {
      "epoch": 640.5839416058394,
      "grad_norm": 19.828508377075195,
      "learning_rate": 1.7970802919708028e-05,
      "loss": 1.0377,
      "step": 87760
    },
    {
      "epoch": 640.6569343065694,
      "grad_norm": 0.078765369951725,
      "learning_rate": 1.7967153284671535e-05,
      "loss": 0.7938,
      "step": 87770
    },
    {
      "epoch": 640.7299270072992,
      "grad_norm": 8.936482429504395,
      "learning_rate": 1.796350364963504e-05,
      "loss": 0.847,
      "step": 87780
    },
    {
      "epoch": 640.8029197080292,
      "grad_norm": 2.6887104511260986,
      "learning_rate": 1.795985401459854e-05,
      "loss": 0.9446,
      "step": 87790
    },
    {
      "epoch": 640.8759124087592,
      "grad_norm": 0.05782966688275337,
      "learning_rate": 1.7956204379562044e-05,
      "loss": 0.5511,
      "step": 87800
    },
    {
      "epoch": 640.948905109489,
      "grad_norm": 8.267616271972656,
      "learning_rate": 1.7952554744525548e-05,
      "loss": 0.8737,
      "step": 87810
    },
    {
      "epoch": 641.021897810219,
      "grad_norm": 0.0999351516366005,
      "learning_rate": 1.7948905109489055e-05,
      "loss": 0.9594,
      "step": 87820
    },
    {
      "epoch": 641.0948905109489,
      "grad_norm": 14.442914009094238,
      "learning_rate": 1.7945255474452555e-05,
      "loss": 1.1763,
      "step": 87830
    },
    {
      "epoch": 641.1678832116788,
      "grad_norm": 13.49163818359375,
      "learning_rate": 1.794160583941606e-05,
      "loss": 0.9327,
      "step": 87840
    },
    {
      "epoch": 641.2408759124088,
      "grad_norm": 14.784405708312988,
      "learning_rate": 1.7937956204379563e-05,
      "loss": 0.6142,
      "step": 87850
    },
    {
      "epoch": 641.3138686131387,
      "grad_norm": 7.501115322113037,
      "learning_rate": 1.7934306569343067e-05,
      "loss": 0.8138,
      "step": 87860
    },
    {
      "epoch": 641.3868613138686,
      "grad_norm": 12.381382942199707,
      "learning_rate": 1.793065693430657e-05,
      "loss": 1.01,
      "step": 87870
    },
    {
      "epoch": 641.4598540145986,
      "grad_norm": 9.955917358398438,
      "learning_rate": 1.7927007299270075e-05,
      "loss": 1.0897,
      "step": 87880
    },
    {
      "epoch": 641.5328467153284,
      "grad_norm": 11.546016693115234,
      "learning_rate": 1.792335766423358e-05,
      "loss": 0.8701,
      "step": 87890
    },
    {
      "epoch": 641.6058394160584,
      "grad_norm": 17.347597122192383,
      "learning_rate": 1.791970802919708e-05,
      "loss": 0.5955,
      "step": 87900
    },
    {
      "epoch": 641.6788321167883,
      "grad_norm": 18.05974769592285,
      "learning_rate": 1.7916058394160583e-05,
      "loss": 0.8936,
      "step": 87910
    },
    {
      "epoch": 641.7518248175182,
      "grad_norm": 12.940839767456055,
      "learning_rate": 1.791240875912409e-05,
      "loss": 0.5406,
      "step": 87920
    },
    {
      "epoch": 641.8248175182482,
      "grad_norm": 12.609291076660156,
      "learning_rate": 1.7908759124087594e-05,
      "loss": 1.1307,
      "step": 87930
    },
    {
      "epoch": 641.8978102189781,
      "grad_norm": 9.0943021774292,
      "learning_rate": 1.7905109489051095e-05,
      "loss": 1.0662,
      "step": 87940
    },
    {
      "epoch": 641.970802919708,
      "grad_norm": 9.735673904418945,
      "learning_rate": 1.79014598540146e-05,
      "loss": 1.0537,
      "step": 87950
    },
    {
      "epoch": 642.043795620438,
      "grad_norm": 3.5282204151153564,
      "learning_rate": 1.7897810218978102e-05,
      "loss": 0.6418,
      "step": 87960
    },
    {
      "epoch": 642.1167883211679,
      "grad_norm": 12.266043663024902,
      "learning_rate": 1.7894160583941606e-05,
      "loss": 1.133,
      "step": 87970
    },
    {
      "epoch": 642.1897810218978,
      "grad_norm": 15.273798942565918,
      "learning_rate": 1.789051094890511e-05,
      "loss": 0.6396,
      "step": 87980
    },
    {
      "epoch": 642.2627737226277,
      "grad_norm": 14.994685173034668,
      "learning_rate": 1.7886861313868614e-05,
      "loss": 0.6118,
      "step": 87990
    },
    {
      "epoch": 642.3357664233577,
      "grad_norm": 7.567470073699951,
      "learning_rate": 1.7883211678832118e-05,
      "loss": 0.932,
      "step": 88000
    },
    {
      "epoch": 642.4087591240876,
      "grad_norm": 16.72098159790039,
      "learning_rate": 1.7879562043795622e-05,
      "loss": 0.9804,
      "step": 88010
    },
    {
      "epoch": 642.4817518248175,
      "grad_norm": 9.957494735717773,
      "learning_rate": 1.7875912408759122e-05,
      "loss": 0.8564,
      "step": 88020
    },
    {
      "epoch": 642.5547445255474,
      "grad_norm": 5.46371603012085,
      "learning_rate": 1.787226277372263e-05,
      "loss": 1.2581,
      "step": 88030
    },
    {
      "epoch": 642.6277372262774,
      "grad_norm": 10.755093574523926,
      "learning_rate": 1.7868613138686133e-05,
      "loss": 0.6894,
      "step": 88040
    },
    {
      "epoch": 642.7007299270073,
      "grad_norm": 19.154277801513672,
      "learning_rate": 1.7864963503649637e-05,
      "loss": 0.9847,
      "step": 88050
    },
    {
      "epoch": 642.7737226277372,
      "grad_norm": 12.642830848693848,
      "learning_rate": 1.7861313868613138e-05,
      "loss": 1.0862,
      "step": 88060
    },
    {
      "epoch": 642.8467153284671,
      "grad_norm": 13.268725395202637,
      "learning_rate": 1.7857664233576642e-05,
      "loss": 0.5798,
      "step": 88070
    },
    {
      "epoch": 642.9197080291971,
      "grad_norm": 0.05338728800415993,
      "learning_rate": 1.785401459854015e-05,
      "loss": 1.1767,
      "step": 88080
    },
    {
      "epoch": 642.992700729927,
      "grad_norm": 12.750893592834473,
      "learning_rate": 1.7850364963503653e-05,
      "loss": 0.9942,
      "step": 88090
    },
    {
      "epoch": 643.0656934306569,
      "grad_norm": 9.775466918945312,
      "learning_rate": 1.7846715328467153e-05,
      "loss": 0.8097,
      "step": 88100
    },
    {
      "epoch": 643.1386861313869,
      "grad_norm": 16.003711700439453,
      "learning_rate": 1.7843065693430657e-05,
      "loss": 0.7329,
      "step": 88110
    },
    {
      "epoch": 643.2116788321168,
      "grad_norm": 12.46468448638916,
      "learning_rate": 1.783941605839416e-05,
      "loss": 1.031,
      "step": 88120
    },
    {
      "epoch": 643.2846715328467,
      "grad_norm": 16.376754760742188,
      "learning_rate": 1.7835766423357665e-05,
      "loss": 0.894,
      "step": 88130
    },
    {
      "epoch": 643.3576642335767,
      "grad_norm": 7.903023719787598,
      "learning_rate": 1.783211678832117e-05,
      "loss": 1.1634,
      "step": 88140
    },
    {
      "epoch": 643.4306569343066,
      "grad_norm": 9.215669631958008,
      "learning_rate": 1.7828467153284673e-05,
      "loss": 1.0837,
      "step": 88150
    },
    {
      "epoch": 643.5036496350365,
      "grad_norm": 0.11880709230899811,
      "learning_rate": 1.7824817518248177e-05,
      "loss": 0.7781,
      "step": 88160
    },
    {
      "epoch": 643.5766423357665,
      "grad_norm": 6.483372688293457,
      "learning_rate": 1.7821167883211677e-05,
      "loss": 0.6688,
      "step": 88170
    },
    {
      "epoch": 643.6496350364963,
      "grad_norm": 13.63872241973877,
      "learning_rate": 1.7817518248175184e-05,
      "loss": 0.9162,
      "step": 88180
    },
    {
      "epoch": 643.7226277372263,
      "grad_norm": 13.244422912597656,
      "learning_rate": 1.7813868613138688e-05,
      "loss": 0.8374,
      "step": 88190
    },
    {
      "epoch": 643.7956204379562,
      "grad_norm": 10.75708293914795,
      "learning_rate": 1.7810218978102192e-05,
      "loss": 0.7462,
      "step": 88200
    },
    {
      "epoch": 643.8686131386861,
      "grad_norm": 0.03215259313583374,
      "learning_rate": 1.7806569343065693e-05,
      "loss": 0.8756,
      "step": 88210
    },
    {
      "epoch": 643.9416058394161,
      "grad_norm": 9.77932071685791,
      "learning_rate": 1.7802919708029197e-05,
      "loss": 0.8607,
      "step": 88220
    },
    {
      "epoch": 644.014598540146,
      "grad_norm": 13.7146577835083,
      "learning_rate": 1.77992700729927e-05,
      "loss": 0.8136,
      "step": 88230
    },
    {
      "epoch": 644.0875912408759,
      "grad_norm": 3.3221547603607178,
      "learning_rate": 1.7795620437956208e-05,
      "loss": 0.6179,
      "step": 88240
    },
    {
      "epoch": 644.1605839416059,
      "grad_norm": 11.52973461151123,
      "learning_rate": 1.7791970802919708e-05,
      "loss": 0.996,
      "step": 88250
    },
    {
      "epoch": 644.2335766423357,
      "grad_norm": 10.399599075317383,
      "learning_rate": 1.7788321167883212e-05,
      "loss": 1.0056,
      "step": 88260
    },
    {
      "epoch": 644.3065693430657,
      "grad_norm": 8.134688377380371,
      "learning_rate": 1.7784671532846716e-05,
      "loss": 0.8237,
      "step": 88270
    },
    {
      "epoch": 644.3795620437957,
      "grad_norm": 0.06260804086923599,
      "learning_rate": 1.778102189781022e-05,
      "loss": 0.8862,
      "step": 88280
    },
    {
      "epoch": 644.4525547445255,
      "grad_norm": 0.04466685280203819,
      "learning_rate": 1.7777372262773724e-05,
      "loss": 0.9092,
      "step": 88290
    },
    {
      "epoch": 644.5255474452555,
      "grad_norm": 20.69248390197754,
      "learning_rate": 1.7773722627737228e-05,
      "loss": 0.9662,
      "step": 88300
    },
    {
      "epoch": 644.5985401459855,
      "grad_norm": 5.50361967086792,
      "learning_rate": 1.777007299270073e-05,
      "loss": 0.9439,
      "step": 88310
    },
    {
      "epoch": 644.6715328467153,
      "grad_norm": 0.05926695466041565,
      "learning_rate": 1.7766423357664232e-05,
      "loss": 0.3164,
      "step": 88320
    },
    {
      "epoch": 644.7445255474453,
      "grad_norm": 18.83359718322754,
      "learning_rate": 1.7762773722627736e-05,
      "loss": 1.3069,
      "step": 88330
    },
    {
      "epoch": 644.8175182481751,
      "grad_norm": 6.636773586273193,
      "learning_rate": 1.7759124087591243e-05,
      "loss": 0.7422,
      "step": 88340
    },
    {
      "epoch": 644.8905109489051,
      "grad_norm": 10.644482612609863,
      "learning_rate": 1.7755474452554747e-05,
      "loss": 1.3844,
      "step": 88350
    },
    {
      "epoch": 644.9635036496351,
      "grad_norm": 9.855387687683105,
      "learning_rate": 1.7751824817518248e-05,
      "loss": 0.9837,
      "step": 88360
    },
    {
      "epoch": 645.0364963503649,
      "grad_norm": 12.397964477539062,
      "learning_rate": 1.774817518248175e-05,
      "loss": 0.7499,
      "step": 88370
    },
    {
      "epoch": 645.1094890510949,
      "grad_norm": 0.054931383579969406,
      "learning_rate": 1.7744525547445255e-05,
      "loss": 0.8609,
      "step": 88380
    },
    {
      "epoch": 645.1824817518249,
      "grad_norm": 5.861306190490723,
      "learning_rate": 1.7740875912408763e-05,
      "loss": 0.8615,
      "step": 88390
    },
    {
      "epoch": 645.2554744525547,
      "grad_norm": 5.239468097686768,
      "learning_rate": 1.7737226277372263e-05,
      "loss": 1.0079,
      "step": 88400
    },
    {
      "epoch": 645.3284671532847,
      "grad_norm": 7.108367443084717,
      "learning_rate": 1.7733576642335767e-05,
      "loss": 0.7022,
      "step": 88410
    },
    {
      "epoch": 645.4014598540145,
      "grad_norm": 4.126201152801514,
      "learning_rate": 1.772992700729927e-05,
      "loss": 0.5588,
      "step": 88420
    },
    {
      "epoch": 645.4744525547445,
      "grad_norm": 16.65962028503418,
      "learning_rate": 1.7726277372262775e-05,
      "loss": 1.2549,
      "step": 88430
    },
    {
      "epoch": 645.5474452554745,
      "grad_norm": 14.696965217590332,
      "learning_rate": 1.772262773722628e-05,
      "loss": 0.8312,
      "step": 88440
    },
    {
      "epoch": 645.6204379562043,
      "grad_norm": 5.802868366241455,
      "learning_rate": 1.7718978102189782e-05,
      "loss": 0.7467,
      "step": 88450
    },
    {
      "epoch": 645.6934306569343,
      "grad_norm": 14.503445625305176,
      "learning_rate": 1.7715328467153286e-05,
      "loss": 0.9213,
      "step": 88460
    },
    {
      "epoch": 645.7664233576643,
      "grad_norm": 12.769286155700684,
      "learning_rate": 1.771167883211679e-05,
      "loss": 0.8728,
      "step": 88470
    },
    {
      "epoch": 645.8394160583941,
      "grad_norm": 0.024221979081630707,
      "learning_rate": 1.770802919708029e-05,
      "loss": 0.6133,
      "step": 88480
    },
    {
      "epoch": 645.9124087591241,
      "grad_norm": 15.087472915649414,
      "learning_rate": 1.7704379562043798e-05,
      "loss": 1.1619,
      "step": 88490
    },
    {
      "epoch": 645.985401459854,
      "grad_norm": 12.238081932067871,
      "learning_rate": 1.7700729927007302e-05,
      "loss": 0.9021,
      "step": 88500
    },
    {
      "epoch": 646.0583941605839,
      "grad_norm": 14.337136268615723,
      "learning_rate": 1.7697080291970806e-05,
      "loss": 1.4965,
      "step": 88510
    },
    {
      "epoch": 646.1313868613139,
      "grad_norm": 7.360200881958008,
      "learning_rate": 1.7693430656934306e-05,
      "loss": 0.897,
      "step": 88520
    },
    {
      "epoch": 646.2043795620438,
      "grad_norm": 7.823410511016846,
      "learning_rate": 1.768978102189781e-05,
      "loss": 0.899,
      "step": 88530
    },
    {
      "epoch": 646.2773722627737,
      "grad_norm": 0.025441132485866547,
      "learning_rate": 1.7686131386861314e-05,
      "loss": 0.9215,
      "step": 88540
    },
    {
      "epoch": 646.3503649635037,
      "grad_norm": 13.46245288848877,
      "learning_rate": 1.7682481751824818e-05,
      "loss": 1.18,
      "step": 88550
    },
    {
      "epoch": 646.4233576642335,
      "grad_norm": 8.005000114440918,
      "learning_rate": 1.7678832116788322e-05,
      "loss": 0.8329,
      "step": 88560
    },
    {
      "epoch": 646.4963503649635,
      "grad_norm": 7.633083343505859,
      "learning_rate": 1.7675182481751826e-05,
      "loss": 1.0104,
      "step": 88570
    },
    {
      "epoch": 646.5693430656934,
      "grad_norm": 12.46214771270752,
      "learning_rate": 1.767153284671533e-05,
      "loss": 0.9435,
      "step": 88580
    },
    {
      "epoch": 646.6423357664233,
      "grad_norm": 14.810768127441406,
      "learning_rate": 1.7667883211678833e-05,
      "loss": 0.5755,
      "step": 88590
    },
    {
      "epoch": 646.7153284671533,
      "grad_norm": 19.086828231811523,
      "learning_rate": 1.7664233576642337e-05,
      "loss": 1.0047,
      "step": 88600
    },
    {
      "epoch": 646.7883211678832,
      "grad_norm": 7.30517053604126,
      "learning_rate": 1.766058394160584e-05,
      "loss": 0.7568,
      "step": 88610
    },
    {
      "epoch": 646.8613138686131,
      "grad_norm": 0.01920117251574993,
      "learning_rate": 1.7656934306569345e-05,
      "loss": 0.8782,
      "step": 88620
    },
    {
      "epoch": 646.9343065693431,
      "grad_norm": 6.731565475463867,
      "learning_rate": 1.7653284671532846e-05,
      "loss": 0.7753,
      "step": 88630
    },
    {
      "epoch": 647.007299270073,
      "grad_norm": 10.981891632080078,
      "learning_rate": 1.764963503649635e-05,
      "loss": 0.7799,
      "step": 88640
    },
    {
      "epoch": 647.0802919708029,
      "grad_norm": 5.395657062530518,
      "learning_rate": 1.7645985401459857e-05,
      "loss": 0.5524,
      "step": 88650
    },
    {
      "epoch": 647.1532846715329,
      "grad_norm": 12.306631088256836,
      "learning_rate": 1.764233576642336e-05,
      "loss": 1.0902,
      "step": 88660
    },
    {
      "epoch": 647.2262773722628,
      "grad_norm": 11.093417167663574,
      "learning_rate": 1.763868613138686e-05,
      "loss": 0.4577,
      "step": 88670
    },
    {
      "epoch": 647.2992700729927,
      "grad_norm": 6.345633029937744,
      "learning_rate": 1.7635036496350365e-05,
      "loss": 0.8567,
      "step": 88680
    },
    {
      "epoch": 647.3722627737226,
      "grad_norm": 8.196207046508789,
      "learning_rate": 1.763138686131387e-05,
      "loss": 0.7665,
      "step": 88690
    },
    {
      "epoch": 647.4452554744526,
      "grad_norm": 10.46951675415039,
      "learning_rate": 1.7627737226277373e-05,
      "loss": 0.931,
      "step": 88700
    },
    {
      "epoch": 647.5182481751825,
      "grad_norm": 17.831893920898438,
      "learning_rate": 1.7624087591240877e-05,
      "loss": 1.009,
      "step": 88710
    },
    {
      "epoch": 647.5912408759124,
      "grad_norm": 12.857084274291992,
      "learning_rate": 1.762043795620438e-05,
      "loss": 0.9596,
      "step": 88720
    },
    {
      "epoch": 647.6642335766423,
      "grad_norm": 11.379404067993164,
      "learning_rate": 1.7616788321167884e-05,
      "loss": 0.8536,
      "step": 88730
    },
    {
      "epoch": 647.7372262773723,
      "grad_norm": 8.421463966369629,
      "learning_rate": 1.7613138686131388e-05,
      "loss": 0.9801,
      "step": 88740
    },
    {
      "epoch": 647.8102189781022,
      "grad_norm": 12.964488983154297,
      "learning_rate": 1.7609489051094892e-05,
      "loss": 1.2789,
      "step": 88750
    },
    {
      "epoch": 647.8832116788321,
      "grad_norm": 0.16684724390506744,
      "learning_rate": 1.7605839416058396e-05,
      "loss": 0.627,
      "step": 88760
    },
    {
      "epoch": 647.956204379562,
      "grad_norm": 14.203665733337402,
      "learning_rate": 1.76021897810219e-05,
      "loss": 1.3048,
      "step": 88770
    },
    {
      "epoch": 648.029197080292,
      "grad_norm": 1.6216588020324707,
      "learning_rate": 1.75985401459854e-05,
      "loss": 0.9021,
      "step": 88780
    },
    {
      "epoch": 648.1021897810219,
      "grad_norm": 13.373047828674316,
      "learning_rate": 1.7594890510948904e-05,
      "loss": 1.1284,
      "step": 88790
    },
    {
      "epoch": 648.1751824817518,
      "grad_norm": 0.04190634563565254,
      "learning_rate": 1.7591240875912408e-05,
      "loss": 0.7968,
      "step": 88800
    },
    {
      "epoch": 648.2481751824818,
      "grad_norm": 0.03373559191823006,
      "learning_rate": 1.7587591240875915e-05,
      "loss": 0.8432,
      "step": 88810
    },
    {
      "epoch": 648.3211678832117,
      "grad_norm": 5.350090503692627,
      "learning_rate": 1.7583941605839416e-05,
      "loss": 0.8604,
      "step": 88820
    },
    {
      "epoch": 648.3941605839416,
      "grad_norm": 0.025526484474539757,
      "learning_rate": 1.758029197080292e-05,
      "loss": 0.8133,
      "step": 88830
    },
    {
      "epoch": 648.4671532846716,
      "grad_norm": 12.476394653320312,
      "learning_rate": 1.7576642335766424e-05,
      "loss": 0.9009,
      "step": 88840
    },
    {
      "epoch": 648.5401459854014,
      "grad_norm": 10.45833969116211,
      "learning_rate": 1.7572992700729928e-05,
      "loss": 0.9769,
      "step": 88850
    },
    {
      "epoch": 648.6131386861314,
      "grad_norm": 9.720783233642578,
      "learning_rate": 1.756934306569343e-05,
      "loss": 0.7234,
      "step": 88860
    },
    {
      "epoch": 648.6861313868613,
      "grad_norm": 10.713435173034668,
      "learning_rate": 1.7565693430656935e-05,
      "loss": 1.0543,
      "step": 88870
    },
    {
      "epoch": 648.7591240875912,
      "grad_norm": 8.987211227416992,
      "learning_rate": 1.756204379562044e-05,
      "loss": 0.8125,
      "step": 88880
    },
    {
      "epoch": 648.8321167883212,
      "grad_norm": 0.010657020844519138,
      "learning_rate": 1.7558394160583943e-05,
      "loss": 1.0843,
      "step": 88890
    },
    {
      "epoch": 648.9051094890511,
      "grad_norm": 11.515361785888672,
      "learning_rate": 1.7554744525547444e-05,
      "loss": 0.8557,
      "step": 88900
    },
    {
      "epoch": 648.978102189781,
      "grad_norm": 8.921037673950195,
      "learning_rate": 1.755109489051095e-05,
      "loss": 1.009,
      "step": 88910
    },
    {
      "epoch": 649.051094890511,
      "grad_norm": 13.858672142028809,
      "learning_rate": 1.7547445255474455e-05,
      "loss": 0.9808,
      "step": 88920
    },
    {
      "epoch": 649.1240875912408,
      "grad_norm": 12.892783164978027,
      "learning_rate": 1.754379562043796e-05,
      "loss": 1.0524,
      "step": 88930
    },
    {
      "epoch": 649.1970802919708,
      "grad_norm": 9.542093276977539,
      "learning_rate": 1.754014598540146e-05,
      "loss": 0.4548,
      "step": 88940
    },
    {
      "epoch": 649.2700729927008,
      "grad_norm": 6.905013084411621,
      "learning_rate": 1.7536496350364963e-05,
      "loss": 0.984,
      "step": 88950
    },
    {
      "epoch": 649.3430656934306,
      "grad_norm": 0.07827704399824142,
      "learning_rate": 1.753284671532847e-05,
      "loss": 0.9298,
      "step": 88960
    },
    {
      "epoch": 649.4160583941606,
      "grad_norm": 17.940139770507812,
      "learning_rate": 1.752919708029197e-05,
      "loss": 1.0859,
      "step": 88970
    },
    {
      "epoch": 649.4890510948906,
      "grad_norm": 7.866001605987549,
      "learning_rate": 1.7525547445255475e-05,
      "loss": 0.697,
      "step": 88980
    },
    {
      "epoch": 649.5620437956204,
      "grad_norm": 0.06033617630600929,
      "learning_rate": 1.752189781021898e-05,
      "loss": 0.6839,
      "step": 88990
    },
    {
      "epoch": 649.6350364963504,
      "grad_norm": 0.01574011705815792,
      "learning_rate": 1.7518248175182482e-05,
      "loss": 0.7762,
      "step": 89000
    },
    {
      "epoch": 649.7080291970802,
      "grad_norm": 6.571910858154297,
      "learning_rate": 1.7514598540145986e-05,
      "loss": 0.9116,
      "step": 89010
    },
    {
      "epoch": 649.7810218978102,
      "grad_norm": 14.125107765197754,
      "learning_rate": 1.751094890510949e-05,
      "loss": 0.909,
      "step": 89020
    },
    {
      "epoch": 649.8540145985402,
      "grad_norm": 10.06829833984375,
      "learning_rate": 1.7507299270072994e-05,
      "loss": 0.9233,
      "step": 89030
    },
    {
      "epoch": 649.92700729927,
      "grad_norm": 13.422560691833496,
      "learning_rate": 1.7503649635036498e-05,
      "loss": 1.1382,
      "step": 89040
    },
    {
      "epoch": 650.0,
      "grad_norm": 0.04781051725149155,
      "learning_rate": 1.75e-05,
      "loss": 1.0057,
      "step": 89050
    },
    {
      "epoch": 650.07299270073,
      "grad_norm": 10.628355026245117,
      "learning_rate": 1.7496350364963506e-05,
      "loss": 0.7874,
      "step": 89060
    },
    {
      "epoch": 650.1459854014598,
      "grad_norm": 11.416768074035645,
      "learning_rate": 1.749270072992701e-05,
      "loss": 0.8039,
      "step": 89070
    },
    {
      "epoch": 650.2189781021898,
      "grad_norm": 19.149070739746094,
      "learning_rate": 1.7489051094890514e-05,
      "loss": 0.9265,
      "step": 89080
    },
    {
      "epoch": 650.2919708029198,
      "grad_norm": 11.237371444702148,
      "learning_rate": 1.7485401459854014e-05,
      "loss": 0.8156,
      "step": 89090
    },
    {
      "epoch": 650.3649635036496,
      "grad_norm": 18.318286895751953,
      "learning_rate": 1.7481751824817518e-05,
      "loss": 1.0866,
      "step": 89100
    },
    {
      "epoch": 650.4379562043796,
      "grad_norm": 6.6710381507873535,
      "learning_rate": 1.7478102189781022e-05,
      "loss": 0.8794,
      "step": 89110
    },
    {
      "epoch": 650.5109489051094,
      "grad_norm": 24.181081771850586,
      "learning_rate": 1.747445255474453e-05,
      "loss": 1.0315,
      "step": 89120
    },
    {
      "epoch": 650.5839416058394,
      "grad_norm": 0.07395607978105545,
      "learning_rate": 1.747080291970803e-05,
      "loss": 1.0491,
      "step": 89130
    },
    {
      "epoch": 650.6569343065694,
      "grad_norm": 6.736997604370117,
      "learning_rate": 1.7467153284671533e-05,
      "loss": 0.9729,
      "step": 89140
    },
    {
      "epoch": 650.7299270072992,
      "grad_norm": 7.4629597663879395,
      "learning_rate": 1.7463503649635037e-05,
      "loss": 0.7738,
      "step": 89150
    },
    {
      "epoch": 650.8029197080292,
      "grad_norm": 0.0805230662226677,
      "learning_rate": 1.745985401459854e-05,
      "loss": 0.8431,
      "step": 89160
    },
    {
      "epoch": 650.8759124087592,
      "grad_norm": 7.247020721435547,
      "learning_rate": 1.7456204379562045e-05,
      "loss": 0.9865,
      "step": 89170
    },
    {
      "epoch": 650.948905109489,
      "grad_norm": 5.3812432289123535,
      "learning_rate": 1.745255474452555e-05,
      "loss": 0.5606,
      "step": 89180
    },
    {
      "epoch": 651.021897810219,
      "grad_norm": 7.3271355628967285,
      "learning_rate": 1.7448905109489053e-05,
      "loss": 0.9187,
      "step": 89190
    },
    {
      "epoch": 651.0948905109489,
      "grad_norm": 9.160184860229492,
      "learning_rate": 1.7445255474452553e-05,
      "loss": 1.0115,
      "step": 89200
    },
    {
      "epoch": 651.1678832116788,
      "grad_norm": 0.11050066351890564,
      "learning_rate": 1.7441605839416057e-05,
      "loss": 0.6342,
      "step": 89210
    },
    {
      "epoch": 651.2408759124088,
      "grad_norm": 8.297060012817383,
      "learning_rate": 1.7437956204379564e-05,
      "loss": 0.6414,
      "step": 89220
    },
    {
      "epoch": 651.3138686131387,
      "grad_norm": 0.1256849765777588,
      "learning_rate": 1.743430656934307e-05,
      "loss": 0.9703,
      "step": 89230
    },
    {
      "epoch": 651.3868613138686,
      "grad_norm": 7.143478870391846,
      "learning_rate": 1.743065693430657e-05,
      "loss": 1.0482,
      "step": 89240
    },
    {
      "epoch": 651.4598540145986,
      "grad_norm": 8.864678382873535,
      "learning_rate": 1.7427007299270073e-05,
      "loss": 1.0775,
      "step": 89250
    },
    {
      "epoch": 651.5328467153284,
      "grad_norm": 10.40258502960205,
      "learning_rate": 1.7423357664233577e-05,
      "loss": 1.151,
      "step": 89260
    },
    {
      "epoch": 651.6058394160584,
      "grad_norm": 0.055881351232528687,
      "learning_rate": 1.741970802919708e-05,
      "loss": 0.9888,
      "step": 89270
    },
    {
      "epoch": 651.6788321167883,
      "grad_norm": 8.943912506103516,
      "learning_rate": 1.7416058394160584e-05,
      "loss": 0.6535,
      "step": 89280
    },
    {
      "epoch": 651.7518248175182,
      "grad_norm": 8.204163551330566,
      "learning_rate": 1.7412408759124088e-05,
      "loss": 0.8783,
      "step": 89290
    },
    {
      "epoch": 651.8248175182482,
      "grad_norm": 18.223711013793945,
      "learning_rate": 1.7408759124087592e-05,
      "loss": 0.6563,
      "step": 89300
    },
    {
      "epoch": 651.8978102189781,
      "grad_norm": 8.039976119995117,
      "learning_rate": 1.7405109489051096e-05,
      "loss": 0.8825,
      "step": 89310
    },
    {
      "epoch": 651.970802919708,
      "grad_norm": 14.669015884399414,
      "learning_rate": 1.74014598540146e-05,
      "loss": 1.1552,
      "step": 89320
    },
    {
      "epoch": 652.043795620438,
      "grad_norm": 11.168513298034668,
      "learning_rate": 1.7397810218978104e-05,
      "loss": 0.7236,
      "step": 89330
    },
    {
      "epoch": 652.1167883211679,
      "grad_norm": 9.84425163269043,
      "learning_rate": 1.7394160583941608e-05,
      "loss": 1.1732,
      "step": 89340
    },
    {
      "epoch": 652.1897810218978,
      "grad_norm": 11.875142097473145,
      "learning_rate": 1.739051094890511e-05,
      "loss": 1.1165,
      "step": 89350
    },
    {
      "epoch": 652.2627737226277,
      "grad_norm": 11.216362953186035,
      "learning_rate": 1.7386861313868612e-05,
      "loss": 1.1217,
      "step": 89360
    },
    {
      "epoch": 652.3357664233577,
      "grad_norm": 10.296255111694336,
      "learning_rate": 1.7383211678832116e-05,
      "loss": 0.6434,
      "step": 89370
    },
    {
      "epoch": 652.4087591240876,
      "grad_norm": 12.573944091796875,
      "learning_rate": 1.7379562043795623e-05,
      "loss": 0.8095,
      "step": 89380
    },
    {
      "epoch": 652.4817518248175,
      "grad_norm": 12.441088676452637,
      "learning_rate": 1.7375912408759127e-05,
      "loss": 1.013,
      "step": 89390
    },
    {
      "epoch": 652.5547445255474,
      "grad_norm": 8.80978012084961,
      "learning_rate": 1.7372262773722628e-05,
      "loss": 1.0822,
      "step": 89400
    },
    {
      "epoch": 652.6277372262774,
      "grad_norm": 0.1013873964548111,
      "learning_rate": 1.736861313868613e-05,
      "loss": 1.0914,
      "step": 89410
    },
    {
      "epoch": 652.7007299270073,
      "grad_norm": 0.0725679099559784,
      "learning_rate": 1.7364963503649635e-05,
      "loss": 0.573,
      "step": 89420
    },
    {
      "epoch": 652.7737226277372,
      "grad_norm": 0.07654368877410889,
      "learning_rate": 1.736131386861314e-05,
      "loss": 0.7522,
      "step": 89430
    },
    {
      "epoch": 652.8467153284671,
      "grad_norm": 7.392014026641846,
      "learning_rate": 1.7357664233576643e-05,
      "loss": 0.9742,
      "step": 89440
    },
    {
      "epoch": 652.9197080291971,
      "grad_norm": 0.7167149186134338,
      "learning_rate": 1.7354014598540147e-05,
      "loss": 0.5127,
      "step": 89450
    },
    {
      "epoch": 652.992700729927,
      "grad_norm": 9.711292266845703,
      "learning_rate": 1.735036496350365e-05,
      "loss": 1.0732,
      "step": 89460
    },
    {
      "epoch": 653.0656934306569,
      "grad_norm": 19.083009719848633,
      "learning_rate": 1.734671532846715e-05,
      "loss": 0.644,
      "step": 89470
    },
    {
      "epoch": 653.1386861313869,
      "grad_norm": 10.890183448791504,
      "learning_rate": 1.734306569343066e-05,
      "loss": 0.5997,
      "step": 89480
    },
    {
      "epoch": 653.2116788321168,
      "grad_norm": 17.97273826599121,
      "learning_rate": 1.7339416058394163e-05,
      "loss": 0.8585,
      "step": 89490
    },
    {
      "epoch": 653.2846715328467,
      "grad_norm": 14.424062728881836,
      "learning_rate": 1.7335766423357666e-05,
      "loss": 1.3075,
      "step": 89500
    },
    {
      "epoch": 653.3576642335767,
      "grad_norm": 6.102091312408447,
      "learning_rate": 1.7332116788321167e-05,
      "loss": 0.8996,
      "step": 89510
    },
    {
      "epoch": 653.4306569343066,
      "grad_norm": 11.475118637084961,
      "learning_rate": 1.732846715328467e-05,
      "loss": 1.3825,
      "step": 89520
    },
    {
      "epoch": 653.5036496350365,
      "grad_norm": 10.49445915222168,
      "learning_rate": 1.7324817518248178e-05,
      "loss": 0.8927,
      "step": 89530
    },
    {
      "epoch": 653.5766423357665,
      "grad_norm": 4.638192176818848,
      "learning_rate": 1.7321167883211682e-05,
      "loss": 0.6104,
      "step": 89540
    },
    {
      "epoch": 653.6496350364963,
      "grad_norm": 11.171601295471191,
      "learning_rate": 1.7317518248175182e-05,
      "loss": 1.0559,
      "step": 89550
    },
    {
      "epoch": 653.7226277372263,
      "grad_norm": 0.4805086851119995,
      "learning_rate": 1.7313868613138686e-05,
      "loss": 0.7562,
      "step": 89560
    },
    {
      "epoch": 653.7956204379562,
      "grad_norm": 6.906876564025879,
      "learning_rate": 1.731021897810219e-05,
      "loss": 0.9792,
      "step": 89570
    },
    {
      "epoch": 653.8686131386861,
      "grad_norm": 11.961350440979004,
      "learning_rate": 1.7306569343065694e-05,
      "loss": 0.9987,
      "step": 89580
    },
    {
      "epoch": 653.9416058394161,
      "grad_norm": 7.024589538574219,
      "learning_rate": 1.7302919708029198e-05,
      "loss": 0.8714,
      "step": 89590
    },
    {
      "epoch": 654.014598540146,
      "grad_norm": 14.890690803527832,
      "learning_rate": 1.7299270072992702e-05,
      "loss": 0.7243,
      "step": 89600
    },
    {
      "epoch": 654.0875912408759,
      "grad_norm": 3.9767258167266846,
      "learning_rate": 1.7295620437956206e-05,
      "loss": 0.845,
      "step": 89610
    },
    {
      "epoch": 654.1605839416059,
      "grad_norm": 8.292313575744629,
      "learning_rate": 1.7291970802919706e-05,
      "loss": 0.883,
      "step": 89620
    },
    {
      "epoch": 654.2335766423357,
      "grad_norm": 13.556408882141113,
      "learning_rate": 1.7288321167883213e-05,
      "loss": 1.1345,
      "step": 89630
    },
    {
      "epoch": 654.3065693430657,
      "grad_norm": 7.378393173217773,
      "learning_rate": 1.7284671532846717e-05,
      "loss": 0.7741,
      "step": 89640
    },
    {
      "epoch": 654.3795620437957,
      "grad_norm": 10.424275398254395,
      "learning_rate": 1.728102189781022e-05,
      "loss": 1.0064,
      "step": 89650
    },
    {
      "epoch": 654.4525547445255,
      "grad_norm": 8.103028297424316,
      "learning_rate": 1.7277372262773722e-05,
      "loss": 0.5902,
      "step": 89660
    },
    {
      "epoch": 654.5255474452555,
      "grad_norm": 6.742340564727783,
      "learning_rate": 1.7273722627737226e-05,
      "loss": 0.8006,
      "step": 89670
    },
    {
      "epoch": 654.5985401459855,
      "grad_norm": 10.33905029296875,
      "learning_rate": 1.727007299270073e-05,
      "loss": 0.7349,
      "step": 89680
    },
    {
      "epoch": 654.6715328467153,
      "grad_norm": 7.113428115844727,
      "learning_rate": 1.7266423357664237e-05,
      "loss": 0.9295,
      "step": 89690
    },
    {
      "epoch": 654.7445255474453,
      "grad_norm": 0.07431899756193161,
      "learning_rate": 1.7262773722627737e-05,
      "loss": 0.95,
      "step": 89700
    },
    {
      "epoch": 654.8175182481751,
      "grad_norm": 9.693024635314941,
      "learning_rate": 1.725912408759124e-05,
      "loss": 0.7997,
      "step": 89710
    },
    {
      "epoch": 654.8905109489051,
      "grad_norm": 14.093814849853516,
      "learning_rate": 1.7255474452554745e-05,
      "loss": 0.9681,
      "step": 89720
    },
    {
      "epoch": 654.9635036496351,
      "grad_norm": 10.689284324645996,
      "learning_rate": 1.725182481751825e-05,
      "loss": 1.0599,
      "step": 89730
    },
    {
      "epoch": 655.0364963503649,
      "grad_norm": 8.138077735900879,
      "learning_rate": 1.7248175182481753e-05,
      "loss": 1.2866,
      "step": 89740
    },
    {
      "epoch": 655.1094890510949,
      "grad_norm": 7.734520435333252,
      "learning_rate": 1.7244525547445257e-05,
      "loss": 1.4797,
      "step": 89750
    },
    {
      "epoch": 655.1824817518249,
      "grad_norm": 2.5392236709594727,
      "learning_rate": 1.724087591240876e-05,
      "loss": 0.661,
      "step": 89760
    },
    {
      "epoch": 655.2554744525547,
      "grad_norm": 10.471385955810547,
      "learning_rate": 1.7237226277372264e-05,
      "loss": 0.801,
      "step": 89770
    },
    {
      "epoch": 655.3284671532847,
      "grad_norm": 13.946504592895508,
      "learning_rate": 1.7233576642335765e-05,
      "loss": 0.7587,
      "step": 89780
    },
    {
      "epoch": 655.4014598540145,
      "grad_norm": 9.410327911376953,
      "learning_rate": 1.7229927007299272e-05,
      "loss": 0.7991,
      "step": 89790
    },
    {
      "epoch": 655.4744525547445,
      "grad_norm": 5.796424865722656,
      "learning_rate": 1.7226277372262776e-05,
      "loss": 0.664,
      "step": 89800
    },
    {
      "epoch": 655.5474452554745,
      "grad_norm": 7.091987609863281,
      "learning_rate": 1.722262773722628e-05,
      "loss": 0.6871,
      "step": 89810
    },
    {
      "epoch": 655.6204379562043,
      "grad_norm": 9.88491439819336,
      "learning_rate": 1.721897810218978e-05,
      "loss": 1.1463,
      "step": 89820
    },
    {
      "epoch": 655.6934306569343,
      "grad_norm": 12.72098159790039,
      "learning_rate": 1.7215328467153284e-05,
      "loss": 0.9265,
      "step": 89830
    },
    {
      "epoch": 655.7664233576643,
      "grad_norm": 15.388724327087402,
      "learning_rate": 1.7211678832116788e-05,
      "loss": 0.8098,
      "step": 89840
    },
    {
      "epoch": 655.8394160583941,
      "grad_norm": 12.225650787353516,
      "learning_rate": 1.7208029197080292e-05,
      "loss": 0.8935,
      "step": 89850
    },
    {
      "epoch": 655.9124087591241,
      "grad_norm": 17.51695442199707,
      "learning_rate": 1.7204379562043796e-05,
      "loss": 0.7942,
      "step": 89860
    },
    {
      "epoch": 655.985401459854,
      "grad_norm": 7.862678050994873,
      "learning_rate": 1.72007299270073e-05,
      "loss": 0.9157,
      "step": 89870
    },
    {
      "epoch": 656.0583941605839,
      "grad_norm": 6.700862884521484,
      "learning_rate": 1.7197080291970804e-05,
      "loss": 0.7881,
      "step": 89880
    },
    {
      "epoch": 656.1313868613139,
      "grad_norm": 8.531353950500488,
      "learning_rate": 1.7193430656934308e-05,
      "loss": 0.8723,
      "step": 89890
    },
    {
      "epoch": 656.2043795620438,
      "grad_norm": 6.618717670440674,
      "learning_rate": 1.718978102189781e-05,
      "loss": 0.8964,
      "step": 89900
    },
    {
      "epoch": 656.2773722627737,
      "grad_norm": 0.08266988396644592,
      "learning_rate": 1.7186131386861315e-05,
      "loss": 0.5526,
      "step": 89910
    },
    {
      "epoch": 656.3503649635037,
      "grad_norm": 10.684057235717773,
      "learning_rate": 1.718248175182482e-05,
      "loss": 1.0867,
      "step": 89920
    },
    {
      "epoch": 656.4233576642335,
      "grad_norm": 8.32830810546875,
      "learning_rate": 1.717883211678832e-05,
      "loss": 1.1972,
      "step": 89930
    },
    {
      "epoch": 656.4963503649635,
      "grad_norm": 10.061957359313965,
      "learning_rate": 1.7175182481751824e-05,
      "loss": 0.8877,
      "step": 89940
    },
    {
      "epoch": 656.5693430656934,
      "grad_norm": 6.991154670715332,
      "learning_rate": 1.717153284671533e-05,
      "loss": 0.8654,
      "step": 89950
    },
    {
      "epoch": 656.6423357664233,
      "grad_norm": 12.507430076599121,
      "learning_rate": 1.7167883211678835e-05,
      "loss": 0.4171,
      "step": 89960
    },
    {
      "epoch": 656.7153284671533,
      "grad_norm": 0.021147042512893677,
      "learning_rate": 1.7164233576642335e-05,
      "loss": 0.8602,
      "step": 89970
    },
    {
      "epoch": 656.7883211678832,
      "grad_norm": 8.04722785949707,
      "learning_rate": 1.716058394160584e-05,
      "loss": 0.7551,
      "step": 89980
    },
    {
      "epoch": 656.8613138686131,
      "grad_norm": 10.422948837280273,
      "learning_rate": 1.7156934306569343e-05,
      "loss": 1.1324,
      "step": 89990
    },
    {
      "epoch": 656.9343065693431,
      "grad_norm": 5.540090084075928,
      "learning_rate": 1.715328467153285e-05,
      "loss": 1.0283,
      "step": 90000
    },
    {
      "epoch": 657.007299270073,
      "grad_norm": 16.655967712402344,
      "learning_rate": 1.714963503649635e-05,
      "loss": 1.0277,
      "step": 90010
    },
    {
      "epoch": 657.0802919708029,
      "grad_norm": 11.025615692138672,
      "learning_rate": 1.7145985401459855e-05,
      "loss": 0.8448,
      "step": 90020
    },
    {
      "epoch": 657.1532846715329,
      "grad_norm": 0.08271688222885132,
      "learning_rate": 1.714233576642336e-05,
      "loss": 1.0199,
      "step": 90030
    },
    {
      "epoch": 657.2262773722628,
      "grad_norm": 6.697883129119873,
      "learning_rate": 1.7138686131386862e-05,
      "loss": 0.4548,
      "step": 90040
    },
    {
      "epoch": 657.2992700729927,
      "grad_norm": 6.972848892211914,
      "learning_rate": 1.7135036496350366e-05,
      "loss": 0.9928,
      "step": 90050
    },
    {
      "epoch": 657.3722627737226,
      "grad_norm": 13.697732925415039,
      "learning_rate": 1.713138686131387e-05,
      "loss": 0.875,
      "step": 90060
    },
    {
      "epoch": 657.4452554744526,
      "grad_norm": 13.355419158935547,
      "learning_rate": 1.7127737226277374e-05,
      "loss": 1.0354,
      "step": 90070
    },
    {
      "epoch": 657.5182481751825,
      "grad_norm": 9.822293281555176,
      "learning_rate": 1.7124087591240875e-05,
      "loss": 0.9175,
      "step": 90080
    },
    {
      "epoch": 657.5912408759124,
      "grad_norm": 8.675558090209961,
      "learning_rate": 1.712043795620438e-05,
      "loss": 0.755,
      "step": 90090
    },
    {
      "epoch": 657.6642335766423,
      "grad_norm": 6.8407673835754395,
      "learning_rate": 1.7116788321167886e-05,
      "loss": 0.8689,
      "step": 90100
    },
    {
      "epoch": 657.7372262773723,
      "grad_norm": 15.173736572265625,
      "learning_rate": 1.711313868613139e-05,
      "loss": 0.9471,
      "step": 90110
    },
    {
      "epoch": 657.8102189781022,
      "grad_norm": 14.267736434936523,
      "learning_rate": 1.710948905109489e-05,
      "loss": 0.9631,
      "step": 90120
    },
    {
      "epoch": 657.8832116788321,
      "grad_norm": 16.20490264892578,
      "learning_rate": 1.7105839416058394e-05,
      "loss": 1.2355,
      "step": 90130
    },
    {
      "epoch": 657.956204379562,
      "grad_norm": 20.783159255981445,
      "learning_rate": 1.7102189781021898e-05,
      "loss": 0.7022,
      "step": 90140
    },
    {
      "epoch": 658.029197080292,
      "grad_norm": 6.135258197784424,
      "learning_rate": 1.7098540145985402e-05,
      "loss": 0.571,
      "step": 90150
    },
    {
      "epoch": 658.1021897810219,
      "grad_norm": 6.655399322509766,
      "learning_rate": 1.7094890510948906e-05,
      "loss": 1.1267,
      "step": 90160
    },
    {
      "epoch": 658.1751824817518,
      "grad_norm": 7.867719650268555,
      "learning_rate": 1.709124087591241e-05,
      "loss": 1.2694,
      "step": 90170
    },
    {
      "epoch": 658.2481751824818,
      "grad_norm": 17.130840301513672,
      "learning_rate": 1.7087591240875913e-05,
      "loss": 0.9771,
      "step": 90180
    },
    {
      "epoch": 658.3211678832117,
      "grad_norm": 12.732175827026367,
      "learning_rate": 1.7083941605839417e-05,
      "loss": 0.6517,
      "step": 90190
    },
    {
      "epoch": 658.3941605839416,
      "grad_norm": 9.432718276977539,
      "learning_rate": 1.708029197080292e-05,
      "loss": 0.6743,
      "step": 90200
    },
    {
      "epoch": 658.4671532846716,
      "grad_norm": 7.838983535766602,
      "learning_rate": 1.7076642335766425e-05,
      "loss": 0.9455,
      "step": 90210
    },
    {
      "epoch": 658.5401459854014,
      "grad_norm": 6.176283836364746,
      "learning_rate": 1.707299270072993e-05,
      "loss": 1.4738,
      "step": 90220
    },
    {
      "epoch": 658.6131386861314,
      "grad_norm": 0.6952417492866516,
      "learning_rate": 1.7069343065693433e-05,
      "loss": 0.4342,
      "step": 90230
    },
    {
      "epoch": 658.6861313868613,
      "grad_norm": 6.5120978355407715,
      "learning_rate": 1.7065693430656933e-05,
      "loss": 0.8575,
      "step": 90240
    },
    {
      "epoch": 658.7591240875912,
      "grad_norm": 7.422309875488281,
      "learning_rate": 1.7062043795620437e-05,
      "loss": 0.9046,
      "step": 90250
    },
    {
      "epoch": 658.8321167883212,
      "grad_norm": 6.078280448913574,
      "learning_rate": 1.7058394160583945e-05,
      "loss": 0.9495,
      "step": 90260
    },
    {
      "epoch": 658.9051094890511,
      "grad_norm": 11.25663948059082,
      "learning_rate": 1.7054744525547445e-05,
      "loss": 1.0342,
      "step": 90270
    },
    {
      "epoch": 658.978102189781,
      "grad_norm": 0.9208843111991882,
      "learning_rate": 1.705109489051095e-05,
      "loss": 0.677,
      "step": 90280
    },
    {
      "epoch": 659.051094890511,
      "grad_norm": 10.202763557434082,
      "learning_rate": 1.7047445255474453e-05,
      "loss": 1.1421,
      "step": 90290
    },
    {
      "epoch": 659.1240875912408,
      "grad_norm": 12.0501070022583,
      "learning_rate": 1.7043795620437957e-05,
      "loss": 1.0923,
      "step": 90300
    },
    {
      "epoch": 659.1970802919708,
      "grad_norm": 0.07019403576850891,
      "learning_rate": 1.704014598540146e-05,
      "loss": 0.8032,
      "step": 90310
    },
    {
      "epoch": 659.2700729927008,
      "grad_norm": 9.681975364685059,
      "learning_rate": 1.7036496350364964e-05,
      "loss": 1.1994,
      "step": 90320
    },
    {
      "epoch": 659.3430656934306,
      "grad_norm": 0.1068132072687149,
      "learning_rate": 1.703284671532847e-05,
      "loss": 0.5066,
      "step": 90330
    },
    {
      "epoch": 659.4160583941606,
      "grad_norm": 17.21786117553711,
      "learning_rate": 1.7029197080291972e-05,
      "loss": 0.9475,
      "step": 90340
    },
    {
      "epoch": 659.4890510948906,
      "grad_norm": 1.738288164138794,
      "learning_rate": 1.7025547445255473e-05,
      "loss": 0.5955,
      "step": 90350
    },
    {
      "epoch": 659.5620437956204,
      "grad_norm": 18.132366180419922,
      "learning_rate": 1.702189781021898e-05,
      "loss": 1.0063,
      "step": 90360
    },
    {
      "epoch": 659.6350364963504,
      "grad_norm": 0.12250160425901413,
      "learning_rate": 1.7018248175182484e-05,
      "loss": 0.4763,
      "step": 90370
    },
    {
      "epoch": 659.7080291970802,
      "grad_norm": 13.850299835205078,
      "learning_rate": 1.7014598540145988e-05,
      "loss": 0.8448,
      "step": 90380
    },
    {
      "epoch": 659.7810218978102,
      "grad_norm": 10.426547050476074,
      "learning_rate": 1.7010948905109488e-05,
      "loss": 0.8913,
      "step": 90390
    },
    {
      "epoch": 659.8540145985402,
      "grad_norm": 16.182096481323242,
      "learning_rate": 1.7007299270072992e-05,
      "loss": 1.3707,
      "step": 90400
    },
    {
      "epoch": 659.92700729927,
      "grad_norm": 10.346341133117676,
      "learning_rate": 1.7003649635036496e-05,
      "loss": 0.9591,
      "step": 90410
    },
    {
      "epoch": 660.0,
      "grad_norm": 29.35055160522461,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.0326,
      "step": 90420
    },
    {
      "epoch": 660.07299270073,
      "grad_norm": 0.08984673023223877,
      "learning_rate": 1.6996350364963504e-05,
      "loss": 0.6489,
      "step": 90430
    },
    {
      "epoch": 660.1459854014598,
      "grad_norm": 12.854482650756836,
      "learning_rate": 1.6992700729927008e-05,
      "loss": 1.0749,
      "step": 90440
    },
    {
      "epoch": 660.2189781021898,
      "grad_norm": 15.997879028320312,
      "learning_rate": 1.698905109489051e-05,
      "loss": 0.9284,
      "step": 90450
    },
    {
      "epoch": 660.2919708029198,
      "grad_norm": 5.194934368133545,
      "learning_rate": 1.6985401459854015e-05,
      "loss": 0.714,
      "step": 90460
    },
    {
      "epoch": 660.3649635036496,
      "grad_norm": 0.13484527170658112,
      "learning_rate": 1.698175182481752e-05,
      "loss": 1.0485,
      "step": 90470
    },
    {
      "epoch": 660.4379562043796,
      "grad_norm": 7.897339820861816,
      "learning_rate": 1.6978102189781023e-05,
      "loss": 0.9735,
      "step": 90480
    },
    {
      "epoch": 660.5109489051094,
      "grad_norm": 9.655550956726074,
      "learning_rate": 1.6974452554744527e-05,
      "loss": 0.7654,
      "step": 90490
    },
    {
      "epoch": 660.5839416058394,
      "grad_norm": 12.480545043945312,
      "learning_rate": 1.6970802919708028e-05,
      "loss": 0.8139,
      "step": 90500
    },
    {
      "epoch": 660.6569343065694,
      "grad_norm": 14.570218086242676,
      "learning_rate": 1.696715328467153e-05,
      "loss": 0.9334,
      "step": 90510
    },
    {
      "epoch": 660.7299270072992,
      "grad_norm": 0.011151128448545933,
      "learning_rate": 1.696350364963504e-05,
      "loss": 0.4321,
      "step": 90520
    },
    {
      "epoch": 660.8029197080292,
      "grad_norm": 6.885778903961182,
      "learning_rate": 1.6959854014598543e-05,
      "loss": 1.3578,
      "step": 90530
    },
    {
      "epoch": 660.8759124087592,
      "grad_norm": 9.020913124084473,
      "learning_rate": 1.6956204379562043e-05,
      "loss": 1.0246,
      "step": 90540
    },
    {
      "epoch": 660.948905109489,
      "grad_norm": 7.217861652374268,
      "learning_rate": 1.6952554744525547e-05,
      "loss": 0.6803,
      "step": 90550
    },
    {
      "epoch": 661.021897810219,
      "grad_norm": 10.645076751708984,
      "learning_rate": 1.694890510948905e-05,
      "loss": 1.1779,
      "step": 90560
    },
    {
      "epoch": 661.0948905109489,
      "grad_norm": 8.707085609436035,
      "learning_rate": 1.6945255474452558e-05,
      "loss": 0.7102,
      "step": 90570
    },
    {
      "epoch": 661.1678832116788,
      "grad_norm": 15.269675254821777,
      "learning_rate": 1.694160583941606e-05,
      "loss": 1.1664,
      "step": 90580
    },
    {
      "epoch": 661.2408759124088,
      "grad_norm": 24.088623046875,
      "learning_rate": 1.6937956204379562e-05,
      "loss": 0.7947,
      "step": 90590
    },
    {
      "epoch": 661.3138686131387,
      "grad_norm": 6.21400785446167,
      "learning_rate": 1.6934306569343066e-05,
      "loss": 0.8616,
      "step": 90600
    },
    {
      "epoch": 661.3868613138686,
      "grad_norm": 6.255107402801514,
      "learning_rate": 1.693065693430657e-05,
      "loss": 1.137,
      "step": 90610
    },
    {
      "epoch": 661.4598540145986,
      "grad_norm": 0.02327081188559532,
      "learning_rate": 1.6927007299270074e-05,
      "loss": 1.0517,
      "step": 90620
    },
    {
      "epoch": 661.5328467153284,
      "grad_norm": 0.05344719812273979,
      "learning_rate": 1.6923357664233578e-05,
      "loss": 0.4229,
      "step": 90630
    },
    {
      "epoch": 661.6058394160584,
      "grad_norm": 6.978017807006836,
      "learning_rate": 1.6919708029197082e-05,
      "loss": 1.1312,
      "step": 90640
    },
    {
      "epoch": 661.6788321167883,
      "grad_norm": 16.144227981567383,
      "learning_rate": 1.6916058394160586e-05,
      "loss": 0.6119,
      "step": 90650
    },
    {
      "epoch": 661.7518248175182,
      "grad_norm": 4.635118007659912,
      "learning_rate": 1.6912408759124086e-05,
      "loss": 0.7631,
      "step": 90660
    },
    {
      "epoch": 661.8248175182482,
      "grad_norm": 8.878251075744629,
      "learning_rate": 1.6908759124087594e-05,
      "loss": 1.0681,
      "step": 90670
    },
    {
      "epoch": 661.8978102189781,
      "grad_norm": 6.046823501586914,
      "learning_rate": 1.6905109489051097e-05,
      "loss": 0.7729,
      "step": 90680
    },
    {
      "epoch": 661.970802919708,
      "grad_norm": 6.644040107727051,
      "learning_rate": 1.6901459854014598e-05,
      "loss": 1.0117,
      "step": 90690
    },
    {
      "epoch": 662.043795620438,
      "grad_norm": 16.653139114379883,
      "learning_rate": 1.6897810218978102e-05,
      "loss": 0.8281,
      "step": 90700
    },
    {
      "epoch": 662.1167883211679,
      "grad_norm": 0.06035354360938072,
      "learning_rate": 1.6894160583941606e-05,
      "loss": 0.626,
      "step": 90710
    },
    {
      "epoch": 662.1897810218978,
      "grad_norm": 9.05492115020752,
      "learning_rate": 1.689051094890511e-05,
      "loss": 0.9313,
      "step": 90720
    },
    {
      "epoch": 662.2627737226277,
      "grad_norm": 9.66226863861084,
      "learning_rate": 1.6886861313868613e-05,
      "loss": 0.9115,
      "step": 90730
    },
    {
      "epoch": 662.3357664233577,
      "grad_norm": 12.744394302368164,
      "learning_rate": 1.6883211678832117e-05,
      "loss": 1.0011,
      "step": 90740
    },
    {
      "epoch": 662.4087591240876,
      "grad_norm": 9.781709671020508,
      "learning_rate": 1.687956204379562e-05,
      "loss": 1.0864,
      "step": 90750
    },
    {
      "epoch": 662.4817518248175,
      "grad_norm": 15.192644119262695,
      "learning_rate": 1.6875912408759125e-05,
      "loss": 0.7109,
      "step": 90760
    },
    {
      "epoch": 662.5547445255474,
      "grad_norm": 8.144970893859863,
      "learning_rate": 1.687226277372263e-05,
      "loss": 0.7598,
      "step": 90770
    },
    {
      "epoch": 662.6277372262774,
      "grad_norm": 13.473908424377441,
      "learning_rate": 1.6868613138686133e-05,
      "loss": 0.9381,
      "step": 90780
    },
    {
      "epoch": 662.7007299270073,
      "grad_norm": 12.16366958618164,
      "learning_rate": 1.6864963503649637e-05,
      "loss": 0.9591,
      "step": 90790
    },
    {
      "epoch": 662.7737226277372,
      "grad_norm": 8.910030364990234,
      "learning_rate": 1.686131386861314e-05,
      "loss": 0.9437,
      "step": 90800
    },
    {
      "epoch": 662.8467153284671,
      "grad_norm": 20.40387725830078,
      "learning_rate": 1.685766423357664e-05,
      "loss": 0.8926,
      "step": 90810
    },
    {
      "epoch": 662.9197080291971,
      "grad_norm": 10.592944145202637,
      "learning_rate": 1.6854014598540145e-05,
      "loss": 1.0875,
      "step": 90820
    },
    {
      "epoch": 662.992700729927,
      "grad_norm": 0.13189151883125305,
      "learning_rate": 1.6850364963503652e-05,
      "loss": 0.6814,
      "step": 90830
    },
    {
      "epoch": 663.0656934306569,
      "grad_norm": 6.482293128967285,
      "learning_rate": 1.6846715328467156e-05,
      "loss": 0.7843,
      "step": 90840
    },
    {
      "epoch": 663.1386861313869,
      "grad_norm": 10.898838996887207,
      "learning_rate": 1.6843065693430657e-05,
      "loss": 0.6744,
      "step": 90850
    },
    {
      "epoch": 663.2116788321168,
      "grad_norm": 9.77160930633545,
      "learning_rate": 1.683941605839416e-05,
      "loss": 0.765,
      "step": 90860
    },
    {
      "epoch": 663.2846715328467,
      "grad_norm": 7.1907806396484375,
      "learning_rate": 1.6835766423357664e-05,
      "loss": 0.9605,
      "step": 90870
    },
    {
      "epoch": 663.3576642335767,
      "grad_norm": 7.231151103973389,
      "learning_rate": 1.683211678832117e-05,
      "loss": 0.7279,
      "step": 90880
    },
    {
      "epoch": 663.4306569343066,
      "grad_norm": 8.30230712890625,
      "learning_rate": 1.6828467153284672e-05,
      "loss": 0.6952,
      "step": 90890
    },
    {
      "epoch": 663.5036496350365,
      "grad_norm": 11.430154800415039,
      "learning_rate": 1.6824817518248176e-05,
      "loss": 1.2522,
      "step": 90900
    },
    {
      "epoch": 663.5766423357665,
      "grad_norm": 14.091293334960938,
      "learning_rate": 1.682116788321168e-05,
      "loss": 1.2075,
      "step": 90910
    },
    {
      "epoch": 663.6496350364963,
      "grad_norm": 11.023761749267578,
      "learning_rate": 1.681751824817518e-05,
      "loss": 0.7235,
      "step": 90920
    },
    {
      "epoch": 663.7226277372263,
      "grad_norm": 12.783112525939941,
      "learning_rate": 1.6813868613138688e-05,
      "loss": 1.1263,
      "step": 90930
    },
    {
      "epoch": 663.7956204379562,
      "grad_norm": 8.408272743225098,
      "learning_rate": 1.681021897810219e-05,
      "loss": 0.7027,
      "step": 90940
    },
    {
      "epoch": 663.8686131386861,
      "grad_norm": 1.237168788909912,
      "learning_rate": 1.6806569343065695e-05,
      "loss": 0.9226,
      "step": 90950
    },
    {
      "epoch": 663.9416058394161,
      "grad_norm": 9.341915130615234,
      "learning_rate": 1.6802919708029196e-05,
      "loss": 0.9733,
      "step": 90960
    },
    {
      "epoch": 664.014598540146,
      "grad_norm": 7.730095386505127,
      "learning_rate": 1.67992700729927e-05,
      "loss": 0.7499,
      "step": 90970
    },
    {
      "epoch": 664.0875912408759,
      "grad_norm": 10.400272369384766,
      "learning_rate": 1.6795620437956207e-05,
      "loss": 0.8983,
      "step": 90980
    },
    {
      "epoch": 664.1605839416059,
      "grad_norm": 6.535984039306641,
      "learning_rate": 1.679197080291971e-05,
      "loss": 0.4462,
      "step": 90990
    },
    {
      "epoch": 664.2335766423357,
      "grad_norm": 6.868465423583984,
      "learning_rate": 1.678832116788321e-05,
      "loss": 1.0237,
      "step": 91000
    },
    {
      "epoch": 664.3065693430657,
      "grad_norm": 7.309083938598633,
      "learning_rate": 1.6784671532846715e-05,
      "loss": 1.3465,
      "step": 91010
    },
    {
      "epoch": 664.3795620437957,
      "grad_norm": 15.94295883178711,
      "learning_rate": 1.678102189781022e-05,
      "loss": 0.6843,
      "step": 91020
    },
    {
      "epoch": 664.4525547445255,
      "grad_norm": 14.890780448913574,
      "learning_rate": 1.6777372262773723e-05,
      "loss": 1.0433,
      "step": 91030
    },
    {
      "epoch": 664.5255474452555,
      "grad_norm": 9.564221382141113,
      "learning_rate": 1.6773722627737227e-05,
      "loss": 0.5774,
      "step": 91040
    },
    {
      "epoch": 664.5985401459855,
      "grad_norm": 12.659796714782715,
      "learning_rate": 1.677007299270073e-05,
      "loss": 0.5348,
      "step": 91050
    },
    {
      "epoch": 664.6715328467153,
      "grad_norm": 0.04509342834353447,
      "learning_rate": 1.6766423357664235e-05,
      "loss": 0.8827,
      "step": 91060
    },
    {
      "epoch": 664.7445255474453,
      "grad_norm": 7.198336124420166,
      "learning_rate": 1.676277372262774e-05,
      "loss": 0.947,
      "step": 91070
    },
    {
      "epoch": 664.8175182481751,
      "grad_norm": 9.270038604736328,
      "learning_rate": 1.6759124087591243e-05,
      "loss": 0.5247,
      "step": 91080
    },
    {
      "epoch": 664.8905109489051,
      "grad_norm": 18.711034774780273,
      "learning_rate": 1.6755474452554746e-05,
      "loss": 1.1767,
      "step": 91090
    },
    {
      "epoch": 664.9635036496351,
      "grad_norm": 7.716512203216553,
      "learning_rate": 1.675182481751825e-05,
      "loss": 1.0769,
      "step": 91100
    },
    {
      "epoch": 665.0364963503649,
      "grad_norm": 0.08116687089204788,
      "learning_rate": 1.6748175182481754e-05,
      "loss": 0.9042,
      "step": 91110
    },
    {
      "epoch": 665.1094890510949,
      "grad_norm": 11.96365737915039,
      "learning_rate": 1.6744525547445255e-05,
      "loss": 0.9021,
      "step": 91120
    },
    {
      "epoch": 665.1824817518249,
      "grad_norm": 10.876508712768555,
      "learning_rate": 1.674087591240876e-05,
      "loss": 0.8892,
      "step": 91130
    },
    {
      "epoch": 665.2554744525547,
      "grad_norm": 12.29859733581543,
      "learning_rate": 1.6737226277372266e-05,
      "loss": 0.3338,
      "step": 91140
    },
    {
      "epoch": 665.3284671532847,
      "grad_norm": 7.945018768310547,
      "learning_rate": 1.6733576642335766e-05,
      "loss": 0.9065,
      "step": 91150
    },
    {
      "epoch": 665.4014598540145,
      "grad_norm": 0.05107526481151581,
      "learning_rate": 1.672992700729927e-05,
      "loss": 1.1537,
      "step": 91160
    },
    {
      "epoch": 665.4744525547445,
      "grad_norm": 10.686330795288086,
      "learning_rate": 1.6726277372262774e-05,
      "loss": 0.8345,
      "step": 91170
    },
    {
      "epoch": 665.5474452554745,
      "grad_norm": 7.443110942840576,
      "learning_rate": 1.6722627737226278e-05,
      "loss": 0.8104,
      "step": 91180
    },
    {
      "epoch": 665.6204379562043,
      "grad_norm": 16.306032180786133,
      "learning_rate": 1.6718978102189782e-05,
      "loss": 0.8742,
      "step": 91190
    },
    {
      "epoch": 665.6934306569343,
      "grad_norm": 18.324512481689453,
      "learning_rate": 1.6715328467153286e-05,
      "loss": 1.0807,
      "step": 91200
    },
    {
      "epoch": 665.7664233576643,
      "grad_norm": 7.135483741760254,
      "learning_rate": 1.671167883211679e-05,
      "loss": 1.216,
      "step": 91210
    },
    {
      "epoch": 665.8394160583941,
      "grad_norm": 9.2299165725708,
      "learning_rate": 1.6708029197080294e-05,
      "loss": 0.8712,
      "step": 91220
    },
    {
      "epoch": 665.9124087591241,
      "grad_norm": 7.1223907470703125,
      "learning_rate": 1.6704379562043794e-05,
      "loss": 0.9053,
      "step": 91230
    },
    {
      "epoch": 665.985401459854,
      "grad_norm": 13.557989120483398,
      "learning_rate": 1.67007299270073e-05,
      "loss": 0.8277,
      "step": 91240
    },
    {
      "epoch": 666.0583941605839,
      "grad_norm": 6.0992207527160645,
      "learning_rate": 1.6697080291970805e-05,
      "loss": 1.0694,
      "step": 91250
    },
    {
      "epoch": 666.1313868613139,
      "grad_norm": 7.526590347290039,
      "learning_rate": 1.669343065693431e-05,
      "loss": 0.9862,
      "step": 91260
    },
    {
      "epoch": 666.2043795620438,
      "grad_norm": 12.654114723205566,
      "learning_rate": 1.668978102189781e-05,
      "loss": 0.965,
      "step": 91270
    },
    {
      "epoch": 666.2773722627737,
      "grad_norm": 9.721359252929688,
      "learning_rate": 1.6686131386861313e-05,
      "loss": 1.0076,
      "step": 91280
    },
    {
      "epoch": 666.3503649635037,
      "grad_norm": 6.41635799407959,
      "learning_rate": 1.6682481751824817e-05,
      "loss": 0.3543,
      "step": 91290
    },
    {
      "epoch": 666.4233576642335,
      "grad_norm": 9.083416938781738,
      "learning_rate": 1.6678832116788325e-05,
      "loss": 1.0008,
      "step": 91300
    },
    {
      "epoch": 666.4963503649635,
      "grad_norm": 11.207291603088379,
      "learning_rate": 1.6675182481751825e-05,
      "loss": 0.8718,
      "step": 91310
    },
    {
      "epoch": 666.5693430656934,
      "grad_norm": 2.117920398712158,
      "learning_rate": 1.667153284671533e-05,
      "loss": 1.1233,
      "step": 91320
    },
    {
      "epoch": 666.6423357664233,
      "grad_norm": 9.79068374633789,
      "learning_rate": 1.6667883211678833e-05,
      "loss": 0.7734,
      "step": 91330
    },
    {
      "epoch": 666.7153284671533,
      "grad_norm": 14.561655044555664,
      "learning_rate": 1.6664233576642337e-05,
      "loss": 0.6536,
      "step": 91340
    },
    {
      "epoch": 666.7883211678832,
      "grad_norm": 17.140901565551758,
      "learning_rate": 1.666058394160584e-05,
      "loss": 0.5887,
      "step": 91350
    },
    {
      "epoch": 666.8613138686131,
      "grad_norm": 8.639829635620117,
      "learning_rate": 1.6656934306569344e-05,
      "loss": 1.1732,
      "step": 91360
    },
    {
      "epoch": 666.9343065693431,
      "grad_norm": 12.430089950561523,
      "learning_rate": 1.665328467153285e-05,
      "loss": 1.0862,
      "step": 91370
    },
    {
      "epoch": 667.007299270073,
      "grad_norm": 15.355535507202148,
      "learning_rate": 1.664963503649635e-05,
      "loss": 1.0723,
      "step": 91380
    },
    {
      "epoch": 667.0802919708029,
      "grad_norm": 11.891725540161133,
      "learning_rate": 1.6645985401459853e-05,
      "loss": 1.0409,
      "step": 91390
    },
    {
      "epoch": 667.1532846715329,
      "grad_norm": 13.682173728942871,
      "learning_rate": 1.664233576642336e-05,
      "loss": 0.8375,
      "step": 91400
    },
    {
      "epoch": 667.2262773722628,
      "grad_norm": 9.021035194396973,
      "learning_rate": 1.6638686131386864e-05,
      "loss": 1.0824,
      "step": 91410
    },
    {
      "epoch": 667.2992700729927,
      "grad_norm": 10.175986289978027,
      "learning_rate": 1.6635036496350364e-05,
      "loss": 0.5278,
      "step": 91420
    },
    {
      "epoch": 667.3722627737226,
      "grad_norm": 0.05218714848160744,
      "learning_rate": 1.6631386861313868e-05,
      "loss": 0.4365,
      "step": 91430
    },
    {
      "epoch": 667.4452554744526,
      "grad_norm": 6.7791972160339355,
      "learning_rate": 1.6627737226277372e-05,
      "loss": 0.9798,
      "step": 91440
    },
    {
      "epoch": 667.5182481751825,
      "grad_norm": 6.127047538757324,
      "learning_rate": 1.662408759124088e-05,
      "loss": 0.8398,
      "step": 91450
    },
    {
      "epoch": 667.5912408759124,
      "grad_norm": 9.81482982635498,
      "learning_rate": 1.662043795620438e-05,
      "loss": 0.8543,
      "step": 91460
    },
    {
      "epoch": 667.6642335766423,
      "grad_norm": 10.668587684631348,
      "learning_rate": 1.6616788321167884e-05,
      "loss": 1.0959,
      "step": 91470
    },
    {
      "epoch": 667.7372262773723,
      "grad_norm": 7.243141174316406,
      "learning_rate": 1.6613138686131388e-05,
      "loss": 0.9509,
      "step": 91480
    },
    {
      "epoch": 667.8102189781022,
      "grad_norm": 9.779026985168457,
      "learning_rate": 1.660948905109489e-05,
      "loss": 1.1904,
      "step": 91490
    },
    {
      "epoch": 667.8832116788321,
      "grad_norm": 13.481441497802734,
      "learning_rate": 1.6605839416058395e-05,
      "loss": 0.9624,
      "step": 91500
    },
    {
      "epoch": 667.956204379562,
      "grad_norm": 6.946136474609375,
      "learning_rate": 1.66021897810219e-05,
      "loss": 0.9092,
      "step": 91510
    },
    {
      "epoch": 668.029197080292,
      "grad_norm": 10.017472267150879,
      "learning_rate": 1.6598540145985403e-05,
      "loss": 1.0683,
      "step": 91520
    },
    {
      "epoch": 668.1021897810219,
      "grad_norm": 0.020251363515853882,
      "learning_rate": 1.6594890510948907e-05,
      "loss": 1.162,
      "step": 91530
    },
    {
      "epoch": 668.1751824817518,
      "grad_norm": 10.549775123596191,
      "learning_rate": 1.6591240875912408e-05,
      "loss": 0.6607,
      "step": 91540
    },
    {
      "epoch": 668.2481751824818,
      "grad_norm": 11.05012035369873,
      "learning_rate": 1.6587591240875915e-05,
      "loss": 1.2537,
      "step": 91550
    },
    {
      "epoch": 668.3211678832117,
      "grad_norm": 16.580259323120117,
      "learning_rate": 1.658394160583942e-05,
      "loss": 1.0553,
      "step": 91560
    },
    {
      "epoch": 668.3941605839416,
      "grad_norm": 7.818461894989014,
      "learning_rate": 1.658029197080292e-05,
      "loss": 1.0406,
      "step": 91570
    },
    {
      "epoch": 668.4671532846716,
      "grad_norm": 9.450460433959961,
      "learning_rate": 1.6576642335766423e-05,
      "loss": 0.7832,
      "step": 91580
    },
    {
      "epoch": 668.5401459854014,
      "grad_norm": 0.09659263491630554,
      "learning_rate": 1.6572992700729927e-05,
      "loss": 1.3322,
      "step": 91590
    },
    {
      "epoch": 668.6131386861314,
      "grad_norm": 9.06143569946289,
      "learning_rate": 1.656934306569343e-05,
      "loss": 0.9993,
      "step": 91600
    },
    {
      "epoch": 668.6861313868613,
      "grad_norm": 6.172882080078125,
      "learning_rate": 1.6565693430656935e-05,
      "loss": 0.8454,
      "step": 91610
    },
    {
      "epoch": 668.7591240875912,
      "grad_norm": 14.494914054870605,
      "learning_rate": 1.656204379562044e-05,
      "loss": 0.7914,
      "step": 91620
    },
    {
      "epoch": 668.8321167883212,
      "grad_norm": 4.248415470123291,
      "learning_rate": 1.6558394160583943e-05,
      "loss": 0.379,
      "step": 91630
    },
    {
      "epoch": 668.9051094890511,
      "grad_norm": 10.4909029006958,
      "learning_rate": 1.6554744525547446e-05,
      "loss": 0.9109,
      "step": 91640
    },
    {
      "epoch": 668.978102189781,
      "grad_norm": 14.600883483886719,
      "learning_rate": 1.655109489051095e-05,
      "loss": 0.8022,
      "step": 91650
    },
    {
      "epoch": 669.051094890511,
      "grad_norm": 8.550970077514648,
      "learning_rate": 1.6547445255474454e-05,
      "loss": 0.6891,
      "step": 91660
    },
    {
      "epoch": 669.1240875912408,
      "grad_norm": 13.41849136352539,
      "learning_rate": 1.6543795620437958e-05,
      "loss": 1.0618,
      "step": 91670
    },
    {
      "epoch": 669.1970802919708,
      "grad_norm": 11.07767391204834,
      "learning_rate": 1.6540145985401462e-05,
      "loss": 0.9495,
      "step": 91680
    },
    {
      "epoch": 669.2700729927008,
      "grad_norm": 11.423294067382812,
      "learning_rate": 1.6536496350364962e-05,
      "loss": 1.5474,
      "step": 91690
    },
    {
      "epoch": 669.3430656934306,
      "grad_norm": 7.745293140411377,
      "learning_rate": 1.6532846715328466e-05,
      "loss": 0.765,
      "step": 91700
    },
    {
      "epoch": 669.4160583941606,
      "grad_norm": 10.435746192932129,
      "learning_rate": 1.6529197080291974e-05,
      "loss": 0.8819,
      "step": 91710
    },
    {
      "epoch": 669.4890510948906,
      "grad_norm": 14.89437484741211,
      "learning_rate": 1.6525547445255477e-05,
      "loss": 0.9316,
      "step": 91720
    },
    {
      "epoch": 669.5620437956204,
      "grad_norm": 12.973024368286133,
      "learning_rate": 1.6521897810218978e-05,
      "loss": 1.2996,
      "step": 91730
    },
    {
      "epoch": 669.6350364963504,
      "grad_norm": 8.941387176513672,
      "learning_rate": 1.6518248175182482e-05,
      "loss": 0.5687,
      "step": 91740
    },
    {
      "epoch": 669.7080291970802,
      "grad_norm": 12.253162384033203,
      "learning_rate": 1.6514598540145986e-05,
      "loss": 0.8432,
      "step": 91750
    },
    {
      "epoch": 669.7810218978102,
      "grad_norm": 12.431132316589355,
      "learning_rate": 1.651094890510949e-05,
      "loss": 0.7869,
      "step": 91760
    },
    {
      "epoch": 669.8540145985402,
      "grad_norm": 17.025585174560547,
      "learning_rate": 1.6507299270072994e-05,
      "loss": 0.8106,
      "step": 91770
    },
    {
      "epoch": 669.92700729927,
      "grad_norm": 13.592401504516602,
      "learning_rate": 1.6503649635036497e-05,
      "loss": 0.5686,
      "step": 91780
    },
    {
      "epoch": 670.0,
      "grad_norm": 0.07754126191139221,
      "learning_rate": 1.65e-05,
      "loss": 0.7839,
      "step": 91790
    },
    {
      "epoch": 670.07299270073,
      "grad_norm": 6.880476474761963,
      "learning_rate": 1.6496350364963502e-05,
      "loss": 0.9408,
      "step": 91800
    },
    {
      "epoch": 670.1459854014598,
      "grad_norm": 0.061220843344926834,
      "learning_rate": 1.649270072992701e-05,
      "loss": 0.7969,
      "step": 91810
    },
    {
      "epoch": 670.2189781021898,
      "grad_norm": 15.549874305725098,
      "learning_rate": 1.6489051094890513e-05,
      "loss": 1.0216,
      "step": 91820
    },
    {
      "epoch": 670.2919708029198,
      "grad_norm": 14.020669937133789,
      "learning_rate": 1.6485401459854017e-05,
      "loss": 1.1738,
      "step": 91830
    },
    {
      "epoch": 670.3649635036496,
      "grad_norm": 9.379871368408203,
      "learning_rate": 1.6481751824817517e-05,
      "loss": 0.8474,
      "step": 91840
    },
    {
      "epoch": 670.4379562043796,
      "grad_norm": 0.11844287067651749,
      "learning_rate": 1.647810218978102e-05,
      "loss": 0.898,
      "step": 91850
    },
    {
      "epoch": 670.5109489051094,
      "grad_norm": 6.345747470855713,
      "learning_rate": 1.6474452554744525e-05,
      "loss": 1.108,
      "step": 91860
    },
    {
      "epoch": 670.5839416058394,
      "grad_norm": 8.039590835571289,
      "learning_rate": 1.6470802919708032e-05,
      "loss": 0.9918,
      "step": 91870
    },
    {
      "epoch": 670.6569343065694,
      "grad_norm": 0.025432270020246506,
      "learning_rate": 1.6467153284671533e-05,
      "loss": 0.7098,
      "step": 91880
    },
    {
      "epoch": 670.7299270072992,
      "grad_norm": 0.5171904563903809,
      "learning_rate": 1.6463503649635037e-05,
      "loss": 1.1036,
      "step": 91890
    },
    {
      "epoch": 670.8029197080292,
      "grad_norm": 0.6658124327659607,
      "learning_rate": 1.645985401459854e-05,
      "loss": 0.5199,
      "step": 91900
    },
    {
      "epoch": 670.8759124087592,
      "grad_norm": 7.961595058441162,
      "learning_rate": 1.6456204379562044e-05,
      "loss": 0.731,
      "step": 91910
    },
    {
      "epoch": 670.948905109489,
      "grad_norm": 7.245010852813721,
      "learning_rate": 1.645255474452555e-05,
      "loss": 0.7973,
      "step": 91920
    },
    {
      "epoch": 671.021897810219,
      "grad_norm": 8.90873908996582,
      "learning_rate": 1.6448905109489052e-05,
      "loss": 1.2096,
      "step": 91930
    },
    {
      "epoch": 671.0948905109489,
      "grad_norm": 8.161436080932617,
      "learning_rate": 1.6445255474452556e-05,
      "loss": 0.7378,
      "step": 91940
    },
    {
      "epoch": 671.1678832116788,
      "grad_norm": 13.788043975830078,
      "learning_rate": 1.644160583941606e-05,
      "loss": 1.2681,
      "step": 91950
    },
    {
      "epoch": 671.2408759124088,
      "grad_norm": 0.0370141975581646,
      "learning_rate": 1.643795620437956e-05,
      "loss": 0.4746,
      "step": 91960
    },
    {
      "epoch": 671.3138686131387,
      "grad_norm": 8.550248146057129,
      "learning_rate": 1.6434306569343068e-05,
      "loss": 0.7374,
      "step": 91970
    },
    {
      "epoch": 671.3868613138686,
      "grad_norm": 12.015145301818848,
      "learning_rate": 1.643065693430657e-05,
      "loss": 1.1713,
      "step": 91980
    },
    {
      "epoch": 671.4598540145986,
      "grad_norm": 7.254220485687256,
      "learning_rate": 1.6427007299270072e-05,
      "loss": 0.9762,
      "step": 91990
    },
    {
      "epoch": 671.5328467153284,
      "grad_norm": 6.988655090332031,
      "learning_rate": 1.6423357664233576e-05,
      "loss": 0.9705,
      "step": 92000
    },
    {
      "epoch": 671.6058394160584,
      "grad_norm": 13.676359176635742,
      "learning_rate": 1.641970802919708e-05,
      "loss": 0.923,
      "step": 92010
    },
    {
      "epoch": 671.6788321167883,
      "grad_norm": 9.609893798828125,
      "learning_rate": 1.6416058394160587e-05,
      "loss": 1.0724,
      "step": 92020
    },
    {
      "epoch": 671.7518248175182,
      "grad_norm": 8.371598243713379,
      "learning_rate": 1.6412408759124088e-05,
      "loss": 0.3685,
      "step": 92030
    },
    {
      "epoch": 671.8248175182482,
      "grad_norm": 9.764596939086914,
      "learning_rate": 1.640875912408759e-05,
      "loss": 0.8643,
      "step": 92040
    },
    {
      "epoch": 671.8978102189781,
      "grad_norm": 8.938563346862793,
      "learning_rate": 1.6405109489051095e-05,
      "loss": 0.9851,
      "step": 92050
    },
    {
      "epoch": 671.970802919708,
      "grad_norm": 8.156902313232422,
      "learning_rate": 1.64014598540146e-05,
      "loss": 0.7526,
      "step": 92060
    },
    {
      "epoch": 672.043795620438,
      "grad_norm": 0.1763845533132553,
      "learning_rate": 1.6397810218978103e-05,
      "loss": 0.9904,
      "step": 92070
    },
    {
      "epoch": 672.1167883211679,
      "grad_norm": 6.751662254333496,
      "learning_rate": 1.6394160583941607e-05,
      "loss": 0.8567,
      "step": 92080
    },
    {
      "epoch": 672.1897810218978,
      "grad_norm": 18.002338409423828,
      "learning_rate": 1.639051094890511e-05,
      "loss": 1.1464,
      "step": 92090
    },
    {
      "epoch": 672.2627737226277,
      "grad_norm": 9.76256275177002,
      "learning_rate": 1.6386861313868615e-05,
      "loss": 1.0905,
      "step": 92100
    },
    {
      "epoch": 672.3357664233577,
      "grad_norm": 0.05134200304746628,
      "learning_rate": 1.6383211678832115e-05,
      "loss": 0.6674,
      "step": 92110
    },
    {
      "epoch": 672.4087591240876,
      "grad_norm": 20.732418060302734,
      "learning_rate": 1.6379562043795623e-05,
      "loss": 1.3751,
      "step": 92120
    },
    {
      "epoch": 672.4817518248175,
      "grad_norm": 15.226329803466797,
      "learning_rate": 1.6375912408759126e-05,
      "loss": 1.1588,
      "step": 92130
    },
    {
      "epoch": 672.5547445255474,
      "grad_norm": 11.69801139831543,
      "learning_rate": 1.637226277372263e-05,
      "loss": 0.7911,
      "step": 92140
    },
    {
      "epoch": 672.6277372262774,
      "grad_norm": 16.068195343017578,
      "learning_rate": 1.636861313868613e-05,
      "loss": 0.9063,
      "step": 92150
    },
    {
      "epoch": 672.7007299270073,
      "grad_norm": 7.800079822540283,
      "learning_rate": 1.6364963503649635e-05,
      "loss": 0.7744,
      "step": 92160
    },
    {
      "epoch": 672.7737226277372,
      "grad_norm": 0.04383176192641258,
      "learning_rate": 1.636131386861314e-05,
      "loss": 0.7356,
      "step": 92170
    },
    {
      "epoch": 672.8467153284671,
      "grad_norm": 7.493318557739258,
      "learning_rate": 1.6357664233576646e-05,
      "loss": 0.838,
      "step": 92180
    },
    {
      "epoch": 672.9197080291971,
      "grad_norm": 15.881333351135254,
      "learning_rate": 1.6354014598540146e-05,
      "loss": 0.6585,
      "step": 92190
    },
    {
      "epoch": 672.992700729927,
      "grad_norm": 13.321964263916016,
      "learning_rate": 1.635036496350365e-05,
      "loss": 0.9464,
      "step": 92200
    },
    {
      "epoch": 673.0656934306569,
      "grad_norm": 0.05180569365620613,
      "learning_rate": 1.6346715328467154e-05,
      "loss": 0.6955,
      "step": 92210
    },
    {
      "epoch": 673.1386861313869,
      "grad_norm": 12.66215705871582,
      "learning_rate": 1.6343065693430658e-05,
      "loss": 0.578,
      "step": 92220
    },
    {
      "epoch": 673.2116788321168,
      "grad_norm": 9.321677207946777,
      "learning_rate": 1.6339416058394162e-05,
      "loss": 0.8083,
      "step": 92230
    },
    {
      "epoch": 673.2846715328467,
      "grad_norm": 8.026847839355469,
      "learning_rate": 1.6335766423357666e-05,
      "loss": 0.9303,
      "step": 92240
    },
    {
      "epoch": 673.3576642335767,
      "grad_norm": 10.759819984436035,
      "learning_rate": 1.633211678832117e-05,
      "loss": 1.0654,
      "step": 92250
    },
    {
      "epoch": 673.4306569343066,
      "grad_norm": 13.235877990722656,
      "learning_rate": 1.632846715328467e-05,
      "loss": 0.6906,
      "step": 92260
    },
    {
      "epoch": 673.5036496350365,
      "grad_norm": 11.443869590759277,
      "learning_rate": 1.6324817518248174e-05,
      "loss": 0.8009,
      "step": 92270
    },
    {
      "epoch": 673.5766423357665,
      "grad_norm": 8.82692813873291,
      "learning_rate": 1.632116788321168e-05,
      "loss": 1.0222,
      "step": 92280
    },
    {
      "epoch": 673.6496350364963,
      "grad_norm": 6.379386901855469,
      "learning_rate": 1.6317518248175185e-05,
      "loss": 0.6442,
      "step": 92290
    },
    {
      "epoch": 673.7226277372263,
      "grad_norm": 12.664632797241211,
      "learning_rate": 1.6313868613138686e-05,
      "loss": 1.0545,
      "step": 92300
    },
    {
      "epoch": 673.7956204379562,
      "grad_norm": 13.980372428894043,
      "learning_rate": 1.631021897810219e-05,
      "loss": 1.1132,
      "step": 92310
    },
    {
      "epoch": 673.8686131386861,
      "grad_norm": 12.59982681274414,
      "learning_rate": 1.6306569343065693e-05,
      "loss": 0.9931,
      "step": 92320
    },
    {
      "epoch": 673.9416058394161,
      "grad_norm": 7.1518235206604,
      "learning_rate": 1.6302919708029197e-05,
      "loss": 0.9092,
      "step": 92330
    },
    {
      "epoch": 674.014598540146,
      "grad_norm": 11.64979362487793,
      "learning_rate": 1.62992700729927e-05,
      "loss": 1.0195,
      "step": 92340
    },
    {
      "epoch": 674.0875912408759,
      "grad_norm": 15.681266784667969,
      "learning_rate": 1.6295620437956205e-05,
      "loss": 1.1894,
      "step": 92350
    },
    {
      "epoch": 674.1605839416059,
      "grad_norm": 7.750143051147461,
      "learning_rate": 1.629197080291971e-05,
      "loss": 1.0151,
      "step": 92360
    },
    {
      "epoch": 674.2335766423357,
      "grad_norm": 7.726573944091797,
      "learning_rate": 1.6288321167883213e-05,
      "loss": 1.0926,
      "step": 92370
    },
    {
      "epoch": 674.3065693430657,
      "grad_norm": 12.252182960510254,
      "learning_rate": 1.6284671532846717e-05,
      "loss": 0.7069,
      "step": 92380
    },
    {
      "epoch": 674.3795620437957,
      "grad_norm": 14.13696575164795,
      "learning_rate": 1.628102189781022e-05,
      "loss": 1.158,
      "step": 92390
    },
    {
      "epoch": 674.4525547445255,
      "grad_norm": 7.9715375900268555,
      "learning_rate": 1.6277372262773725e-05,
      "loss": 0.7598,
      "step": 92400
    },
    {
      "epoch": 674.5255474452555,
      "grad_norm": 9.49502182006836,
      "learning_rate": 1.627372262773723e-05,
      "loss": 1.0246,
      "step": 92410
    },
    {
      "epoch": 674.5985401459855,
      "grad_norm": 8.513787269592285,
      "learning_rate": 1.627007299270073e-05,
      "loss": 0.8889,
      "step": 92420
    },
    {
      "epoch": 674.6715328467153,
      "grad_norm": 10.096443176269531,
      "learning_rate": 1.6266423357664233e-05,
      "loss": 0.8987,
      "step": 92430
    },
    {
      "epoch": 674.7445255474453,
      "grad_norm": 11.853256225585938,
      "learning_rate": 1.626277372262774e-05,
      "loss": 0.9437,
      "step": 92440
    },
    {
      "epoch": 674.8175182481751,
      "grad_norm": 11.302810668945312,
      "learning_rate": 1.625912408759124e-05,
      "loss": 1.058,
      "step": 92450
    },
    {
      "epoch": 674.8905109489051,
      "grad_norm": 6.309175968170166,
      "learning_rate": 1.6255474452554744e-05,
      "loss": 0.6365,
      "step": 92460
    },
    {
      "epoch": 674.9635036496351,
      "grad_norm": 0.056370582431554794,
      "learning_rate": 1.625182481751825e-05,
      "loss": 0.7167,
      "step": 92470
    },
    {
      "epoch": 675.0364963503649,
      "grad_norm": 5.684427261352539,
      "learning_rate": 1.6248175182481752e-05,
      "loss": 0.612,
      "step": 92480
    },
    {
      "epoch": 675.1094890510949,
      "grad_norm": 0.034175436943769455,
      "learning_rate": 1.6244525547445256e-05,
      "loss": 1.0815,
      "step": 92490
    },
    {
      "epoch": 675.1824817518249,
      "grad_norm": 9.749526023864746,
      "learning_rate": 1.624087591240876e-05,
      "loss": 1.0638,
      "step": 92500
    },
    {
      "epoch": 675.2554744525547,
      "grad_norm": 10.760952949523926,
      "learning_rate": 1.6237226277372264e-05,
      "loss": 0.9284,
      "step": 92510
    },
    {
      "epoch": 675.3284671532847,
      "grad_norm": 0.08277298510074615,
      "learning_rate": 1.6233576642335768e-05,
      "loss": 0.8527,
      "step": 92520
    },
    {
      "epoch": 675.4014598540145,
      "grad_norm": 6.578732490539551,
      "learning_rate": 1.6229927007299268e-05,
      "loss": 0.7843,
      "step": 92530
    },
    {
      "epoch": 675.4744525547445,
      "grad_norm": 10.101522445678711,
      "learning_rate": 1.6226277372262776e-05,
      "loss": 0.7686,
      "step": 92540
    },
    {
      "epoch": 675.5474452554745,
      "grad_norm": 13.293923377990723,
      "learning_rate": 1.622262773722628e-05,
      "loss": 0.7685,
      "step": 92550
    },
    {
      "epoch": 675.6204379562043,
      "grad_norm": 10.800116539001465,
      "learning_rate": 1.6218978102189783e-05,
      "loss": 1.0226,
      "step": 92560
    },
    {
      "epoch": 675.6934306569343,
      "grad_norm": 15.663527488708496,
      "learning_rate": 1.6215328467153284e-05,
      "loss": 0.7731,
      "step": 92570
    },
    {
      "epoch": 675.7664233576643,
      "grad_norm": 7.165264129638672,
      "learning_rate": 1.6211678832116788e-05,
      "loss": 0.725,
      "step": 92580
    },
    {
      "epoch": 675.8394160583941,
      "grad_norm": 16.668352127075195,
      "learning_rate": 1.6208029197080295e-05,
      "loss": 1.2502,
      "step": 92590
    },
    {
      "epoch": 675.9124087591241,
      "grad_norm": 11.555596351623535,
      "learning_rate": 1.62043795620438e-05,
      "loss": 0.9122,
      "step": 92600
    },
    {
      "epoch": 675.985401459854,
      "grad_norm": 7.336474418640137,
      "learning_rate": 1.62007299270073e-05,
      "loss": 0.9785,
      "step": 92610
    },
    {
      "epoch": 676.0583941605839,
      "grad_norm": 15.000776290893555,
      "learning_rate": 1.6197080291970803e-05,
      "loss": 0.7231,
      "step": 92620
    },
    {
      "epoch": 676.1313868613139,
      "grad_norm": 17.77373695373535,
      "learning_rate": 1.6193430656934307e-05,
      "loss": 0.6324,
      "step": 92630
    },
    {
      "epoch": 676.2043795620438,
      "grad_norm": 8.955378532409668,
      "learning_rate": 1.618978102189781e-05,
      "loss": 1.0388,
      "step": 92640
    },
    {
      "epoch": 676.2773722627737,
      "grad_norm": 8.090517044067383,
      "learning_rate": 1.6186131386861315e-05,
      "loss": 1.2649,
      "step": 92650
    },
    {
      "epoch": 676.3503649635037,
      "grad_norm": 12.983927726745605,
      "learning_rate": 1.618248175182482e-05,
      "loss": 0.7654,
      "step": 92660
    },
    {
      "epoch": 676.4233576642335,
      "grad_norm": 5.151241779327393,
      "learning_rate": 1.6178832116788323e-05,
      "loss": 0.6353,
      "step": 92670
    },
    {
      "epoch": 676.4963503649635,
      "grad_norm": 10.953987121582031,
      "learning_rate": 1.6175182481751823e-05,
      "loss": 0.9149,
      "step": 92680
    },
    {
      "epoch": 676.5693430656934,
      "grad_norm": 12.45489501953125,
      "learning_rate": 1.617153284671533e-05,
      "loss": 0.7204,
      "step": 92690
    },
    {
      "epoch": 676.6423357664233,
      "grad_norm": 20.295013427734375,
      "learning_rate": 1.6167883211678834e-05,
      "loss": 0.6372,
      "step": 92700
    },
    {
      "epoch": 676.7153284671533,
      "grad_norm": 5.722512245178223,
      "learning_rate": 1.6164233576642338e-05,
      "loss": 1.4642,
      "step": 92710
    },
    {
      "epoch": 676.7883211678832,
      "grad_norm": 17.707576751708984,
      "learning_rate": 1.616058394160584e-05,
      "loss": 1.089,
      "step": 92720
    },
    {
      "epoch": 676.8613138686131,
      "grad_norm": 16.39661979675293,
      "learning_rate": 1.6156934306569342e-05,
      "loss": 0.8161,
      "step": 92730
    },
    {
      "epoch": 676.9343065693431,
      "grad_norm": 13.411064147949219,
      "learning_rate": 1.6153284671532846e-05,
      "loss": 1.1515,
      "step": 92740
    },
    {
      "epoch": 677.007299270073,
      "grad_norm": 9.368661880493164,
      "learning_rate": 1.6149635036496354e-05,
      "loss": 0.7311,
      "step": 92750
    },
    {
      "epoch": 677.0802919708029,
      "grad_norm": 9.529742240905762,
      "learning_rate": 1.6145985401459854e-05,
      "loss": 0.7314,
      "step": 92760
    },
    {
      "epoch": 677.1532846715329,
      "grad_norm": 8.286179542541504,
      "learning_rate": 1.6142335766423358e-05,
      "loss": 1.1629,
      "step": 92770
    },
    {
      "epoch": 677.2262773722628,
      "grad_norm": 0.14796969294548035,
      "learning_rate": 1.6138686131386862e-05,
      "loss": 1.0298,
      "step": 92780
    },
    {
      "epoch": 677.2992700729927,
      "grad_norm": 0.05296628922224045,
      "learning_rate": 1.6135036496350366e-05,
      "loss": 0.4827,
      "step": 92790
    },
    {
      "epoch": 677.3722627737226,
      "grad_norm": 0.03640870004892349,
      "learning_rate": 1.613138686131387e-05,
      "loss": 0.4541,
      "step": 92800
    },
    {
      "epoch": 677.4452554744526,
      "grad_norm": 9.933625221252441,
      "learning_rate": 1.6127737226277374e-05,
      "loss": 0.8631,
      "step": 92810
    },
    {
      "epoch": 677.5182481751825,
      "grad_norm": 14.581048965454102,
      "learning_rate": 1.6124087591240877e-05,
      "loss": 0.8515,
      "step": 92820
    },
    {
      "epoch": 677.5912408759124,
      "grad_norm": 0.5755236148834229,
      "learning_rate": 1.612043795620438e-05,
      "loss": 0.8613,
      "step": 92830
    },
    {
      "epoch": 677.6642335766423,
      "grad_norm": 7.006239891052246,
      "learning_rate": 1.6116788321167882e-05,
      "loss": 1.0259,
      "step": 92840
    },
    {
      "epoch": 677.7372262773723,
      "grad_norm": 12.413753509521484,
      "learning_rate": 1.611313868613139e-05,
      "loss": 0.9451,
      "step": 92850
    },
    {
      "epoch": 677.8102189781022,
      "grad_norm": 10.980860710144043,
      "learning_rate": 1.6109489051094893e-05,
      "loss": 0.8183,
      "step": 92860
    },
    {
      "epoch": 677.8832116788321,
      "grad_norm": 13.940340042114258,
      "learning_rate": 1.6105839416058393e-05,
      "loss": 0.6678,
      "step": 92870
    },
    {
      "epoch": 677.956204379562,
      "grad_norm": 10.995660781860352,
      "learning_rate": 1.6102189781021897e-05,
      "loss": 1.3111,
      "step": 92880
    },
    {
      "epoch": 678.029197080292,
      "grad_norm": 8.715805053710938,
      "learning_rate": 1.60985401459854e-05,
      "loss": 0.9635,
      "step": 92890
    },
    {
      "epoch": 678.1021897810219,
      "grad_norm": 15.245376586914062,
      "learning_rate": 1.6094890510948905e-05,
      "loss": 0.4089,
      "step": 92900
    },
    {
      "epoch": 678.1751824817518,
      "grad_norm": 0.018055325374007225,
      "learning_rate": 1.609124087591241e-05,
      "loss": 1.0995,
      "step": 92910
    },
    {
      "epoch": 678.2481751824818,
      "grad_norm": 7.252749443054199,
      "learning_rate": 1.6087591240875913e-05,
      "loss": 0.6094,
      "step": 92920
    },
    {
      "epoch": 678.3211678832117,
      "grad_norm": 0.9236410856246948,
      "learning_rate": 1.6083941605839417e-05,
      "loss": 0.9343,
      "step": 92930
    },
    {
      "epoch": 678.3941605839416,
      "grad_norm": 13.363997459411621,
      "learning_rate": 1.608029197080292e-05,
      "loss": 0.7133,
      "step": 92940
    },
    {
      "epoch": 678.4671532846716,
      "grad_norm": 0.04643888399004936,
      "learning_rate": 1.6076642335766425e-05,
      "loss": 0.6263,
      "step": 92950
    },
    {
      "epoch": 678.5401459854014,
      "grad_norm": 9.501282691955566,
      "learning_rate": 1.607299270072993e-05,
      "loss": 0.65,
      "step": 92960
    },
    {
      "epoch": 678.6131386861314,
      "grad_norm": 8.505781173706055,
      "learning_rate": 1.6069343065693432e-05,
      "loss": 1.2425,
      "step": 92970
    },
    {
      "epoch": 678.6861313868613,
      "grad_norm": 0.0798003077507019,
      "learning_rate": 1.6065693430656936e-05,
      "loss": 0.9657,
      "step": 92980
    },
    {
      "epoch": 678.7591240875912,
      "grad_norm": 13.650273323059082,
      "learning_rate": 1.6062043795620437e-05,
      "loss": 1.0007,
      "step": 92990
    },
    {
      "epoch": 678.8321167883212,
      "grad_norm": 11.000489234924316,
      "learning_rate": 1.605839416058394e-05,
      "loss": 1.3369,
      "step": 93000
    },
    {
      "epoch": 678.9051094890511,
      "grad_norm": 0.0636606365442276,
      "learning_rate": 1.6054744525547448e-05,
      "loss": 0.9697,
      "step": 93010
    },
    {
      "epoch": 678.978102189781,
      "grad_norm": 10.113438606262207,
      "learning_rate": 1.6051094890510952e-05,
      "loss": 0.8605,
      "step": 93020
    },
    {
      "epoch": 679.051094890511,
      "grad_norm": 0.041928891092538834,
      "learning_rate": 1.6047445255474452e-05,
      "loss": 1.0847,
      "step": 93030
    },
    {
      "epoch": 679.1240875912408,
      "grad_norm": 12.620766639709473,
      "learning_rate": 1.6043795620437956e-05,
      "loss": 0.9566,
      "step": 93040
    },
    {
      "epoch": 679.1970802919708,
      "grad_norm": 8.895191192626953,
      "learning_rate": 1.604014598540146e-05,
      "loss": 1.2119,
      "step": 93050
    },
    {
      "epoch": 679.2700729927008,
      "grad_norm": 0.020030422136187553,
      "learning_rate": 1.6036496350364964e-05,
      "loss": 0.6687,
      "step": 93060
    },
    {
      "epoch": 679.3430656934306,
      "grad_norm": 9.644635200500488,
      "learning_rate": 1.6032846715328468e-05,
      "loss": 0.605,
      "step": 93070
    },
    {
      "epoch": 679.4160583941606,
      "grad_norm": 10.25459098815918,
      "learning_rate": 1.602919708029197e-05,
      "loss": 0.953,
      "step": 93080
    },
    {
      "epoch": 679.4890510948906,
      "grad_norm": 10.772445678710938,
      "learning_rate": 1.6025547445255475e-05,
      "loss": 0.8451,
      "step": 93090
    },
    {
      "epoch": 679.5620437956204,
      "grad_norm": 12.2742919921875,
      "learning_rate": 1.6021897810218976e-05,
      "loss": 0.844,
      "step": 93100
    },
    {
      "epoch": 679.6350364963504,
      "grad_norm": 17.108062744140625,
      "learning_rate": 1.6018248175182483e-05,
      "loss": 0.9738,
      "step": 93110
    },
    {
      "epoch": 679.7080291970802,
      "grad_norm": 6.950369834899902,
      "learning_rate": 1.6014598540145987e-05,
      "loss": 0.9669,
      "step": 93120
    },
    {
      "epoch": 679.7810218978102,
      "grad_norm": 9.954501152038574,
      "learning_rate": 1.601094890510949e-05,
      "loss": 0.9405,
      "step": 93130
    },
    {
      "epoch": 679.8540145985402,
      "grad_norm": 17.415836334228516,
      "learning_rate": 1.600729927007299e-05,
      "loss": 0.9756,
      "step": 93140
    },
    {
      "epoch": 679.92700729927,
      "grad_norm": 0.5687264800071716,
      "learning_rate": 1.6003649635036495e-05,
      "loss": 0.7806,
      "step": 93150
    },
    {
      "epoch": 680.0,
      "grad_norm": 2.0710601806640625,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.6732,
      "step": 93160
    },
    {
      "epoch": 680.07299270073,
      "grad_norm": 10.83938980102539,
      "learning_rate": 1.5996350364963507e-05,
      "loss": 0.5278,
      "step": 93170
    },
    {
      "epoch": 680.1459854014598,
      "grad_norm": 17.85979461669922,
      "learning_rate": 1.5992700729927007e-05,
      "loss": 0.759,
      "step": 93180
    },
    {
      "epoch": 680.2189781021898,
      "grad_norm": 7.466771602630615,
      "learning_rate": 1.598905109489051e-05,
      "loss": 1.117,
      "step": 93190
    },
    {
      "epoch": 680.2919708029198,
      "grad_norm": 13.82730484008789,
      "learning_rate": 1.5985401459854015e-05,
      "loss": 0.7235,
      "step": 93200
    },
    {
      "epoch": 680.3649635036496,
      "grad_norm": 13.036428451538086,
      "learning_rate": 1.598175182481752e-05,
      "loss": 1.2175,
      "step": 93210
    },
    {
      "epoch": 680.4379562043796,
      "grad_norm": 9.443596839904785,
      "learning_rate": 1.5978102189781023e-05,
      "loss": 0.922,
      "step": 93220
    },
    {
      "epoch": 680.5109489051094,
      "grad_norm": 12.255603790283203,
      "learning_rate": 1.5974452554744526e-05,
      "loss": 0.8089,
      "step": 93230
    },
    {
      "epoch": 680.5839416058394,
      "grad_norm": 18.78300666809082,
      "learning_rate": 1.597080291970803e-05,
      "loss": 0.6561,
      "step": 93240
    },
    {
      "epoch": 680.6569343065694,
      "grad_norm": 12.110452651977539,
      "learning_rate": 1.5967153284671534e-05,
      "loss": 0.8,
      "step": 93250
    },
    {
      "epoch": 680.7299270072992,
      "grad_norm": 9.895025253295898,
      "learning_rate": 1.5963503649635038e-05,
      "loss": 1.224,
      "step": 93260
    },
    {
      "epoch": 680.8029197080292,
      "grad_norm": 12.717621803283691,
      "learning_rate": 1.5959854014598542e-05,
      "loss": 1.0713,
      "step": 93270
    },
    {
      "epoch": 680.8759124087592,
      "grad_norm": 8.106183052062988,
      "learning_rate": 1.5956204379562046e-05,
      "loss": 0.6304,
      "step": 93280
    },
    {
      "epoch": 680.948905109489,
      "grad_norm": 7.309571743011475,
      "learning_rate": 1.5952554744525546e-05,
      "loss": 0.8804,
      "step": 93290
    },
    {
      "epoch": 681.021897810219,
      "grad_norm": 12.395771026611328,
      "learning_rate": 1.594890510948905e-05,
      "loss": 0.7209,
      "step": 93300
    },
    {
      "epoch": 681.0948905109489,
      "grad_norm": 6.614755630493164,
      "learning_rate": 1.5945255474452554e-05,
      "loss": 0.6664,
      "step": 93310
    },
    {
      "epoch": 681.1678832116788,
      "grad_norm": 9.600748062133789,
      "learning_rate": 1.594160583941606e-05,
      "loss": 0.8772,
      "step": 93320
    },
    {
      "epoch": 681.2408759124088,
      "grad_norm": 12.366888046264648,
      "learning_rate": 1.5937956204379562e-05,
      "loss": 0.6366,
      "step": 93330
    },
    {
      "epoch": 681.3138686131387,
      "grad_norm": 10.729619026184082,
      "learning_rate": 1.5934306569343066e-05,
      "loss": 0.884,
      "step": 93340
    },
    {
      "epoch": 681.3868613138686,
      "grad_norm": 4.351720809936523,
      "learning_rate": 1.593065693430657e-05,
      "loss": 0.7842,
      "step": 93350
    },
    {
      "epoch": 681.4598540145986,
      "grad_norm": 0.046445414423942566,
      "learning_rate": 1.5927007299270074e-05,
      "loss": 1.0702,
      "step": 93360
    },
    {
      "epoch": 681.5328467153284,
      "grad_norm": 5.8432159423828125,
      "learning_rate": 1.5923357664233577e-05,
      "loss": 0.7409,
      "step": 93370
    },
    {
      "epoch": 681.6058394160584,
      "grad_norm": 12.038125991821289,
      "learning_rate": 1.591970802919708e-05,
      "loss": 1.1003,
      "step": 93380
    },
    {
      "epoch": 681.6788321167883,
      "grad_norm": 0.06413864344358444,
      "learning_rate": 1.5916058394160585e-05,
      "loss": 0.5632,
      "step": 93390
    },
    {
      "epoch": 681.7518248175182,
      "grad_norm": 5.99621057510376,
      "learning_rate": 1.591240875912409e-05,
      "loss": 1.0968,
      "step": 93400
    },
    {
      "epoch": 681.8248175182482,
      "grad_norm": 7.108036518096924,
      "learning_rate": 1.590875912408759e-05,
      "loss": 0.974,
      "step": 93410
    },
    {
      "epoch": 681.8978102189781,
      "grad_norm": 0.10375610738992691,
      "learning_rate": 1.5905109489051097e-05,
      "loss": 0.9556,
      "step": 93420
    },
    {
      "epoch": 681.970802919708,
      "grad_norm": 8.889363288879395,
      "learning_rate": 1.59014598540146e-05,
      "loss": 0.9392,
      "step": 93430
    },
    {
      "epoch": 682.043795620438,
      "grad_norm": 9.45064926147461,
      "learning_rate": 1.5897810218978105e-05,
      "loss": 1.2109,
      "step": 93440
    },
    {
      "epoch": 682.1167883211679,
      "grad_norm": 9.078804016113281,
      "learning_rate": 1.5894160583941605e-05,
      "loss": 0.8581,
      "step": 93450
    },
    {
      "epoch": 682.1897810218978,
      "grad_norm": 14.264269828796387,
      "learning_rate": 1.589051094890511e-05,
      "loss": 0.7043,
      "step": 93460
    },
    {
      "epoch": 682.2627737226277,
      "grad_norm": 0.22088249027729034,
      "learning_rate": 1.5886861313868613e-05,
      "loss": 1.1537,
      "step": 93470
    },
    {
      "epoch": 682.3357664233577,
      "grad_norm": 13.743277549743652,
      "learning_rate": 1.588321167883212e-05,
      "loss": 0.9556,
      "step": 93480
    },
    {
      "epoch": 682.4087591240876,
      "grad_norm": 14.464794158935547,
      "learning_rate": 1.587956204379562e-05,
      "loss": 1.1948,
      "step": 93490
    },
    {
      "epoch": 682.4817518248175,
      "grad_norm": 12.209622383117676,
      "learning_rate": 1.5875912408759125e-05,
      "loss": 0.9065,
      "step": 93500
    },
    {
      "epoch": 682.5547445255474,
      "grad_norm": 5.626969337463379,
      "learning_rate": 1.587226277372263e-05,
      "loss": 1.1011,
      "step": 93510
    },
    {
      "epoch": 682.6277372262774,
      "grad_norm": 2.230062961578369,
      "learning_rate": 1.5868613138686132e-05,
      "loss": 0.7245,
      "step": 93520
    },
    {
      "epoch": 682.7007299270073,
      "grad_norm": 1.8349368572235107,
      "learning_rate": 1.5864963503649636e-05,
      "loss": 0.8561,
      "step": 93530
    },
    {
      "epoch": 682.7737226277372,
      "grad_norm": 7.075666904449463,
      "learning_rate": 1.586131386861314e-05,
      "loss": 0.73,
      "step": 93540
    },
    {
      "epoch": 682.8467153284671,
      "grad_norm": 9.932840347290039,
      "learning_rate": 1.5857664233576644e-05,
      "loss": 0.906,
      "step": 93550
    },
    {
      "epoch": 682.9197080291971,
      "grad_norm": 8.978058815002441,
      "learning_rate": 1.5854014598540144e-05,
      "loss": 0.6306,
      "step": 93560
    },
    {
      "epoch": 682.992700729927,
      "grad_norm": 9.598726272583008,
      "learning_rate": 1.5850364963503648e-05,
      "loss": 0.8099,
      "step": 93570
    },
    {
      "epoch": 683.0656934306569,
      "grad_norm": 11.009190559387207,
      "learning_rate": 1.5846715328467156e-05,
      "loss": 1.0319,
      "step": 93580
    },
    {
      "epoch": 683.1386861313869,
      "grad_norm": 10.708444595336914,
      "learning_rate": 1.584306569343066e-05,
      "loss": 0.6376,
      "step": 93590
    },
    {
      "epoch": 683.2116788321168,
      "grad_norm": 13.252843856811523,
      "learning_rate": 1.583941605839416e-05,
      "loss": 1.1633,
      "step": 93600
    },
    {
      "epoch": 683.2846715328467,
      "grad_norm": 6.981738567352295,
      "learning_rate": 1.5835766423357664e-05,
      "loss": 0.7565,
      "step": 93610
    },
    {
      "epoch": 683.3576642335767,
      "grad_norm": 12.623222351074219,
      "learning_rate": 1.5832116788321168e-05,
      "loss": 0.9998,
      "step": 93620
    },
    {
      "epoch": 683.4306569343066,
      "grad_norm": 11.002827644348145,
      "learning_rate": 1.5828467153284675e-05,
      "loss": 1.1554,
      "step": 93630
    },
    {
      "epoch": 683.5036496350365,
      "grad_norm": 12.45114517211914,
      "learning_rate": 1.5824817518248175e-05,
      "loss": 1.02,
      "step": 93640
    },
    {
      "epoch": 683.5766423357665,
      "grad_norm": 15.61497688293457,
      "learning_rate": 1.582116788321168e-05,
      "loss": 1.4042,
      "step": 93650
    },
    {
      "epoch": 683.6496350364963,
      "grad_norm": 10.747849464416504,
      "learning_rate": 1.5817518248175183e-05,
      "loss": 0.5209,
      "step": 93660
    },
    {
      "epoch": 683.7226277372263,
      "grad_norm": 5.537622928619385,
      "learning_rate": 1.5813868613138687e-05,
      "loss": 0.6152,
      "step": 93670
    },
    {
      "epoch": 683.7956204379562,
      "grad_norm": 7.225399971008301,
      "learning_rate": 1.581021897810219e-05,
      "loss": 1.0314,
      "step": 93680
    },
    {
      "epoch": 683.8686131386861,
      "grad_norm": 6.633587837219238,
      "learning_rate": 1.5806569343065695e-05,
      "loss": 0.899,
      "step": 93690
    },
    {
      "epoch": 683.9416058394161,
      "grad_norm": 0.05758535489439964,
      "learning_rate": 1.58029197080292e-05,
      "loss": 0.5535,
      "step": 93700
    },
    {
      "epoch": 684.014598540146,
      "grad_norm": 11.507380485534668,
      "learning_rate": 1.57992700729927e-05,
      "loss": 0.7768,
      "step": 93710
    },
    {
      "epoch": 684.0875912408759,
      "grad_norm": 15.441669464111328,
      "learning_rate": 1.5795620437956203e-05,
      "loss": 0.9688,
      "step": 93720
    },
    {
      "epoch": 684.1605839416059,
      "grad_norm": 17.36653709411621,
      "learning_rate": 1.579197080291971e-05,
      "loss": 1.1455,
      "step": 93730
    },
    {
      "epoch": 684.2335766423357,
      "grad_norm": 8.922316551208496,
      "learning_rate": 1.5788321167883214e-05,
      "loss": 0.9863,
      "step": 93740
    },
    {
      "epoch": 684.3065693430657,
      "grad_norm": 0.13414804637432098,
      "learning_rate": 1.5784671532846715e-05,
      "loss": 0.6067,
      "step": 93750
    },
    {
      "epoch": 684.3795620437957,
      "grad_norm": 3.9737493991851807,
      "learning_rate": 1.578102189781022e-05,
      "loss": 0.7876,
      "step": 93760
    },
    {
      "epoch": 684.4525547445255,
      "grad_norm": 0.12177149206399918,
      "learning_rate": 1.5777372262773723e-05,
      "loss": 1.1121,
      "step": 93770
    },
    {
      "epoch": 684.5255474452555,
      "grad_norm": 9.061663627624512,
      "learning_rate": 1.5773722627737226e-05,
      "loss": 0.8357,
      "step": 93780
    },
    {
      "epoch": 684.5985401459855,
      "grad_norm": 0.08644615858793259,
      "learning_rate": 1.577007299270073e-05,
      "loss": 0.7595,
      "step": 93790
    },
    {
      "epoch": 684.6715328467153,
      "grad_norm": 6.7592267990112305,
      "learning_rate": 1.5766423357664234e-05,
      "loss": 0.6623,
      "step": 93800
    },
    {
      "epoch": 684.7445255474453,
      "grad_norm": 6.247246265411377,
      "learning_rate": 1.5762773722627738e-05,
      "loss": 0.7073,
      "step": 93810
    },
    {
      "epoch": 684.8175182481751,
      "grad_norm": 7.689968585968018,
      "learning_rate": 1.5759124087591242e-05,
      "loss": 0.9683,
      "step": 93820
    },
    {
      "epoch": 684.8905109489051,
      "grad_norm": 8.172928810119629,
      "learning_rate": 1.5755474452554746e-05,
      "loss": 0.8218,
      "step": 93830
    },
    {
      "epoch": 684.9635036496351,
      "grad_norm": 8.041428565979004,
      "learning_rate": 1.575182481751825e-05,
      "loss": 1.1336,
      "step": 93840
    },
    {
      "epoch": 685.0364963503649,
      "grad_norm": 6.371333122253418,
      "learning_rate": 1.5748175182481754e-05,
      "loss": 0.8295,
      "step": 93850
    },
    {
      "epoch": 685.1094890510949,
      "grad_norm": 8.594094276428223,
      "learning_rate": 1.5744525547445257e-05,
      "loss": 0.9975,
      "step": 93860
    },
    {
      "epoch": 685.1824817518249,
      "grad_norm": 9.545631408691406,
      "learning_rate": 1.5740875912408758e-05,
      "loss": 0.9812,
      "step": 93870
    },
    {
      "epoch": 685.2554744525547,
      "grad_norm": 2.281604290008545,
      "learning_rate": 1.5737226277372262e-05,
      "loss": 1.2268,
      "step": 93880
    },
    {
      "epoch": 685.3284671532847,
      "grad_norm": 6.539612770080566,
      "learning_rate": 1.573357664233577e-05,
      "loss": 0.815,
      "step": 93890
    },
    {
      "epoch": 685.4014598540145,
      "grad_norm": 13.706677436828613,
      "learning_rate": 1.5729927007299273e-05,
      "loss": 0.7929,
      "step": 93900
    },
    {
      "epoch": 685.4744525547445,
      "grad_norm": 0.08346135169267654,
      "learning_rate": 1.5726277372262774e-05,
      "loss": 0.9349,
      "step": 93910
    },
    {
      "epoch": 685.5474452554745,
      "grad_norm": 12.667367935180664,
      "learning_rate": 1.5722627737226277e-05,
      "loss": 0.5729,
      "step": 93920
    },
    {
      "epoch": 685.6204379562043,
      "grad_norm": 9.060879707336426,
      "learning_rate": 1.571897810218978e-05,
      "loss": 0.9004,
      "step": 93930
    },
    {
      "epoch": 685.6934306569343,
      "grad_norm": 12.531636238098145,
      "learning_rate": 1.5715328467153285e-05,
      "loss": 1.2429,
      "step": 93940
    },
    {
      "epoch": 685.7664233576643,
      "grad_norm": 11.661473274230957,
      "learning_rate": 1.571167883211679e-05,
      "loss": 0.9292,
      "step": 93950
    },
    {
      "epoch": 685.8394160583941,
      "grad_norm": 0.05874935910105705,
      "learning_rate": 1.5708029197080293e-05,
      "loss": 0.475,
      "step": 93960
    },
    {
      "epoch": 685.9124087591241,
      "grad_norm": 12.563704490661621,
      "learning_rate": 1.5704379562043797e-05,
      "loss": 0.7428,
      "step": 93970
    },
    {
      "epoch": 685.985401459854,
      "grad_norm": 8.256621360778809,
      "learning_rate": 1.5700729927007297e-05,
      "loss": 0.8795,
      "step": 93980
    },
    {
      "epoch": 686.0583941605839,
      "grad_norm": 12.688411712646484,
      "learning_rate": 1.5697080291970805e-05,
      "loss": 0.7674,
      "step": 93990
    },
    {
      "epoch": 686.1313868613139,
      "grad_norm": 8.675813674926758,
      "learning_rate": 1.569343065693431e-05,
      "loss": 0.9643,
      "step": 94000
    },
    {
      "epoch": 686.2043795620438,
      "grad_norm": 19.41986846923828,
      "learning_rate": 1.5689781021897812e-05,
      "loss": 0.9994,
      "step": 94010
    },
    {
      "epoch": 686.2773722627737,
      "grad_norm": 7.948812007904053,
      "learning_rate": 1.5686131386861313e-05,
      "loss": 0.6884,
      "step": 94020
    },
    {
      "epoch": 686.3503649635037,
      "grad_norm": 12.49246597290039,
      "learning_rate": 1.5682481751824817e-05,
      "loss": 1.1936,
      "step": 94030
    },
    {
      "epoch": 686.4233576642335,
      "grad_norm": 7.1962151527404785,
      "learning_rate": 1.5678832116788324e-05,
      "loss": 0.7468,
      "step": 94040
    },
    {
      "epoch": 686.4963503649635,
      "grad_norm": 16.49773406982422,
      "learning_rate": 1.5675182481751828e-05,
      "loss": 1.018,
      "step": 94050
    },
    {
      "epoch": 686.5693430656934,
      "grad_norm": 13.413128852844238,
      "learning_rate": 1.567153284671533e-05,
      "loss": 0.9333,
      "step": 94060
    },
    {
      "epoch": 686.6423357664233,
      "grad_norm": 8.935754776000977,
      "learning_rate": 1.5667883211678832e-05,
      "loss": 1.0013,
      "step": 94070
    },
    {
      "epoch": 686.7153284671533,
      "grad_norm": 12.553727149963379,
      "learning_rate": 1.5664233576642336e-05,
      "loss": 0.666,
      "step": 94080
    },
    {
      "epoch": 686.7883211678832,
      "grad_norm": 13.041232109069824,
      "learning_rate": 1.566058394160584e-05,
      "loss": 0.7182,
      "step": 94090
    },
    {
      "epoch": 686.8613138686131,
      "grad_norm": 9.483508110046387,
      "learning_rate": 1.5656934306569344e-05,
      "loss": 0.7966,
      "step": 94100
    },
    {
      "epoch": 686.9343065693431,
      "grad_norm": 10.088695526123047,
      "learning_rate": 1.5653284671532848e-05,
      "loss": 0.9276,
      "step": 94110
    },
    {
      "epoch": 687.007299270073,
      "grad_norm": 10.535470962524414,
      "learning_rate": 1.564963503649635e-05,
      "loss": 1.3316,
      "step": 94120
    },
    {
      "epoch": 687.0802919708029,
      "grad_norm": 9.309861183166504,
      "learning_rate": 1.5645985401459856e-05,
      "loss": 0.8979,
      "step": 94130
    },
    {
      "epoch": 687.1532846715329,
      "grad_norm": 7.547370910644531,
      "learning_rate": 1.564233576642336e-05,
      "loss": 0.8641,
      "step": 94140
    },
    {
      "epoch": 687.2262773722628,
      "grad_norm": 13.626850128173828,
      "learning_rate": 1.5638686131386863e-05,
      "loss": 1.0048,
      "step": 94150
    },
    {
      "epoch": 687.2992700729927,
      "grad_norm": 1.0076977014541626,
      "learning_rate": 1.5635036496350367e-05,
      "loss": 0.6554,
      "step": 94160
    },
    {
      "epoch": 687.3722627737226,
      "grad_norm": 8.095682144165039,
      "learning_rate": 1.5631386861313868e-05,
      "loss": 1.2472,
      "step": 94170
    },
    {
      "epoch": 687.4452554744526,
      "grad_norm": 8.091110229492188,
      "learning_rate": 1.562773722627737e-05,
      "loss": 0.6689,
      "step": 94180
    },
    {
      "epoch": 687.5182481751825,
      "grad_norm": 9.772848129272461,
      "learning_rate": 1.5624087591240875e-05,
      "loss": 0.9885,
      "step": 94190
    },
    {
      "epoch": 687.5912408759124,
      "grad_norm": 12.323141098022461,
      "learning_rate": 1.5620437956204383e-05,
      "loss": 0.9416,
      "step": 94200
    },
    {
      "epoch": 687.6642335766423,
      "grad_norm": 7.026002407073975,
      "learning_rate": 1.5616788321167883e-05,
      "loss": 0.7939,
      "step": 94210
    },
    {
      "epoch": 687.7372262773723,
      "grad_norm": 0.10639183968305588,
      "learning_rate": 1.5613138686131387e-05,
      "loss": 0.3093,
      "step": 94220
    },
    {
      "epoch": 687.8102189781022,
      "grad_norm": 8.892996788024902,
      "learning_rate": 1.560948905109489e-05,
      "loss": 0.6885,
      "step": 94230
    },
    {
      "epoch": 687.8832116788321,
      "grad_norm": 0.09732941538095474,
      "learning_rate": 1.5605839416058395e-05,
      "loss": 1.3959,
      "step": 94240
    },
    {
      "epoch": 687.956204379562,
      "grad_norm": 9.629289627075195,
      "learning_rate": 1.56021897810219e-05,
      "loss": 1.0288,
      "step": 94250
    },
    {
      "epoch": 688.029197080292,
      "grad_norm": 8.6681489944458,
      "learning_rate": 1.5598540145985403e-05,
      "loss": 0.8611,
      "step": 94260
    },
    {
      "epoch": 688.1021897810219,
      "grad_norm": 5.093156337738037,
      "learning_rate": 1.5594890510948907e-05,
      "loss": 0.8536,
      "step": 94270
    },
    {
      "epoch": 688.1751824817518,
      "grad_norm": 7.797277450561523,
      "learning_rate": 1.559124087591241e-05,
      "loss": 0.8703,
      "step": 94280
    },
    {
      "epoch": 688.2481751824818,
      "grad_norm": 13.137399673461914,
      "learning_rate": 1.558759124087591e-05,
      "loss": 0.8146,
      "step": 94290
    },
    {
      "epoch": 688.3211678832117,
      "grad_norm": 11.66827392578125,
      "learning_rate": 1.5583941605839418e-05,
      "loss": 0.7064,
      "step": 94300
    },
    {
      "epoch": 688.3941605839416,
      "grad_norm": 13.311079025268555,
      "learning_rate": 1.5580291970802922e-05,
      "loss": 0.9774,
      "step": 94310
    },
    {
      "epoch": 688.4671532846716,
      "grad_norm": 11.541851043701172,
      "learning_rate": 1.5576642335766426e-05,
      "loss": 0.4965,
      "step": 94320
    },
    {
      "epoch": 688.5401459854014,
      "grad_norm": 9.508208274841309,
      "learning_rate": 1.5572992700729926e-05,
      "loss": 1.1124,
      "step": 94330
    },
    {
      "epoch": 688.6131386861314,
      "grad_norm": 8.111897468566895,
      "learning_rate": 1.556934306569343e-05,
      "loss": 1.0689,
      "step": 94340
    },
    {
      "epoch": 688.6861313868613,
      "grad_norm": 10.514411926269531,
      "learning_rate": 1.5565693430656934e-05,
      "loss": 0.6689,
      "step": 94350
    },
    {
      "epoch": 688.7591240875912,
      "grad_norm": 7.143200874328613,
      "learning_rate": 1.5562043795620438e-05,
      "loss": 1.0108,
      "step": 94360
    },
    {
      "epoch": 688.8321167883212,
      "grad_norm": 11.092010498046875,
      "learning_rate": 1.5558394160583942e-05,
      "loss": 1.0652,
      "step": 94370
    },
    {
      "epoch": 688.9051094890511,
      "grad_norm": 10.735811233520508,
      "learning_rate": 1.5554744525547446e-05,
      "loss": 1.1667,
      "step": 94380
    },
    {
      "epoch": 688.978102189781,
      "grad_norm": 11.56208324432373,
      "learning_rate": 1.555109489051095e-05,
      "loss": 0.8789,
      "step": 94390
    },
    {
      "epoch": 689.051094890511,
      "grad_norm": 14.645379066467285,
      "learning_rate": 1.5547445255474454e-05,
      "loss": 0.828,
      "step": 94400
    },
    {
      "epoch": 689.1240875912408,
      "grad_norm": 7.674464702606201,
      "learning_rate": 1.5543795620437957e-05,
      "loss": 0.6315,
      "step": 94410
    },
    {
      "epoch": 689.1970802919708,
      "grad_norm": 2.6029653549194336,
      "learning_rate": 1.554014598540146e-05,
      "loss": 0.7257,
      "step": 94420
    },
    {
      "epoch": 689.2700729927008,
      "grad_norm": 10.411874771118164,
      "learning_rate": 1.5536496350364965e-05,
      "loss": 0.8909,
      "step": 94430
    },
    {
      "epoch": 689.3430656934306,
      "grad_norm": 18.96397590637207,
      "learning_rate": 1.5532846715328466e-05,
      "loss": 1.3695,
      "step": 94440
    },
    {
      "epoch": 689.4160583941606,
      "grad_norm": 5.91082239151001,
      "learning_rate": 1.552919708029197e-05,
      "loss": 1.1437,
      "step": 94450
    },
    {
      "epoch": 689.4890510948906,
      "grad_norm": 13.10491943359375,
      "learning_rate": 1.5525547445255477e-05,
      "loss": 1.1614,
      "step": 94460
    },
    {
      "epoch": 689.5620437956204,
      "grad_norm": 11.385388374328613,
      "learning_rate": 1.552189781021898e-05,
      "loss": 0.9219,
      "step": 94470
    },
    {
      "epoch": 689.6350364963504,
      "grad_norm": 0.0888378694653511,
      "learning_rate": 1.551824817518248e-05,
      "loss": 0.6511,
      "step": 94480
    },
    {
      "epoch": 689.7080291970802,
      "grad_norm": 6.964787483215332,
      "learning_rate": 1.5514598540145985e-05,
      "loss": 0.6948,
      "step": 94490
    },
    {
      "epoch": 689.7810218978102,
      "grad_norm": 7.982028961181641,
      "learning_rate": 1.551094890510949e-05,
      "loss": 0.8431,
      "step": 94500
    },
    {
      "epoch": 689.8540145985402,
      "grad_norm": 3.557415723800659,
      "learning_rate": 1.5507299270072996e-05,
      "loss": 0.8398,
      "step": 94510
    },
    {
      "epoch": 689.92700729927,
      "grad_norm": 19.460721969604492,
      "learning_rate": 1.5503649635036497e-05,
      "loss": 0.7876,
      "step": 94520
    },
    {
      "epoch": 690.0,
      "grad_norm": 9.26577091217041,
      "learning_rate": 1.55e-05,
      "loss": 1.0344,
      "step": 94530
    },
    {
      "epoch": 690.07299270073,
      "grad_norm": 6.6647443771362305,
      "learning_rate": 1.5496350364963505e-05,
      "loss": 0.9593,
      "step": 94540
    },
    {
      "epoch": 690.1459854014598,
      "grad_norm": 5.039515972137451,
      "learning_rate": 1.549270072992701e-05,
      "loss": 0.9545,
      "step": 94550
    },
    {
      "epoch": 690.2189781021898,
      "grad_norm": 10.917875289916992,
      "learning_rate": 1.5489051094890512e-05,
      "loss": 0.7899,
      "step": 94560
    },
    {
      "epoch": 690.2919708029198,
      "grad_norm": 7.467840194702148,
      "learning_rate": 1.5485401459854016e-05,
      "loss": 0.9217,
      "step": 94570
    },
    {
      "epoch": 690.3649635036496,
      "grad_norm": 1.1145219802856445,
      "learning_rate": 1.548175182481752e-05,
      "loss": 0.9705,
      "step": 94580
    },
    {
      "epoch": 690.4379562043796,
      "grad_norm": 0.0512663759291172,
      "learning_rate": 1.547810218978102e-05,
      "loss": 0.7607,
      "step": 94590
    },
    {
      "epoch": 690.5109489051094,
      "grad_norm": 5.446284294128418,
      "learning_rate": 1.5474452554744524e-05,
      "loss": 0.6137,
      "step": 94600
    },
    {
      "epoch": 690.5839416058394,
      "grad_norm": 8.746953964233398,
      "learning_rate": 1.5470802919708032e-05,
      "loss": 1.0801,
      "step": 94610
    },
    {
      "epoch": 690.6569343065694,
      "grad_norm": 9.809303283691406,
      "learning_rate": 1.5467153284671536e-05,
      "loss": 1.0612,
      "step": 94620
    },
    {
      "epoch": 690.7299270072992,
      "grad_norm": 7.07644510269165,
      "learning_rate": 1.5463503649635036e-05,
      "loss": 0.6383,
      "step": 94630
    },
    {
      "epoch": 690.8029197080292,
      "grad_norm": 12.847545623779297,
      "learning_rate": 1.545985401459854e-05,
      "loss": 1.306,
      "step": 94640
    },
    {
      "epoch": 690.8759124087592,
      "grad_norm": 17.779972076416016,
      "learning_rate": 1.5456204379562044e-05,
      "loss": 0.637,
      "step": 94650
    },
    {
      "epoch": 690.948905109489,
      "grad_norm": 7.345996379852295,
      "learning_rate": 1.5452554744525548e-05,
      "loss": 0.7384,
      "step": 94660
    },
    {
      "epoch": 691.021897810219,
      "grad_norm": 0.06828861683607101,
      "learning_rate": 1.544890510948905e-05,
      "loss": 1.1859,
      "step": 94670
    },
    {
      "epoch": 691.0948905109489,
      "grad_norm": 12.488775253295898,
      "learning_rate": 1.5445255474452556e-05,
      "loss": 0.9998,
      "step": 94680
    },
    {
      "epoch": 691.1678832116788,
      "grad_norm": 0.16449376940727234,
      "learning_rate": 1.544160583941606e-05,
      "loss": 0.9267,
      "step": 94690
    },
    {
      "epoch": 691.2408759124088,
      "grad_norm": 10.069709777832031,
      "learning_rate": 1.5437956204379563e-05,
      "loss": 0.6972,
      "step": 94700
    },
    {
      "epoch": 691.3138686131387,
      "grad_norm": 10.812235832214355,
      "learning_rate": 1.5434306569343067e-05,
      "loss": 0.9772,
      "step": 94710
    },
    {
      "epoch": 691.3868613138686,
      "grad_norm": 10.081412315368652,
      "learning_rate": 1.543065693430657e-05,
      "loss": 1.0443,
      "step": 94720
    },
    {
      "epoch": 691.4598540145986,
      "grad_norm": 8.020950317382812,
      "learning_rate": 1.5427007299270075e-05,
      "loss": 0.6287,
      "step": 94730
    },
    {
      "epoch": 691.5328467153284,
      "grad_norm": 12.835176467895508,
      "learning_rate": 1.542335766423358e-05,
      "loss": 0.9519,
      "step": 94740
    },
    {
      "epoch": 691.6058394160584,
      "grad_norm": 12.727360725402832,
      "learning_rate": 1.541970802919708e-05,
      "loss": 0.8435,
      "step": 94750
    },
    {
      "epoch": 691.6788321167883,
      "grad_norm": 9.89902114868164,
      "learning_rate": 1.5416058394160583e-05,
      "loss": 0.6486,
      "step": 94760
    },
    {
      "epoch": 691.7518248175182,
      "grad_norm": 10.105128288269043,
      "learning_rate": 1.541240875912409e-05,
      "loss": 0.9272,
      "step": 94770
    },
    {
      "epoch": 691.8248175182482,
      "grad_norm": 13.622953414916992,
      "learning_rate": 1.540875912408759e-05,
      "loss": 1.051,
      "step": 94780
    },
    {
      "epoch": 691.8978102189781,
      "grad_norm": 16.356962203979492,
      "learning_rate": 1.5405109489051095e-05,
      "loss": 1.0855,
      "step": 94790
    },
    {
      "epoch": 691.970802919708,
      "grad_norm": 11.297798156738281,
      "learning_rate": 1.54014598540146e-05,
      "loss": 0.7647,
      "step": 94800
    },
    {
      "epoch": 692.043795620438,
      "grad_norm": 5.186453819274902,
      "learning_rate": 1.5397810218978103e-05,
      "loss": 0.5192,
      "step": 94810
    },
    {
      "epoch": 692.1167883211679,
      "grad_norm": 10.67062759399414,
      "learning_rate": 1.5394160583941606e-05,
      "loss": 0.5754,
      "step": 94820
    },
    {
      "epoch": 692.1897810218978,
      "grad_norm": 0.056561652570962906,
      "learning_rate": 1.539051094890511e-05,
      "loss": 0.7852,
      "step": 94830
    },
    {
      "epoch": 692.2627737226277,
      "grad_norm": 4.9831461906433105,
      "learning_rate": 1.5386861313868614e-05,
      "loss": 0.7742,
      "step": 94840
    },
    {
      "epoch": 692.3357664233577,
      "grad_norm": 12.960990905761719,
      "learning_rate": 1.5383211678832118e-05,
      "loss": 0.9356,
      "step": 94850
    },
    {
      "epoch": 692.4087591240876,
      "grad_norm": 0.5949885249137878,
      "learning_rate": 1.537956204379562e-05,
      "loss": 0.8968,
      "step": 94860
    },
    {
      "epoch": 692.4817518248175,
      "grad_norm": 7.471829891204834,
      "learning_rate": 1.5375912408759126e-05,
      "loss": 1.4474,
      "step": 94870
    },
    {
      "epoch": 692.5547445255474,
      "grad_norm": 6.021839618682861,
      "learning_rate": 1.537226277372263e-05,
      "loss": 0.8733,
      "step": 94880
    },
    {
      "epoch": 692.6277372262774,
      "grad_norm": 7.426735877990723,
      "learning_rate": 1.5368613138686134e-05,
      "loss": 0.6357,
      "step": 94890
    },
    {
      "epoch": 692.7007299270073,
      "grad_norm": 7.698672771453857,
      "learning_rate": 1.5364963503649634e-05,
      "loss": 1.085,
      "step": 94900
    },
    {
      "epoch": 692.7737226277372,
      "grad_norm": 8.042972564697266,
      "learning_rate": 1.5361313868613138e-05,
      "loss": 1.1695,
      "step": 94910
    },
    {
      "epoch": 692.8467153284671,
      "grad_norm": 14.35197925567627,
      "learning_rate": 1.5357664233576642e-05,
      "loss": 0.8182,
      "step": 94920
    },
    {
      "epoch": 692.9197080291971,
      "grad_norm": 14.309986114501953,
      "learning_rate": 1.535401459854015e-05,
      "loss": 0.9773,
      "step": 94930
    },
    {
      "epoch": 692.992700729927,
      "grad_norm": 14.376790046691895,
      "learning_rate": 1.535036496350365e-05,
      "loss": 0.9327,
      "step": 94940
    },
    {
      "epoch": 693.0656934306569,
      "grad_norm": 0.05171295627951622,
      "learning_rate": 1.5346715328467154e-05,
      "loss": 0.8421,
      "step": 94950
    },
    {
      "epoch": 693.1386861313869,
      "grad_norm": 0.01732875593006611,
      "learning_rate": 1.5343065693430657e-05,
      "loss": 0.6861,
      "step": 94960
    },
    {
      "epoch": 693.2116788321168,
      "grad_norm": 16.16918182373047,
      "learning_rate": 1.533941605839416e-05,
      "loss": 1.287,
      "step": 94970
    },
    {
      "epoch": 693.2846715328467,
      "grad_norm": 11.19189453125,
      "learning_rate": 1.5335766423357665e-05,
      "loss": 0.9219,
      "step": 94980
    },
    {
      "epoch": 693.3576642335767,
      "grad_norm": 10.720248222351074,
      "learning_rate": 1.533211678832117e-05,
      "loss": 0.8749,
      "step": 94990
    },
    {
      "epoch": 693.4306569343066,
      "grad_norm": 0.01983865536749363,
      "learning_rate": 1.5328467153284673e-05,
      "loss": 0.8915,
      "step": 95000
    },
    {
      "epoch": 693.5036496350365,
      "grad_norm": 16.336408615112305,
      "learning_rate": 1.5324817518248173e-05,
      "loss": 0.9618,
      "step": 95010
    },
    {
      "epoch": 693.5766423357665,
      "grad_norm": 3.950605869293213,
      "learning_rate": 1.5321167883211677e-05,
      "loss": 0.8565,
      "step": 95020
    },
    {
      "epoch": 693.6496350364963,
      "grad_norm": 7.666001319885254,
      "learning_rate": 1.5317518248175185e-05,
      "loss": 0.9017,
      "step": 95030
    },
    {
      "epoch": 693.7226277372263,
      "grad_norm": 0.8236197233200073,
      "learning_rate": 1.531386861313869e-05,
      "loss": 0.9717,
      "step": 95040
    },
    {
      "epoch": 693.7956204379562,
      "grad_norm": 9.871770858764648,
      "learning_rate": 1.531021897810219e-05,
      "loss": 1.0616,
      "step": 95050
    },
    {
      "epoch": 693.8686131386861,
      "grad_norm": 7.5180768966674805,
      "learning_rate": 1.5306569343065693e-05,
      "loss": 1.0252,
      "step": 95060
    },
    {
      "epoch": 693.9416058394161,
      "grad_norm": 12.771360397338867,
      "learning_rate": 1.5302919708029197e-05,
      "loss": 0.9431,
      "step": 95070
    },
    {
      "epoch": 694.014598540146,
      "grad_norm": 11.251872062683105,
      "learning_rate": 1.5299270072992704e-05,
      "loss": 0.7218,
      "step": 95080
    },
    {
      "epoch": 694.0875912408759,
      "grad_norm": 9.90089225769043,
      "learning_rate": 1.5295620437956205e-05,
      "loss": 0.9787,
      "step": 95090
    },
    {
      "epoch": 694.1605839416059,
      "grad_norm": 7.02314567565918,
      "learning_rate": 1.529197080291971e-05,
      "loss": 0.6758,
      "step": 95100
    },
    {
      "epoch": 694.2335766423357,
      "grad_norm": 0.022123252972960472,
      "learning_rate": 1.5288321167883212e-05,
      "loss": 1.0089,
      "step": 95110
    },
    {
      "epoch": 694.3065693430657,
      "grad_norm": 2.155649423599243,
      "learning_rate": 1.5284671532846716e-05,
      "loss": 0.4987,
      "step": 95120
    },
    {
      "epoch": 694.3795620437957,
      "grad_norm": 7.2055253982543945,
      "learning_rate": 1.528102189781022e-05,
      "loss": 0.9763,
      "step": 95130
    },
    {
      "epoch": 694.4525547445255,
      "grad_norm": 0.06033245101571083,
      "learning_rate": 1.5277372262773724e-05,
      "loss": 0.4628,
      "step": 95140
    },
    {
      "epoch": 694.5255474452555,
      "grad_norm": 5.220654487609863,
      "learning_rate": 1.5273722627737228e-05,
      "loss": 0.796,
      "step": 95150
    },
    {
      "epoch": 694.5985401459855,
      "grad_norm": 7.809412479400635,
      "learning_rate": 1.5270072992700732e-05,
      "loss": 0.9166,
      "step": 95160
    },
    {
      "epoch": 694.6715328467153,
      "grad_norm": 14.062418937683105,
      "learning_rate": 1.5266423357664232e-05,
      "loss": 1.2572,
      "step": 95170
    },
    {
      "epoch": 694.7445255474453,
      "grad_norm": 13.279067993164062,
      "learning_rate": 1.526277372262774e-05,
      "loss": 0.7598,
      "step": 95180
    },
    {
      "epoch": 694.8175182481751,
      "grad_norm": 13.399815559387207,
      "learning_rate": 1.5259124087591243e-05,
      "loss": 1.0738,
      "step": 95190
    },
    {
      "epoch": 694.8905109489051,
      "grad_norm": 5.5883941650390625,
      "learning_rate": 1.5255474452554747e-05,
      "loss": 0.8334,
      "step": 95200
    },
    {
      "epoch": 694.9635036496351,
      "grad_norm": 0.024590924382209778,
      "learning_rate": 1.5251824817518248e-05,
      "loss": 0.9627,
      "step": 95210
    },
    {
      "epoch": 695.0364963503649,
      "grad_norm": 8.290975570678711,
      "learning_rate": 1.5248175182481753e-05,
      "loss": 1.6943,
      "step": 95220
    },
    {
      "epoch": 695.1094890510949,
      "grad_norm": 9.997873306274414,
      "learning_rate": 1.5244525547445257e-05,
      "loss": 0.9303,
      "step": 95230
    },
    {
      "epoch": 695.1824817518249,
      "grad_norm": 0.10668710619211197,
      "learning_rate": 1.524087591240876e-05,
      "loss": 1.23,
      "step": 95240
    },
    {
      "epoch": 695.2554744525547,
      "grad_norm": 9.694007873535156,
      "learning_rate": 1.5237226277372263e-05,
      "loss": 0.6331,
      "step": 95250
    },
    {
      "epoch": 695.3284671532847,
      "grad_norm": 8.551926612854004,
      "learning_rate": 1.5233576642335767e-05,
      "loss": 0.9335,
      "step": 95260
    },
    {
      "epoch": 695.4014598540145,
      "grad_norm": 15.18074893951416,
      "learning_rate": 1.5229927007299271e-05,
      "loss": 1.1854,
      "step": 95270
    },
    {
      "epoch": 695.4744525547445,
      "grad_norm": 7.361375331878662,
      "learning_rate": 1.5226277372262773e-05,
      "loss": 0.9084,
      "step": 95280
    },
    {
      "epoch": 695.5474452554745,
      "grad_norm": 10.500831604003906,
      "learning_rate": 1.5222627737226277e-05,
      "loss": 0.7949,
      "step": 95290
    },
    {
      "epoch": 695.6204379562043,
      "grad_norm": 13.374903678894043,
      "learning_rate": 1.5218978102189783e-05,
      "loss": 1.024,
      "step": 95300
    },
    {
      "epoch": 695.6934306569343,
      "grad_norm": 15.605853080749512,
      "learning_rate": 1.5215328467153287e-05,
      "loss": 0.9885,
      "step": 95310
    },
    {
      "epoch": 695.7664233576643,
      "grad_norm": 0.02916954644024372,
      "learning_rate": 1.5211678832116789e-05,
      "loss": 0.7718,
      "step": 95320
    },
    {
      "epoch": 695.8394160583941,
      "grad_norm": 0.024582896381616592,
      "learning_rate": 1.5208029197080293e-05,
      "loss": 0.609,
      "step": 95330
    },
    {
      "epoch": 695.9124087591241,
      "grad_norm": 9.936162948608398,
      "learning_rate": 1.5204379562043797e-05,
      "loss": 0.645,
      "step": 95340
    },
    {
      "epoch": 695.985401459854,
      "grad_norm": 7.751425266265869,
      "learning_rate": 1.52007299270073e-05,
      "loss": 0.8542,
      "step": 95350
    },
    {
      "epoch": 696.0583941605839,
      "grad_norm": 12.455477714538574,
      "learning_rate": 1.5197080291970803e-05,
      "loss": 0.8969,
      "step": 95360
    },
    {
      "epoch": 696.1313868613139,
      "grad_norm": 9.538239479064941,
      "learning_rate": 1.5193430656934306e-05,
      "loss": 0.5968,
      "step": 95370
    },
    {
      "epoch": 696.2043795620438,
      "grad_norm": 6.501976490020752,
      "learning_rate": 1.5189781021897812e-05,
      "loss": 0.5291,
      "step": 95380
    },
    {
      "epoch": 696.2773722627737,
      "grad_norm": 15.584795951843262,
      "learning_rate": 1.5186131386861316e-05,
      "loss": 1.0738,
      "step": 95390
    },
    {
      "epoch": 696.3503649635037,
      "grad_norm": 9.857061386108398,
      "learning_rate": 1.5182481751824818e-05,
      "loss": 0.6577,
      "step": 95400
    },
    {
      "epoch": 696.4233576642335,
      "grad_norm": 9.715460777282715,
      "learning_rate": 1.5178832116788322e-05,
      "loss": 1.1319,
      "step": 95410
    },
    {
      "epoch": 696.4963503649635,
      "grad_norm": 9.377257347106934,
      "learning_rate": 1.5175182481751826e-05,
      "loss": 0.9674,
      "step": 95420
    },
    {
      "epoch": 696.5693430656934,
      "grad_norm": 14.488317489624023,
      "learning_rate": 1.5171532846715328e-05,
      "loss": 0.8925,
      "step": 95430
    },
    {
      "epoch": 696.6423357664233,
      "grad_norm": 11.536725997924805,
      "learning_rate": 1.5167883211678832e-05,
      "loss": 1.1828,
      "step": 95440
    },
    {
      "epoch": 696.7153284671533,
      "grad_norm": 12.622760772705078,
      "learning_rate": 1.5164233576642336e-05,
      "loss": 0.783,
      "step": 95450
    },
    {
      "epoch": 696.7883211678832,
      "grad_norm": 0.08905677497386932,
      "learning_rate": 1.5160583941605841e-05,
      "loss": 0.5252,
      "step": 95460
    },
    {
      "epoch": 696.8613138686131,
      "grad_norm": 13.4007568359375,
      "learning_rate": 1.5156934306569342e-05,
      "loss": 0.6905,
      "step": 95470
    },
    {
      "epoch": 696.9343065693431,
      "grad_norm": 0.027134397998452187,
      "learning_rate": 1.5153284671532847e-05,
      "loss": 1.1398,
      "step": 95480
    },
    {
      "epoch": 697.007299270073,
      "grad_norm": 6.557363033294678,
      "learning_rate": 1.5149635036496351e-05,
      "loss": 1.0921,
      "step": 95490
    },
    {
      "epoch": 697.0802919708029,
      "grad_norm": 8.671771049499512,
      "learning_rate": 1.5145985401459855e-05,
      "loss": 0.7225,
      "step": 95500
    },
    {
      "epoch": 697.1532846715329,
      "grad_norm": 5.584451198577881,
      "learning_rate": 1.5142335766423357e-05,
      "loss": 0.9601,
      "step": 95510
    },
    {
      "epoch": 697.2262773722628,
      "grad_norm": 0.0656348392367363,
      "learning_rate": 1.5138686131386861e-05,
      "loss": 0.9633,
      "step": 95520
    },
    {
      "epoch": 697.2992700729927,
      "grad_norm": 6.618007659912109,
      "learning_rate": 1.5135036496350365e-05,
      "loss": 0.7744,
      "step": 95530
    },
    {
      "epoch": 697.3722627737226,
      "grad_norm": 0.4711422622203827,
      "learning_rate": 1.513138686131387e-05,
      "loss": 0.4253,
      "step": 95540
    },
    {
      "epoch": 697.4452554744526,
      "grad_norm": 6.912496089935303,
      "learning_rate": 1.5127737226277371e-05,
      "loss": 0.9552,
      "step": 95550
    },
    {
      "epoch": 697.5182481751825,
      "grad_norm": 12.735308647155762,
      "learning_rate": 1.5124087591240877e-05,
      "loss": 1.0919,
      "step": 95560
    },
    {
      "epoch": 697.5912408759124,
      "grad_norm": 6.997195243835449,
      "learning_rate": 1.512043795620438e-05,
      "loss": 1.0082,
      "step": 95570
    },
    {
      "epoch": 697.6642335766423,
      "grad_norm": 0.026186872273683548,
      "learning_rate": 1.5116788321167885e-05,
      "loss": 0.9983,
      "step": 95580
    },
    {
      "epoch": 697.7372262773723,
      "grad_norm": 0.16368266940116882,
      "learning_rate": 1.5113138686131387e-05,
      "loss": 0.8478,
      "step": 95590
    },
    {
      "epoch": 697.8102189781022,
      "grad_norm": 10.814970016479492,
      "learning_rate": 1.510948905109489e-05,
      "loss": 1.2421,
      "step": 95600
    },
    {
      "epoch": 697.8832116788321,
      "grad_norm": 7.21981954574585,
      "learning_rate": 1.5105839416058396e-05,
      "loss": 1.0543,
      "step": 95610
    },
    {
      "epoch": 697.956204379562,
      "grad_norm": 13.439547538757324,
      "learning_rate": 1.51021897810219e-05,
      "loss": 1.2725,
      "step": 95620
    },
    {
      "epoch": 698.029197080292,
      "grad_norm": 13.497934341430664,
      "learning_rate": 1.50985401459854e-05,
      "loss": 0.6728,
      "step": 95630
    },
    {
      "epoch": 698.1021897810219,
      "grad_norm": 16.5365047454834,
      "learning_rate": 1.5094890510948906e-05,
      "loss": 0.9211,
      "step": 95640
    },
    {
      "epoch": 698.1751824817518,
      "grad_norm": 0.02940371446311474,
      "learning_rate": 1.509124087591241e-05,
      "loss": 0.7752,
      "step": 95650
    },
    {
      "epoch": 698.2481751824818,
      "grad_norm": 7.575867176055908,
      "learning_rate": 1.5087591240875912e-05,
      "loss": 0.7988,
      "step": 95660
    },
    {
      "epoch": 698.3211678832117,
      "grad_norm": 0.016602933406829834,
      "learning_rate": 1.5083941605839416e-05,
      "loss": 0.6012,
      "step": 95670
    },
    {
      "epoch": 698.3941605839416,
      "grad_norm": 11.471314430236816,
      "learning_rate": 1.508029197080292e-05,
      "loss": 0.7886,
      "step": 95680
    },
    {
      "epoch": 698.4671532846716,
      "grad_norm": 7.43477725982666,
      "learning_rate": 1.5076642335766426e-05,
      "loss": 0.7182,
      "step": 95690
    },
    {
      "epoch": 698.5401459854014,
      "grad_norm": 0.07280924916267395,
      "learning_rate": 1.5072992700729926e-05,
      "loss": 1.1314,
      "step": 95700
    },
    {
      "epoch": 698.6131386861314,
      "grad_norm": 12.896406173706055,
      "learning_rate": 1.5069343065693432e-05,
      "loss": 1.1189,
      "step": 95710
    },
    {
      "epoch": 698.6861313868613,
      "grad_norm": 0.12443926930427551,
      "learning_rate": 1.5065693430656936e-05,
      "loss": 0.9477,
      "step": 95720
    },
    {
      "epoch": 698.7591240875912,
      "grad_norm": 13.009439468383789,
      "learning_rate": 1.506204379562044e-05,
      "loss": 0.9474,
      "step": 95730
    },
    {
      "epoch": 698.8321167883212,
      "grad_norm": 17.941057205200195,
      "learning_rate": 1.5058394160583942e-05,
      "loss": 0.9723,
      "step": 95740
    },
    {
      "epoch": 698.9051094890511,
      "grad_norm": 0.054830752313137054,
      "learning_rate": 1.5054744525547446e-05,
      "loss": 1.1147,
      "step": 95750
    },
    {
      "epoch": 698.978102189781,
      "grad_norm": 0.01870136521756649,
      "learning_rate": 1.505109489051095e-05,
      "loss": 0.8981,
      "step": 95760
    },
    {
      "epoch": 699.051094890511,
      "grad_norm": 20.329238891601562,
      "learning_rate": 1.5047445255474455e-05,
      "loss": 0.7024,
      "step": 95770
    },
    {
      "epoch": 699.1240875912408,
      "grad_norm": 12.57669734954834,
      "learning_rate": 1.5043795620437955e-05,
      "loss": 0.8634,
      "step": 95780
    },
    {
      "epoch": 699.1970802919708,
      "grad_norm": 21.16802406311035,
      "learning_rate": 1.5040145985401461e-05,
      "loss": 1.5794,
      "step": 95790
    },
    {
      "epoch": 699.2700729927008,
      "grad_norm": 8.183147430419922,
      "learning_rate": 1.5036496350364965e-05,
      "loss": 0.4687,
      "step": 95800
    },
    {
      "epoch": 699.3430656934306,
      "grad_norm": 11.263612747192383,
      "learning_rate": 1.5032846715328469e-05,
      "loss": 1.005,
      "step": 95810
    },
    {
      "epoch": 699.4160583941606,
      "grad_norm": 6.561662673950195,
      "learning_rate": 1.5029197080291971e-05,
      "loss": 0.8804,
      "step": 95820
    },
    {
      "epoch": 699.4890510948906,
      "grad_norm": 12.08080005645752,
      "learning_rate": 1.5025547445255475e-05,
      "loss": 1.0588,
      "step": 95830
    },
    {
      "epoch": 699.5620437956204,
      "grad_norm": 7.014745235443115,
      "learning_rate": 1.5021897810218979e-05,
      "loss": 1.0667,
      "step": 95840
    },
    {
      "epoch": 699.6350364963504,
      "grad_norm": 7.610304355621338,
      "learning_rate": 1.5018248175182484e-05,
      "loss": 1.0014,
      "step": 95850
    },
    {
      "epoch": 699.7080291970802,
      "grad_norm": 16.033092498779297,
      "learning_rate": 1.5014598540145985e-05,
      "loss": 0.7811,
      "step": 95860
    },
    {
      "epoch": 699.7810218978102,
      "grad_norm": 20.539613723754883,
      "learning_rate": 1.501094890510949e-05,
      "loss": 1.255,
      "step": 95870
    },
    {
      "epoch": 699.8540145985402,
      "grad_norm": 12.151433944702148,
      "learning_rate": 1.5007299270072994e-05,
      "loss": 0.5661,
      "step": 95880
    },
    {
      "epoch": 699.92700729927,
      "grad_norm": 13.31921672821045,
      "learning_rate": 1.5003649635036496e-05,
      "loss": 0.9009,
      "step": 95890
    },
    {
      "epoch": 700.0,
      "grad_norm": 15.789214134216309,
      "learning_rate": 1.5e-05,
      "loss": 0.6874,
      "step": 95900
    },
    {
      "epoch": 700.07299270073,
      "grad_norm": 0.11687445640563965,
      "learning_rate": 1.4996350364963504e-05,
      "loss": 0.9231,
      "step": 95910
    },
    {
      "epoch": 700.1459854014598,
      "grad_norm": 15.185442924499512,
      "learning_rate": 1.4992700729927008e-05,
      "loss": 0.8519,
      "step": 95920
    },
    {
      "epoch": 700.2189781021898,
      "grad_norm": 0.11142134666442871,
      "learning_rate": 1.498905109489051e-05,
      "loss": 0.3652,
      "step": 95930
    },
    {
      "epoch": 700.2919708029198,
      "grad_norm": 6.9485321044921875,
      "learning_rate": 1.4985401459854014e-05,
      "loss": 1.0481,
      "step": 95940
    },
    {
      "epoch": 700.3649635036496,
      "grad_norm": 7.551177501678467,
      "learning_rate": 1.498175182481752e-05,
      "loss": 0.9664,
      "step": 95950
    },
    {
      "epoch": 700.4379562043796,
      "grad_norm": 7.83229398727417,
      "learning_rate": 1.4978102189781024e-05,
      "loss": 0.773,
      "step": 95960
    },
    {
      "epoch": 700.5109489051094,
      "grad_norm": 9.905197143554688,
      "learning_rate": 1.4974452554744526e-05,
      "loss": 0.9075,
      "step": 95970
    },
    {
      "epoch": 700.5839416058394,
      "grad_norm": 11.486283302307129,
      "learning_rate": 1.497080291970803e-05,
      "loss": 0.7663,
      "step": 95980
    },
    {
      "epoch": 700.6569343065694,
      "grad_norm": 7.7363715171813965,
      "learning_rate": 1.4967153284671534e-05,
      "loss": 0.8702,
      "step": 95990
    },
    {
      "epoch": 700.7299270072992,
      "grad_norm": 7.795340538024902,
      "learning_rate": 1.496350364963504e-05,
      "loss": 0.7686,
      "step": 96000
    },
    {
      "epoch": 700.8029197080292,
      "grad_norm": 11.764256477355957,
      "learning_rate": 1.495985401459854e-05,
      "loss": 0.9284,
      "step": 96010
    },
    {
      "epoch": 700.8759124087592,
      "grad_norm": 10.43863296508789,
      "learning_rate": 1.4956204379562044e-05,
      "loss": 1.1214,
      "step": 96020
    },
    {
      "epoch": 700.948905109489,
      "grad_norm": 12.604869842529297,
      "learning_rate": 1.495255474452555e-05,
      "loss": 1.1767,
      "step": 96030
    },
    {
      "epoch": 701.021897810219,
      "grad_norm": 7.797330379486084,
      "learning_rate": 1.4948905109489053e-05,
      "loss": 1.1471,
      "step": 96040
    },
    {
      "epoch": 701.0948905109489,
      "grad_norm": 6.94890022277832,
      "learning_rate": 1.4945255474452555e-05,
      "loss": 0.4621,
      "step": 96050
    },
    {
      "epoch": 701.1678832116788,
      "grad_norm": 6.874515056610107,
      "learning_rate": 1.4941605839416059e-05,
      "loss": 0.8865,
      "step": 96060
    },
    {
      "epoch": 701.2408759124088,
      "grad_norm": 12.563875198364258,
      "learning_rate": 1.4937956204379563e-05,
      "loss": 0.9377,
      "step": 96070
    },
    {
      "epoch": 701.3138686131387,
      "grad_norm": 7.2653045654296875,
      "learning_rate": 1.4934306569343065e-05,
      "loss": 1.0831,
      "step": 96080
    },
    {
      "epoch": 701.3868613138686,
      "grad_norm": 1.5317643880844116,
      "learning_rate": 1.4930656934306569e-05,
      "loss": 0.5256,
      "step": 96090
    },
    {
      "epoch": 701.4598540145986,
      "grad_norm": 9.130857467651367,
      "learning_rate": 1.4927007299270075e-05,
      "loss": 0.8608,
      "step": 96100
    },
    {
      "epoch": 701.5328467153284,
      "grad_norm": 8.151460647583008,
      "learning_rate": 1.4923357664233579e-05,
      "loss": 1.0337,
      "step": 96110
    },
    {
      "epoch": 701.6058394160584,
      "grad_norm": 7.105827808380127,
      "learning_rate": 1.4919708029197079e-05,
      "loss": 0.6365,
      "step": 96120
    },
    {
      "epoch": 701.6788321167883,
      "grad_norm": 13.62234115600586,
      "learning_rate": 1.4916058394160585e-05,
      "loss": 1.0068,
      "step": 96130
    },
    {
      "epoch": 701.7518248175182,
      "grad_norm": 13.817428588867188,
      "learning_rate": 1.4912408759124088e-05,
      "loss": 0.8155,
      "step": 96140
    },
    {
      "epoch": 701.8248175182482,
      "grad_norm": 13.106060981750488,
      "learning_rate": 1.4908759124087592e-05,
      "loss": 1.0252,
      "step": 96150
    },
    {
      "epoch": 701.8978102189781,
      "grad_norm": 17.147552490234375,
      "learning_rate": 1.4905109489051095e-05,
      "loss": 1.0815,
      "step": 96160
    },
    {
      "epoch": 701.970802919708,
      "grad_norm": 0.0807456523180008,
      "learning_rate": 1.4901459854014598e-05,
      "loss": 0.9358,
      "step": 96170
    },
    {
      "epoch": 702.043795620438,
      "grad_norm": 7.815658092498779,
      "learning_rate": 1.4897810218978104e-05,
      "loss": 0.6916,
      "step": 96180
    },
    {
      "epoch": 702.1167883211679,
      "grad_norm": 4.188173770904541,
      "learning_rate": 1.4894160583941608e-05,
      "loss": 0.9899,
      "step": 96190
    },
    {
      "epoch": 702.1897810218978,
      "grad_norm": 14.305022239685059,
      "learning_rate": 1.4890510948905108e-05,
      "loss": 1.0732,
      "step": 96200
    },
    {
      "epoch": 702.2627737226277,
      "grad_norm": 0.5226206183433533,
      "learning_rate": 1.4886861313868614e-05,
      "loss": 0.845,
      "step": 96210
    },
    {
      "epoch": 702.3357664233577,
      "grad_norm": 10.662260055541992,
      "learning_rate": 1.4883211678832118e-05,
      "loss": 0.7754,
      "step": 96220
    },
    {
      "epoch": 702.4087591240876,
      "grad_norm": 6.743717670440674,
      "learning_rate": 1.4879562043795622e-05,
      "loss": 0.5905,
      "step": 96230
    },
    {
      "epoch": 702.4817518248175,
      "grad_norm": 13.522378921508789,
      "learning_rate": 1.4875912408759124e-05,
      "loss": 0.9681,
      "step": 96240
    },
    {
      "epoch": 702.5547445255474,
      "grad_norm": 14.104513168334961,
      "learning_rate": 1.4872262773722628e-05,
      "loss": 0.9257,
      "step": 96250
    },
    {
      "epoch": 702.6277372262774,
      "grad_norm": 8.870635986328125,
      "learning_rate": 1.4868613138686133e-05,
      "loss": 1.0115,
      "step": 96260
    },
    {
      "epoch": 702.7007299270073,
      "grad_norm": 12.18837833404541,
      "learning_rate": 1.4864963503649637e-05,
      "loss": 0.9496,
      "step": 96270
    },
    {
      "epoch": 702.7737226277372,
      "grad_norm": 7.725560665130615,
      "learning_rate": 1.486131386861314e-05,
      "loss": 1.1708,
      "step": 96280
    },
    {
      "epoch": 702.8467153284671,
      "grad_norm": 7.603423118591309,
      "learning_rate": 1.4857664233576643e-05,
      "loss": 0.6119,
      "step": 96290
    },
    {
      "epoch": 702.9197080291971,
      "grad_norm": 6.053908348083496,
      "learning_rate": 1.4854014598540147e-05,
      "loss": 1.143,
      "step": 96300
    },
    {
      "epoch": 702.992700729927,
      "grad_norm": 0.08706792443990707,
      "learning_rate": 1.485036496350365e-05,
      "loss": 0.6919,
      "step": 96310
    },
    {
      "epoch": 703.0656934306569,
      "grad_norm": 7.648214340209961,
      "learning_rate": 1.4846715328467153e-05,
      "loss": 1.2051,
      "step": 96320
    },
    {
      "epoch": 703.1386861313869,
      "grad_norm": 1.7205849885940552,
      "learning_rate": 1.4843065693430657e-05,
      "loss": 0.8772,
      "step": 96330
    },
    {
      "epoch": 703.2116788321168,
      "grad_norm": 0.06972428411245346,
      "learning_rate": 1.4839416058394163e-05,
      "loss": 0.8748,
      "step": 96340
    },
    {
      "epoch": 703.2846715328467,
      "grad_norm": 12.705344200134277,
      "learning_rate": 1.4835766423357663e-05,
      "loss": 0.5771,
      "step": 96350
    },
    {
      "epoch": 703.3576642335767,
      "grad_norm": 9.295868873596191,
      "learning_rate": 1.4832116788321169e-05,
      "loss": 1.2458,
      "step": 96360
    },
    {
      "epoch": 703.4306569343066,
      "grad_norm": 17.897762298583984,
      "learning_rate": 1.4828467153284673e-05,
      "loss": 1.2252,
      "step": 96370
    },
    {
      "epoch": 703.5036496350365,
      "grad_norm": 7.689267635345459,
      "learning_rate": 1.4824817518248177e-05,
      "loss": 1.1721,
      "step": 96380
    },
    {
      "epoch": 703.5766423357665,
      "grad_norm": 8.22140884399414,
      "learning_rate": 1.4821167883211679e-05,
      "loss": 0.6958,
      "step": 96390
    },
    {
      "epoch": 703.6496350364963,
      "grad_norm": 10.148207664489746,
      "learning_rate": 1.4817518248175183e-05,
      "loss": 0.7638,
      "step": 96400
    },
    {
      "epoch": 703.7226277372263,
      "grad_norm": 7.299142837524414,
      "learning_rate": 1.4813868613138687e-05,
      "loss": 1.0424,
      "step": 96410
    },
    {
      "epoch": 703.7956204379562,
      "grad_norm": 6.829585075378418,
      "learning_rate": 1.4810218978102192e-05,
      "loss": 0.8508,
      "step": 96420
    },
    {
      "epoch": 703.8686131386861,
      "grad_norm": 9.49105167388916,
      "learning_rate": 1.4806569343065693e-05,
      "loss": 0.7903,
      "step": 96430
    },
    {
      "epoch": 703.9416058394161,
      "grad_norm": 9.32565689086914,
      "learning_rate": 1.4802919708029198e-05,
      "loss": 0.8419,
      "step": 96440
    },
    {
      "epoch": 704.014598540146,
      "grad_norm": 11.714797973632812,
      "learning_rate": 1.4799270072992702e-05,
      "loss": 1.1049,
      "step": 96450
    },
    {
      "epoch": 704.0875912408759,
      "grad_norm": 6.473848342895508,
      "learning_rate": 1.4795620437956206e-05,
      "loss": 1.2492,
      "step": 96460
    },
    {
      "epoch": 704.1605839416059,
      "grad_norm": 6.3964009284973145,
      "learning_rate": 1.4791970802919708e-05,
      "loss": 0.7019,
      "step": 96470
    },
    {
      "epoch": 704.2335766423357,
      "grad_norm": 0.03439110144972801,
      "learning_rate": 1.4788321167883212e-05,
      "loss": 0.5321,
      "step": 96480
    },
    {
      "epoch": 704.3065693430657,
      "grad_norm": 1.6210638284683228,
      "learning_rate": 1.4784671532846716e-05,
      "loss": 1.0775,
      "step": 96490
    },
    {
      "epoch": 704.3795620437957,
      "grad_norm": 11.878878593444824,
      "learning_rate": 1.4781021897810221e-05,
      "loss": 1.1357,
      "step": 96500
    },
    {
      "epoch": 704.4525547445255,
      "grad_norm": 12.085667610168457,
      "learning_rate": 1.4777372262773722e-05,
      "loss": 0.9567,
      "step": 96510
    },
    {
      "epoch": 704.5255474452555,
      "grad_norm": 7.506523609161377,
      "learning_rate": 1.4773722627737228e-05,
      "loss": 0.7733,
      "step": 96520
    },
    {
      "epoch": 704.5985401459855,
      "grad_norm": 14.84958267211914,
      "learning_rate": 1.4770072992700731e-05,
      "loss": 0.6332,
      "step": 96530
    },
    {
      "epoch": 704.6715328467153,
      "grad_norm": 12.073800086975098,
      "learning_rate": 1.4766423357664234e-05,
      "loss": 1.2924,
      "step": 96540
    },
    {
      "epoch": 704.7445255474453,
      "grad_norm": 13.552982330322266,
      "learning_rate": 1.4762773722627737e-05,
      "loss": 0.7976,
      "step": 96550
    },
    {
      "epoch": 704.8175182481751,
      "grad_norm": 18.57533836364746,
      "learning_rate": 1.4759124087591241e-05,
      "loss": 0.788,
      "step": 96560
    },
    {
      "epoch": 704.8905109489051,
      "grad_norm": 6.527493000030518,
      "learning_rate": 1.4755474452554747e-05,
      "loss": 1.0952,
      "step": 96570
    },
    {
      "epoch": 704.9635036496351,
      "grad_norm": 0.046873871237039566,
      "learning_rate": 1.4751824817518247e-05,
      "loss": 0.4956,
      "step": 96580
    },
    {
      "epoch": 705.0364963503649,
      "grad_norm": 6.424609661102295,
      "learning_rate": 1.4748175182481751e-05,
      "loss": 1.2435,
      "step": 96590
    },
    {
      "epoch": 705.1094890510949,
      "grad_norm": 7.557681083679199,
      "learning_rate": 1.4744525547445257e-05,
      "loss": 0.7542,
      "step": 96600
    },
    {
      "epoch": 705.1824817518249,
      "grad_norm": 9.570719718933105,
      "learning_rate": 1.474087591240876e-05,
      "loss": 1.4626,
      "step": 96610
    },
    {
      "epoch": 705.2554744525547,
      "grad_norm": 15.632061958312988,
      "learning_rate": 1.4737226277372263e-05,
      "loss": 0.9096,
      "step": 96620
    },
    {
      "epoch": 705.3284671532847,
      "grad_norm": 19.088937759399414,
      "learning_rate": 1.4733576642335767e-05,
      "loss": 0.791,
      "step": 96630
    },
    {
      "epoch": 705.4014598540145,
      "grad_norm": 9.837580680847168,
      "learning_rate": 1.472992700729927e-05,
      "loss": 0.7197,
      "step": 96640
    },
    {
      "epoch": 705.4744525547445,
      "grad_norm": 7.895803451538086,
      "learning_rate": 1.4726277372262776e-05,
      "loss": 0.9864,
      "step": 96650
    },
    {
      "epoch": 705.5474452554745,
      "grad_norm": 20.43094825744629,
      "learning_rate": 1.4722627737226277e-05,
      "loss": 0.6818,
      "step": 96660
    },
    {
      "epoch": 705.6204379562043,
      "grad_norm": 0.1397707611322403,
      "learning_rate": 1.4718978102189782e-05,
      "loss": 0.8782,
      "step": 96670
    },
    {
      "epoch": 705.6934306569343,
      "grad_norm": 8.384350776672363,
      "learning_rate": 1.4715328467153286e-05,
      "loss": 0.8016,
      "step": 96680
    },
    {
      "epoch": 705.7664233576643,
      "grad_norm": 21.693004608154297,
      "learning_rate": 1.471167883211679e-05,
      "loss": 0.8837,
      "step": 96690
    },
    {
      "epoch": 705.8394160583941,
      "grad_norm": 9.887126922607422,
      "learning_rate": 1.4708029197080292e-05,
      "loss": 0.9761,
      "step": 96700
    },
    {
      "epoch": 705.9124087591241,
      "grad_norm": 8.534220695495605,
      "learning_rate": 1.4704379562043796e-05,
      "loss": 0.8083,
      "step": 96710
    },
    {
      "epoch": 705.985401459854,
      "grad_norm": 0.07751171290874481,
      "learning_rate": 1.47007299270073e-05,
      "loss": 0.6051,
      "step": 96720
    },
    {
      "epoch": 706.0583941605839,
      "grad_norm": 11.024389266967773,
      "learning_rate": 1.4697080291970802e-05,
      "loss": 0.8551,
      "step": 96730
    },
    {
      "epoch": 706.1313868613139,
      "grad_norm": 10.476664543151855,
      "learning_rate": 1.4693430656934306e-05,
      "loss": 0.7186,
      "step": 96740
    },
    {
      "epoch": 706.2043795620438,
      "grad_norm": 8.48235034942627,
      "learning_rate": 1.4689781021897812e-05,
      "loss": 0.9209,
      "step": 96750
    },
    {
      "epoch": 706.2773722627737,
      "grad_norm": 0.17239412665367126,
      "learning_rate": 1.4686131386861316e-05,
      "loss": 0.8324,
      "step": 96760
    },
    {
      "epoch": 706.3503649635037,
      "grad_norm": 17.218076705932617,
      "learning_rate": 1.4682481751824818e-05,
      "loss": 0.9042,
      "step": 96770
    },
    {
      "epoch": 706.4233576642335,
      "grad_norm": 10.779637336730957,
      "learning_rate": 1.4678832116788322e-05,
      "loss": 0.9567,
      "step": 96780
    },
    {
      "epoch": 706.4963503649635,
      "grad_norm": 10.084335327148438,
      "learning_rate": 1.4675182481751826e-05,
      "loss": 0.8938,
      "step": 96790
    },
    {
      "epoch": 706.5693430656934,
      "grad_norm": 14.620577812194824,
      "learning_rate": 1.467153284671533e-05,
      "loss": 0.7401,
      "step": 96800
    },
    {
      "epoch": 706.6423357664233,
      "grad_norm": 7.908123016357422,
      "learning_rate": 1.4667883211678832e-05,
      "loss": 0.9741,
      "step": 96810
    },
    {
      "epoch": 706.7153284671533,
      "grad_norm": 6.407804012298584,
      "learning_rate": 1.4664233576642336e-05,
      "loss": 1.2337,
      "step": 96820
    },
    {
      "epoch": 706.7883211678832,
      "grad_norm": 0.016508495435118675,
      "learning_rate": 1.4660583941605841e-05,
      "loss": 0.5553,
      "step": 96830
    },
    {
      "epoch": 706.8613138686131,
      "grad_norm": 8.012336730957031,
      "learning_rate": 1.4656934306569345e-05,
      "loss": 0.7447,
      "step": 96840
    },
    {
      "epoch": 706.9343065693431,
      "grad_norm": 9.17755126953125,
      "learning_rate": 1.4653284671532847e-05,
      "loss": 0.87,
      "step": 96850
    },
    {
      "epoch": 707.007299270073,
      "grad_norm": 1.4052900075912476,
      "learning_rate": 1.4649635036496351e-05,
      "loss": 1.0057,
      "step": 96860
    },
    {
      "epoch": 707.0802919708029,
      "grad_norm": 7.0833611488342285,
      "learning_rate": 1.4645985401459855e-05,
      "loss": 0.6722,
      "step": 96870
    },
    {
      "epoch": 707.1532846715329,
      "grad_norm": 18.66544532775879,
      "learning_rate": 1.4642335766423359e-05,
      "loss": 1.1708,
      "step": 96880
    },
    {
      "epoch": 707.2262773722628,
      "grad_norm": 12.53812026977539,
      "learning_rate": 1.4638686131386861e-05,
      "loss": 1.1104,
      "step": 96890
    },
    {
      "epoch": 707.2992700729927,
      "grad_norm": 8.674849510192871,
      "learning_rate": 1.4635036496350365e-05,
      "loss": 1.0254,
      "step": 96900
    },
    {
      "epoch": 707.3722627737226,
      "grad_norm": 9.362870216369629,
      "learning_rate": 1.463138686131387e-05,
      "loss": 0.7997,
      "step": 96910
    },
    {
      "epoch": 707.4452554744526,
      "grad_norm": 7.077062606811523,
      "learning_rate": 1.4627737226277374e-05,
      "loss": 0.6668,
      "step": 96920
    },
    {
      "epoch": 707.5182481751825,
      "grad_norm": 6.402164459228516,
      "learning_rate": 1.4624087591240877e-05,
      "loss": 0.7264,
      "step": 96930
    },
    {
      "epoch": 707.5912408759124,
      "grad_norm": 11.174155235290527,
      "learning_rate": 1.462043795620438e-05,
      "loss": 0.8568,
      "step": 96940
    },
    {
      "epoch": 707.6642335766423,
      "grad_norm": 10.044419288635254,
      "learning_rate": 1.4616788321167884e-05,
      "loss": 1.0068,
      "step": 96950
    },
    {
      "epoch": 707.7372262773723,
      "grad_norm": 13.53388500213623,
      "learning_rate": 1.4613138686131387e-05,
      "loss": 1.0738,
      "step": 96960
    },
    {
      "epoch": 707.8102189781022,
      "grad_norm": 11.001275062561035,
      "learning_rate": 1.460948905109489e-05,
      "loss": 1.1325,
      "step": 96970
    },
    {
      "epoch": 707.8832116788321,
      "grad_norm": 6.548931121826172,
      "learning_rate": 1.4605839416058394e-05,
      "loss": 0.867,
      "step": 96980
    },
    {
      "epoch": 707.956204379562,
      "grad_norm": 0.02700338140130043,
      "learning_rate": 1.46021897810219e-05,
      "loss": 0.7105,
      "step": 96990
    },
    {
      "epoch": 708.029197080292,
      "grad_norm": 10.744382858276367,
      "learning_rate": 1.45985401459854e-05,
      "loss": 0.9837,
      "step": 97000
    },
    {
      "epoch": 708.1021897810219,
      "grad_norm": 4.668579578399658,
      "learning_rate": 1.4594890510948906e-05,
      "loss": 0.8061,
      "step": 97010
    },
    {
      "epoch": 708.1751824817518,
      "grad_norm": 10.24266242980957,
      "learning_rate": 1.459124087591241e-05,
      "loss": 1.2959,
      "step": 97020
    },
    {
      "epoch": 708.2481751824818,
      "grad_norm": 9.663833618164062,
      "learning_rate": 1.4587591240875914e-05,
      "loss": 0.9565,
      "step": 97030
    },
    {
      "epoch": 708.3211678832117,
      "grad_norm": 8.230740547180176,
      "learning_rate": 1.4583941605839416e-05,
      "loss": 0.5467,
      "step": 97040
    },
    {
      "epoch": 708.3941605839416,
      "grad_norm": 10.419271469116211,
      "learning_rate": 1.458029197080292e-05,
      "loss": 0.7474,
      "step": 97050
    },
    {
      "epoch": 708.4671532846716,
      "grad_norm": 12.938885688781738,
      "learning_rate": 1.4576642335766424e-05,
      "loss": 0.6735,
      "step": 97060
    },
    {
      "epoch": 708.5401459854014,
      "grad_norm": 0.020526675507426262,
      "learning_rate": 1.457299270072993e-05,
      "loss": 1.0233,
      "step": 97070
    },
    {
      "epoch": 708.6131386861314,
      "grad_norm": 5.808627128601074,
      "learning_rate": 1.456934306569343e-05,
      "loss": 0.9963,
      "step": 97080
    },
    {
      "epoch": 708.6861313868613,
      "grad_norm": 7.799991607666016,
      "learning_rate": 1.4565693430656935e-05,
      "loss": 1.1719,
      "step": 97090
    },
    {
      "epoch": 708.7591240875912,
      "grad_norm": 9.36115550994873,
      "learning_rate": 1.456204379562044e-05,
      "loss": 1.0372,
      "step": 97100
    },
    {
      "epoch": 708.8321167883212,
      "grad_norm": 15.992507934570312,
      "learning_rate": 1.4558394160583943e-05,
      "loss": 0.6881,
      "step": 97110
    },
    {
      "epoch": 708.9051094890511,
      "grad_norm": 8.152140617370605,
      "learning_rate": 1.4554744525547445e-05,
      "loss": 0.9233,
      "step": 97120
    },
    {
      "epoch": 708.978102189781,
      "grad_norm": 6.781582355499268,
      "learning_rate": 1.4551094890510949e-05,
      "loss": 0.7284,
      "step": 97130
    },
    {
      "epoch": 709.051094890511,
      "grad_norm": 15.568121910095215,
      "learning_rate": 1.4547445255474455e-05,
      "loss": 1.2247,
      "step": 97140
    },
    {
      "epoch": 709.1240875912408,
      "grad_norm": 7.647365093231201,
      "learning_rate": 1.4543795620437955e-05,
      "loss": 1.1304,
      "step": 97150
    },
    {
      "epoch": 709.1970802919708,
      "grad_norm": 8.936805725097656,
      "learning_rate": 1.4540145985401459e-05,
      "loss": 0.7694,
      "step": 97160
    },
    {
      "epoch": 709.2700729927008,
      "grad_norm": 14.144664764404297,
      "learning_rate": 1.4536496350364965e-05,
      "loss": 0.9488,
      "step": 97170
    },
    {
      "epoch": 709.3430656934306,
      "grad_norm": 6.948390483856201,
      "learning_rate": 1.4532846715328469e-05,
      "loss": 0.9532,
      "step": 97180
    },
    {
      "epoch": 709.4160583941606,
      "grad_norm": 9.830276489257812,
      "learning_rate": 1.452919708029197e-05,
      "loss": 0.7217,
      "step": 97190
    },
    {
      "epoch": 709.4890510948906,
      "grad_norm": 0.07883720099925995,
      "learning_rate": 1.4525547445255475e-05,
      "loss": 0.644,
      "step": 97200
    },
    {
      "epoch": 709.5620437956204,
      "grad_norm": 12.134596824645996,
      "learning_rate": 1.4521897810218978e-05,
      "loss": 0.8115,
      "step": 97210
    },
    {
      "epoch": 709.6350364963504,
      "grad_norm": 9.828880310058594,
      "learning_rate": 1.4518248175182484e-05,
      "loss": 0.9671,
      "step": 97220
    },
    {
      "epoch": 709.7080291970802,
      "grad_norm": 6.156907558441162,
      "learning_rate": 1.4514598540145985e-05,
      "loss": 0.562,
      "step": 97230
    },
    {
      "epoch": 709.7810218978102,
      "grad_norm": 16.533409118652344,
      "learning_rate": 1.451094890510949e-05,
      "loss": 0.953,
      "step": 97240
    },
    {
      "epoch": 709.8540145985402,
      "grad_norm": 11.137843132019043,
      "learning_rate": 1.4507299270072994e-05,
      "loss": 1.2078,
      "step": 97250
    },
    {
      "epoch": 709.92700729927,
      "grad_norm": 17.467086791992188,
      "learning_rate": 1.4503649635036498e-05,
      "loss": 1.0747,
      "step": 97260
    },
    {
      "epoch": 710.0,
      "grad_norm": 10.286297798156738,
      "learning_rate": 1.45e-05,
      "loss": 1.0116,
      "step": 97270
    },
    {
      "epoch": 710.07299270073,
      "grad_norm": 12.543423652648926,
      "learning_rate": 1.4496350364963504e-05,
      "loss": 0.6722,
      "step": 97280
    },
    {
      "epoch": 710.1459854014598,
      "grad_norm": 5.235803127288818,
      "learning_rate": 1.4492700729927008e-05,
      "loss": 0.9403,
      "step": 97290
    },
    {
      "epoch": 710.2189781021898,
      "grad_norm": 13.805173873901367,
      "learning_rate": 1.4489051094890513e-05,
      "loss": 0.6255,
      "step": 97300
    },
    {
      "epoch": 710.2919708029198,
      "grad_norm": 8.068724632263184,
      "learning_rate": 1.4485401459854014e-05,
      "loss": 0.8658,
      "step": 97310
    },
    {
      "epoch": 710.3649635036496,
      "grad_norm": 7.294890403747559,
      "learning_rate": 1.448175182481752e-05,
      "loss": 0.9455,
      "step": 97320
    },
    {
      "epoch": 710.4379562043796,
      "grad_norm": 11.852206230163574,
      "learning_rate": 1.4478102189781023e-05,
      "loss": 0.9041,
      "step": 97330
    },
    {
      "epoch": 710.5109489051094,
      "grad_norm": 8.129605293273926,
      "learning_rate": 1.4474452554744527e-05,
      "loss": 0.5521,
      "step": 97340
    },
    {
      "epoch": 710.5839416058394,
      "grad_norm": 16.250633239746094,
      "learning_rate": 1.447080291970803e-05,
      "loss": 1.6646,
      "step": 97350
    },
    {
      "epoch": 710.6569343065694,
      "grad_norm": 16.981264114379883,
      "learning_rate": 1.4467153284671533e-05,
      "loss": 1.2895,
      "step": 97360
    },
    {
      "epoch": 710.7299270072992,
      "grad_norm": 0.4337667226791382,
      "learning_rate": 1.4463503649635037e-05,
      "loss": 0.7175,
      "step": 97370
    },
    {
      "epoch": 710.8029197080292,
      "grad_norm": 7.689077854156494,
      "learning_rate": 1.445985401459854e-05,
      "loss": 0.9568,
      "step": 97380
    },
    {
      "epoch": 710.8759124087592,
      "grad_norm": 9.451536178588867,
      "learning_rate": 1.4456204379562043e-05,
      "loss": 0.9633,
      "step": 97390
    },
    {
      "epoch": 710.948905109489,
      "grad_norm": 15.28848934173584,
      "learning_rate": 1.4452554744525549e-05,
      "loss": 1.0507,
      "step": 97400
    },
    {
      "epoch": 711.021897810219,
      "grad_norm": 10.281537055969238,
      "learning_rate": 1.4448905109489053e-05,
      "loss": 0.6998,
      "step": 97410
    },
    {
      "epoch": 711.0948905109489,
      "grad_norm": 6.318106651306152,
      "learning_rate": 1.4445255474452555e-05,
      "loss": 0.987,
      "step": 97420
    },
    {
      "epoch": 711.1678832116788,
      "grad_norm": 7.902390956878662,
      "learning_rate": 1.4441605839416059e-05,
      "loss": 0.377,
      "step": 97430
    },
    {
      "epoch": 711.2408759124088,
      "grad_norm": 13.31498908996582,
      "learning_rate": 1.4437956204379563e-05,
      "loss": 1.1164,
      "step": 97440
    },
    {
      "epoch": 711.3138686131387,
      "grad_norm": 0.08824783563613892,
      "learning_rate": 1.4434306569343067e-05,
      "loss": 0.7012,
      "step": 97450
    },
    {
      "epoch": 711.3868613138686,
      "grad_norm": 0.13256290555000305,
      "learning_rate": 1.4430656934306569e-05,
      "loss": 0.6312,
      "step": 97460
    },
    {
      "epoch": 711.4598540145986,
      "grad_norm": 0.017611032351851463,
      "learning_rate": 1.4427007299270073e-05,
      "loss": 1.0083,
      "step": 97470
    },
    {
      "epoch": 711.5328467153284,
      "grad_norm": 0.08744607120752335,
      "learning_rate": 1.4423357664233578e-05,
      "loss": 0.7038,
      "step": 97480
    },
    {
      "epoch": 711.6058394160584,
      "grad_norm": 7.057565212249756,
      "learning_rate": 1.4419708029197082e-05,
      "loss": 0.7559,
      "step": 97490
    },
    {
      "epoch": 711.6788321167883,
      "grad_norm": 16.914363861083984,
      "learning_rate": 1.4416058394160584e-05,
      "loss": 1.4268,
      "step": 97500
    },
    {
      "epoch": 711.7518248175182,
      "grad_norm": 14.928309440612793,
      "learning_rate": 1.4412408759124088e-05,
      "loss": 0.7354,
      "step": 97510
    },
    {
      "epoch": 711.8248175182482,
      "grad_norm": 13.02234935760498,
      "learning_rate": 1.4408759124087592e-05,
      "loss": 0.8063,
      "step": 97520
    },
    {
      "epoch": 711.8978102189781,
      "grad_norm": 0.016379596665501595,
      "learning_rate": 1.4405109489051098e-05,
      "loss": 0.7866,
      "step": 97530
    },
    {
      "epoch": 711.970802919708,
      "grad_norm": 13.286795616149902,
      "learning_rate": 1.4401459854014598e-05,
      "loss": 0.8803,
      "step": 97540
    },
    {
      "epoch": 712.043795620438,
      "grad_norm": 14.105781555175781,
      "learning_rate": 1.4397810218978102e-05,
      "loss": 1.3167,
      "step": 97550
    },
    {
      "epoch": 712.1167883211679,
      "grad_norm": 4.010779857635498,
      "learning_rate": 1.4394160583941608e-05,
      "loss": 0.7565,
      "step": 97560
    },
    {
      "epoch": 712.1897810218978,
      "grad_norm": 18.722332000732422,
      "learning_rate": 1.4390510948905111e-05,
      "loss": 1.1798,
      "step": 97570
    },
    {
      "epoch": 712.2627737226277,
      "grad_norm": 8.728877067565918,
      "learning_rate": 1.4386861313868614e-05,
      "loss": 1.0152,
      "step": 97580
    },
    {
      "epoch": 712.3357664233577,
      "grad_norm": 0.07559465616941452,
      "learning_rate": 1.4383211678832118e-05,
      "loss": 0.7515,
      "step": 97590
    },
    {
      "epoch": 712.4087591240876,
      "grad_norm": 0.059247471392154694,
      "learning_rate": 1.4379562043795621e-05,
      "loss": 0.5695,
      "step": 97600
    },
    {
      "epoch": 712.4817518248175,
      "grad_norm": 0.031227905303239822,
      "learning_rate": 1.4375912408759124e-05,
      "loss": 0.452,
      "step": 97610
    },
    {
      "epoch": 712.5547445255474,
      "grad_norm": 10.107603073120117,
      "learning_rate": 1.4372262773722627e-05,
      "loss": 1.066,
      "step": 97620
    },
    {
      "epoch": 712.6277372262774,
      "grad_norm": 18.659563064575195,
      "learning_rate": 1.4368613138686133e-05,
      "loss": 1.1456,
      "step": 97630
    },
    {
      "epoch": 712.7007299270073,
      "grad_norm": 8.70562744140625,
      "learning_rate": 1.4364963503649637e-05,
      "loss": 0.8132,
      "step": 97640
    },
    {
      "epoch": 712.7737226277372,
      "grad_norm": 0.028096910566091537,
      "learning_rate": 1.4361313868613137e-05,
      "loss": 0.9229,
      "step": 97650
    },
    {
      "epoch": 712.8467153284671,
      "grad_norm": 8.03327465057373,
      "learning_rate": 1.4357664233576643e-05,
      "loss": 0.8535,
      "step": 97660
    },
    {
      "epoch": 712.9197080291971,
      "grad_norm": 0.01389640849083662,
      "learning_rate": 1.4354014598540147e-05,
      "loss": 0.9161,
      "step": 97670
    },
    {
      "epoch": 712.992700729927,
      "grad_norm": 19.45038414001465,
      "learning_rate": 1.435036496350365e-05,
      "loss": 1.1118,
      "step": 97680
    },
    {
      "epoch": 713.0656934306569,
      "grad_norm": 5.518428802490234,
      "learning_rate": 1.4346715328467153e-05,
      "loss": 0.9222,
      "step": 97690
    },
    {
      "epoch": 713.1386861313869,
      "grad_norm": 11.877135276794434,
      "learning_rate": 1.4343065693430657e-05,
      "loss": 1.1304,
      "step": 97700
    },
    {
      "epoch": 713.2116788321168,
      "grad_norm": 3.5255002975463867,
      "learning_rate": 1.4339416058394162e-05,
      "loss": 0.7631,
      "step": 97710
    },
    {
      "epoch": 713.2846715328467,
      "grad_norm": 0.06790612637996674,
      "learning_rate": 1.4335766423357666e-05,
      "loss": 0.8231,
      "step": 97720
    },
    {
      "epoch": 713.3576642335767,
      "grad_norm": 12.372065544128418,
      "learning_rate": 1.4332116788321167e-05,
      "loss": 0.8154,
      "step": 97730
    },
    {
      "epoch": 713.4306569343066,
      "grad_norm": 10.028352737426758,
      "learning_rate": 1.4328467153284672e-05,
      "loss": 0.8989,
      "step": 97740
    },
    {
      "epoch": 713.5036496350365,
      "grad_norm": 8.448465347290039,
      "learning_rate": 1.4324817518248176e-05,
      "loss": 1.0833,
      "step": 97750
    },
    {
      "epoch": 713.5766423357665,
      "grad_norm": 11.472908020019531,
      "learning_rate": 1.432116788321168e-05,
      "loss": 0.6948,
      "step": 97760
    },
    {
      "epoch": 713.6496350364963,
      "grad_norm": 16.02590560913086,
      "learning_rate": 1.4317518248175182e-05,
      "loss": 0.5699,
      "step": 97770
    },
    {
      "epoch": 713.7226277372263,
      "grad_norm": 14.329408645629883,
      "learning_rate": 1.4313868613138686e-05,
      "loss": 0.7897,
      "step": 97780
    },
    {
      "epoch": 713.7956204379562,
      "grad_norm": 9.144417762756348,
      "learning_rate": 1.4310218978102192e-05,
      "loss": 0.9031,
      "step": 97790
    },
    {
      "epoch": 713.8686131386861,
      "grad_norm": 11.805778503417969,
      "learning_rate": 1.4306569343065692e-05,
      "loss": 1.1217,
      "step": 97800
    },
    {
      "epoch": 713.9416058394161,
      "grad_norm": 9.922627449035645,
      "learning_rate": 1.4302919708029198e-05,
      "loss": 1.0179,
      "step": 97810
    },
    {
      "epoch": 714.014598540146,
      "grad_norm": 10.222646713256836,
      "learning_rate": 1.4299270072992702e-05,
      "loss": 0.9832,
      "step": 97820
    },
    {
      "epoch": 714.0875912408759,
      "grad_norm": 4.328538417816162,
      "learning_rate": 1.4295620437956206e-05,
      "loss": 0.655,
      "step": 97830
    },
    {
      "epoch": 714.1605839416059,
      "grad_norm": 9.480037689208984,
      "learning_rate": 1.4291970802919708e-05,
      "loss": 1.082,
      "step": 97840
    },
    {
      "epoch": 714.2335766423357,
      "grad_norm": 18.804101943969727,
      "learning_rate": 1.4288321167883212e-05,
      "loss": 0.6164,
      "step": 97850
    },
    {
      "epoch": 714.3065693430657,
      "grad_norm": 0.04233246296644211,
      "learning_rate": 1.4284671532846716e-05,
      "loss": 0.5818,
      "step": 97860
    },
    {
      "epoch": 714.3795620437957,
      "grad_norm": 14.729449272155762,
      "learning_rate": 1.4281021897810221e-05,
      "loss": 0.7017,
      "step": 97870
    },
    {
      "epoch": 714.4525547445255,
      "grad_norm": 10.926375389099121,
      "learning_rate": 1.4277372262773722e-05,
      "loss": 1.0912,
      "step": 97880
    },
    {
      "epoch": 714.5255474452555,
      "grad_norm": 7.806282997131348,
      "learning_rate": 1.4273722627737227e-05,
      "loss": 0.9697,
      "step": 97890
    },
    {
      "epoch": 714.5985401459855,
      "grad_norm": 14.724600791931152,
      "learning_rate": 1.4270072992700731e-05,
      "loss": 1.1321,
      "step": 97900
    },
    {
      "epoch": 714.6715328467153,
      "grad_norm": 7.663480758666992,
      "learning_rate": 1.4266423357664235e-05,
      "loss": 1.1082,
      "step": 97910
    },
    {
      "epoch": 714.7445255474453,
      "grad_norm": 0.1182306632399559,
      "learning_rate": 1.4262773722627737e-05,
      "loss": 0.6776,
      "step": 97920
    },
    {
      "epoch": 714.8175182481751,
      "grad_norm": 11.11040210723877,
      "learning_rate": 1.4259124087591241e-05,
      "loss": 1.0866,
      "step": 97930
    },
    {
      "epoch": 714.8905109489051,
      "grad_norm": 9.14704704284668,
      "learning_rate": 1.4255474452554745e-05,
      "loss": 0.8197,
      "step": 97940
    },
    {
      "epoch": 714.9635036496351,
      "grad_norm": 10.758378982543945,
      "learning_rate": 1.425182481751825e-05,
      "loss": 0.9371,
      "step": 97950
    },
    {
      "epoch": 715.0364963503649,
      "grad_norm": 13.40912914276123,
      "learning_rate": 1.4248175182481751e-05,
      "loss": 1.0151,
      "step": 97960
    },
    {
      "epoch": 715.1094890510949,
      "grad_norm": 5.712222099304199,
      "learning_rate": 1.4244525547445257e-05,
      "loss": 0.7851,
      "step": 97970
    },
    {
      "epoch": 715.1824817518249,
      "grad_norm": 14.213434219360352,
      "learning_rate": 1.424087591240876e-05,
      "loss": 0.7852,
      "step": 97980
    },
    {
      "epoch": 715.2554744525547,
      "grad_norm": 9.448412895202637,
      "learning_rate": 1.4237226277372264e-05,
      "loss": 0.9651,
      "step": 97990
    },
    {
      "epoch": 715.3284671532847,
      "grad_norm": 7.685040473937988,
      "learning_rate": 1.4233576642335767e-05,
      "loss": 0.9417,
      "step": 98000
    },
    {
      "epoch": 715.4014598540145,
      "grad_norm": 9.595979690551758,
      "learning_rate": 1.422992700729927e-05,
      "loss": 0.8518,
      "step": 98010
    },
    {
      "epoch": 715.4744525547445,
      "grad_norm": 9.506093978881836,
      "learning_rate": 1.4226277372262774e-05,
      "loss": 0.6216,
      "step": 98020
    },
    {
      "epoch": 715.5474452554745,
      "grad_norm": 0.08835416287183762,
      "learning_rate": 1.4222627737226277e-05,
      "loss": 0.6382,
      "step": 98030
    },
    {
      "epoch": 715.6204379562043,
      "grad_norm": 7.316016674041748,
      "learning_rate": 1.421897810218978e-05,
      "loss": 0.9997,
      "step": 98040
    },
    {
      "epoch": 715.6934306569343,
      "grad_norm": 15.49048137664795,
      "learning_rate": 1.4215328467153286e-05,
      "loss": 0.9877,
      "step": 98050
    },
    {
      "epoch": 715.7664233576643,
      "grad_norm": 10.316608428955078,
      "learning_rate": 1.421167883211679e-05,
      "loss": 0.9957,
      "step": 98060
    },
    {
      "epoch": 715.8394160583941,
      "grad_norm": 11.312797546386719,
      "learning_rate": 1.4208029197080292e-05,
      "loss": 1.4881,
      "step": 98070
    },
    {
      "epoch": 715.9124087591241,
      "grad_norm": 11.941667556762695,
      "learning_rate": 1.4204379562043796e-05,
      "loss": 1.0103,
      "step": 98080
    },
    {
      "epoch": 715.985401459854,
      "grad_norm": 11.068215370178223,
      "learning_rate": 1.42007299270073e-05,
      "loss": 0.7915,
      "step": 98090
    },
    {
      "epoch": 716.0583941605839,
      "grad_norm": 0.02343590185046196,
      "learning_rate": 1.4197080291970805e-05,
      "loss": 0.3381,
      "step": 98100
    },
    {
      "epoch": 716.1313868613139,
      "grad_norm": 5.831239223480225,
      "learning_rate": 1.4193430656934306e-05,
      "loss": 0.8225,
      "step": 98110
    },
    {
      "epoch": 716.2043795620438,
      "grad_norm": 6.979445457458496,
      "learning_rate": 1.418978102189781e-05,
      "loss": 0.7449,
      "step": 98120
    },
    {
      "epoch": 716.2773722627737,
      "grad_norm": 12.996610641479492,
      "learning_rate": 1.4186131386861315e-05,
      "loss": 1.2152,
      "step": 98130
    },
    {
      "epoch": 716.3503649635037,
      "grad_norm": 11.093548774719238,
      "learning_rate": 1.418248175182482e-05,
      "loss": 0.6485,
      "step": 98140
    },
    {
      "epoch": 716.4233576642335,
      "grad_norm": 10.591612815856934,
      "learning_rate": 1.4178832116788321e-05,
      "loss": 1.1005,
      "step": 98150
    },
    {
      "epoch": 716.4963503649635,
      "grad_norm": 0.07322774827480316,
      "learning_rate": 1.4175182481751825e-05,
      "loss": 0.8649,
      "step": 98160
    },
    {
      "epoch": 716.5693430656934,
      "grad_norm": 16.64143180847168,
      "learning_rate": 1.417153284671533e-05,
      "loss": 0.8533,
      "step": 98170
    },
    {
      "epoch": 716.6423357664233,
      "grad_norm": 0.043748289346694946,
      "learning_rate": 1.4167883211678835e-05,
      "loss": 0.8258,
      "step": 98180
    },
    {
      "epoch": 716.7153284671533,
      "grad_norm": 0.12547117471694946,
      "learning_rate": 1.4164233576642335e-05,
      "loss": 0.898,
      "step": 98190
    },
    {
      "epoch": 716.7883211678832,
      "grad_norm": 11.83568000793457,
      "learning_rate": 1.416058394160584e-05,
      "loss": 0.9587,
      "step": 98200
    },
    {
      "epoch": 716.8613138686131,
      "grad_norm": 16.523588180541992,
      "learning_rate": 1.4156934306569345e-05,
      "loss": 0.9173,
      "step": 98210
    },
    {
      "epoch": 716.9343065693431,
      "grad_norm": 14.460272789001465,
      "learning_rate": 1.4153284671532849e-05,
      "loss": 0.8837,
      "step": 98220
    },
    {
      "epoch": 717.007299270073,
      "grad_norm": 10.575636863708496,
      "learning_rate": 1.414963503649635e-05,
      "loss": 0.7861,
      "step": 98230
    },
    {
      "epoch": 717.0802919708029,
      "grad_norm": 21.08509063720703,
      "learning_rate": 1.4145985401459855e-05,
      "loss": 0.719,
      "step": 98240
    },
    {
      "epoch": 717.1532846715329,
      "grad_norm": 13.771674156188965,
      "learning_rate": 1.4142335766423359e-05,
      "loss": 0.9409,
      "step": 98250
    },
    {
      "epoch": 717.2262773722628,
      "grad_norm": 0.047816406935453415,
      "learning_rate": 1.413868613138686e-05,
      "loss": 0.5766,
      "step": 98260
    },
    {
      "epoch": 717.2992700729927,
      "grad_norm": 9.909546852111816,
      "learning_rate": 1.4135036496350365e-05,
      "loss": 0.8634,
      "step": 98270
    },
    {
      "epoch": 717.3722627737226,
      "grad_norm": 6.867712020874023,
      "learning_rate": 1.413138686131387e-05,
      "loss": 1.0192,
      "step": 98280
    },
    {
      "epoch": 717.4452554744526,
      "grad_norm": 11.58903694152832,
      "learning_rate": 1.4127737226277374e-05,
      "loss": 1.2317,
      "step": 98290
    },
    {
      "epoch": 717.5182481751825,
      "grad_norm": 17.39264678955078,
      "learning_rate": 1.4124087591240876e-05,
      "loss": 1.0774,
      "step": 98300
    },
    {
      "epoch": 717.5912408759124,
      "grad_norm": 17.366214752197266,
      "learning_rate": 1.412043795620438e-05,
      "loss": 0.7197,
      "step": 98310
    },
    {
      "epoch": 717.6642335766423,
      "grad_norm": 0.0330280065536499,
      "learning_rate": 1.4116788321167884e-05,
      "loss": 0.8132,
      "step": 98320
    },
    {
      "epoch": 717.7372262773723,
      "grad_norm": 5.630892276763916,
      "learning_rate": 1.4113138686131388e-05,
      "loss": 0.7462,
      "step": 98330
    },
    {
      "epoch": 717.8102189781022,
      "grad_norm": 13.89456844329834,
      "learning_rate": 1.410948905109489e-05,
      "loss": 1.1963,
      "step": 98340
    },
    {
      "epoch": 717.8832116788321,
      "grad_norm": 9.234454154968262,
      "learning_rate": 1.4105839416058394e-05,
      "loss": 0.8966,
      "step": 98350
    },
    {
      "epoch": 717.956204379562,
      "grad_norm": 7.809572219848633,
      "learning_rate": 1.41021897810219e-05,
      "loss": 0.715,
      "step": 98360
    },
    {
      "epoch": 718.029197080292,
      "grad_norm": 0.7500191926956177,
      "learning_rate": 1.4098540145985403e-05,
      "loss": 0.8362,
      "step": 98370
    },
    {
      "epoch": 718.1021897810219,
      "grad_norm": 8.001912117004395,
      "learning_rate": 1.4094890510948906e-05,
      "loss": 0.7336,
      "step": 98380
    },
    {
      "epoch": 718.1751824817518,
      "grad_norm": 0.06538733839988708,
      "learning_rate": 1.409124087591241e-05,
      "loss": 1.0829,
      "step": 98390
    },
    {
      "epoch": 718.2481751824818,
      "grad_norm": 6.961586952209473,
      "learning_rate": 1.4087591240875913e-05,
      "loss": 0.3746,
      "step": 98400
    },
    {
      "epoch": 718.3211678832117,
      "grad_norm": 13.14661693572998,
      "learning_rate": 1.4083941605839417e-05,
      "loss": 0.7239,
      "step": 98410
    },
    {
      "epoch": 718.3941605839416,
      "grad_norm": 0.06746386736631393,
      "learning_rate": 1.408029197080292e-05,
      "loss": 0.6938,
      "step": 98420
    },
    {
      "epoch": 718.4671532846716,
      "grad_norm": 7.8648810386657715,
      "learning_rate": 1.4076642335766423e-05,
      "loss": 1.1339,
      "step": 98430
    },
    {
      "epoch": 718.5401459854014,
      "grad_norm": 8.008723258972168,
      "learning_rate": 1.4072992700729929e-05,
      "loss": 0.6864,
      "step": 98440
    },
    {
      "epoch": 718.6131386861314,
      "grad_norm": 24.721681594848633,
      "learning_rate": 1.406934306569343e-05,
      "loss": 0.9868,
      "step": 98450
    },
    {
      "epoch": 718.6861313868613,
      "grad_norm": 10.807172775268555,
      "learning_rate": 1.4065693430656935e-05,
      "loss": 1.1714,
      "step": 98460
    },
    {
      "epoch": 718.7591240875912,
      "grad_norm": 12.40937328338623,
      "learning_rate": 1.4062043795620439e-05,
      "loss": 1.4562,
      "step": 98470
    },
    {
      "epoch": 718.8321167883212,
      "grad_norm": 8.87592887878418,
      "learning_rate": 1.4058394160583943e-05,
      "loss": 0.956,
      "step": 98480
    },
    {
      "epoch": 718.9051094890511,
      "grad_norm": 14.74228286743164,
      "learning_rate": 1.4054744525547445e-05,
      "loss": 0.9922,
      "step": 98490
    },
    {
      "epoch": 718.978102189781,
      "grad_norm": 8.555277824401855,
      "learning_rate": 1.4051094890510949e-05,
      "loss": 0.7111,
      "step": 98500
    },
    {
      "epoch": 719.051094890511,
      "grad_norm": 0.047533269971609116,
      "learning_rate": 1.4047445255474453e-05,
      "loss": 0.6101,
      "step": 98510
    },
    {
      "epoch": 719.1240875912408,
      "grad_norm": 13.206334114074707,
      "learning_rate": 1.4043795620437958e-05,
      "loss": 0.8784,
      "step": 98520
    },
    {
      "epoch": 719.1970802919708,
      "grad_norm": 14.482673645019531,
      "learning_rate": 1.4040145985401459e-05,
      "loss": 0.8725,
      "step": 98530
    },
    {
      "epoch": 719.2700729927008,
      "grad_norm": 8.94741153717041,
      "learning_rate": 1.4036496350364964e-05,
      "loss": 1.1156,
      "step": 98540
    },
    {
      "epoch": 719.3430656934306,
      "grad_norm": 10.452737808227539,
      "learning_rate": 1.4032846715328468e-05,
      "loss": 1.1196,
      "step": 98550
    },
    {
      "epoch": 719.4160583941606,
      "grad_norm": 0.10459981113672256,
      "learning_rate": 1.4029197080291972e-05,
      "loss": 0.9081,
      "step": 98560
    },
    {
      "epoch": 719.4890510948906,
      "grad_norm": 0.02384374290704727,
      "learning_rate": 1.4025547445255474e-05,
      "loss": 0.9529,
      "step": 98570
    },
    {
      "epoch": 719.5620437956204,
      "grad_norm": 12.965577125549316,
      "learning_rate": 1.4021897810218978e-05,
      "loss": 0.9613,
      "step": 98580
    },
    {
      "epoch": 719.6350364963504,
      "grad_norm": 12.799840927124023,
      "learning_rate": 1.4018248175182482e-05,
      "loss": 0.9472,
      "step": 98590
    },
    {
      "epoch": 719.7080291970802,
      "grad_norm": 9.558297157287598,
      "learning_rate": 1.4014598540145988e-05,
      "loss": 0.8878,
      "step": 98600
    },
    {
      "epoch": 719.7810218978102,
      "grad_norm": 8.48693561553955,
      "learning_rate": 1.4010948905109488e-05,
      "loss": 0.5875,
      "step": 98610
    },
    {
      "epoch": 719.8540145985402,
      "grad_norm": 6.785435199737549,
      "learning_rate": 1.4007299270072994e-05,
      "loss": 0.7802,
      "step": 98620
    },
    {
      "epoch": 719.92700729927,
      "grad_norm": 10.084464073181152,
      "learning_rate": 1.4003649635036498e-05,
      "loss": 1.0128,
      "step": 98630
    },
    {
      "epoch": 720.0,
      "grad_norm": 30.11574935913086,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.209,
      "step": 98640
    },
    {
      "epoch": 720.07299270073,
      "grad_norm": 12.749345779418945,
      "learning_rate": 1.3996350364963504e-05,
      "loss": 1.0601,
      "step": 98650
    },
    {
      "epoch": 720.1459854014598,
      "grad_norm": 6.043766498565674,
      "learning_rate": 1.3992700729927008e-05,
      "loss": 0.9978,
      "step": 98660
    },
    {
      "epoch": 720.2189781021898,
      "grad_norm": 0.03243233263492584,
      "learning_rate": 1.3989051094890513e-05,
      "loss": 0.6756,
      "step": 98670
    },
    {
      "epoch": 720.2919708029198,
      "grad_norm": 0.021115880459547043,
      "learning_rate": 1.3985401459854014e-05,
      "loss": 0.6765,
      "step": 98680
    },
    {
      "epoch": 720.3649635036496,
      "grad_norm": 16.83071517944336,
      "learning_rate": 1.3981751824817518e-05,
      "loss": 1.082,
      "step": 98690
    },
    {
      "epoch": 720.4379562043796,
      "grad_norm": 5.686342716217041,
      "learning_rate": 1.3978102189781023e-05,
      "loss": 0.7678,
      "step": 98700
    },
    {
      "epoch": 720.5109489051094,
      "grad_norm": 4.454283714294434,
      "learning_rate": 1.3974452554744527e-05,
      "loss": 0.9952,
      "step": 98710
    },
    {
      "epoch": 720.5839416058394,
      "grad_norm": 12.438880920410156,
      "learning_rate": 1.397080291970803e-05,
      "loss": 0.9487,
      "step": 98720
    },
    {
      "epoch": 720.6569343065694,
      "grad_norm": 10.036796569824219,
      "learning_rate": 1.3967153284671533e-05,
      "loss": 0.9734,
      "step": 98730
    },
    {
      "epoch": 720.7299270072992,
      "grad_norm": 7.429378509521484,
      "learning_rate": 1.3963503649635037e-05,
      "loss": 0.7895,
      "step": 98740
    },
    {
      "epoch": 720.8029197080292,
      "grad_norm": 7.796339988708496,
      "learning_rate": 1.3959854014598542e-05,
      "loss": 0.9064,
      "step": 98750
    },
    {
      "epoch": 720.8759124087592,
      "grad_norm": 13.429394721984863,
      "learning_rate": 1.3956204379562043e-05,
      "loss": 0.6684,
      "step": 98760
    },
    {
      "epoch": 720.948905109489,
      "grad_norm": 8.463065147399902,
      "learning_rate": 1.3952554744525549e-05,
      "loss": 1.0162,
      "step": 98770
    },
    {
      "epoch": 721.021897810219,
      "grad_norm": 8.676570892333984,
      "learning_rate": 1.3948905109489052e-05,
      "loss": 0.8247,
      "step": 98780
    },
    {
      "epoch": 721.0948905109489,
      "grad_norm": 7.784233093261719,
      "learning_rate": 1.3945255474452556e-05,
      "loss": 1.246,
      "step": 98790
    },
    {
      "epoch": 721.1678832116788,
      "grad_norm": 6.121840476989746,
      "learning_rate": 1.3941605839416059e-05,
      "loss": 0.9067,
      "step": 98800
    },
    {
      "epoch": 721.2408759124088,
      "grad_norm": 9.96437931060791,
      "learning_rate": 1.3937956204379562e-05,
      "loss": 0.8869,
      "step": 98810
    },
    {
      "epoch": 721.3138686131387,
      "grad_norm": 0.9235116839408875,
      "learning_rate": 1.3934306569343066e-05,
      "loss": 0.9263,
      "step": 98820
    },
    {
      "epoch": 721.3868613138686,
      "grad_norm": 4.173246383666992,
      "learning_rate": 1.3930656934306572e-05,
      "loss": 0.656,
      "step": 98830
    },
    {
      "epoch": 721.4598540145986,
      "grad_norm": 5.390522480010986,
      "learning_rate": 1.3927007299270072e-05,
      "loss": 1.0138,
      "step": 98840
    },
    {
      "epoch": 721.5328467153284,
      "grad_norm": 9.191459655761719,
      "learning_rate": 1.3923357664233578e-05,
      "loss": 0.8527,
      "step": 98850
    },
    {
      "epoch": 721.6058394160584,
      "grad_norm": 0.06673822551965714,
      "learning_rate": 1.3919708029197082e-05,
      "loss": 0.8657,
      "step": 98860
    },
    {
      "epoch": 721.6788321167883,
      "grad_norm": 11.326789855957031,
      "learning_rate": 1.3916058394160586e-05,
      "loss": 0.8559,
      "step": 98870
    },
    {
      "epoch": 721.7518248175182,
      "grad_norm": 8.951321601867676,
      "learning_rate": 1.3912408759124088e-05,
      "loss": 0.6975,
      "step": 98880
    },
    {
      "epoch": 721.8248175182482,
      "grad_norm": 13.782772064208984,
      "learning_rate": 1.3908759124087592e-05,
      "loss": 0.9665,
      "step": 98890
    },
    {
      "epoch": 721.8978102189781,
      "grad_norm": 6.551776885986328,
      "learning_rate": 1.3905109489051096e-05,
      "loss": 0.9845,
      "step": 98900
    },
    {
      "epoch": 721.970802919708,
      "grad_norm": 0.15723708271980286,
      "learning_rate": 1.3901459854014598e-05,
      "loss": 0.7941,
      "step": 98910
    },
    {
      "epoch": 722.043795620438,
      "grad_norm": 0.07360801845788956,
      "learning_rate": 1.3897810218978102e-05,
      "loss": 0.7967,
      "step": 98920
    },
    {
      "epoch": 722.1167883211679,
      "grad_norm": 15.244621276855469,
      "learning_rate": 1.3894160583941607e-05,
      "loss": 1.0158,
      "step": 98930
    },
    {
      "epoch": 722.1897810218978,
      "grad_norm": 17.03362464904785,
      "learning_rate": 1.3890510948905111e-05,
      "loss": 0.9649,
      "step": 98940
    },
    {
      "epoch": 722.2627737226277,
      "grad_norm": 11.251258850097656,
      "learning_rate": 1.3886861313868613e-05,
      "loss": 0.7464,
      "step": 98950
    },
    {
      "epoch": 722.3357664233577,
      "grad_norm": 11.399147033691406,
      "learning_rate": 1.3883211678832117e-05,
      "loss": 0.9939,
      "step": 98960
    },
    {
      "epoch": 722.4087591240876,
      "grad_norm": 10.887991905212402,
      "learning_rate": 1.3879562043795621e-05,
      "loss": 1.1212,
      "step": 98970
    },
    {
      "epoch": 722.4817518248175,
      "grad_norm": 9.58292293548584,
      "learning_rate": 1.3875912408759125e-05,
      "loss": 0.8095,
      "step": 98980
    },
    {
      "epoch": 722.5547445255474,
      "grad_norm": 13.193428039550781,
      "learning_rate": 1.3872262773722627e-05,
      "loss": 0.8495,
      "step": 98990
    },
    {
      "epoch": 722.6277372262774,
      "grad_norm": 0.08418858796358109,
      "learning_rate": 1.3868613138686131e-05,
      "loss": 1.1612,
      "step": 99000
    },
    {
      "epoch": 722.7007299270073,
      "grad_norm": 8.73940658569336,
      "learning_rate": 1.3864963503649637e-05,
      "loss": 0.9973,
      "step": 99010
    },
    {
      "epoch": 722.7737226277372,
      "grad_norm": 12.411117553710938,
      "learning_rate": 1.386131386861314e-05,
      "loss": 0.6673,
      "step": 99020
    },
    {
      "epoch": 722.8467153284671,
      "grad_norm": 7.98859977722168,
      "learning_rate": 1.3857664233576643e-05,
      "loss": 1.1558,
      "step": 99030
    },
    {
      "epoch": 722.9197080291971,
      "grad_norm": 9.070670127868652,
      "learning_rate": 1.3854014598540147e-05,
      "loss": 0.6056,
      "step": 99040
    },
    {
      "epoch": 722.992700729927,
      "grad_norm": 9.285064697265625,
      "learning_rate": 1.385036496350365e-05,
      "loss": 0.7225,
      "step": 99050
    },
    {
      "epoch": 723.0656934306569,
      "grad_norm": 0.01905141770839691,
      "learning_rate": 1.3846715328467156e-05,
      "loss": 0.7157,
      "step": 99060
    },
    {
      "epoch": 723.1386861313869,
      "grad_norm": 0.06719378381967545,
      "learning_rate": 1.3843065693430657e-05,
      "loss": 0.5723,
      "step": 99070
    },
    {
      "epoch": 723.2116788321168,
      "grad_norm": 5.098932266235352,
      "learning_rate": 1.383941605839416e-05,
      "loss": 0.6894,
      "step": 99080
    },
    {
      "epoch": 723.2846715328467,
      "grad_norm": 12.28797435760498,
      "learning_rate": 1.3835766423357666e-05,
      "loss": 1.04,
      "step": 99090
    },
    {
      "epoch": 723.3576642335767,
      "grad_norm": 12.633027076721191,
      "learning_rate": 1.3832116788321167e-05,
      "loss": 0.9173,
      "step": 99100
    },
    {
      "epoch": 723.4306569343066,
      "grad_norm": 11.728118896484375,
      "learning_rate": 1.3828467153284672e-05,
      "loss": 0.7308,
      "step": 99110
    },
    {
      "epoch": 723.5036496350365,
      "grad_norm": 19.399906158447266,
      "learning_rate": 1.3824817518248176e-05,
      "loss": 0.9634,
      "step": 99120
    },
    {
      "epoch": 723.5766423357665,
      "grad_norm": 13.202831268310547,
      "learning_rate": 1.382116788321168e-05,
      "loss": 1.0946,
      "step": 99130
    },
    {
      "epoch": 723.6496350364963,
      "grad_norm": 6.932795524597168,
      "learning_rate": 1.3817518248175182e-05,
      "loss": 0.8083,
      "step": 99140
    },
    {
      "epoch": 723.7226277372263,
      "grad_norm": 16.75298500061035,
      "learning_rate": 1.3813868613138686e-05,
      "loss": 1.2989,
      "step": 99150
    },
    {
      "epoch": 723.7956204379562,
      "grad_norm": 7.625823974609375,
      "learning_rate": 1.3810218978102192e-05,
      "loss": 0.5088,
      "step": 99160
    },
    {
      "epoch": 723.8686131386861,
      "grad_norm": 7.693366527557373,
      "learning_rate": 1.3806569343065695e-05,
      "loss": 1.2058,
      "step": 99170
    },
    {
      "epoch": 723.9416058394161,
      "grad_norm": 6.355737686157227,
      "learning_rate": 1.3802919708029196e-05,
      "loss": 0.7514,
      "step": 99180
    },
    {
      "epoch": 724.014598540146,
      "grad_norm": 14.967702865600586,
      "learning_rate": 1.3799270072992701e-05,
      "loss": 1.1416,
      "step": 99190
    },
    {
      "epoch": 724.0875912408759,
      "grad_norm": 0.052287258207798004,
      "learning_rate": 1.3795620437956205e-05,
      "loss": 0.681,
      "step": 99200
    },
    {
      "epoch": 724.1605839416059,
      "grad_norm": 7.40155029296875,
      "learning_rate": 1.379197080291971e-05,
      "loss": 0.6378,
      "step": 99210
    },
    {
      "epoch": 724.2335766423357,
      "grad_norm": 10.74842357635498,
      "learning_rate": 1.3788321167883211e-05,
      "loss": 0.884,
      "step": 99220
    },
    {
      "epoch": 724.3065693430657,
      "grad_norm": 0.040675580501556396,
      "learning_rate": 1.3784671532846715e-05,
      "loss": 0.8671,
      "step": 99230
    },
    {
      "epoch": 724.3795620437957,
      "grad_norm": 5.412572383880615,
      "learning_rate": 1.3781021897810221e-05,
      "loss": 0.988,
      "step": 99240
    },
    {
      "epoch": 724.4525547445255,
      "grad_norm": 8.724089622497559,
      "learning_rate": 1.3777372262773725e-05,
      "loss": 1.3844,
      "step": 99250
    },
    {
      "epoch": 724.5255474452555,
      "grad_norm": 15.65321159362793,
      "learning_rate": 1.3773722627737227e-05,
      "loss": 1.0346,
      "step": 99260
    },
    {
      "epoch": 724.5985401459855,
      "grad_norm": 11.044631004333496,
      "learning_rate": 1.377007299270073e-05,
      "loss": 0.7336,
      "step": 99270
    },
    {
      "epoch": 724.6715328467153,
      "grad_norm": 11.802465438842773,
      "learning_rate": 1.3766423357664235e-05,
      "loss": 0.989,
      "step": 99280
    },
    {
      "epoch": 724.7445255474453,
      "grad_norm": 12.5497465133667,
      "learning_rate": 1.3762773722627739e-05,
      "loss": 1.2714,
      "step": 99290
    },
    {
      "epoch": 724.8175182481751,
      "grad_norm": 17.116348266601562,
      "learning_rate": 1.375912408759124e-05,
      "loss": 1.0683,
      "step": 99300
    },
    {
      "epoch": 724.8905109489051,
      "grad_norm": 5.879697322845459,
      "learning_rate": 1.3755474452554745e-05,
      "loss": 0.9697,
      "step": 99310
    },
    {
      "epoch": 724.9635036496351,
      "grad_norm": 14.44308090209961,
      "learning_rate": 1.375182481751825e-05,
      "loss": 0.5103,
      "step": 99320
    },
    {
      "epoch": 725.0364963503649,
      "grad_norm": 8.377634048461914,
      "learning_rate": 1.374817518248175e-05,
      "loss": 0.6169,
      "step": 99330
    },
    {
      "epoch": 725.1094890510949,
      "grad_norm": 9.959612846374512,
      "learning_rate": 1.3744525547445256e-05,
      "loss": 0.9309,
      "step": 99340
    },
    {
      "epoch": 725.1824817518249,
      "grad_norm": 7.2111711502075195,
      "learning_rate": 1.374087591240876e-05,
      "loss": 0.8173,
      "step": 99350
    },
    {
      "epoch": 725.2554744525547,
      "grad_norm": 7.641873359680176,
      "learning_rate": 1.3737226277372264e-05,
      "loss": 0.8991,
      "step": 99360
    },
    {
      "epoch": 725.3284671532847,
      "grad_norm": 5.072699546813965,
      "learning_rate": 1.3733576642335766e-05,
      "loss": 0.688,
      "step": 99370
    },
    {
      "epoch": 725.4014598540145,
      "grad_norm": 9.754678726196289,
      "learning_rate": 1.372992700729927e-05,
      "loss": 1.223,
      "step": 99380
    },
    {
      "epoch": 725.4744525547445,
      "grad_norm": 10.704178810119629,
      "learning_rate": 1.3726277372262774e-05,
      "loss": 0.9899,
      "step": 99390
    },
    {
      "epoch": 725.5474452554745,
      "grad_norm": 9.317697525024414,
      "learning_rate": 1.372262773722628e-05,
      "loss": 0.7755,
      "step": 99400
    },
    {
      "epoch": 725.6204379562043,
      "grad_norm": 7.006680011749268,
      "learning_rate": 1.371897810218978e-05,
      "loss": 1.213,
      "step": 99410
    },
    {
      "epoch": 725.6934306569343,
      "grad_norm": 10.294867515563965,
      "learning_rate": 1.3715328467153286e-05,
      "loss": 1.1667,
      "step": 99420
    },
    {
      "epoch": 725.7664233576643,
      "grad_norm": 7.735812187194824,
      "learning_rate": 1.371167883211679e-05,
      "loss": 1.0168,
      "step": 99430
    },
    {
      "epoch": 725.8394160583941,
      "grad_norm": 10.154626846313477,
      "learning_rate": 1.3708029197080293e-05,
      "loss": 0.4227,
      "step": 99440
    },
    {
      "epoch": 725.9124087591241,
      "grad_norm": 6.584987640380859,
      "learning_rate": 1.3704379562043796e-05,
      "loss": 0.6751,
      "step": 99450
    },
    {
      "epoch": 725.985401459854,
      "grad_norm": 9.437252044677734,
      "learning_rate": 1.37007299270073e-05,
      "loss": 1.1708,
      "step": 99460
    },
    {
      "epoch": 726.0583941605839,
      "grad_norm": 13.100885391235352,
      "learning_rate": 1.3697080291970803e-05,
      "loss": 0.6474,
      "step": 99470
    },
    {
      "epoch": 726.1313868613139,
      "grad_norm": 0.5894122123718262,
      "learning_rate": 1.3693430656934309e-05,
      "loss": 0.8571,
      "step": 99480
    },
    {
      "epoch": 726.2043795620438,
      "grad_norm": 0.09640344232320786,
      "learning_rate": 1.368978102189781e-05,
      "loss": 0.8112,
      "step": 99490
    },
    {
      "epoch": 726.2773722627737,
      "grad_norm": 0.0312652513384819,
      "learning_rate": 1.3686131386861315e-05,
      "loss": 1.0276,
      "step": 99500
    },
    {
      "epoch": 726.3503649635037,
      "grad_norm": 19.1218204498291,
      "learning_rate": 1.3682481751824819e-05,
      "loss": 0.8057,
      "step": 99510
    },
    {
      "epoch": 726.4233576642335,
      "grad_norm": 16.41798210144043,
      "learning_rate": 1.3678832116788321e-05,
      "loss": 0.9748,
      "step": 99520
    },
    {
      "epoch": 726.4963503649635,
      "grad_norm": 11.892088890075684,
      "learning_rate": 1.3675182481751825e-05,
      "loss": 0.9349,
      "step": 99530
    },
    {
      "epoch": 726.5693430656934,
      "grad_norm": 15.461974143981934,
      "learning_rate": 1.3671532846715329e-05,
      "loss": 1.2009,
      "step": 99540
    },
    {
      "epoch": 726.6423357664233,
      "grad_norm": 17.60104751586914,
      "learning_rate": 1.3667883211678833e-05,
      "loss": 0.8967,
      "step": 99550
    },
    {
      "epoch": 726.7153284671533,
      "grad_norm": 0.2809876799583435,
      "learning_rate": 1.3664233576642335e-05,
      "loss": 0.8626,
      "step": 99560
    },
    {
      "epoch": 726.7883211678832,
      "grad_norm": 8.081901550292969,
      "learning_rate": 1.3660583941605839e-05,
      "loss": 1.0886,
      "step": 99570
    },
    {
      "epoch": 726.8613138686131,
      "grad_norm": 8.181467056274414,
      "learning_rate": 1.3656934306569344e-05,
      "loss": 1.0686,
      "step": 99580
    },
    {
      "epoch": 726.9343065693431,
      "grad_norm": 6.299496650695801,
      "learning_rate": 1.3653284671532848e-05,
      "loss": 0.7723,
      "step": 99590
    },
    {
      "epoch": 727.007299270073,
      "grad_norm": 12.16811752319336,
      "learning_rate": 1.364963503649635e-05,
      "loss": 0.6282,
      "step": 99600
    },
    {
      "epoch": 727.0802919708029,
      "grad_norm": 8.475963592529297,
      "learning_rate": 1.3645985401459854e-05,
      "loss": 0.994,
      "step": 99610
    },
    {
      "epoch": 727.1532846715329,
      "grad_norm": 6.961045742034912,
      "learning_rate": 1.3642335766423358e-05,
      "loss": 0.9479,
      "step": 99620
    },
    {
      "epoch": 727.2262773722628,
      "grad_norm": 8.457965850830078,
      "learning_rate": 1.3638686131386864e-05,
      "loss": 1.1102,
      "step": 99630
    },
    {
      "epoch": 727.2992700729927,
      "grad_norm": 9.99583911895752,
      "learning_rate": 1.3635036496350364e-05,
      "loss": 1.2925,
      "step": 99640
    },
    {
      "epoch": 727.3722627737226,
      "grad_norm": 0.02282441407442093,
      "learning_rate": 1.3631386861313868e-05,
      "loss": 0.934,
      "step": 99650
    },
    {
      "epoch": 727.4452554744526,
      "grad_norm": 7.4319748878479,
      "learning_rate": 1.3627737226277374e-05,
      "loss": 0.8515,
      "step": 99660
    },
    {
      "epoch": 727.5182481751825,
      "grad_norm": 12.39687442779541,
      "learning_rate": 1.3624087591240878e-05,
      "loss": 0.7699,
      "step": 99670
    },
    {
      "epoch": 727.5912408759124,
      "grad_norm": 9.34329891204834,
      "learning_rate": 1.362043795620438e-05,
      "loss": 0.3816,
      "step": 99680
    },
    {
      "epoch": 727.6642335766423,
      "grad_norm": 8.051554679870605,
      "learning_rate": 1.3616788321167884e-05,
      "loss": 1.1573,
      "step": 99690
    },
    {
      "epoch": 727.7372262773723,
      "grad_norm": 7.744778633117676,
      "learning_rate": 1.3613138686131388e-05,
      "loss": 0.7033,
      "step": 99700
    },
    {
      "epoch": 727.8102189781022,
      "grad_norm": 5.1819329261779785,
      "learning_rate": 1.3609489051094893e-05,
      "loss": 0.6189,
      "step": 99710
    },
    {
      "epoch": 727.8832116788321,
      "grad_norm": 1.0973390340805054,
      "learning_rate": 1.3605839416058394e-05,
      "loss": 0.7521,
      "step": 99720
    },
    {
      "epoch": 727.956204379562,
      "grad_norm": 12.602807998657227,
      "learning_rate": 1.36021897810219e-05,
      "loss": 1.0294,
      "step": 99730
    },
    {
      "epoch": 728.029197080292,
      "grad_norm": 8.347546577453613,
      "learning_rate": 1.3598540145985403e-05,
      "loss": 1.2821,
      "step": 99740
    },
    {
      "epoch": 728.1021897810219,
      "grad_norm": 6.673648834228516,
      "learning_rate": 1.3594890510948904e-05,
      "loss": 0.525,
      "step": 99750
    },
    {
      "epoch": 728.1751824817518,
      "grad_norm": 4.878573417663574,
      "learning_rate": 1.359124087591241e-05,
      "loss": 0.8091,
      "step": 99760
    },
    {
      "epoch": 728.2481751824818,
      "grad_norm": 6.2629313468933105,
      "learning_rate": 1.3587591240875913e-05,
      "loss": 1.0229,
      "step": 99770
    },
    {
      "epoch": 728.3211678832117,
      "grad_norm": 7.927766799926758,
      "learning_rate": 1.3583941605839417e-05,
      "loss": 0.7496,
      "step": 99780
    },
    {
      "epoch": 728.3941605839416,
      "grad_norm": 7.367319583892822,
      "learning_rate": 1.358029197080292e-05,
      "loss": 0.6963,
      "step": 99790
    },
    {
      "epoch": 728.4671532846716,
      "grad_norm": 8.114738464355469,
      "learning_rate": 1.3576642335766423e-05,
      "loss": 0.8669,
      "step": 99800
    },
    {
      "epoch": 728.5401459854014,
      "grad_norm": 6.416287422180176,
      "learning_rate": 1.3572992700729929e-05,
      "loss": 1.1579,
      "step": 99810
    },
    {
      "epoch": 728.6131386861314,
      "grad_norm": 10.68120288848877,
      "learning_rate": 1.3569343065693433e-05,
      "loss": 1.0754,
      "step": 99820
    },
    {
      "epoch": 728.6861313868613,
      "grad_norm": 5.076837539672852,
      "learning_rate": 1.3565693430656935e-05,
      "loss": 0.7066,
      "step": 99830
    },
    {
      "epoch": 728.7591240875912,
      "grad_norm": 7.867159366607666,
      "learning_rate": 1.3562043795620439e-05,
      "loss": 0.8107,
      "step": 99840
    },
    {
      "epoch": 728.8321167883212,
      "grad_norm": 6.480545520782471,
      "learning_rate": 1.3558394160583942e-05,
      "loss": 0.9265,
      "step": 99850
    },
    {
      "epoch": 728.9051094890511,
      "grad_norm": 7.893552303314209,
      "learning_rate": 1.3554744525547446e-05,
      "loss": 1.1582,
      "step": 99860
    },
    {
      "epoch": 728.978102189781,
      "grad_norm": 0.33135369420051575,
      "learning_rate": 1.3551094890510949e-05,
      "loss": 0.62,
      "step": 99870
    },
    {
      "epoch": 729.051094890511,
      "grad_norm": 13.458136558532715,
      "learning_rate": 1.3547445255474452e-05,
      "loss": 1.2992,
      "step": 99880
    },
    {
      "epoch": 729.1240875912408,
      "grad_norm": 0.046981893479824066,
      "learning_rate": 1.3543795620437958e-05,
      "loss": 1.1405,
      "step": 99890
    },
    {
      "epoch": 729.1970802919708,
      "grad_norm": 5.866264343261719,
      "learning_rate": 1.3540145985401462e-05,
      "loss": 0.8734,
      "step": 99900
    },
    {
      "epoch": 729.2700729927008,
      "grad_norm": 5.281427383422852,
      "learning_rate": 1.3536496350364964e-05,
      "loss": 0.282,
      "step": 99910
    },
    {
      "epoch": 729.3430656934306,
      "grad_norm": 9.578141212463379,
      "learning_rate": 1.3532846715328468e-05,
      "loss": 0.6711,
      "step": 99920
    },
    {
      "epoch": 729.4160583941606,
      "grad_norm": 9.155089378356934,
      "learning_rate": 1.3529197080291972e-05,
      "loss": 0.6431,
      "step": 99930
    },
    {
      "epoch": 729.4890510948906,
      "grad_norm": 14.978177070617676,
      "learning_rate": 1.3525547445255476e-05,
      "loss": 1.4893,
      "step": 99940
    },
    {
      "epoch": 729.5620437956204,
      "grad_norm": 6.613650321960449,
      "learning_rate": 1.3521897810218978e-05,
      "loss": 1.2289,
      "step": 99950
    },
    {
      "epoch": 729.6350364963504,
      "grad_norm": 0.06622184813022614,
      "learning_rate": 1.3518248175182482e-05,
      "loss": 0.758,
      "step": 99960
    },
    {
      "epoch": 729.7080291970802,
      "grad_norm": 5.133216857910156,
      "learning_rate": 1.3514598540145987e-05,
      "loss": 0.6548,
      "step": 99970
    },
    {
      "epoch": 729.7810218978102,
      "grad_norm": 10.65248966217041,
      "learning_rate": 1.3510948905109488e-05,
      "loss": 1.0355,
      "step": 99980
    },
    {
      "epoch": 729.8540145985402,
      "grad_norm": 5.033464431762695,
      "learning_rate": 1.3507299270072993e-05,
      "loss": 0.8942,
      "step": 99990
    },
    {
      "epoch": 729.92700729927,
      "grad_norm": 12.646065711975098,
      "learning_rate": 1.3503649635036497e-05,
      "loss": 0.7871,
      "step": 100000
    },
    {
      "epoch": 730.0,
      "grad_norm": 0.05726027861237526,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.9253,
      "step": 100010
    },
    {
      "epoch": 730.07299270073,
      "grad_norm": 10.96877670288086,
      "learning_rate": 1.3496350364963503e-05,
      "loss": 1.0651,
      "step": 100020
    },
    {
      "epoch": 730.1459854014598,
      "grad_norm": 12.991582870483398,
      "learning_rate": 1.3492700729927007e-05,
      "loss": 0.7969,
      "step": 100030
    },
    {
      "epoch": 730.2189781021898,
      "grad_norm": 12.958747863769531,
      "learning_rate": 1.3489051094890511e-05,
      "loss": 1.1063,
      "step": 100040
    },
    {
      "epoch": 730.2919708029198,
      "grad_norm": 6.988288402557373,
      "learning_rate": 1.3485401459854017e-05,
      "loss": 0.7776,
      "step": 100050
    },
    {
      "epoch": 730.3649635036496,
      "grad_norm": 7.9052863121032715,
      "learning_rate": 1.3481751824817517e-05,
      "loss": 0.8708,
      "step": 100060
    },
    {
      "epoch": 730.4379562043796,
      "grad_norm": 9.064369201660156,
      "learning_rate": 1.3478102189781023e-05,
      "loss": 0.8889,
      "step": 100070
    },
    {
      "epoch": 730.5109489051094,
      "grad_norm": 12.991333961486816,
      "learning_rate": 1.3474452554744527e-05,
      "loss": 0.6924,
      "step": 100080
    },
    {
      "epoch": 730.5839416058394,
      "grad_norm": 16.198156356811523,
      "learning_rate": 1.347080291970803e-05,
      "loss": 0.6531,
      "step": 100090
    },
    {
      "epoch": 730.6569343065694,
      "grad_norm": 10.533510208129883,
      "learning_rate": 1.3467153284671533e-05,
      "loss": 0.9148,
      "step": 100100
    },
    {
      "epoch": 730.7299270072992,
      "grad_norm": 16.62944984436035,
      "learning_rate": 1.3463503649635037e-05,
      "loss": 0.9539,
      "step": 100110
    },
    {
      "epoch": 730.8029197080292,
      "grad_norm": 6.838685989379883,
      "learning_rate": 1.345985401459854e-05,
      "loss": 0.8255,
      "step": 100120
    },
    {
      "epoch": 730.8759124087592,
      "grad_norm": 8.214557647705078,
      "learning_rate": 1.3456204379562046e-05,
      "loss": 0.814,
      "step": 100130
    },
    {
      "epoch": 730.948905109489,
      "grad_norm": 12.80888843536377,
      "learning_rate": 1.3452554744525547e-05,
      "loss": 1.1262,
      "step": 100140
    },
    {
      "epoch": 731.021897810219,
      "grad_norm": 7.6103739738464355,
      "learning_rate": 1.3448905109489052e-05,
      "loss": 0.8418,
      "step": 100150
    },
    {
      "epoch": 731.0948905109489,
      "grad_norm": 0.09283049404621124,
      "learning_rate": 1.3445255474452556e-05,
      "loss": 0.9718,
      "step": 100160
    },
    {
      "epoch": 731.1678832116788,
      "grad_norm": 16.238832473754883,
      "learning_rate": 1.3441605839416058e-05,
      "loss": 1.3057,
      "step": 100170
    },
    {
      "epoch": 731.2408759124088,
      "grad_norm": 15.147260665893555,
      "learning_rate": 1.3437956204379562e-05,
      "loss": 1.1257,
      "step": 100180
    },
    {
      "epoch": 731.3138686131387,
      "grad_norm": 8.716146469116211,
      "learning_rate": 1.3434306569343066e-05,
      "loss": 1.117,
      "step": 100190
    },
    {
      "epoch": 731.3868613138686,
      "grad_norm": 8.845951080322266,
      "learning_rate": 1.3430656934306572e-05,
      "loss": 1.1416,
      "step": 100200
    },
    {
      "epoch": 731.4598540145986,
      "grad_norm": 0.020646292716264725,
      "learning_rate": 1.3427007299270072e-05,
      "loss": 0.6114,
      "step": 100210
    },
    {
      "epoch": 731.5328467153284,
      "grad_norm": 7.492231369018555,
      "learning_rate": 1.3423357664233576e-05,
      "loss": 0.8289,
      "step": 100220
    },
    {
      "epoch": 731.6058394160584,
      "grad_norm": 0.04321685805916786,
      "learning_rate": 1.3419708029197082e-05,
      "loss": 0.5026,
      "step": 100230
    },
    {
      "epoch": 731.6788321167883,
      "grad_norm": 11.434674263000488,
      "learning_rate": 1.3416058394160585e-05,
      "loss": 0.9364,
      "step": 100240
    },
    {
      "epoch": 731.7518248175182,
      "grad_norm": 10.284521102905273,
      "learning_rate": 1.3412408759124088e-05,
      "loss": 1.1591,
      "step": 100250
    },
    {
      "epoch": 731.8248175182482,
      "grad_norm": 8.204402923583984,
      "learning_rate": 1.3408759124087591e-05,
      "loss": 0.7907,
      "step": 100260
    },
    {
      "epoch": 731.8978102189781,
      "grad_norm": 10.811529159545898,
      "learning_rate": 1.3405109489051095e-05,
      "loss": 0.9057,
      "step": 100270
    },
    {
      "epoch": 731.970802919708,
      "grad_norm": 11.346290588378906,
      "learning_rate": 1.3401459854014601e-05,
      "loss": 0.7712,
      "step": 100280
    },
    {
      "epoch": 732.043795620438,
      "grad_norm": 14.989068984985352,
      "learning_rate": 1.3397810218978101e-05,
      "loss": 0.9696,
      "step": 100290
    },
    {
      "epoch": 732.1167883211679,
      "grad_norm": 0.127301424741745,
      "learning_rate": 1.3394160583941607e-05,
      "loss": 0.4969,
      "step": 100300
    },
    {
      "epoch": 732.1897810218978,
      "grad_norm": 7.136111259460449,
      "learning_rate": 1.3390510948905111e-05,
      "loss": 0.7881,
      "step": 100310
    },
    {
      "epoch": 732.2627737226277,
      "grad_norm": 9.795296669006348,
      "learning_rate": 1.3386861313868615e-05,
      "loss": 0.4962,
      "step": 100320
    },
    {
      "epoch": 732.3357664233577,
      "grad_norm": 10.279121398925781,
      "learning_rate": 1.3383211678832117e-05,
      "loss": 0.7822,
      "step": 100330
    },
    {
      "epoch": 732.4087591240876,
      "grad_norm": 10.293266296386719,
      "learning_rate": 1.337956204379562e-05,
      "loss": 0.7462,
      "step": 100340
    },
    {
      "epoch": 732.4817518248175,
      "grad_norm": 13.767742156982422,
      "learning_rate": 1.3375912408759125e-05,
      "loss": 1.1743,
      "step": 100350
    },
    {
      "epoch": 732.5547445255474,
      "grad_norm": 12.426844596862793,
      "learning_rate": 1.337226277372263e-05,
      "loss": 0.7977,
      "step": 100360
    },
    {
      "epoch": 732.6277372262774,
      "grad_norm": 10.966814994812012,
      "learning_rate": 1.336861313868613e-05,
      "loss": 0.8733,
      "step": 100370
    },
    {
      "epoch": 732.7007299270073,
      "grad_norm": 15.11194133758545,
      "learning_rate": 1.3364963503649636e-05,
      "loss": 0.9824,
      "step": 100380
    },
    {
      "epoch": 732.7737226277372,
      "grad_norm": 9.223773002624512,
      "learning_rate": 1.336131386861314e-05,
      "loss": 1.0077,
      "step": 100390
    },
    {
      "epoch": 732.8467153284671,
      "grad_norm": 11.717120170593262,
      "learning_rate": 1.3357664233576642e-05,
      "loss": 1.2682,
      "step": 100400
    },
    {
      "epoch": 732.9197080291971,
      "grad_norm": 10.509215354919434,
      "learning_rate": 1.3354014598540146e-05,
      "loss": 1.1027,
      "step": 100410
    },
    {
      "epoch": 732.992700729927,
      "grad_norm": 11.8463716506958,
      "learning_rate": 1.335036496350365e-05,
      "loss": 0.8686,
      "step": 100420
    },
    {
      "epoch": 733.0656934306569,
      "grad_norm": 10.226844787597656,
      "learning_rate": 1.3346715328467154e-05,
      "loss": 1.0148,
      "step": 100430
    },
    {
      "epoch": 733.1386861313869,
      "grad_norm": 0.023417657241225243,
      "learning_rate": 1.3343065693430656e-05,
      "loss": 0.9495,
      "step": 100440
    },
    {
      "epoch": 733.2116788321168,
      "grad_norm": 7.975705623626709,
      "learning_rate": 1.333941605839416e-05,
      "loss": 0.7615,
      "step": 100450
    },
    {
      "epoch": 733.2846715328467,
      "grad_norm": 0.12888173758983612,
      "learning_rate": 1.3335766423357666e-05,
      "loss": 0.3701,
      "step": 100460
    },
    {
      "epoch": 733.3576642335767,
      "grad_norm": 7.333659648895264,
      "learning_rate": 1.333211678832117e-05,
      "loss": 0.682,
      "step": 100470
    },
    {
      "epoch": 733.4306569343066,
      "grad_norm": 12.0283203125,
      "learning_rate": 1.3328467153284672e-05,
      "loss": 1.492,
      "step": 100480
    },
    {
      "epoch": 733.5036496350365,
      "grad_norm": 0.031823962926864624,
      "learning_rate": 1.3324817518248176e-05,
      "loss": 0.8131,
      "step": 100490
    },
    {
      "epoch": 733.5766423357665,
      "grad_norm": 10.1773042678833,
      "learning_rate": 1.332116788321168e-05,
      "loss": 1.1838,
      "step": 100500
    },
    {
      "epoch": 733.6496350364963,
      "grad_norm": 6.243880748748779,
      "learning_rate": 1.3317518248175183e-05,
      "loss": 0.7315,
      "step": 100510
    },
    {
      "epoch": 733.7226277372263,
      "grad_norm": 14.149459838867188,
      "learning_rate": 1.3313868613138686e-05,
      "loss": 1.0212,
      "step": 100520
    },
    {
      "epoch": 733.7956204379562,
      "grad_norm": 15.915700912475586,
      "learning_rate": 1.331021897810219e-05,
      "loss": 0.8251,
      "step": 100530
    },
    {
      "epoch": 733.8686131386861,
      "grad_norm": 14.460226058959961,
      "learning_rate": 1.3306569343065695e-05,
      "loss": 0.9507,
      "step": 100540
    },
    {
      "epoch": 733.9416058394161,
      "grad_norm": 13.48696517944336,
      "learning_rate": 1.3302919708029199e-05,
      "loss": 1.1145,
      "step": 100550
    },
    {
      "epoch": 734.014598540146,
      "grad_norm": 5.73976993560791,
      "learning_rate": 1.3299270072992701e-05,
      "loss": 0.8344,
      "step": 100560
    },
    {
      "epoch": 734.0875912408759,
      "grad_norm": 15.677437782287598,
      "learning_rate": 1.3295620437956205e-05,
      "loss": 0.8461,
      "step": 100570
    },
    {
      "epoch": 734.1605839416059,
      "grad_norm": 14.920228004455566,
      "learning_rate": 1.3291970802919709e-05,
      "loss": 1.3309,
      "step": 100580
    },
    {
      "epoch": 734.2335766423357,
      "grad_norm": 11.792584419250488,
      "learning_rate": 1.3288321167883215e-05,
      "loss": 1.0963,
      "step": 100590
    },
    {
      "epoch": 734.3065693430657,
      "grad_norm": 0.03995886445045471,
      "learning_rate": 1.3284671532846715e-05,
      "loss": 0.8469,
      "step": 100600
    },
    {
      "epoch": 734.3795620437957,
      "grad_norm": 0.09526047110557556,
      "learning_rate": 1.3281021897810219e-05,
      "loss": 0.8931,
      "step": 100610
    },
    {
      "epoch": 734.4525547445255,
      "grad_norm": 0.021422995254397392,
      "learning_rate": 1.3277372262773724e-05,
      "loss": 0.5781,
      "step": 100620
    },
    {
      "epoch": 734.5255474452555,
      "grad_norm": 21.014249801635742,
      "learning_rate": 1.3273722627737225e-05,
      "loss": 0.7895,
      "step": 100630
    },
    {
      "epoch": 734.5985401459855,
      "grad_norm": 7.917107105255127,
      "learning_rate": 1.327007299270073e-05,
      "loss": 0.9064,
      "step": 100640
    },
    {
      "epoch": 734.6715328467153,
      "grad_norm": 8.408734321594238,
      "learning_rate": 1.3266423357664234e-05,
      "loss": 0.9774,
      "step": 100650
    },
    {
      "epoch": 734.7445255474453,
      "grad_norm": 16.028244018554688,
      "learning_rate": 1.3262773722627738e-05,
      "loss": 0.7714,
      "step": 100660
    },
    {
      "epoch": 734.8175182481751,
      "grad_norm": 14.462533950805664,
      "learning_rate": 1.325912408759124e-05,
      "loss": 0.807,
      "step": 100670
    },
    {
      "epoch": 734.8905109489051,
      "grad_norm": 8.864622116088867,
      "learning_rate": 1.3255474452554744e-05,
      "loss": 0.8966,
      "step": 100680
    },
    {
      "epoch": 734.9635036496351,
      "grad_norm": 11.913627624511719,
      "learning_rate": 1.325182481751825e-05,
      "loss": 1.2556,
      "step": 100690
    },
    {
      "epoch": 735.0364963503649,
      "grad_norm": 11.016932487487793,
      "learning_rate": 1.3248175182481754e-05,
      "loss": 0.5895,
      "step": 100700
    },
    {
      "epoch": 735.1094890510949,
      "grad_norm": 7.753310680389404,
      "learning_rate": 1.3244525547445254e-05,
      "loss": 0.7235,
      "step": 100710
    },
    {
      "epoch": 735.1824817518249,
      "grad_norm": 11.814712524414062,
      "learning_rate": 1.324087591240876e-05,
      "loss": 1.0094,
      "step": 100720
    },
    {
      "epoch": 735.2554744525547,
      "grad_norm": 9.403491020202637,
      "learning_rate": 1.3237226277372264e-05,
      "loss": 1.1254,
      "step": 100730
    },
    {
      "epoch": 735.3284671532847,
      "grad_norm": 8.201953887939453,
      "learning_rate": 1.3233576642335768e-05,
      "loss": 0.4456,
      "step": 100740
    },
    {
      "epoch": 735.4014598540145,
      "grad_norm": 10.220658302307129,
      "learning_rate": 1.322992700729927e-05,
      "loss": 0.8674,
      "step": 100750
    },
    {
      "epoch": 735.4744525547445,
      "grad_norm": 7.1006951332092285,
      "learning_rate": 1.3226277372262774e-05,
      "loss": 0.8375,
      "step": 100760
    },
    {
      "epoch": 735.5474452554745,
      "grad_norm": 15.92899227142334,
      "learning_rate": 1.322262773722628e-05,
      "loss": 0.8081,
      "step": 100770
    },
    {
      "epoch": 735.6204379562043,
      "grad_norm": 13.332202911376953,
      "learning_rate": 1.3218978102189783e-05,
      "loss": 1.2649,
      "step": 100780
    },
    {
      "epoch": 735.6934306569343,
      "grad_norm": 4.808169841766357,
      "learning_rate": 1.3215328467153285e-05,
      "loss": 0.7097,
      "step": 100790
    },
    {
      "epoch": 735.7664233576643,
      "grad_norm": 8.554908752441406,
      "learning_rate": 1.321167883211679e-05,
      "loss": 0.5421,
      "step": 100800
    },
    {
      "epoch": 735.8394160583941,
      "grad_norm": 14.85577392578125,
      "learning_rate": 1.3208029197080293e-05,
      "loss": 0.8931,
      "step": 100810
    },
    {
      "epoch": 735.9124087591241,
      "grad_norm": 12.042017936706543,
      "learning_rate": 1.3204379562043795e-05,
      "loss": 0.8607,
      "step": 100820
    },
    {
      "epoch": 735.985401459854,
      "grad_norm": 12.90487003326416,
      "learning_rate": 1.32007299270073e-05,
      "loss": 1.1375,
      "step": 100830
    },
    {
      "epoch": 736.0583941605839,
      "grad_norm": 18.08064842224121,
      "learning_rate": 1.3197080291970803e-05,
      "loss": 1.3248,
      "step": 100840
    },
    {
      "epoch": 736.1313868613139,
      "grad_norm": 10.47941780090332,
      "learning_rate": 1.3193430656934309e-05,
      "loss": 0.9609,
      "step": 100850
    },
    {
      "epoch": 736.2043795620438,
      "grad_norm": 7.671816349029541,
      "learning_rate": 1.318978102189781e-05,
      "loss": 0.9092,
      "step": 100860
    },
    {
      "epoch": 736.2773722627737,
      "grad_norm": 7.3608174324035645,
      "learning_rate": 1.3186131386861315e-05,
      "loss": 0.7525,
      "step": 100870
    },
    {
      "epoch": 736.3503649635037,
      "grad_norm": 6.263529300689697,
      "learning_rate": 1.3182481751824819e-05,
      "loss": 0.8865,
      "step": 100880
    },
    {
      "epoch": 736.4233576642335,
      "grad_norm": 5.830512046813965,
      "learning_rate": 1.3178832116788323e-05,
      "loss": 0.8145,
      "step": 100890
    },
    {
      "epoch": 736.4963503649635,
      "grad_norm": 15.461847305297852,
      "learning_rate": 1.3175182481751825e-05,
      "loss": 0.8133,
      "step": 100900
    },
    {
      "epoch": 736.5693430656934,
      "grad_norm": 10.604476928710938,
      "learning_rate": 1.3171532846715329e-05,
      "loss": 0.7746,
      "step": 100910
    },
    {
      "epoch": 736.6423357664233,
      "grad_norm": 0.04165106639266014,
      "learning_rate": 1.3167883211678832e-05,
      "loss": 0.8191,
      "step": 100920
    },
    {
      "epoch": 736.7153284671533,
      "grad_norm": 10.08638858795166,
      "learning_rate": 1.3164233576642338e-05,
      "loss": 0.8963,
      "step": 100930
    },
    {
      "epoch": 736.7883211678832,
      "grad_norm": 7.377933502197266,
      "learning_rate": 1.3160583941605839e-05,
      "loss": 0.6612,
      "step": 100940
    },
    {
      "epoch": 736.8613138686131,
      "grad_norm": 0.07417616993188858,
      "learning_rate": 1.3156934306569344e-05,
      "loss": 0.8125,
      "step": 100950
    },
    {
      "epoch": 736.9343065693431,
      "grad_norm": 7.961746692657471,
      "learning_rate": 1.3153284671532848e-05,
      "loss": 1.2598,
      "step": 100960
    },
    {
      "epoch": 737.007299270073,
      "grad_norm": 10.28314208984375,
      "learning_rate": 1.3149635036496352e-05,
      "loss": 0.6655,
      "step": 100970
    },
    {
      "epoch": 737.0802919708029,
      "grad_norm": 0.010979348793625832,
      "learning_rate": 1.3145985401459854e-05,
      "loss": 0.7934,
      "step": 100980
    },
    {
      "epoch": 737.1532846715329,
      "grad_norm": 7.188759803771973,
      "learning_rate": 1.3142335766423358e-05,
      "loss": 0.9637,
      "step": 100990
    },
    {
      "epoch": 737.2262773722628,
      "grad_norm": 12.825556755065918,
      "learning_rate": 1.3138686131386862e-05,
      "loss": 0.8841,
      "step": 101000
    },
    {
      "epoch": 737.2992700729927,
      "grad_norm": 0.03126222640275955,
      "learning_rate": 1.3135036496350367e-05,
      "loss": 0.7938,
      "step": 101010
    },
    {
      "epoch": 737.3722627737226,
      "grad_norm": 6.048185348510742,
      "learning_rate": 1.3131386861313868e-05,
      "loss": 0.5813,
      "step": 101020
    },
    {
      "epoch": 737.4452554744526,
      "grad_norm": 8.053133010864258,
      "learning_rate": 1.3127737226277373e-05,
      "loss": 1.1204,
      "step": 101030
    },
    {
      "epoch": 737.5182481751825,
      "grad_norm": 0.06129045784473419,
      "learning_rate": 1.3124087591240877e-05,
      "loss": 0.9942,
      "step": 101040
    },
    {
      "epoch": 737.5912408759124,
      "grad_norm": 11.331687927246094,
      "learning_rate": 1.312043795620438e-05,
      "loss": 0.8953,
      "step": 101050
    },
    {
      "epoch": 737.6642335766423,
      "grad_norm": 12.776577949523926,
      "learning_rate": 1.3116788321167883e-05,
      "loss": 1.106,
      "step": 101060
    },
    {
      "epoch": 737.7372262773723,
      "grad_norm": 6.538780212402344,
      "learning_rate": 1.3113138686131387e-05,
      "loss": 1.0029,
      "step": 101070
    },
    {
      "epoch": 737.8102189781022,
      "grad_norm": 14.078404426574707,
      "learning_rate": 1.3109489051094891e-05,
      "loss": 0.8625,
      "step": 101080
    },
    {
      "epoch": 737.8832116788321,
      "grad_norm": 9.917120933532715,
      "learning_rate": 1.3105839416058393e-05,
      "loss": 0.9631,
      "step": 101090
    },
    {
      "epoch": 737.956204379562,
      "grad_norm": 0.010233904235064983,
      "learning_rate": 1.3102189781021897e-05,
      "loss": 0.9593,
      "step": 101100
    },
    {
      "epoch": 738.029197080292,
      "grad_norm": 9.393180847167969,
      "learning_rate": 1.3098540145985403e-05,
      "loss": 0.7662,
      "step": 101110
    },
    {
      "epoch": 738.1021897810219,
      "grad_norm": 12.555758476257324,
      "learning_rate": 1.3094890510948907e-05,
      "loss": 1.1335,
      "step": 101120
    },
    {
      "epoch": 738.1751824817518,
      "grad_norm": 17.17645263671875,
      "learning_rate": 1.3091240875912409e-05,
      "loss": 0.9034,
      "step": 101130
    },
    {
      "epoch": 738.2481751824818,
      "grad_norm": 9.588665008544922,
      "learning_rate": 1.3087591240875913e-05,
      "loss": 0.6868,
      "step": 101140
    },
    {
      "epoch": 738.3211678832117,
      "grad_norm": 16.72372055053711,
      "learning_rate": 1.3083941605839417e-05,
      "loss": 0.6533,
      "step": 101150
    },
    {
      "epoch": 738.3941605839416,
      "grad_norm": 11.82969856262207,
      "learning_rate": 1.3080291970802922e-05,
      "loss": 0.9676,
      "step": 101160
    },
    {
      "epoch": 738.4671532846716,
      "grad_norm": 4.14209508895874,
      "learning_rate": 1.3076642335766423e-05,
      "loss": 0.9796,
      "step": 101170
    },
    {
      "epoch": 738.5401459854014,
      "grad_norm": 19.226276397705078,
      "learning_rate": 1.3072992700729927e-05,
      "loss": 0.9136,
      "step": 101180
    },
    {
      "epoch": 738.6131386861314,
      "grad_norm": 11.242679595947266,
      "learning_rate": 1.3069343065693432e-05,
      "loss": 0.9569,
      "step": 101190
    },
    {
      "epoch": 738.6861313868613,
      "grad_norm": 11.69552230834961,
      "learning_rate": 1.3065693430656936e-05,
      "loss": 0.9682,
      "step": 101200
    },
    {
      "epoch": 738.7591240875912,
      "grad_norm": 12.559625625610352,
      "learning_rate": 1.3062043795620438e-05,
      "loss": 1.1405,
      "step": 101210
    },
    {
      "epoch": 738.8321167883212,
      "grad_norm": 12.035673141479492,
      "learning_rate": 1.3058394160583942e-05,
      "loss": 0.7688,
      "step": 101220
    },
    {
      "epoch": 738.9051094890511,
      "grad_norm": 14.901019096374512,
      "learning_rate": 1.3054744525547446e-05,
      "loss": 0.8934,
      "step": 101230
    },
    {
      "epoch": 738.978102189781,
      "grad_norm": 9.435510635375977,
      "learning_rate": 1.3051094890510952e-05,
      "loss": 0.8419,
      "step": 101240
    },
    {
      "epoch": 739.051094890511,
      "grad_norm": 10.10970401763916,
      "learning_rate": 1.3047445255474452e-05,
      "loss": 0.8675,
      "step": 101250
    },
    {
      "epoch": 739.1240875912408,
      "grad_norm": 6.604050636291504,
      "learning_rate": 1.3043795620437958e-05,
      "loss": 0.9343,
      "step": 101260
    },
    {
      "epoch": 739.1970802919708,
      "grad_norm": 7.452175140380859,
      "learning_rate": 1.3040145985401462e-05,
      "loss": 0.9082,
      "step": 101270
    },
    {
      "epoch": 739.2700729927008,
      "grad_norm": 9.28406810760498,
      "learning_rate": 1.3036496350364962e-05,
      "loss": 1.1131,
      "step": 101280
    },
    {
      "epoch": 739.3430656934306,
      "grad_norm": 8.720726013183594,
      "learning_rate": 1.3032846715328468e-05,
      "loss": 1.0165,
      "step": 101290
    },
    {
      "epoch": 739.4160583941606,
      "grad_norm": 12.24527359008789,
      "learning_rate": 1.3029197080291972e-05,
      "loss": 0.9531,
      "step": 101300
    },
    {
      "epoch": 739.4890510948906,
      "grad_norm": 8.385725975036621,
      "learning_rate": 1.3025547445255475e-05,
      "loss": 0.8837,
      "step": 101310
    },
    {
      "epoch": 739.5620437956204,
      "grad_norm": 6.388294696807861,
      "learning_rate": 1.3021897810218978e-05,
      "loss": 0.7786,
      "step": 101320
    },
    {
      "epoch": 739.6350364963504,
      "grad_norm": 0.09772077202796936,
      "learning_rate": 1.3018248175182481e-05,
      "loss": 0.656,
      "step": 101330
    },
    {
      "epoch": 739.7080291970802,
      "grad_norm": 7.6331353187561035,
      "learning_rate": 1.3014598540145987e-05,
      "loss": 0.8977,
      "step": 101340
    },
    {
      "epoch": 739.7810218978102,
      "grad_norm": 14.850152969360352,
      "learning_rate": 1.3010948905109491e-05,
      "loss": 0.4924,
      "step": 101350
    },
    {
      "epoch": 739.8540145985402,
      "grad_norm": 13.96119499206543,
      "learning_rate": 1.3007299270072993e-05,
      "loss": 0.7838,
      "step": 101360
    },
    {
      "epoch": 739.92700729927,
      "grad_norm": 2.982954502105713,
      "learning_rate": 1.3003649635036497e-05,
      "loss": 0.9379,
      "step": 101370
    },
    {
      "epoch": 740.0,
      "grad_norm": 17.360572814941406,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.0472,
      "step": 101380
    },
    {
      "epoch": 740.07299270073,
      "grad_norm": 0.007481788285076618,
      "learning_rate": 1.2996350364963505e-05,
      "loss": 0.6653,
      "step": 101390
    },
    {
      "epoch": 740.1459854014598,
      "grad_norm": 7.620318412780762,
      "learning_rate": 1.2992700729927007e-05,
      "loss": 0.9656,
      "step": 101400
    },
    {
      "epoch": 740.2189781021898,
      "grad_norm": 10.327220916748047,
      "learning_rate": 1.2989051094890511e-05,
      "loss": 0.7796,
      "step": 101410
    },
    {
      "epoch": 740.2919708029198,
      "grad_norm": 11.632027626037598,
      "learning_rate": 1.2985401459854016e-05,
      "loss": 1.2721,
      "step": 101420
    },
    {
      "epoch": 740.3649635036496,
      "grad_norm": 7.117374420166016,
      "learning_rate": 1.298175182481752e-05,
      "loss": 0.8479,
      "step": 101430
    },
    {
      "epoch": 740.4379562043796,
      "grad_norm": 10.757548332214355,
      "learning_rate": 1.2978102189781022e-05,
      "loss": 0.9155,
      "step": 101440
    },
    {
      "epoch": 740.5109489051094,
      "grad_norm": 0.0604977160692215,
      "learning_rate": 1.2974452554744526e-05,
      "loss": 0.8818,
      "step": 101450
    },
    {
      "epoch": 740.5839416058394,
      "grad_norm": 6.645974636077881,
      "learning_rate": 1.297080291970803e-05,
      "loss": 0.9728,
      "step": 101460
    },
    {
      "epoch": 740.6569343065694,
      "grad_norm": 13.596649169921875,
      "learning_rate": 1.2967153284671532e-05,
      "loss": 0.9202,
      "step": 101470
    },
    {
      "epoch": 740.7299270072992,
      "grad_norm": 0.026328476145863533,
      "learning_rate": 1.2963503649635036e-05,
      "loss": 0.7878,
      "step": 101480
    },
    {
      "epoch": 740.8029197080292,
      "grad_norm": 4.0651984214782715,
      "learning_rate": 1.295985401459854e-05,
      "loss": 0.8183,
      "step": 101490
    },
    {
      "epoch": 740.8759124087592,
      "grad_norm": 9.931356430053711,
      "learning_rate": 1.2956204379562046e-05,
      "loss": 0.8789,
      "step": 101500
    },
    {
      "epoch": 740.948905109489,
      "grad_norm": 14.53122615814209,
      "learning_rate": 1.2952554744525546e-05,
      "loss": 1.2424,
      "step": 101510
    },
    {
      "epoch": 741.021897810219,
      "grad_norm": 13.110350608825684,
      "learning_rate": 1.2948905109489052e-05,
      "loss": 0.7335,
      "step": 101520
    },
    {
      "epoch": 741.0948905109489,
      "grad_norm": 0.05320974439382553,
      "learning_rate": 1.2945255474452556e-05,
      "loss": 1.149,
      "step": 101530
    },
    {
      "epoch": 741.1678832116788,
      "grad_norm": 11.166842460632324,
      "learning_rate": 1.294160583941606e-05,
      "loss": 0.9985,
      "step": 101540
    },
    {
      "epoch": 741.2408759124088,
      "grad_norm": 15.31576919555664,
      "learning_rate": 1.2937956204379562e-05,
      "loss": 1.1821,
      "step": 101550
    },
    {
      "epoch": 741.3138686131387,
      "grad_norm": 11.00094985961914,
      "learning_rate": 1.2934306569343066e-05,
      "loss": 1.0185,
      "step": 101560
    },
    {
      "epoch": 741.3868613138686,
      "grad_norm": 8.811446189880371,
      "learning_rate": 1.293065693430657e-05,
      "loss": 1.1574,
      "step": 101570
    },
    {
      "epoch": 741.4598540145986,
      "grad_norm": 12.157764434814453,
      "learning_rate": 1.2927007299270075e-05,
      "loss": 0.9589,
      "step": 101580
    },
    {
      "epoch": 741.5328467153284,
      "grad_norm": 4.6052422523498535,
      "learning_rate": 1.2923357664233576e-05,
      "loss": 0.8668,
      "step": 101590
    },
    {
      "epoch": 741.6058394160584,
      "grad_norm": 8.338512420654297,
      "learning_rate": 1.2919708029197081e-05,
      "loss": 0.6985,
      "step": 101600
    },
    {
      "epoch": 741.6788321167883,
      "grad_norm": 7.185252666473389,
      "learning_rate": 1.2916058394160585e-05,
      "loss": 0.7518,
      "step": 101610
    },
    {
      "epoch": 741.7518248175182,
      "grad_norm": 6.867691993713379,
      "learning_rate": 1.2912408759124089e-05,
      "loss": 0.8877,
      "step": 101620
    },
    {
      "epoch": 741.8248175182482,
      "grad_norm": 6.946660041809082,
      "learning_rate": 1.2908759124087591e-05,
      "loss": 0.6884,
      "step": 101630
    },
    {
      "epoch": 741.8978102189781,
      "grad_norm": 14.320344924926758,
      "learning_rate": 1.2905109489051095e-05,
      "loss": 0.9653,
      "step": 101640
    },
    {
      "epoch": 741.970802919708,
      "grad_norm": 2.3709938526153564,
      "learning_rate": 1.2901459854014599e-05,
      "loss": 0.5045,
      "step": 101650
    },
    {
      "epoch": 742.043795620438,
      "grad_norm": 11.074084281921387,
      "learning_rate": 1.2897810218978105e-05,
      "loss": 1.0066,
      "step": 101660
    },
    {
      "epoch": 742.1167883211679,
      "grad_norm": 13.032408714294434,
      "learning_rate": 1.2894160583941605e-05,
      "loss": 0.9405,
      "step": 101670
    },
    {
      "epoch": 742.1897810218978,
      "grad_norm": 12.897307395935059,
      "learning_rate": 1.289051094890511e-05,
      "loss": 0.8518,
      "step": 101680
    },
    {
      "epoch": 742.2627737226277,
      "grad_norm": 16.46646499633789,
      "learning_rate": 1.2886861313868614e-05,
      "loss": 1.0118,
      "step": 101690
    },
    {
      "epoch": 742.3357664233577,
      "grad_norm": 0.04642786458134651,
      "learning_rate": 1.2883211678832117e-05,
      "loss": 0.9003,
      "step": 101700
    },
    {
      "epoch": 742.4087591240876,
      "grad_norm": 14.666326522827148,
      "learning_rate": 1.287956204379562e-05,
      "loss": 0.9436,
      "step": 101710
    },
    {
      "epoch": 742.4817518248175,
      "grad_norm": 12.24007797241211,
      "learning_rate": 1.2875912408759124e-05,
      "loss": 0.9583,
      "step": 101720
    },
    {
      "epoch": 742.5547445255474,
      "grad_norm": 11.30872917175293,
      "learning_rate": 1.287226277372263e-05,
      "loss": 0.9957,
      "step": 101730
    },
    {
      "epoch": 742.6277372262774,
      "grad_norm": 19.238449096679688,
      "learning_rate": 1.286861313868613e-05,
      "loss": 1.1468,
      "step": 101740
    },
    {
      "epoch": 742.7007299270073,
      "grad_norm": 7.276492595672607,
      "learning_rate": 1.2864963503649634e-05,
      "loss": 0.544,
      "step": 101750
    },
    {
      "epoch": 742.7737226277372,
      "grad_norm": 0.133127361536026,
      "learning_rate": 1.286131386861314e-05,
      "loss": 0.8796,
      "step": 101760
    },
    {
      "epoch": 742.8467153284671,
      "grad_norm": 0.05165792629122734,
      "learning_rate": 1.2857664233576644e-05,
      "loss": 0.6626,
      "step": 101770
    },
    {
      "epoch": 742.9197080291971,
      "grad_norm": 11.298454284667969,
      "learning_rate": 1.2854014598540146e-05,
      "loss": 0.8382,
      "step": 101780
    },
    {
      "epoch": 742.992700729927,
      "grad_norm": 12.379246711730957,
      "learning_rate": 1.285036496350365e-05,
      "loss": 0.5131,
      "step": 101790
    },
    {
      "epoch": 743.0656934306569,
      "grad_norm": 9.434618949890137,
      "learning_rate": 1.2846715328467154e-05,
      "loss": 1.3527,
      "step": 101800
    },
    {
      "epoch": 743.1386861313869,
      "grad_norm": 12.207950592041016,
      "learning_rate": 1.284306569343066e-05,
      "loss": 0.8548,
      "step": 101810
    },
    {
      "epoch": 743.2116788321168,
      "grad_norm": 6.921069145202637,
      "learning_rate": 1.283941605839416e-05,
      "loss": 0.7768,
      "step": 101820
    },
    {
      "epoch": 743.2846715328467,
      "grad_norm": 8.576250076293945,
      "learning_rate": 1.2835766423357665e-05,
      "loss": 0.9291,
      "step": 101830
    },
    {
      "epoch": 743.3576642335767,
      "grad_norm": 6.4170823097229,
      "learning_rate": 1.283211678832117e-05,
      "loss": 1.1272,
      "step": 101840
    },
    {
      "epoch": 743.4306569343066,
      "grad_norm": 0.01837516948580742,
      "learning_rate": 1.2828467153284673e-05,
      "loss": 0.9348,
      "step": 101850
    },
    {
      "epoch": 743.5036496350365,
      "grad_norm": 13.705495834350586,
      "learning_rate": 1.2824817518248175e-05,
      "loss": 0.9918,
      "step": 101860
    },
    {
      "epoch": 743.5766423357665,
      "grad_norm": 7.428184986114502,
      "learning_rate": 1.282116788321168e-05,
      "loss": 0.6591,
      "step": 101870
    },
    {
      "epoch": 743.6496350364963,
      "grad_norm": 7.668964862823486,
      "learning_rate": 1.2817518248175183e-05,
      "loss": 0.8346,
      "step": 101880
    },
    {
      "epoch": 743.7226277372263,
      "grad_norm": 11.146531105041504,
      "learning_rate": 1.2813868613138685e-05,
      "loss": 0.9321,
      "step": 101890
    },
    {
      "epoch": 743.7956204379562,
      "grad_norm": 0.2606828808784485,
      "learning_rate": 1.281021897810219e-05,
      "loss": 0.948,
      "step": 101900
    },
    {
      "epoch": 743.8686131386861,
      "grad_norm": 0.08199763298034668,
      "learning_rate": 1.2806569343065695e-05,
      "loss": 0.5303,
      "step": 101910
    },
    {
      "epoch": 743.9416058394161,
      "grad_norm": 8.661748886108398,
      "learning_rate": 1.2802919708029199e-05,
      "loss": 0.8013,
      "step": 101920
    },
    {
      "epoch": 744.014598540146,
      "grad_norm": 10.71215534210205,
      "learning_rate": 1.2799270072992701e-05,
      "loss": 0.7954,
      "step": 101930
    },
    {
      "epoch": 744.0875912408759,
      "grad_norm": 13.434135437011719,
      "learning_rate": 1.2795620437956205e-05,
      "loss": 0.7456,
      "step": 101940
    },
    {
      "epoch": 744.1605839416059,
      "grad_norm": 10.233370780944824,
      "learning_rate": 1.2791970802919709e-05,
      "loss": 0.8937,
      "step": 101950
    },
    {
      "epoch": 744.2335766423357,
      "grad_norm": 15.746493339538574,
      "learning_rate": 1.2788321167883213e-05,
      "loss": 1.09,
      "step": 101960
    },
    {
      "epoch": 744.3065693430657,
      "grad_norm": 11.181849479675293,
      "learning_rate": 1.2784671532846715e-05,
      "loss": 1.3142,
      "step": 101970
    },
    {
      "epoch": 744.3795620437957,
      "grad_norm": 10.225720405578613,
      "learning_rate": 1.2781021897810219e-05,
      "loss": 0.9017,
      "step": 101980
    },
    {
      "epoch": 744.4525547445255,
      "grad_norm": 12.558097839355469,
      "learning_rate": 1.2777372262773724e-05,
      "loss": 1.189,
      "step": 101990
    },
    {
      "epoch": 744.5255474452555,
      "grad_norm": 0.056777939200401306,
      "learning_rate": 1.2773722627737228e-05,
      "loss": 0.6493,
      "step": 102000
    },
    {
      "epoch": 744.5985401459855,
      "grad_norm": 14.677990913391113,
      "learning_rate": 1.277007299270073e-05,
      "loss": 0.9238,
      "step": 102010
    },
    {
      "epoch": 744.6715328467153,
      "grad_norm": 8.09196662902832,
      "learning_rate": 1.2766423357664234e-05,
      "loss": 1.2587,
      "step": 102020
    },
    {
      "epoch": 744.7445255474453,
      "grad_norm": 13.43422794342041,
      "learning_rate": 1.2762773722627738e-05,
      "loss": 0.4142,
      "step": 102030
    },
    {
      "epoch": 744.8175182481751,
      "grad_norm": 0.15052664279937744,
      "learning_rate": 1.2759124087591242e-05,
      "loss": 0.5158,
      "step": 102040
    },
    {
      "epoch": 744.8905109489051,
      "grad_norm": 8.617745399475098,
      "learning_rate": 1.2755474452554744e-05,
      "loss": 0.7484,
      "step": 102050
    },
    {
      "epoch": 744.9635036496351,
      "grad_norm": 0.0379362553358078,
      "learning_rate": 1.2751824817518248e-05,
      "loss": 0.8494,
      "step": 102060
    },
    {
      "epoch": 745.0364963503649,
      "grad_norm": 0.07243983447551727,
      "learning_rate": 1.2748175182481754e-05,
      "loss": 0.6383,
      "step": 102070
    },
    {
      "epoch": 745.1094890510949,
      "grad_norm": 12.205343246459961,
      "learning_rate": 1.2744525547445257e-05,
      "loss": 0.7817,
      "step": 102080
    },
    {
      "epoch": 745.1824817518249,
      "grad_norm": 9.375907897949219,
      "learning_rate": 1.274087591240876e-05,
      "loss": 1.113,
      "step": 102090
    },
    {
      "epoch": 745.2554744525547,
      "grad_norm": 9.486709594726562,
      "learning_rate": 1.2737226277372263e-05,
      "loss": 1.502,
      "step": 102100
    },
    {
      "epoch": 745.3284671532847,
      "grad_norm": 12.562283515930176,
      "learning_rate": 1.2733576642335767e-05,
      "loss": 0.4539,
      "step": 102110
    },
    {
      "epoch": 745.4014598540145,
      "grad_norm": 4.60447359085083,
      "learning_rate": 1.272992700729927e-05,
      "loss": 0.9652,
      "step": 102120
    },
    {
      "epoch": 745.4744525547445,
      "grad_norm": 7.678520679473877,
      "learning_rate": 1.2726277372262773e-05,
      "loss": 0.842,
      "step": 102130
    },
    {
      "epoch": 745.5474452554745,
      "grad_norm": 10.793354034423828,
      "learning_rate": 1.2722627737226277e-05,
      "loss": 0.6033,
      "step": 102140
    },
    {
      "epoch": 745.6204379562043,
      "grad_norm": 0.07100926339626312,
      "learning_rate": 1.2718978102189783e-05,
      "loss": 0.991,
      "step": 102150
    },
    {
      "epoch": 745.6934306569343,
      "grad_norm": 8.170401573181152,
      "learning_rate": 1.2715328467153283e-05,
      "loss": 1.178,
      "step": 102160
    },
    {
      "epoch": 745.7664233576643,
      "grad_norm": 7.881475448608398,
      "learning_rate": 1.2711678832116789e-05,
      "loss": 0.6181,
      "step": 102170
    },
    {
      "epoch": 745.8394160583941,
      "grad_norm": 10.59436321258545,
      "learning_rate": 1.2708029197080293e-05,
      "loss": 0.9198,
      "step": 102180
    },
    {
      "epoch": 745.9124087591241,
      "grad_norm": 8.97523307800293,
      "learning_rate": 1.2704379562043797e-05,
      "loss": 1.081,
      "step": 102190
    },
    {
      "epoch": 745.985401459854,
      "grad_norm": 10.042756080627441,
      "learning_rate": 1.2700729927007299e-05,
      "loss": 0.4329,
      "step": 102200
    },
    {
      "epoch": 746.0583941605839,
      "grad_norm": 13.430047988891602,
      "learning_rate": 1.2697080291970803e-05,
      "loss": 0.8786,
      "step": 102210
    },
    {
      "epoch": 746.1313868613139,
      "grad_norm": 12.490642547607422,
      "learning_rate": 1.2693430656934308e-05,
      "loss": 0.9133,
      "step": 102220
    },
    {
      "epoch": 746.2043795620438,
      "grad_norm": 11.917695999145508,
      "learning_rate": 1.2689781021897812e-05,
      "loss": 1.1593,
      "step": 102230
    },
    {
      "epoch": 746.2773722627737,
      "grad_norm": 11.058809280395508,
      "learning_rate": 1.2686131386861313e-05,
      "loss": 0.7289,
      "step": 102240
    },
    {
      "epoch": 746.3503649635037,
      "grad_norm": 9.515216827392578,
      "learning_rate": 1.2682481751824818e-05,
      "loss": 1.1142,
      "step": 102250
    },
    {
      "epoch": 746.4233576642335,
      "grad_norm": 6.960914134979248,
      "learning_rate": 1.2678832116788322e-05,
      "loss": 0.7765,
      "step": 102260
    },
    {
      "epoch": 746.4963503649635,
      "grad_norm": 5.759553909301758,
      "learning_rate": 1.2675182481751826e-05,
      "loss": 0.7758,
      "step": 102270
    },
    {
      "epoch": 746.5693430656934,
      "grad_norm": 18.20252227783203,
      "learning_rate": 1.2671532846715328e-05,
      "loss": 1.0712,
      "step": 102280
    },
    {
      "epoch": 746.6423357664233,
      "grad_norm": 12.248127937316895,
      "learning_rate": 1.2667883211678832e-05,
      "loss": 0.8508,
      "step": 102290
    },
    {
      "epoch": 746.7153284671533,
      "grad_norm": 9.201139450073242,
      "learning_rate": 1.2664233576642338e-05,
      "loss": 0.7668,
      "step": 102300
    },
    {
      "epoch": 746.7883211678832,
      "grad_norm": 8.770103454589844,
      "learning_rate": 1.2660583941605842e-05,
      "loss": 0.9391,
      "step": 102310
    },
    {
      "epoch": 746.8613138686131,
      "grad_norm": 0.09616024792194366,
      "learning_rate": 1.2656934306569344e-05,
      "loss": 0.6093,
      "step": 102320
    },
    {
      "epoch": 746.9343065693431,
      "grad_norm": 12.838484764099121,
      "learning_rate": 1.2653284671532848e-05,
      "loss": 0.8125,
      "step": 102330
    },
    {
      "epoch": 747.007299270073,
      "grad_norm": 15.971697807312012,
      "learning_rate": 1.2649635036496352e-05,
      "loss": 0.6816,
      "step": 102340
    },
    {
      "epoch": 747.0802919708029,
      "grad_norm": 11.197395324707031,
      "learning_rate": 1.2645985401459854e-05,
      "loss": 1.0198,
      "step": 102350
    },
    {
      "epoch": 747.1532846715329,
      "grad_norm": 0.0614999383687973,
      "learning_rate": 1.2642335766423358e-05,
      "loss": 0.4755,
      "step": 102360
    },
    {
      "epoch": 747.2262773722628,
      "grad_norm": 0.025318395346403122,
      "learning_rate": 1.2638686131386862e-05,
      "loss": 0.7936,
      "step": 102370
    },
    {
      "epoch": 747.2992700729927,
      "grad_norm": 12.56861686706543,
      "learning_rate": 1.2635036496350367e-05,
      "loss": 1.198,
      "step": 102380
    },
    {
      "epoch": 747.3722627737226,
      "grad_norm": 0.04514072835445404,
      "learning_rate": 1.2631386861313868e-05,
      "loss": 1.1464,
      "step": 102390
    },
    {
      "epoch": 747.4452554744526,
      "grad_norm": 7.754119873046875,
      "learning_rate": 1.2627737226277373e-05,
      "loss": 0.7811,
      "step": 102400
    },
    {
      "epoch": 747.5182481751825,
      "grad_norm": 10.917695045471191,
      "learning_rate": 1.2624087591240877e-05,
      "loss": 1.0466,
      "step": 102410
    },
    {
      "epoch": 747.5912408759124,
      "grad_norm": 0.16417774558067322,
      "learning_rate": 1.2620437956204381e-05,
      "loss": 0.6281,
      "step": 102420
    },
    {
      "epoch": 747.6642335766423,
      "grad_norm": 8.580400466918945,
      "learning_rate": 1.2616788321167883e-05,
      "loss": 0.7723,
      "step": 102430
    },
    {
      "epoch": 747.7372262773723,
      "grad_norm": 0.056614942848682404,
      "learning_rate": 1.2613138686131387e-05,
      "loss": 0.7672,
      "step": 102440
    },
    {
      "epoch": 747.8102189781022,
      "grad_norm": 6.999942779541016,
      "learning_rate": 1.2609489051094891e-05,
      "loss": 0.8861,
      "step": 102450
    },
    {
      "epoch": 747.8832116788321,
      "grad_norm": 0.06539882719516754,
      "learning_rate": 1.2605839416058396e-05,
      "loss": 1.1641,
      "step": 102460
    },
    {
      "epoch": 747.956204379562,
      "grad_norm": 12.824137687683105,
      "learning_rate": 1.2602189781021897e-05,
      "loss": 0.6724,
      "step": 102470
    },
    {
      "epoch": 748.029197080292,
      "grad_norm": 10.0407075881958,
      "learning_rate": 1.2598540145985403e-05,
      "loss": 0.9862,
      "step": 102480
    },
    {
      "epoch": 748.1021897810219,
      "grad_norm": 0.02824680134654045,
      "learning_rate": 1.2594890510948906e-05,
      "loss": 0.7608,
      "step": 102490
    },
    {
      "epoch": 748.1751824817518,
      "grad_norm": 12.275138854980469,
      "learning_rate": 1.259124087591241e-05,
      "loss": 0.8462,
      "step": 102500
    },
    {
      "epoch": 748.2481751824818,
      "grad_norm": 14.670400619506836,
      "learning_rate": 1.2587591240875913e-05,
      "loss": 1.2021,
      "step": 102510
    },
    {
      "epoch": 748.3211678832117,
      "grad_norm": 8.3651704788208,
      "learning_rate": 1.2583941605839416e-05,
      "loss": 0.8266,
      "step": 102520
    },
    {
      "epoch": 748.3941605839416,
      "grad_norm": 11.084673881530762,
      "learning_rate": 1.258029197080292e-05,
      "loss": 0.8605,
      "step": 102530
    },
    {
      "epoch": 748.4671532846716,
      "grad_norm": 10.819451332092285,
      "learning_rate": 1.2576642335766422e-05,
      "loss": 1.3527,
      "step": 102540
    },
    {
      "epoch": 748.5401459854014,
      "grad_norm": 0.023718031123280525,
      "learning_rate": 1.2572992700729926e-05,
      "loss": 0.904,
      "step": 102550
    },
    {
      "epoch": 748.6131386861314,
      "grad_norm": 8.751977920532227,
      "learning_rate": 1.2569343065693432e-05,
      "loss": 0.6837,
      "step": 102560
    },
    {
      "epoch": 748.6861313868613,
      "grad_norm": 11.474189758300781,
      "learning_rate": 1.2565693430656936e-05,
      "loss": 0.7581,
      "step": 102570
    },
    {
      "epoch": 748.7591240875912,
      "grad_norm": 14.19577693939209,
      "learning_rate": 1.2562043795620438e-05,
      "loss": 0.872,
      "step": 102580
    },
    {
      "epoch": 748.8321167883212,
      "grad_norm": 15.852356910705566,
      "learning_rate": 1.2558394160583942e-05,
      "loss": 0.9271,
      "step": 102590
    },
    {
      "epoch": 748.9051094890511,
      "grad_norm": 12.833067893981934,
      "learning_rate": 1.2554744525547446e-05,
      "loss": 1.1011,
      "step": 102600
    },
    {
      "epoch": 748.978102189781,
      "grad_norm": 8.099149703979492,
      "learning_rate": 1.255109489051095e-05,
      "loss": 0.7394,
      "step": 102610
    },
    {
      "epoch": 749.051094890511,
      "grad_norm": 10.804071426391602,
      "learning_rate": 1.2547445255474452e-05,
      "loss": 0.5986,
      "step": 102620
    },
    {
      "epoch": 749.1240875912408,
      "grad_norm": 7.759954452514648,
      "learning_rate": 1.2543795620437956e-05,
      "loss": 0.8534,
      "step": 102630
    },
    {
      "epoch": 749.1970802919708,
      "grad_norm": 0.042093969881534576,
      "learning_rate": 1.2540145985401461e-05,
      "loss": 1.1599,
      "step": 102640
    },
    {
      "epoch": 749.2700729927008,
      "grad_norm": 0.028226692229509354,
      "learning_rate": 1.2536496350364965e-05,
      "loss": 0.4123,
      "step": 102650
    },
    {
      "epoch": 749.3430656934306,
      "grad_norm": 10.93204116821289,
      "learning_rate": 1.2532846715328467e-05,
      "loss": 1.0069,
      "step": 102660
    },
    {
      "epoch": 749.4160583941606,
      "grad_norm": 10.776525497436523,
      "learning_rate": 1.2529197080291971e-05,
      "loss": 1.016,
      "step": 102670
    },
    {
      "epoch": 749.4890510948906,
      "grad_norm": 9.69039535522461,
      "learning_rate": 1.2525547445255475e-05,
      "loss": 0.7739,
      "step": 102680
    },
    {
      "epoch": 749.5620437956204,
      "grad_norm": 10.705645561218262,
      "learning_rate": 1.252189781021898e-05,
      "loss": 1.1022,
      "step": 102690
    },
    {
      "epoch": 749.6350364963504,
      "grad_norm": 11.906132698059082,
      "learning_rate": 1.2518248175182481e-05,
      "loss": 0.9236,
      "step": 102700
    },
    {
      "epoch": 749.7080291970802,
      "grad_norm": 0.15709668397903442,
      "learning_rate": 1.2514598540145985e-05,
      "loss": 0.6267,
      "step": 102710
    },
    {
      "epoch": 749.7810218978102,
      "grad_norm": 13.750699996948242,
      "learning_rate": 1.251094890510949e-05,
      "loss": 1.2577,
      "step": 102720
    },
    {
      "epoch": 749.8540145985402,
      "grad_norm": 11.223603248596191,
      "learning_rate": 1.2507299270072995e-05,
      "loss": 0.9871,
      "step": 102730
    },
    {
      "epoch": 749.92700729927,
      "grad_norm": 11.742876052856445,
      "learning_rate": 1.2503649635036497e-05,
      "loss": 0.7047,
      "step": 102740
    },
    {
      "epoch": 750.0,
      "grad_norm": 0.27754104137420654,
      "learning_rate": 1.25e-05,
      "loss": 0.9401,
      "step": 102750
    },
    {
      "epoch": 750.07299270073,
      "grad_norm": 9.052994728088379,
      "learning_rate": 1.2496350364963503e-05,
      "loss": 1.1234,
      "step": 102760
    },
    {
      "epoch": 750.1459854014598,
      "grad_norm": 0.02196682058274746,
      "learning_rate": 1.2492700729927008e-05,
      "loss": 0.8516,
      "step": 102770
    },
    {
      "epoch": 750.2189781021898,
      "grad_norm": 13.457307815551758,
      "learning_rate": 1.248905109489051e-05,
      "loss": 1.1182,
      "step": 102780
    },
    {
      "epoch": 750.2919708029198,
      "grad_norm": 11.382186889648438,
      "learning_rate": 1.2485401459854016e-05,
      "loss": 0.8031,
      "step": 102790
    },
    {
      "epoch": 750.3649635036496,
      "grad_norm": 9.666186332702637,
      "learning_rate": 1.2481751824817518e-05,
      "loss": 0.7995,
      "step": 102800
    },
    {
      "epoch": 750.4379562043796,
      "grad_norm": 0.01444142498075962,
      "learning_rate": 1.2478102189781022e-05,
      "loss": 0.8424,
      "step": 102810
    },
    {
      "epoch": 750.5109489051094,
      "grad_norm": 0.015456077642738819,
      "learning_rate": 1.2474452554744526e-05,
      "loss": 0.6965,
      "step": 102820
    },
    {
      "epoch": 750.5839416058394,
      "grad_norm": 8.858579635620117,
      "learning_rate": 1.247080291970803e-05,
      "loss": 0.5439,
      "step": 102830
    },
    {
      "epoch": 750.6569343065694,
      "grad_norm": 14.160252571105957,
      "learning_rate": 1.2467153284671534e-05,
      "loss": 0.673,
      "step": 102840
    },
    {
      "epoch": 750.7299270072992,
      "grad_norm": 6.310795783996582,
      "learning_rate": 1.2463503649635038e-05,
      "loss": 0.8701,
      "step": 102850
    },
    {
      "epoch": 750.8029197080292,
      "grad_norm": 12.805944442749023,
      "learning_rate": 1.245985401459854e-05,
      "loss": 1.0085,
      "step": 102860
    },
    {
      "epoch": 750.8759124087592,
      "grad_norm": 12.515978813171387,
      "learning_rate": 1.2456204379562045e-05,
      "loss": 0.9708,
      "step": 102870
    },
    {
      "epoch": 750.948905109489,
      "grad_norm": 11.543862342834473,
      "learning_rate": 1.2452554744525548e-05,
      "loss": 1.1441,
      "step": 102880
    },
    {
      "epoch": 751.021897810219,
      "grad_norm": 10.429152488708496,
      "learning_rate": 1.2448905109489052e-05,
      "loss": 1.0139,
      "step": 102890
    },
    {
      "epoch": 751.0948905109489,
      "grad_norm": 11.18710708618164,
      "learning_rate": 1.2445255474452555e-05,
      "loss": 1.2973,
      "step": 102900
    },
    {
      "epoch": 751.1678832116788,
      "grad_norm": 7.181594371795654,
      "learning_rate": 1.244160583941606e-05,
      "loss": 0.4653,
      "step": 102910
    },
    {
      "epoch": 751.2408759124088,
      "grad_norm": 13.170329093933105,
      "learning_rate": 1.2437956204379563e-05,
      "loss": 0.4292,
      "step": 102920
    },
    {
      "epoch": 751.3138686131387,
      "grad_norm": 15.876479148864746,
      "learning_rate": 1.2434306569343067e-05,
      "loss": 1.0705,
      "step": 102930
    },
    {
      "epoch": 751.3868613138686,
      "grad_norm": 16.995201110839844,
      "learning_rate": 1.243065693430657e-05,
      "loss": 1.0756,
      "step": 102940
    },
    {
      "epoch": 751.4598540145986,
      "grad_norm": 5.949367523193359,
      "learning_rate": 1.2427007299270075e-05,
      "loss": 0.9602,
      "step": 102950
    },
    {
      "epoch": 751.5328467153284,
      "grad_norm": 12.410059928894043,
      "learning_rate": 1.2423357664233577e-05,
      "loss": 1.3211,
      "step": 102960
    },
    {
      "epoch": 751.6058394160584,
      "grad_norm": 13.851402282714844,
      "learning_rate": 1.2419708029197081e-05,
      "loss": 0.7404,
      "step": 102970
    },
    {
      "epoch": 751.6788321167883,
      "grad_norm": 7.674837589263916,
      "learning_rate": 1.2416058394160585e-05,
      "loss": 0.4734,
      "step": 102980
    },
    {
      "epoch": 751.7518248175182,
      "grad_norm": 7.53635835647583,
      "learning_rate": 1.2412408759124087e-05,
      "loss": 0.8959,
      "step": 102990
    },
    {
      "epoch": 751.8248175182482,
      "grad_norm": 11.45132064819336,
      "learning_rate": 1.2408759124087593e-05,
      "loss": 0.9516,
      "step": 103000
    },
    {
      "epoch": 751.8978102189781,
      "grad_norm": 12.953556060791016,
      "learning_rate": 1.2405109489051095e-05,
      "loss": 1.12,
      "step": 103010
    },
    {
      "epoch": 751.970802919708,
      "grad_norm": 0.02239878848195076,
      "learning_rate": 1.2401459854014599e-05,
      "loss": 0.9923,
      "step": 103020
    },
    {
      "epoch": 752.043795620438,
      "grad_norm": 12.265806198120117,
      "learning_rate": 1.2397810218978103e-05,
      "loss": 1.1231,
      "step": 103030
    },
    {
      "epoch": 752.1167883211679,
      "grad_norm": 14.891594886779785,
      "learning_rate": 1.2394160583941606e-05,
      "loss": 0.6967,
      "step": 103040
    },
    {
      "epoch": 752.1897810218978,
      "grad_norm": 0.03673906251788139,
      "learning_rate": 1.239051094890511e-05,
      "loss": 0.9213,
      "step": 103050
    },
    {
      "epoch": 752.2627737226277,
      "grad_norm": 13.961304664611816,
      "learning_rate": 1.2386861313868614e-05,
      "loss": 0.8167,
      "step": 103060
    },
    {
      "epoch": 752.3357664233577,
      "grad_norm": 7.7572407722473145,
      "learning_rate": 1.2383211678832116e-05,
      "loss": 0.8675,
      "step": 103070
    },
    {
      "epoch": 752.4087591240876,
      "grad_norm": 17.530582427978516,
      "learning_rate": 1.2379562043795622e-05,
      "loss": 0.9977,
      "step": 103080
    },
    {
      "epoch": 752.4817518248175,
      "grad_norm": 17.81591796875,
      "learning_rate": 1.2375912408759124e-05,
      "loss": 0.7681,
      "step": 103090
    },
    {
      "epoch": 752.5547445255474,
      "grad_norm": 11.043050765991211,
      "learning_rate": 1.2372262773722628e-05,
      "loss": 1.4048,
      "step": 103100
    },
    {
      "epoch": 752.6277372262774,
      "grad_norm": 0.029428089037537575,
      "learning_rate": 1.2368613138686132e-05,
      "loss": 0.7926,
      "step": 103110
    },
    {
      "epoch": 752.7007299270073,
      "grad_norm": 15.238495826721191,
      "learning_rate": 1.2364963503649636e-05,
      "loss": 0.7602,
      "step": 103120
    },
    {
      "epoch": 752.7737226277372,
      "grad_norm": 13.712024688720703,
      "learning_rate": 1.236131386861314e-05,
      "loss": 0.9287,
      "step": 103130
    },
    {
      "epoch": 752.8467153284671,
      "grad_norm": 0.08382929861545563,
      "learning_rate": 1.2357664233576644e-05,
      "loss": 0.9565,
      "step": 103140
    },
    {
      "epoch": 752.9197080291971,
      "grad_norm": 11.220580101013184,
      "learning_rate": 1.2354014598540146e-05,
      "loss": 1.0753,
      "step": 103150
    },
    {
      "epoch": 752.992700729927,
      "grad_norm": 7.797469615936279,
      "learning_rate": 1.2350364963503651e-05,
      "loss": 0.8292,
      "step": 103160
    },
    {
      "epoch": 753.0656934306569,
      "grad_norm": 1.4632431268692017,
      "learning_rate": 1.2346715328467153e-05,
      "loss": 0.8926,
      "step": 103170
    },
    {
      "epoch": 753.1386861313869,
      "grad_norm": 0.2110152393579483,
      "learning_rate": 1.2343065693430657e-05,
      "loss": 0.6836,
      "step": 103180
    },
    {
      "epoch": 753.2116788321168,
      "grad_norm": 14.346465110778809,
      "learning_rate": 1.2339416058394161e-05,
      "loss": 1.1435,
      "step": 103190
    },
    {
      "epoch": 753.2846715328467,
      "grad_norm": 10.372880935668945,
      "learning_rate": 1.2335766423357663e-05,
      "loss": 1.0462,
      "step": 103200
    },
    {
      "epoch": 753.3576642335767,
      "grad_norm": 17.7203426361084,
      "learning_rate": 1.2332116788321169e-05,
      "loss": 1.0459,
      "step": 103210
    },
    {
      "epoch": 753.4306569343066,
      "grad_norm": 7.389354705810547,
      "learning_rate": 1.2328467153284671e-05,
      "loss": 0.5078,
      "step": 103220
    },
    {
      "epoch": 753.5036496350365,
      "grad_norm": 11.253853797912598,
      "learning_rate": 1.2324817518248175e-05,
      "loss": 0.7079,
      "step": 103230
    },
    {
      "epoch": 753.5766423357665,
      "grad_norm": 0.04891446232795715,
      "learning_rate": 1.2321167883211679e-05,
      "loss": 0.7469,
      "step": 103240
    },
    {
      "epoch": 753.6496350364963,
      "grad_norm": 8.83545970916748,
      "learning_rate": 1.2317518248175183e-05,
      "loss": 1.0018,
      "step": 103250
    },
    {
      "epoch": 753.7226277372263,
      "grad_norm": 0.038634493947029114,
      "learning_rate": 1.2313868613138687e-05,
      "loss": 0.4617,
      "step": 103260
    },
    {
      "epoch": 753.7956204379562,
      "grad_norm": 10.031903266906738,
      "learning_rate": 1.231021897810219e-05,
      "loss": 0.9995,
      "step": 103270
    },
    {
      "epoch": 753.8686131386861,
      "grad_norm": 8.006631851196289,
      "learning_rate": 1.2306569343065693e-05,
      "loss": 0.8913,
      "step": 103280
    },
    {
      "epoch": 753.9416058394161,
      "grad_norm": 17.052371978759766,
      "learning_rate": 1.2302919708029198e-05,
      "loss": 1.3009,
      "step": 103290
    },
    {
      "epoch": 754.014598540146,
      "grad_norm": 14.3430814743042,
      "learning_rate": 1.22992700729927e-05,
      "loss": 1.2453,
      "step": 103300
    },
    {
      "epoch": 754.0875912408759,
      "grad_norm": 12.254108428955078,
      "learning_rate": 1.2295620437956206e-05,
      "loss": 0.9083,
      "step": 103310
    },
    {
      "epoch": 754.1605839416059,
      "grad_norm": 14.107261657714844,
      "learning_rate": 1.2291970802919708e-05,
      "loss": 1.0071,
      "step": 103320
    },
    {
      "epoch": 754.2335766423357,
      "grad_norm": 12.914488792419434,
      "learning_rate": 1.2288321167883212e-05,
      "loss": 1.0774,
      "step": 103330
    },
    {
      "epoch": 754.3065693430657,
      "grad_norm": 14.587456703186035,
      "learning_rate": 1.2284671532846716e-05,
      "loss": 0.8035,
      "step": 103340
    },
    {
      "epoch": 754.3795620437957,
      "grad_norm": 14.400543212890625,
      "learning_rate": 1.228102189781022e-05,
      "loss": 1.0951,
      "step": 103350
    },
    {
      "epoch": 754.4525547445255,
      "grad_norm": 7.382172584533691,
      "learning_rate": 1.2277372262773724e-05,
      "loss": 0.6441,
      "step": 103360
    },
    {
      "epoch": 754.5255474452555,
      "grad_norm": 0.027058716863393784,
      "learning_rate": 1.2273722627737228e-05,
      "loss": 0.7433,
      "step": 103370
    },
    {
      "epoch": 754.5985401459855,
      "grad_norm": 8.27424144744873,
      "learning_rate": 1.227007299270073e-05,
      "loss": 0.7684,
      "step": 103380
    },
    {
      "epoch": 754.6715328467153,
      "grad_norm": 7.726959228515625,
      "learning_rate": 1.2266423357664236e-05,
      "loss": 0.9558,
      "step": 103390
    },
    {
      "epoch": 754.7445255474453,
      "grad_norm": 8.150014877319336,
      "learning_rate": 1.2262773722627738e-05,
      "loss": 0.9316,
      "step": 103400
    },
    {
      "epoch": 754.8175182481751,
      "grad_norm": 12.583678245544434,
      "learning_rate": 1.2259124087591242e-05,
      "loss": 0.8909,
      "step": 103410
    },
    {
      "epoch": 754.8905109489051,
      "grad_norm": 10.196918487548828,
      "learning_rate": 1.2255474452554745e-05,
      "loss": 0.7103,
      "step": 103420
    },
    {
      "epoch": 754.9635036496351,
      "grad_norm": 4.31468391418457,
      "learning_rate": 1.2251824817518248e-05,
      "loss": 0.7128,
      "step": 103430
    },
    {
      "epoch": 755.0364963503649,
      "grad_norm": 0.14291781187057495,
      "learning_rate": 1.2248175182481753e-05,
      "loss": 0.6203,
      "step": 103440
    },
    {
      "epoch": 755.1094890510949,
      "grad_norm": 8.3694486618042,
      "learning_rate": 1.2244525547445255e-05,
      "loss": 0.9419,
      "step": 103450
    },
    {
      "epoch": 755.1824817518249,
      "grad_norm": 9.576624870300293,
      "learning_rate": 1.224087591240876e-05,
      "loss": 0.9214,
      "step": 103460
    },
    {
      "epoch": 755.2554744525547,
      "grad_norm": 9.112650871276855,
      "learning_rate": 1.2237226277372263e-05,
      "loss": 1.2091,
      "step": 103470
    },
    {
      "epoch": 755.3284671532847,
      "grad_norm": 11.13044548034668,
      "learning_rate": 1.2233576642335767e-05,
      "loss": 0.6884,
      "step": 103480
    },
    {
      "epoch": 755.4014598540145,
      "grad_norm": 10.583894729614258,
      "learning_rate": 1.2229927007299271e-05,
      "loss": 0.9807,
      "step": 103490
    },
    {
      "epoch": 755.4744525547445,
      "grad_norm": 13.065834999084473,
      "learning_rate": 1.2226277372262775e-05,
      "loss": 0.9052,
      "step": 103500
    },
    {
      "epoch": 755.5474452554745,
      "grad_norm": 10.36292839050293,
      "learning_rate": 1.2222627737226277e-05,
      "loss": 1.2255,
      "step": 103510
    },
    {
      "epoch": 755.6204379562043,
      "grad_norm": 6.398107528686523,
      "learning_rate": 1.2218978102189783e-05,
      "loss": 0.7908,
      "step": 103520
    },
    {
      "epoch": 755.6934306569343,
      "grad_norm": 14.476393699645996,
      "learning_rate": 1.2215328467153285e-05,
      "loss": 0.9424,
      "step": 103530
    },
    {
      "epoch": 755.7664233576643,
      "grad_norm": 8.872364044189453,
      "learning_rate": 1.2211678832116789e-05,
      "loss": 0.8018,
      "step": 103540
    },
    {
      "epoch": 755.8394160583941,
      "grad_norm": 5.684816837310791,
      "learning_rate": 1.2208029197080293e-05,
      "loss": 0.6892,
      "step": 103550
    },
    {
      "epoch": 755.9124087591241,
      "grad_norm": 7.505003452301025,
      "learning_rate": 1.2204379562043796e-05,
      "loss": 0.985,
      "step": 103560
    },
    {
      "epoch": 755.985401459854,
      "grad_norm": 12.454307556152344,
      "learning_rate": 1.22007299270073e-05,
      "loss": 0.7463,
      "step": 103570
    },
    {
      "epoch": 756.0583941605839,
      "grad_norm": 9.180139541625977,
      "learning_rate": 1.2197080291970804e-05,
      "loss": 0.7879,
      "step": 103580
    },
    {
      "epoch": 756.1313868613139,
      "grad_norm": 0.10520175099372864,
      "learning_rate": 1.2193430656934306e-05,
      "loss": 0.6524,
      "step": 103590
    },
    {
      "epoch": 756.2043795620438,
      "grad_norm": 12.540386199951172,
      "learning_rate": 1.2189781021897812e-05,
      "loss": 1.0684,
      "step": 103600
    },
    {
      "epoch": 756.2773722627737,
      "grad_norm": 12.954773902893066,
      "learning_rate": 1.2186131386861314e-05,
      "loss": 0.8117,
      "step": 103610
    },
    {
      "epoch": 756.3503649635037,
      "grad_norm": 13.361258506774902,
      "learning_rate": 1.2182481751824818e-05,
      "loss": 0.742,
      "step": 103620
    },
    {
      "epoch": 756.4233576642335,
      "grad_norm": 0.0492001436650753,
      "learning_rate": 1.2178832116788322e-05,
      "loss": 0.6218,
      "step": 103630
    },
    {
      "epoch": 756.4963503649635,
      "grad_norm": 12.024624824523926,
      "learning_rate": 1.2175182481751824e-05,
      "loss": 0.9202,
      "step": 103640
    },
    {
      "epoch": 756.5693430656934,
      "grad_norm": 0.013001732528209686,
      "learning_rate": 1.217153284671533e-05,
      "loss": 0.8196,
      "step": 103650
    },
    {
      "epoch": 756.6423357664233,
      "grad_norm": 13.01815128326416,
      "learning_rate": 1.2167883211678832e-05,
      "loss": 0.9929,
      "step": 103660
    },
    {
      "epoch": 756.7153284671533,
      "grad_norm": 13.007128715515137,
      "learning_rate": 1.2164233576642336e-05,
      "loss": 0.9747,
      "step": 103670
    },
    {
      "epoch": 756.7883211678832,
      "grad_norm": 6.777097702026367,
      "learning_rate": 1.216058394160584e-05,
      "loss": 0.5821,
      "step": 103680
    },
    {
      "epoch": 756.8613138686131,
      "grad_norm": 7.004776954650879,
      "learning_rate": 1.2156934306569344e-05,
      "loss": 1.0789,
      "step": 103690
    },
    {
      "epoch": 756.9343065693431,
      "grad_norm": 10.522550582885742,
      "learning_rate": 1.2153284671532847e-05,
      "loss": 1.0637,
      "step": 103700
    },
    {
      "epoch": 757.007299270073,
      "grad_norm": 7.866142749786377,
      "learning_rate": 1.2149635036496351e-05,
      "loss": 1.1943,
      "step": 103710
    },
    {
      "epoch": 757.0802919708029,
      "grad_norm": 9.850380897521973,
      "learning_rate": 1.2145985401459853e-05,
      "loss": 0.7381,
      "step": 103720
    },
    {
      "epoch": 757.1532846715329,
      "grad_norm": 8.824995994567871,
      "learning_rate": 1.2142335766423359e-05,
      "loss": 0.8519,
      "step": 103730
    },
    {
      "epoch": 757.2262773722628,
      "grad_norm": 9.179473876953125,
      "learning_rate": 1.2138686131386861e-05,
      "loss": 0.7574,
      "step": 103740
    },
    {
      "epoch": 757.2992700729927,
      "grad_norm": 5.3462233543396,
      "learning_rate": 1.2135036496350367e-05,
      "loss": 0.7954,
      "step": 103750
    },
    {
      "epoch": 757.3722627737226,
      "grad_norm": 1.9491980075836182,
      "learning_rate": 1.2131386861313869e-05,
      "loss": 1.0978,
      "step": 103760
    },
    {
      "epoch": 757.4452554744526,
      "grad_norm": 0.05787920206785202,
      "learning_rate": 1.2127737226277373e-05,
      "loss": 0.9479,
      "step": 103770
    },
    {
      "epoch": 757.5182481751825,
      "grad_norm": 0.04387682303786278,
      "learning_rate": 1.2124087591240877e-05,
      "loss": 0.843,
      "step": 103780
    },
    {
      "epoch": 757.5912408759124,
      "grad_norm": 10.317107200622559,
      "learning_rate": 1.212043795620438e-05,
      "loss": 1.1283,
      "step": 103790
    },
    {
      "epoch": 757.6642335766423,
      "grad_norm": 10.758910179138184,
      "learning_rate": 1.2116788321167885e-05,
      "loss": 0.9949,
      "step": 103800
    },
    {
      "epoch": 757.7372262773723,
      "grad_norm": 17.02698516845703,
      "learning_rate": 1.2113138686131388e-05,
      "loss": 0.8405,
      "step": 103810
    },
    {
      "epoch": 757.8102189781022,
      "grad_norm": 6.756100654602051,
      "learning_rate": 1.210948905109489e-05,
      "loss": 1.117,
      "step": 103820
    },
    {
      "epoch": 757.8832116788321,
      "grad_norm": 7.3177313804626465,
      "learning_rate": 1.2105839416058394e-05,
      "loss": 0.7279,
      "step": 103830
    },
    {
      "epoch": 757.956204379562,
      "grad_norm": 6.881035804748535,
      "learning_rate": 1.2102189781021898e-05,
      "loss": 0.6317,
      "step": 103840
    },
    {
      "epoch": 758.029197080292,
      "grad_norm": 0.042386673390865326,
      "learning_rate": 1.2098540145985402e-05,
      "loss": 0.8892,
      "step": 103850
    },
    {
      "epoch": 758.1021897810219,
      "grad_norm": 11.322014808654785,
      "learning_rate": 1.2094890510948906e-05,
      "loss": 1.1421,
      "step": 103860
    },
    {
      "epoch": 758.1751824817518,
      "grad_norm": 0.7284553050994873,
      "learning_rate": 1.2091240875912408e-05,
      "loss": 0.3669,
      "step": 103870
    },
    {
      "epoch": 758.2481751824818,
      "grad_norm": 7.0920729637146,
      "learning_rate": 1.2087591240875914e-05,
      "loss": 0.7878,
      "step": 103880
    },
    {
      "epoch": 758.3211678832117,
      "grad_norm": 0.06779800355434418,
      "learning_rate": 1.2083941605839416e-05,
      "loss": 0.5979,
      "step": 103890
    },
    {
      "epoch": 758.3941605839416,
      "grad_norm": 20.289318084716797,
      "learning_rate": 1.208029197080292e-05,
      "loss": 1.4489,
      "step": 103900
    },
    {
      "epoch": 758.4671532846716,
      "grad_norm": 0.06359825283288956,
      "learning_rate": 1.2076642335766424e-05,
      "loss": 1.16,
      "step": 103910
    },
    {
      "epoch": 758.5401459854014,
      "grad_norm": 18.986644744873047,
      "learning_rate": 1.2072992700729928e-05,
      "loss": 0.7423,
      "step": 103920
    },
    {
      "epoch": 758.6131386861314,
      "grad_norm": 5.067933082580566,
      "learning_rate": 1.2069343065693432e-05,
      "loss": 0.7191,
      "step": 103930
    },
    {
      "epoch": 758.6861313868613,
      "grad_norm": 2.2111423015594482,
      "learning_rate": 1.2065693430656936e-05,
      "loss": 0.8396,
      "step": 103940
    },
    {
      "epoch": 758.7591240875912,
      "grad_norm": 11.628870964050293,
      "learning_rate": 1.2062043795620438e-05,
      "loss": 0.8403,
      "step": 103950
    },
    {
      "epoch": 758.8321167883212,
      "grad_norm": 9.733107566833496,
      "learning_rate": 1.2058394160583943e-05,
      "loss": 1.0758,
      "step": 103960
    },
    {
      "epoch": 758.9051094890511,
      "grad_norm": 13.05667495727539,
      "learning_rate": 1.2054744525547445e-05,
      "loss": 1.0749,
      "step": 103970
    },
    {
      "epoch": 758.978102189781,
      "grad_norm": 11.520862579345703,
      "learning_rate": 1.205109489051095e-05,
      "loss": 1.0404,
      "step": 103980
    },
    {
      "epoch": 759.051094890511,
      "grad_norm": 10.910325050354004,
      "learning_rate": 1.2047445255474453e-05,
      "loss": 0.6903,
      "step": 103990
    },
    {
      "epoch": 759.1240875912408,
      "grad_norm": 11.821290969848633,
      "learning_rate": 1.2043795620437957e-05,
      "loss": 0.6539,
      "step": 104000
    },
    {
      "epoch": 759.1970802919708,
      "grad_norm": 12.81454849243164,
      "learning_rate": 1.2040145985401461e-05,
      "loss": 0.8791,
      "step": 104010
    },
    {
      "epoch": 759.2700729927008,
      "grad_norm": 7.420270919799805,
      "learning_rate": 1.2036496350364965e-05,
      "loss": 0.7778,
      "step": 104020
    },
    {
      "epoch": 759.3430656934306,
      "grad_norm": 9.494612693786621,
      "learning_rate": 1.2032846715328467e-05,
      "loss": 1.0842,
      "step": 104030
    },
    {
      "epoch": 759.4160583941606,
      "grad_norm": 11.718172073364258,
      "learning_rate": 1.2029197080291973e-05,
      "loss": 0.79,
      "step": 104040
    },
    {
      "epoch": 759.4890510948906,
      "grad_norm": 6.577500820159912,
      "learning_rate": 1.2025547445255475e-05,
      "loss": 0.9933,
      "step": 104050
    },
    {
      "epoch": 759.5620437956204,
      "grad_norm": 9.172398567199707,
      "learning_rate": 1.2021897810218979e-05,
      "loss": 1.1987,
      "step": 104060
    },
    {
      "epoch": 759.6350364963504,
      "grad_norm": 5.329580783843994,
      "learning_rate": 1.2018248175182483e-05,
      "loss": 0.5517,
      "step": 104070
    },
    {
      "epoch": 759.7080291970802,
      "grad_norm": 0.011680545285344124,
      "learning_rate": 1.2014598540145985e-05,
      "loss": 0.497,
      "step": 104080
    },
    {
      "epoch": 759.7810218978102,
      "grad_norm": 11.99350643157959,
      "learning_rate": 1.201094890510949e-05,
      "loss": 0.8331,
      "step": 104090
    },
    {
      "epoch": 759.8540145985402,
      "grad_norm": 5.452323913574219,
      "learning_rate": 1.2007299270072993e-05,
      "loss": 0.853,
      "step": 104100
    },
    {
      "epoch": 759.92700729927,
      "grad_norm": 7.82268762588501,
      "learning_rate": 1.2003649635036496e-05,
      "loss": 1.1365,
      "step": 104110
    },
    {
      "epoch": 760.0,
      "grad_norm": 13.910928726196289,
      "learning_rate": 1.2e-05,
      "loss": 1.0275,
      "step": 104120
    },
    {
      "epoch": 760.07299270073,
      "grad_norm": 9.973347663879395,
      "learning_rate": 1.1996350364963504e-05,
      "loss": 1.3835,
      "step": 104130
    },
    {
      "epoch": 760.1459854014598,
      "grad_norm": 7.444057941436768,
      "learning_rate": 1.1992700729927008e-05,
      "loss": 1.0371,
      "step": 104140
    },
    {
      "epoch": 760.2189781021898,
      "grad_norm": 7.656272888183594,
      "learning_rate": 1.1989051094890512e-05,
      "loss": 0.849,
      "step": 104150
    },
    {
      "epoch": 760.2919708029198,
      "grad_norm": 10.312616348266602,
      "learning_rate": 1.1985401459854014e-05,
      "loss": 0.9822,
      "step": 104160
    },
    {
      "epoch": 760.3649635036496,
      "grad_norm": 0.1108224019408226,
      "learning_rate": 1.198175182481752e-05,
      "loss": 0.8018,
      "step": 104170
    },
    {
      "epoch": 760.4379562043796,
      "grad_norm": 6.917783260345459,
      "learning_rate": 1.1978102189781022e-05,
      "loss": 0.6021,
      "step": 104180
    },
    {
      "epoch": 760.5109489051094,
      "grad_norm": 0.041414402425289154,
      "learning_rate": 1.1974452554744526e-05,
      "loss": 0.8617,
      "step": 104190
    },
    {
      "epoch": 760.5839416058394,
      "grad_norm": 14.4229154586792,
      "learning_rate": 1.197080291970803e-05,
      "loss": 1.0252,
      "step": 104200
    },
    {
      "epoch": 760.6569343065694,
      "grad_norm": 0.015519164502620697,
      "learning_rate": 1.1967153284671534e-05,
      "loss": 0.8287,
      "step": 104210
    },
    {
      "epoch": 760.7299270072992,
      "grad_norm": 6.87240743637085,
      "learning_rate": 1.1963503649635037e-05,
      "loss": 0.7544,
      "step": 104220
    },
    {
      "epoch": 760.8029197080292,
      "grad_norm": 5.4985032081604,
      "learning_rate": 1.1959854014598541e-05,
      "loss": 0.6388,
      "step": 104230
    },
    {
      "epoch": 760.8759124087592,
      "grad_norm": 7.946933746337891,
      "learning_rate": 1.1956204379562044e-05,
      "loss": 0.9973,
      "step": 104240
    },
    {
      "epoch": 760.948905109489,
      "grad_norm": 9.819775581359863,
      "learning_rate": 1.1952554744525549e-05,
      "loss": 0.773,
      "step": 104250
    },
    {
      "epoch": 761.021897810219,
      "grad_norm": 0.021662907674908638,
      "learning_rate": 1.1948905109489051e-05,
      "loss": 0.5373,
      "step": 104260
    },
    {
      "epoch": 761.0948905109489,
      "grad_norm": 15.143403053283691,
      "learning_rate": 1.1945255474452555e-05,
      "loss": 1.1242,
      "step": 104270
    },
    {
      "epoch": 761.1678832116788,
      "grad_norm": 14.879404067993164,
      "learning_rate": 1.1941605839416059e-05,
      "loss": 0.87,
      "step": 104280
    },
    {
      "epoch": 761.2408759124088,
      "grad_norm": 0.07129068672657013,
      "learning_rate": 1.1937956204379561e-05,
      "loss": 0.8418,
      "step": 104290
    },
    {
      "epoch": 761.3138686131387,
      "grad_norm": 0.058125488460063934,
      "learning_rate": 1.1934306569343067e-05,
      "loss": 0.7985,
      "step": 104300
    },
    {
      "epoch": 761.3868613138686,
      "grad_norm": 11.973458290100098,
      "learning_rate": 1.1930656934306569e-05,
      "loss": 0.5419,
      "step": 104310
    },
    {
      "epoch": 761.4598540145986,
      "grad_norm": 0.07564599066972733,
      "learning_rate": 1.1927007299270075e-05,
      "loss": 0.9938,
      "step": 104320
    },
    {
      "epoch": 761.5328467153284,
      "grad_norm": 0.0430242083966732,
      "learning_rate": 1.1923357664233577e-05,
      "loss": 0.7283,
      "step": 104330
    },
    {
      "epoch": 761.6058394160584,
      "grad_norm": 0.11326874792575836,
      "learning_rate": 1.191970802919708e-05,
      "loss": 0.5196,
      "step": 104340
    },
    {
      "epoch": 761.6788321167883,
      "grad_norm": 11.720285415649414,
      "learning_rate": 1.1916058394160585e-05,
      "loss": 1.2495,
      "step": 104350
    },
    {
      "epoch": 761.7518248175182,
      "grad_norm": 10.450587272644043,
      "learning_rate": 1.1912408759124088e-05,
      "loss": 0.7998,
      "step": 104360
    },
    {
      "epoch": 761.8248175182482,
      "grad_norm": 10.61074161529541,
      "learning_rate": 1.1908759124087592e-05,
      "loss": 0.888,
      "step": 104370
    },
    {
      "epoch": 761.8978102189781,
      "grad_norm": 12.03663444519043,
      "learning_rate": 1.1905109489051096e-05,
      "loss": 1.0047,
      "step": 104380
    },
    {
      "epoch": 761.970802919708,
      "grad_norm": 8.426392555236816,
      "learning_rate": 1.1901459854014598e-05,
      "loss": 1.0153,
      "step": 104390
    },
    {
      "epoch": 762.043795620438,
      "grad_norm": 5.483845233917236,
      "learning_rate": 1.1897810218978104e-05,
      "loss": 1.0198,
      "step": 104400
    },
    {
      "epoch": 762.1167883211679,
      "grad_norm": 21.095306396484375,
      "learning_rate": 1.1894160583941606e-05,
      "loss": 1.1646,
      "step": 104410
    },
    {
      "epoch": 762.1897810218978,
      "grad_norm": 12.52837085723877,
      "learning_rate": 1.189051094890511e-05,
      "loss": 0.8004,
      "step": 104420
    },
    {
      "epoch": 762.2627737226277,
      "grad_norm": 7.977776050567627,
      "learning_rate": 1.1886861313868614e-05,
      "loss": 0.8088,
      "step": 104430
    },
    {
      "epoch": 762.3357664233577,
      "grad_norm": 13.192413330078125,
      "learning_rate": 1.1883211678832118e-05,
      "loss": 0.6274,
      "step": 104440
    },
    {
      "epoch": 762.4087591240876,
      "grad_norm": 6.654861927032471,
      "learning_rate": 1.1879562043795622e-05,
      "loss": 0.7681,
      "step": 104450
    },
    {
      "epoch": 762.4817518248175,
      "grad_norm": 9.907700538635254,
      "learning_rate": 1.1875912408759126e-05,
      "loss": 1.0256,
      "step": 104460
    },
    {
      "epoch": 762.5547445255474,
      "grad_norm": 19.136079788208008,
      "learning_rate": 1.1872262773722628e-05,
      "loss": 0.9178,
      "step": 104470
    },
    {
      "epoch": 762.6277372262774,
      "grad_norm": 0.042200133204460144,
      "learning_rate": 1.1868613138686132e-05,
      "loss": 0.4377,
      "step": 104480
    },
    {
      "epoch": 762.7007299270073,
      "grad_norm": 10.702101707458496,
      "learning_rate": 1.1864963503649635e-05,
      "loss": 1.099,
      "step": 104490
    },
    {
      "epoch": 762.7737226277372,
      "grad_norm": 6.14249324798584,
      "learning_rate": 1.186131386861314e-05,
      "loss": 0.847,
      "step": 104500
    },
    {
      "epoch": 762.8467153284671,
      "grad_norm": 8.923595428466797,
      "learning_rate": 1.1857664233576643e-05,
      "loss": 1.1054,
      "step": 104510
    },
    {
      "epoch": 762.9197080291971,
      "grad_norm": 11.582496643066406,
      "learning_rate": 1.1854014598540145e-05,
      "loss": 1.1711,
      "step": 104520
    },
    {
      "epoch": 762.992700729927,
      "grad_norm": 5.047048091888428,
      "learning_rate": 1.1850364963503651e-05,
      "loss": 0.9386,
      "step": 104530
    },
    {
      "epoch": 763.0656934306569,
      "grad_norm": 6.45654821395874,
      "learning_rate": 1.1846715328467153e-05,
      "loss": 0.5626,
      "step": 104540
    },
    {
      "epoch": 763.1386861313869,
      "grad_norm": 6.578017234802246,
      "learning_rate": 1.1843065693430657e-05,
      "loss": 0.9332,
      "step": 104550
    },
    {
      "epoch": 763.2116788321168,
      "grad_norm": 0.03578855097293854,
      "learning_rate": 1.1839416058394161e-05,
      "loss": 1.0008,
      "step": 104560
    },
    {
      "epoch": 763.2846715328467,
      "grad_norm": 6.703489303588867,
      "learning_rate": 1.1835766423357665e-05,
      "loss": 0.8718,
      "step": 104570
    },
    {
      "epoch": 763.3576642335767,
      "grad_norm": 0.13398469984531403,
      "learning_rate": 1.1832116788321169e-05,
      "loss": 0.7614,
      "step": 104580
    },
    {
      "epoch": 763.4306569343066,
      "grad_norm": 9.999923706054688,
      "learning_rate": 1.1828467153284673e-05,
      "loss": 0.9476,
      "step": 104590
    },
    {
      "epoch": 763.5036496350365,
      "grad_norm": 11.774700164794922,
      "learning_rate": 1.1824817518248175e-05,
      "loss": 0.7893,
      "step": 104600
    },
    {
      "epoch": 763.5766423357665,
      "grad_norm": 8.125114440917969,
      "learning_rate": 1.182116788321168e-05,
      "loss": 0.7206,
      "step": 104610
    },
    {
      "epoch": 763.6496350364963,
      "grad_norm": 9.127439498901367,
      "learning_rate": 1.1817518248175183e-05,
      "loss": 1.1773,
      "step": 104620
    },
    {
      "epoch": 763.7226277372263,
      "grad_norm": 20.17341423034668,
      "learning_rate": 1.1813868613138686e-05,
      "loss": 0.8802,
      "step": 104630
    },
    {
      "epoch": 763.7956204379562,
      "grad_norm": 14.867829322814941,
      "learning_rate": 1.181021897810219e-05,
      "loss": 1.221,
      "step": 104640
    },
    {
      "epoch": 763.8686131386861,
      "grad_norm": 5.414787292480469,
      "learning_rate": 1.1806569343065694e-05,
      "loss": 0.7791,
      "step": 104650
    },
    {
      "epoch": 763.9416058394161,
      "grad_norm": 5.643714427947998,
      "learning_rate": 1.1802919708029198e-05,
      "loss": 0.9813,
      "step": 104660
    },
    {
      "epoch": 764.014598540146,
      "grad_norm": 7.16201639175415,
      "learning_rate": 1.1799270072992702e-05,
      "loss": 1.1346,
      "step": 104670
    },
    {
      "epoch": 764.0875912408759,
      "grad_norm": 7.528119087219238,
      "learning_rate": 1.1795620437956204e-05,
      "loss": 0.559,
      "step": 104680
    },
    {
      "epoch": 764.1605839416059,
      "grad_norm": 7.061924457550049,
      "learning_rate": 1.1791970802919708e-05,
      "loss": 0.7798,
      "step": 104690
    },
    {
      "epoch": 764.2335766423357,
      "grad_norm": 15.296056747436523,
      "learning_rate": 1.1788321167883212e-05,
      "loss": 1.0807,
      "step": 104700
    },
    {
      "epoch": 764.3065693430657,
      "grad_norm": 12.893523216247559,
      "learning_rate": 1.1784671532846716e-05,
      "loss": 1.0159,
      "step": 104710
    },
    {
      "epoch": 764.3795620437957,
      "grad_norm": 0.03053114004433155,
      "learning_rate": 1.178102189781022e-05,
      "loss": 0.6915,
      "step": 104720
    },
    {
      "epoch": 764.4525547445255,
      "grad_norm": 23.579269409179688,
      "learning_rate": 1.1777372262773722e-05,
      "loss": 0.8408,
      "step": 104730
    },
    {
      "epoch": 764.5255474452555,
      "grad_norm": 9.66905689239502,
      "learning_rate": 1.1773722627737227e-05,
      "loss": 0.9556,
      "step": 104740
    },
    {
      "epoch": 764.5985401459855,
      "grad_norm": 9.008936882019043,
      "learning_rate": 1.177007299270073e-05,
      "loss": 0.7269,
      "step": 104750
    },
    {
      "epoch": 764.6715328467153,
      "grad_norm": 5.997959613800049,
      "learning_rate": 1.1766423357664234e-05,
      "loss": 1.0265,
      "step": 104760
    },
    {
      "epoch": 764.7445255474453,
      "grad_norm": 10.91576862335205,
      "learning_rate": 1.1762773722627737e-05,
      "loss": 0.8695,
      "step": 104770
    },
    {
      "epoch": 764.8175182481751,
      "grad_norm": 18.393177032470703,
      "learning_rate": 1.1759124087591241e-05,
      "loss": 0.7058,
      "step": 104780
    },
    {
      "epoch": 764.8905109489051,
      "grad_norm": 6.832064151763916,
      "learning_rate": 1.1755474452554745e-05,
      "loss": 0.8722,
      "step": 104790
    },
    {
      "epoch": 764.9635036496351,
      "grad_norm": 9.56099796295166,
      "learning_rate": 1.1751824817518249e-05,
      "loss": 1.3258,
      "step": 104800
    },
    {
      "epoch": 765.0364963503649,
      "grad_norm": 21.666139602661133,
      "learning_rate": 1.1748175182481751e-05,
      "loss": 0.881,
      "step": 104810
    },
    {
      "epoch": 765.1094890510949,
      "grad_norm": 7.406291484832764,
      "learning_rate": 1.1744525547445257e-05,
      "loss": 1.33,
      "step": 104820
    },
    {
      "epoch": 765.1824817518249,
      "grad_norm": 10.255855560302734,
      "learning_rate": 1.1740875912408759e-05,
      "loss": 0.714,
      "step": 104830
    },
    {
      "epoch": 765.2554744525547,
      "grad_norm": 12.48621654510498,
      "learning_rate": 1.1737226277372265e-05,
      "loss": 1.2811,
      "step": 104840
    },
    {
      "epoch": 765.3284671532847,
      "grad_norm": 12.477316856384277,
      "learning_rate": 1.1733576642335767e-05,
      "loss": 1.0402,
      "step": 104850
    },
    {
      "epoch": 765.4014598540145,
      "grad_norm": 10.949707984924316,
      "learning_rate": 1.172992700729927e-05,
      "loss": 0.9396,
      "step": 104860
    },
    {
      "epoch": 765.4744525547445,
      "grad_norm": 13.244446754455566,
      "learning_rate": 1.1726277372262775e-05,
      "loss": 0.791,
      "step": 104870
    },
    {
      "epoch": 765.5474452554745,
      "grad_norm": 6.5273027420043945,
      "learning_rate": 1.1722627737226278e-05,
      "loss": 0.6705,
      "step": 104880
    },
    {
      "epoch": 765.6204379562043,
      "grad_norm": 0.027323555201292038,
      "learning_rate": 1.1718978102189782e-05,
      "loss": 0.7746,
      "step": 104890
    },
    {
      "epoch": 765.6934306569343,
      "grad_norm": 17.025239944458008,
      "learning_rate": 1.1715328467153286e-05,
      "loss": 0.6569,
      "step": 104900
    },
    {
      "epoch": 765.7664233576643,
      "grad_norm": 7.803407669067383,
      "learning_rate": 1.1711678832116788e-05,
      "loss": 0.8869,
      "step": 104910
    },
    {
      "epoch": 765.8394160583941,
      "grad_norm": 0.03531588241457939,
      "learning_rate": 1.1708029197080292e-05,
      "loss": 0.8452,
      "step": 104920
    },
    {
      "epoch": 765.9124087591241,
      "grad_norm": 13.209207534790039,
      "learning_rate": 1.1704379562043796e-05,
      "loss": 0.9529,
      "step": 104930
    },
    {
      "epoch": 765.985401459854,
      "grad_norm": 13.59020709991455,
      "learning_rate": 1.17007299270073e-05,
      "loss": 0.6222,
      "step": 104940
    },
    {
      "epoch": 766.0583941605839,
      "grad_norm": 10.74738883972168,
      "learning_rate": 1.1697080291970804e-05,
      "loss": 1.3104,
      "step": 104950
    },
    {
      "epoch": 766.1313868613139,
      "grad_norm": 13.569109916687012,
      "learning_rate": 1.1693430656934306e-05,
      "loss": 0.8363,
      "step": 104960
    },
    {
      "epoch": 766.2043795620438,
      "grad_norm": 9.12791633605957,
      "learning_rate": 1.1689781021897812e-05,
      "loss": 0.6183,
      "step": 104970
    },
    {
      "epoch": 766.2773722627737,
      "grad_norm": 11.539514541625977,
      "learning_rate": 1.1686131386861314e-05,
      "loss": 0.9553,
      "step": 104980
    },
    {
      "epoch": 766.3503649635037,
      "grad_norm": 7.072796821594238,
      "learning_rate": 1.1682481751824818e-05,
      "loss": 1.125,
      "step": 104990
    },
    {
      "epoch": 766.4233576642335,
      "grad_norm": 19.227266311645508,
      "learning_rate": 1.1678832116788322e-05,
      "loss": 0.9658,
      "step": 105000
    },
    {
      "epoch": 766.4963503649635,
      "grad_norm": 12.99207878112793,
      "learning_rate": 1.1675182481751826e-05,
      "loss": 0.8023,
      "step": 105010
    },
    {
      "epoch": 766.5693430656934,
      "grad_norm": 8.047234535217285,
      "learning_rate": 1.167153284671533e-05,
      "loss": 0.9064,
      "step": 105020
    },
    {
      "epoch": 766.6423357664233,
      "grad_norm": 14.015985488891602,
      "learning_rate": 1.1667883211678833e-05,
      "loss": 0.957,
      "step": 105030
    },
    {
      "epoch": 766.7153284671533,
      "grad_norm": 7.8094916343688965,
      "learning_rate": 1.1664233576642335e-05,
      "loss": 0.7944,
      "step": 105040
    },
    {
      "epoch": 766.7883211678832,
      "grad_norm": 12.477837562561035,
      "learning_rate": 1.1660583941605841e-05,
      "loss": 1.1308,
      "step": 105050
    },
    {
      "epoch": 766.8613138686131,
      "grad_norm": 7.635035991668701,
      "learning_rate": 1.1656934306569343e-05,
      "loss": 0.5487,
      "step": 105060
    },
    {
      "epoch": 766.9343065693431,
      "grad_norm": 8.064740180969238,
      "learning_rate": 1.1653284671532847e-05,
      "loss": 0.8937,
      "step": 105070
    },
    {
      "epoch": 767.007299270073,
      "grad_norm": 0.06059287488460541,
      "learning_rate": 1.1649635036496351e-05,
      "loss": 0.8704,
      "step": 105080
    },
    {
      "epoch": 767.0802919708029,
      "grad_norm": 14.63216781616211,
      "learning_rate": 1.1645985401459855e-05,
      "loss": 0.7404,
      "step": 105090
    },
    {
      "epoch": 767.1532846715329,
      "grad_norm": 7.813122749328613,
      "learning_rate": 1.1642335766423359e-05,
      "loss": 1.1226,
      "step": 105100
    },
    {
      "epoch": 767.2262773722628,
      "grad_norm": 8.733247756958008,
      "learning_rate": 1.1638686131386863e-05,
      "loss": 1.0816,
      "step": 105110
    },
    {
      "epoch": 767.2992700729927,
      "grad_norm": 13.82436752319336,
      "learning_rate": 1.1635036496350365e-05,
      "loss": 0.8805,
      "step": 105120
    },
    {
      "epoch": 767.3722627737226,
      "grad_norm": 9.571210861206055,
      "learning_rate": 1.1631386861313869e-05,
      "loss": 0.9354,
      "step": 105130
    },
    {
      "epoch": 767.4452554744526,
      "grad_norm": 0.1157020851969719,
      "learning_rate": 1.1627737226277373e-05,
      "loss": 1.0919,
      "step": 105140
    },
    {
      "epoch": 767.5182481751825,
      "grad_norm": 11.083575248718262,
      "learning_rate": 1.1624087591240876e-05,
      "loss": 0.7418,
      "step": 105150
    },
    {
      "epoch": 767.5912408759124,
      "grad_norm": 0.882469654083252,
      "learning_rate": 1.162043795620438e-05,
      "loss": 0.7799,
      "step": 105160
    },
    {
      "epoch": 767.6642335766423,
      "grad_norm": 10.418990135192871,
      "learning_rate": 1.1616788321167883e-05,
      "loss": 0.929,
      "step": 105170
    },
    {
      "epoch": 767.7372262773723,
      "grad_norm": 9.579569816589355,
      "learning_rate": 1.1613138686131388e-05,
      "loss": 1.1903,
      "step": 105180
    },
    {
      "epoch": 767.8102189781022,
      "grad_norm": 0.02096211351454258,
      "learning_rate": 1.160948905109489e-05,
      "loss": 0.85,
      "step": 105190
    },
    {
      "epoch": 767.8832116788321,
      "grad_norm": 10.630549430847168,
      "learning_rate": 1.1605839416058394e-05,
      "loss": 1.066,
      "step": 105200
    },
    {
      "epoch": 767.956204379562,
      "grad_norm": 0.05741465836763382,
      "learning_rate": 1.1602189781021898e-05,
      "loss": 0.7536,
      "step": 105210
    },
    {
      "epoch": 768.029197080292,
      "grad_norm": 0.032487038522958755,
      "learning_rate": 1.1598540145985402e-05,
      "loss": 0.8066,
      "step": 105220
    },
    {
      "epoch": 768.1021897810219,
      "grad_norm": 0.044167280197143555,
      "learning_rate": 1.1594890510948906e-05,
      "loss": 0.6644,
      "step": 105230
    },
    {
      "epoch": 768.1751824817518,
      "grad_norm": 0.01620274968445301,
      "learning_rate": 1.159124087591241e-05,
      "loss": 1.0879,
      "step": 105240
    },
    {
      "epoch": 768.2481751824818,
      "grad_norm": 7.218910217285156,
      "learning_rate": 1.1587591240875912e-05,
      "loss": 0.8907,
      "step": 105250
    },
    {
      "epoch": 768.3211678832117,
      "grad_norm": 7.895970344543457,
      "learning_rate": 1.1583941605839417e-05,
      "loss": 0.9475,
      "step": 105260
    },
    {
      "epoch": 768.3941605839416,
      "grad_norm": 12.569934844970703,
      "learning_rate": 1.158029197080292e-05,
      "loss": 0.8428,
      "step": 105270
    },
    {
      "epoch": 768.4671532846716,
      "grad_norm": 23.948322296142578,
      "learning_rate": 1.1576642335766425e-05,
      "loss": 1.1744,
      "step": 105280
    },
    {
      "epoch": 768.5401459854014,
      "grad_norm": 6.328995227813721,
      "learning_rate": 1.1572992700729927e-05,
      "loss": 0.8778,
      "step": 105290
    },
    {
      "epoch": 768.6131386861314,
      "grad_norm": 7.117366313934326,
      "learning_rate": 1.1569343065693431e-05,
      "loss": 0.8657,
      "step": 105300
    },
    {
      "epoch": 768.6861313868613,
      "grad_norm": 16.33611488342285,
      "learning_rate": 1.1565693430656935e-05,
      "loss": 1.0877,
      "step": 105310
    },
    {
      "epoch": 768.7591240875912,
      "grad_norm": 11.307300567626953,
      "learning_rate": 1.1562043795620439e-05,
      "loss": 0.7298,
      "step": 105320
    },
    {
      "epoch": 768.8321167883212,
      "grad_norm": 8.113404273986816,
      "learning_rate": 1.1558394160583943e-05,
      "loss": 0.888,
      "step": 105330
    },
    {
      "epoch": 768.9051094890511,
      "grad_norm": 8.141900062561035,
      "learning_rate": 1.1554744525547445e-05,
      "loss": 0.6048,
      "step": 105340
    },
    {
      "epoch": 768.978102189781,
      "grad_norm": 11.408817291259766,
      "learning_rate": 1.1551094890510949e-05,
      "loss": 0.9599,
      "step": 105350
    },
    {
      "epoch": 769.051094890511,
      "grad_norm": 6.077935218811035,
      "learning_rate": 1.1547445255474453e-05,
      "loss": 0.9611,
      "step": 105360
    },
    {
      "epoch": 769.1240875912408,
      "grad_norm": 16.921405792236328,
      "learning_rate": 1.1543795620437957e-05,
      "loss": 1.0709,
      "step": 105370
    },
    {
      "epoch": 769.1970802919708,
      "grad_norm": 7.746833324432373,
      "learning_rate": 1.154014598540146e-05,
      "loss": 0.8998,
      "step": 105380
    },
    {
      "epoch": 769.2700729927008,
      "grad_norm": 11.632159233093262,
      "learning_rate": 1.1536496350364965e-05,
      "loss": 0.732,
      "step": 105390
    },
    {
      "epoch": 769.3430656934306,
      "grad_norm": 9.597784042358398,
      "learning_rate": 1.1532846715328467e-05,
      "loss": 1.1142,
      "step": 105400
    },
    {
      "epoch": 769.4160583941606,
      "grad_norm": 9.863348960876465,
      "learning_rate": 1.1529197080291972e-05,
      "loss": 1.181,
      "step": 105410
    },
    {
      "epoch": 769.4890510948906,
      "grad_norm": 12.294612884521484,
      "learning_rate": 1.1525547445255475e-05,
      "loss": 1.1037,
      "step": 105420
    },
    {
      "epoch": 769.5620437956204,
      "grad_norm": 7.527115821838379,
      "learning_rate": 1.1521897810218978e-05,
      "loss": 0.9769,
      "step": 105430
    },
    {
      "epoch": 769.6350364963504,
      "grad_norm": 8.557304382324219,
      "learning_rate": 1.1518248175182482e-05,
      "loss": 0.8858,
      "step": 105440
    },
    {
      "epoch": 769.7080291970802,
      "grad_norm": 7.644931793212891,
      "learning_rate": 1.1514598540145986e-05,
      "loss": 0.6479,
      "step": 105450
    },
    {
      "epoch": 769.7810218978102,
      "grad_norm": 9.063374519348145,
      "learning_rate": 1.151094890510949e-05,
      "loss": 0.8375,
      "step": 105460
    },
    {
      "epoch": 769.8540145985402,
      "grad_norm": 10.783223152160645,
      "learning_rate": 1.1507299270072994e-05,
      "loss": 0.8464,
      "step": 105470
    },
    {
      "epoch": 769.92700729927,
      "grad_norm": 7.4022040367126465,
      "learning_rate": 1.1503649635036496e-05,
      "loss": 0.6727,
      "step": 105480
    },
    {
      "epoch": 770.0,
      "grad_norm": 0.026494931429624557,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.8237,
      "step": 105490
    },
    {
      "epoch": 770.07299270073,
      "grad_norm": 8.890240669250488,
      "learning_rate": 1.1496350364963504e-05,
      "loss": 0.9254,
      "step": 105500
    },
    {
      "epoch": 770.1459854014598,
      "grad_norm": 9.87637710571289,
      "learning_rate": 1.1492700729927008e-05,
      "loss": 1.1169,
      "step": 105510
    },
    {
      "epoch": 770.2189781021898,
      "grad_norm": 7.769195079803467,
      "learning_rate": 1.1489051094890512e-05,
      "loss": 0.6797,
      "step": 105520
    },
    {
      "epoch": 770.2919708029198,
      "grad_norm": 6.805264949798584,
      "learning_rate": 1.1485401459854016e-05,
      "loss": 0.8533,
      "step": 105530
    },
    {
      "epoch": 770.3649635036496,
      "grad_norm": 9.822327613830566,
      "learning_rate": 1.148175182481752e-05,
      "loss": 0.7215,
      "step": 105540
    },
    {
      "epoch": 770.4379562043796,
      "grad_norm": 9.37713623046875,
      "learning_rate": 1.1478102189781023e-05,
      "loss": 0.6935,
      "step": 105550
    },
    {
      "epoch": 770.5109489051094,
      "grad_norm": 7.079909801483154,
      "learning_rate": 1.1474452554744525e-05,
      "loss": 0.5672,
      "step": 105560
    },
    {
      "epoch": 770.5839416058394,
      "grad_norm": 17.51825714111328,
      "learning_rate": 1.147080291970803e-05,
      "loss": 0.7333,
      "step": 105570
    },
    {
      "epoch": 770.6569343065694,
      "grad_norm": 0.028531935065984726,
      "learning_rate": 1.1467153284671533e-05,
      "loss": 0.8466,
      "step": 105580
    },
    {
      "epoch": 770.7299270072992,
      "grad_norm": 0.03752247244119644,
      "learning_rate": 1.1463503649635037e-05,
      "loss": 1.3441,
      "step": 105590
    },
    {
      "epoch": 770.8029197080292,
      "grad_norm": 9.838732719421387,
      "learning_rate": 1.1459854014598541e-05,
      "loss": 1.0136,
      "step": 105600
    },
    {
      "epoch": 770.8759124087592,
      "grad_norm": 20.862733840942383,
      "learning_rate": 1.1456204379562043e-05,
      "loss": 0.8014,
      "step": 105610
    },
    {
      "epoch": 770.948905109489,
      "grad_norm": 6.625167369842529,
      "learning_rate": 1.1452554744525549e-05,
      "loss": 0.6056,
      "step": 105620
    },
    {
      "epoch": 771.021897810219,
      "grad_norm": 16.228866577148438,
      "learning_rate": 1.1448905109489051e-05,
      "loss": 1.3774,
      "step": 105630
    },
    {
      "epoch": 771.0948905109489,
      "grad_norm": 6.7882795333862305,
      "learning_rate": 1.1445255474452555e-05,
      "loss": 0.9928,
      "step": 105640
    },
    {
      "epoch": 771.1678832116788,
      "grad_norm": 0.018544858321547508,
      "learning_rate": 1.1441605839416059e-05,
      "loss": 0.8198,
      "step": 105650
    },
    {
      "epoch": 771.2408759124088,
      "grad_norm": 0.030754581093788147,
      "learning_rate": 1.1437956204379563e-05,
      "loss": 0.8502,
      "step": 105660
    },
    {
      "epoch": 771.3138686131387,
      "grad_norm": 14.891240119934082,
      "learning_rate": 1.1434306569343067e-05,
      "loss": 0.7579,
      "step": 105670
    },
    {
      "epoch": 771.3868613138686,
      "grad_norm": 7.1037092208862305,
      "learning_rate": 1.143065693430657e-05,
      "loss": 1.1577,
      "step": 105680
    },
    {
      "epoch": 771.4598540145986,
      "grad_norm": 10.816571235656738,
      "learning_rate": 1.1427007299270073e-05,
      "loss": 0.8844,
      "step": 105690
    },
    {
      "epoch": 771.5328467153284,
      "grad_norm": 15.088506698608398,
      "learning_rate": 1.1423357664233578e-05,
      "loss": 1.0398,
      "step": 105700
    },
    {
      "epoch": 771.6058394160584,
      "grad_norm": 13.709545135498047,
      "learning_rate": 1.141970802919708e-05,
      "loss": 0.8253,
      "step": 105710
    },
    {
      "epoch": 771.6788321167883,
      "grad_norm": 12.091241836547852,
      "learning_rate": 1.1416058394160584e-05,
      "loss": 0.6019,
      "step": 105720
    },
    {
      "epoch": 771.7518248175182,
      "grad_norm": 8.725467681884766,
      "learning_rate": 1.1412408759124088e-05,
      "loss": 1.0671,
      "step": 105730
    },
    {
      "epoch": 771.8248175182482,
      "grad_norm": 6.6023430824279785,
      "learning_rate": 1.1408759124087592e-05,
      "loss": 1.2033,
      "step": 105740
    },
    {
      "epoch": 771.8978102189781,
      "grad_norm": 10.085772514343262,
      "learning_rate": 1.1405109489051096e-05,
      "loss": 0.7075,
      "step": 105750
    },
    {
      "epoch": 771.970802919708,
      "grad_norm": 0.11589550226926804,
      "learning_rate": 1.14014598540146e-05,
      "loss": 0.7571,
      "step": 105760
    },
    {
      "epoch": 772.043795620438,
      "grad_norm": 8.783863067626953,
      "learning_rate": 1.1397810218978102e-05,
      "loss": 0.6012,
      "step": 105770
    },
    {
      "epoch": 772.1167883211679,
      "grad_norm": 15.66093921661377,
      "learning_rate": 1.1394160583941606e-05,
      "loss": 0.8684,
      "step": 105780
    },
    {
      "epoch": 772.1897810218978,
      "grad_norm": 8.833429336547852,
      "learning_rate": 1.139051094890511e-05,
      "loss": 0.8819,
      "step": 105790
    },
    {
      "epoch": 772.2627737226277,
      "grad_norm": 9.965377807617188,
      "learning_rate": 1.1386861313868614e-05,
      "loss": 0.9461,
      "step": 105800
    },
    {
      "epoch": 772.3357664233577,
      "grad_norm": 7.545290470123291,
      "learning_rate": 1.1383211678832117e-05,
      "loss": 0.9332,
      "step": 105810
    },
    {
      "epoch": 772.4087591240876,
      "grad_norm": 9.937234878540039,
      "learning_rate": 1.137956204379562e-05,
      "loss": 0.7933,
      "step": 105820
    },
    {
      "epoch": 772.4817518248175,
      "grad_norm": 16.223384857177734,
      "learning_rate": 1.1375912408759125e-05,
      "loss": 0.7349,
      "step": 105830
    },
    {
      "epoch": 772.5547445255474,
      "grad_norm": 11.264327049255371,
      "learning_rate": 1.1372262773722627e-05,
      "loss": 0.9264,
      "step": 105840
    },
    {
      "epoch": 772.6277372262774,
      "grad_norm": 7.148344993591309,
      "learning_rate": 1.1368613138686133e-05,
      "loss": 0.918,
      "step": 105850
    },
    {
      "epoch": 772.7007299270073,
      "grad_norm": 11.117505073547363,
      "learning_rate": 1.1364963503649635e-05,
      "loss": 0.8693,
      "step": 105860
    },
    {
      "epoch": 772.7737226277372,
      "grad_norm": 12.389934539794922,
      "learning_rate": 1.1361313868613139e-05,
      "loss": 0.9697,
      "step": 105870
    },
    {
      "epoch": 772.8467153284671,
      "grad_norm": 6.3862738609313965,
      "learning_rate": 1.1357664233576643e-05,
      "loss": 1.0656,
      "step": 105880
    },
    {
      "epoch": 772.9197080291971,
      "grad_norm": 12.637190818786621,
      "learning_rate": 1.1354014598540147e-05,
      "loss": 0.9209,
      "step": 105890
    },
    {
      "epoch": 772.992700729927,
      "grad_norm": 9.809102058410645,
      "learning_rate": 1.135036496350365e-05,
      "loss": 1.0754,
      "step": 105900
    },
    {
      "epoch": 773.0656934306569,
      "grad_norm": 0.21772384643554688,
      "learning_rate": 1.1346715328467155e-05,
      "loss": 0.9152,
      "step": 105910
    },
    {
      "epoch": 773.1386861313869,
      "grad_norm": 10.183171272277832,
      "learning_rate": 1.1343065693430657e-05,
      "loss": 0.6779,
      "step": 105920
    },
    {
      "epoch": 773.2116788321168,
      "grad_norm": 7.292636394500732,
      "learning_rate": 1.1339416058394162e-05,
      "loss": 0.7695,
      "step": 105930
    },
    {
      "epoch": 773.2846715328467,
      "grad_norm": 11.630416870117188,
      "learning_rate": 1.1335766423357665e-05,
      "loss": 1.0862,
      "step": 105940
    },
    {
      "epoch": 773.3576642335767,
      "grad_norm": 0.06384369730949402,
      "learning_rate": 1.1332116788321168e-05,
      "loss": 0.6681,
      "step": 105950
    },
    {
      "epoch": 773.4306569343066,
      "grad_norm": 12.866049766540527,
      "learning_rate": 1.1328467153284672e-05,
      "loss": 0.7364,
      "step": 105960
    },
    {
      "epoch": 773.5036496350365,
      "grad_norm": 10.222285270690918,
      "learning_rate": 1.1324817518248176e-05,
      "loss": 0.7312,
      "step": 105970
    },
    {
      "epoch": 773.5766423357665,
      "grad_norm": 9.509092330932617,
      "learning_rate": 1.132116788321168e-05,
      "loss": 1.1413,
      "step": 105980
    },
    {
      "epoch": 773.6496350364963,
      "grad_norm": 12.835378646850586,
      "learning_rate": 1.1317518248175182e-05,
      "loss": 1.1837,
      "step": 105990
    },
    {
      "epoch": 773.7226277372263,
      "grad_norm": 8.39491081237793,
      "learning_rate": 1.1313868613138686e-05,
      "loss": 0.7492,
      "step": 106000
    },
    {
      "epoch": 773.7956204379562,
      "grad_norm": 12.862977981567383,
      "learning_rate": 1.131021897810219e-05,
      "loss": 0.7371,
      "step": 106010
    },
    {
      "epoch": 773.8686131386861,
      "grad_norm": 18.36211585998535,
      "learning_rate": 1.1306569343065694e-05,
      "loss": 1.211,
      "step": 106020
    },
    {
      "epoch": 773.9416058394161,
      "grad_norm": 12.044927597045898,
      "learning_rate": 1.1302919708029198e-05,
      "loss": 1.0725,
      "step": 106030
    },
    {
      "epoch": 774.014598540146,
      "grad_norm": 7.27868127822876,
      "learning_rate": 1.1299270072992702e-05,
      "loss": 1.0362,
      "step": 106040
    },
    {
      "epoch": 774.0875912408759,
      "grad_norm": 13.862553596496582,
      "learning_rate": 1.1295620437956204e-05,
      "loss": 0.949,
      "step": 106050
    },
    {
      "epoch": 774.1605839416059,
      "grad_norm": 0.014707351103425026,
      "learning_rate": 1.129197080291971e-05,
      "loss": 0.5299,
      "step": 106060
    },
    {
      "epoch": 774.2335766423357,
      "grad_norm": 0.09717104583978653,
      "learning_rate": 1.1288321167883212e-05,
      "loss": 0.7072,
      "step": 106070
    },
    {
      "epoch": 774.3065693430657,
      "grad_norm": 8.63987922668457,
      "learning_rate": 1.1284671532846716e-05,
      "loss": 0.7127,
      "step": 106080
    },
    {
      "epoch": 774.3795620437957,
      "grad_norm": 5.387814521789551,
      "learning_rate": 1.128102189781022e-05,
      "loss": 1.2476,
      "step": 106090
    },
    {
      "epoch": 774.4525547445255,
      "grad_norm": 11.203473091125488,
      "learning_rate": 1.1277372262773723e-05,
      "loss": 1.1391,
      "step": 106100
    },
    {
      "epoch": 774.5255474452555,
      "grad_norm": 6.589915752410889,
      "learning_rate": 1.1273722627737227e-05,
      "loss": 0.8619,
      "step": 106110
    },
    {
      "epoch": 774.5985401459855,
      "grad_norm": 5.615202903747559,
      "learning_rate": 1.1270072992700731e-05,
      "loss": 0.6431,
      "step": 106120
    },
    {
      "epoch": 774.6715328467153,
      "grad_norm": 8.612570762634277,
      "learning_rate": 1.1266423357664233e-05,
      "loss": 0.8769,
      "step": 106130
    },
    {
      "epoch": 774.7445255474453,
      "grad_norm": 10.535493850708008,
      "learning_rate": 1.1262773722627739e-05,
      "loss": 1.155,
      "step": 106140
    },
    {
      "epoch": 774.8175182481751,
      "grad_norm": 14.341682434082031,
      "learning_rate": 1.1259124087591241e-05,
      "loss": 0.9624,
      "step": 106150
    },
    {
      "epoch": 774.8905109489051,
      "grad_norm": 10.161160469055176,
      "learning_rate": 1.1255474452554745e-05,
      "loss": 0.661,
      "step": 106160
    },
    {
      "epoch": 774.9635036496351,
      "grad_norm": 0.07039983570575714,
      "learning_rate": 1.1251824817518249e-05,
      "loss": 0.6881,
      "step": 106170
    },
    {
      "epoch": 775.0364963503649,
      "grad_norm": 14.287074089050293,
      "learning_rate": 1.1248175182481753e-05,
      "loss": 0.9885,
      "step": 106180
    },
    {
      "epoch": 775.1094890510949,
      "grad_norm": 6.432412624359131,
      "learning_rate": 1.1244525547445257e-05,
      "loss": 1.1274,
      "step": 106190
    },
    {
      "epoch": 775.1824817518249,
      "grad_norm": 10.49203109741211,
      "learning_rate": 1.1240875912408759e-05,
      "loss": 0.7161,
      "step": 106200
    },
    {
      "epoch": 775.2554744525547,
      "grad_norm": 13.446864128112793,
      "learning_rate": 1.1237226277372263e-05,
      "loss": 0.8745,
      "step": 106210
    },
    {
      "epoch": 775.3284671532847,
      "grad_norm": 12.992622375488281,
      "learning_rate": 1.1233576642335766e-05,
      "loss": 0.957,
      "step": 106220
    },
    {
      "epoch": 775.4014598540145,
      "grad_norm": 4.726580619812012,
      "learning_rate": 1.122992700729927e-05,
      "loss": 1.077,
      "step": 106230
    },
    {
      "epoch": 775.4744525547445,
      "grad_norm": 7.792605400085449,
      "learning_rate": 1.1226277372262774e-05,
      "loss": 0.6698,
      "step": 106240
    },
    {
      "epoch": 775.5474452554745,
      "grad_norm": 14.79342269897461,
      "learning_rate": 1.1222627737226278e-05,
      "loss": 1.3701,
      "step": 106250
    },
    {
      "epoch": 775.6204379562043,
      "grad_norm": 5.49468469619751,
      "learning_rate": 1.121897810218978e-05,
      "loss": 0.7472,
      "step": 106260
    },
    {
      "epoch": 775.6934306569343,
      "grad_norm": 4.010918140411377,
      "learning_rate": 1.1215328467153286e-05,
      "loss": 0.8941,
      "step": 106270
    },
    {
      "epoch": 775.7664233576643,
      "grad_norm": 3.2655253410339355,
      "learning_rate": 1.1211678832116788e-05,
      "loss": 0.8991,
      "step": 106280
    },
    {
      "epoch": 775.8394160583941,
      "grad_norm": 0.042292676866054535,
      "learning_rate": 1.1208029197080292e-05,
      "loss": 0.6555,
      "step": 106290
    },
    {
      "epoch": 775.9124087591241,
      "grad_norm": 20.768470764160156,
      "learning_rate": 1.1204379562043796e-05,
      "loss": 0.7671,
      "step": 106300
    },
    {
      "epoch": 775.985401459854,
      "grad_norm": 8.64508056640625,
      "learning_rate": 1.12007299270073e-05,
      "loss": 0.9205,
      "step": 106310
    },
    {
      "epoch": 776.0583941605839,
      "grad_norm": 6.276505470275879,
      "learning_rate": 1.1197080291970804e-05,
      "loss": 0.7994,
      "step": 106320
    },
    {
      "epoch": 776.1313868613139,
      "grad_norm": 15.303606033325195,
      "learning_rate": 1.1193430656934307e-05,
      "loss": 1.3874,
      "step": 106330
    },
    {
      "epoch": 776.2043795620438,
      "grad_norm": 0.036608241498470306,
      "learning_rate": 1.118978102189781e-05,
      "loss": 0.693,
      "step": 106340
    },
    {
      "epoch": 776.2773722627737,
      "grad_norm": 10.406584739685059,
      "learning_rate": 1.1186131386861315e-05,
      "loss": 0.9665,
      "step": 106350
    },
    {
      "epoch": 776.3503649635037,
      "grad_norm": 10.412053108215332,
      "learning_rate": 1.1182481751824817e-05,
      "loss": 0.7095,
      "step": 106360
    },
    {
      "epoch": 776.4233576642335,
      "grad_norm": 15.890514373779297,
      "learning_rate": 1.1178832116788323e-05,
      "loss": 0.8036,
      "step": 106370
    },
    {
      "epoch": 776.4963503649635,
      "grad_norm": 0.03159302473068237,
      "learning_rate": 1.1175182481751825e-05,
      "loss": 0.7299,
      "step": 106380
    },
    {
      "epoch": 776.5693430656934,
      "grad_norm": 12.681683540344238,
      "learning_rate": 1.1171532846715329e-05,
      "loss": 0.9149,
      "step": 106390
    },
    {
      "epoch": 776.6423357664233,
      "grad_norm": 10.935595512390137,
      "learning_rate": 1.1167883211678833e-05,
      "loss": 0.6184,
      "step": 106400
    },
    {
      "epoch": 776.7153284671533,
      "grad_norm": 19.395648956298828,
      "learning_rate": 1.1164233576642337e-05,
      "loss": 1.0978,
      "step": 106410
    },
    {
      "epoch": 776.7883211678832,
      "grad_norm": 10.853859901428223,
      "learning_rate": 1.116058394160584e-05,
      "loss": 1.0039,
      "step": 106420
    },
    {
      "epoch": 776.8613138686131,
      "grad_norm": 0.032513584941625595,
      "learning_rate": 1.1156934306569343e-05,
      "loss": 0.6746,
      "step": 106430
    },
    {
      "epoch": 776.9343065693431,
      "grad_norm": 8.156025886535645,
      "learning_rate": 1.1153284671532847e-05,
      "loss": 1.0592,
      "step": 106440
    },
    {
      "epoch": 777.007299270073,
      "grad_norm": 12.605708122253418,
      "learning_rate": 1.114963503649635e-05,
      "loss": 0.6865,
      "step": 106450
    },
    {
      "epoch": 777.0802919708029,
      "grad_norm": 13.103378295898438,
      "learning_rate": 1.1145985401459855e-05,
      "loss": 0.8323,
      "step": 106460
    },
    {
      "epoch": 777.1532846715329,
      "grad_norm": 5.3352484703063965,
      "learning_rate": 1.1142335766423358e-05,
      "loss": 1.0679,
      "step": 106470
    },
    {
      "epoch": 777.2262773722628,
      "grad_norm": 7.865196704864502,
      "learning_rate": 1.1138686131386862e-05,
      "loss": 0.9484,
      "step": 106480
    },
    {
      "epoch": 777.2992700729927,
      "grad_norm": 7.023332595825195,
      "learning_rate": 1.1135036496350365e-05,
      "loss": 0.703,
      "step": 106490
    },
    {
      "epoch": 777.3722627737226,
      "grad_norm": 12.643418312072754,
      "learning_rate": 1.113138686131387e-05,
      "loss": 0.7243,
      "step": 106500
    },
    {
      "epoch": 777.4452554744526,
      "grad_norm": 10.314021110534668,
      "learning_rate": 1.1127737226277372e-05,
      "loss": 0.8308,
      "step": 106510
    },
    {
      "epoch": 777.5182481751825,
      "grad_norm": 9.723207473754883,
      "learning_rate": 1.1124087591240876e-05,
      "loss": 0.7719,
      "step": 106520
    },
    {
      "epoch": 777.5912408759124,
      "grad_norm": 8.561551094055176,
      "learning_rate": 1.112043795620438e-05,
      "loss": 0.8877,
      "step": 106530
    },
    {
      "epoch": 777.6642335766423,
      "grad_norm": 14.498687744140625,
      "learning_rate": 1.1116788321167884e-05,
      "loss": 0.8468,
      "step": 106540
    },
    {
      "epoch": 777.7372262773723,
      "grad_norm": 14.750894546508789,
      "learning_rate": 1.1113138686131388e-05,
      "loss": 0.6484,
      "step": 106550
    },
    {
      "epoch": 777.8102189781022,
      "grad_norm": 20.652538299560547,
      "learning_rate": 1.1109489051094892e-05,
      "loss": 0.9794,
      "step": 106560
    },
    {
      "epoch": 777.8832116788321,
      "grad_norm": 6.323790073394775,
      "learning_rate": 1.1105839416058394e-05,
      "loss": 0.9667,
      "step": 106570
    },
    {
      "epoch": 777.956204379562,
      "grad_norm": 13.038602828979492,
      "learning_rate": 1.11021897810219e-05,
      "loss": 0.8223,
      "step": 106580
    },
    {
      "epoch": 778.029197080292,
      "grad_norm": 7.347866058349609,
      "learning_rate": 1.1098540145985402e-05,
      "loss": 1.1194,
      "step": 106590
    },
    {
      "epoch": 778.1021897810219,
      "grad_norm": 10.111031532287598,
      "learning_rate": 1.1094890510948906e-05,
      "loss": 0.9736,
      "step": 106600
    },
    {
      "epoch": 778.1751824817518,
      "grad_norm": 16.987834930419922,
      "learning_rate": 1.109124087591241e-05,
      "loss": 1.1714,
      "step": 106610
    },
    {
      "epoch": 778.2481751824818,
      "grad_norm": 13.474883079528809,
      "learning_rate": 1.1087591240875913e-05,
      "loss": 1.0181,
      "step": 106620
    },
    {
      "epoch": 778.3211678832117,
      "grad_norm": 11.729215621948242,
      "learning_rate": 1.1083941605839417e-05,
      "loss": 0.674,
      "step": 106630
    },
    {
      "epoch": 778.3941605839416,
      "grad_norm": 14.398835182189941,
      "learning_rate": 1.108029197080292e-05,
      "loss": 0.8459,
      "step": 106640
    },
    {
      "epoch": 778.4671532846716,
      "grad_norm": 4.967983722686768,
      "learning_rate": 1.1076642335766423e-05,
      "loss": 0.6933,
      "step": 106650
    },
    {
      "epoch": 778.5401459854014,
      "grad_norm": 12.765348434448242,
      "learning_rate": 1.1072992700729927e-05,
      "loss": 0.6523,
      "step": 106660
    },
    {
      "epoch": 778.6131386861314,
      "grad_norm": 8.867293357849121,
      "learning_rate": 1.1069343065693431e-05,
      "loss": 1.2428,
      "step": 106670
    },
    {
      "epoch": 778.6861313868613,
      "grad_norm": 1.5525175333023071,
      "learning_rate": 1.1065693430656935e-05,
      "loss": 1.0752,
      "step": 106680
    },
    {
      "epoch": 778.7591240875912,
      "grad_norm": 8.627717018127441,
      "learning_rate": 1.1062043795620439e-05,
      "loss": 0.8357,
      "step": 106690
    },
    {
      "epoch": 778.8321167883212,
      "grad_norm": 10.435853004455566,
      "learning_rate": 1.1058394160583941e-05,
      "loss": 0.821,
      "step": 106700
    },
    {
      "epoch": 778.9051094890511,
      "grad_norm": 10.03351879119873,
      "learning_rate": 1.1054744525547447e-05,
      "loss": 0.8932,
      "step": 106710
    },
    {
      "epoch": 778.978102189781,
      "grad_norm": 0.028617652133107185,
      "learning_rate": 1.1051094890510949e-05,
      "loss": 0.8679,
      "step": 106720
    },
    {
      "epoch": 779.051094890511,
      "grad_norm": 0.033782824873924255,
      "learning_rate": 1.1047445255474453e-05,
      "loss": 0.6581,
      "step": 106730
    },
    {
      "epoch": 779.1240875912408,
      "grad_norm": 11.5421781539917,
      "learning_rate": 1.1043795620437957e-05,
      "loss": 0.7375,
      "step": 106740
    },
    {
      "epoch": 779.1970802919708,
      "grad_norm": 12.149774551391602,
      "learning_rate": 1.104014598540146e-05,
      "loss": 1.4952,
      "step": 106750
    },
    {
      "epoch": 779.2700729927008,
      "grad_norm": 0.06467603147029877,
      "learning_rate": 1.1036496350364964e-05,
      "loss": 0.82,
      "step": 106760
    },
    {
      "epoch": 779.3430656934306,
      "grad_norm": 4.323707103729248,
      "learning_rate": 1.1032846715328468e-05,
      "loss": 0.9773,
      "step": 106770
    },
    {
      "epoch": 779.4160583941606,
      "grad_norm": 9.950640678405762,
      "learning_rate": 1.102919708029197e-05,
      "loss": 0.9772,
      "step": 106780
    },
    {
      "epoch": 779.4890510948906,
      "grad_norm": 9.404476165771484,
      "learning_rate": 1.1025547445255476e-05,
      "loss": 0.7199,
      "step": 106790
    },
    {
      "epoch": 779.5620437956204,
      "grad_norm": 6.0349602699279785,
      "learning_rate": 1.1021897810218978e-05,
      "loss": 0.9686,
      "step": 106800
    },
    {
      "epoch": 779.6350364963504,
      "grad_norm": 8.788320541381836,
      "learning_rate": 1.1018248175182484e-05,
      "loss": 1.0539,
      "step": 106810
    },
    {
      "epoch": 779.7080291970802,
      "grad_norm": 7.893530368804932,
      "learning_rate": 1.1014598540145986e-05,
      "loss": 0.7545,
      "step": 106820
    },
    {
      "epoch": 779.7810218978102,
      "grad_norm": 0.02317993901669979,
      "learning_rate": 1.101094890510949e-05,
      "loss": 0.4359,
      "step": 106830
    },
    {
      "epoch": 779.8540145985402,
      "grad_norm": 6.593856334686279,
      "learning_rate": 1.1007299270072994e-05,
      "loss": 0.7308,
      "step": 106840
    },
    {
      "epoch": 779.92700729927,
      "grad_norm": 13.823765754699707,
      "learning_rate": 1.1003649635036496e-05,
      "loss": 0.8634,
      "step": 106850
    },
    {
      "epoch": 780.0,
      "grad_norm": 0.020992286503314972,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.8724,
      "step": 106860
    },
    {
      "epoch": 780.07299270073,
      "grad_norm": 9.588847160339355,
      "learning_rate": 1.0996350364963504e-05,
      "loss": 0.9543,
      "step": 106870
    },
    {
      "epoch": 780.1459854014598,
      "grad_norm": 6.641359329223633,
      "learning_rate": 1.0992700729927007e-05,
      "loss": 0.6534,
      "step": 106880
    },
    {
      "epoch": 780.2189781021898,
      "grad_norm": 0.02143017388880253,
      "learning_rate": 1.0989051094890511e-05,
      "loss": 1.1038,
      "step": 106890
    },
    {
      "epoch": 780.2919708029198,
      "grad_norm": 14.582571029663086,
      "learning_rate": 1.0985401459854015e-05,
      "loss": 0.8576,
      "step": 106900
    },
    {
      "epoch": 780.3649635036496,
      "grad_norm": 9.401257514953613,
      "learning_rate": 1.0981751824817519e-05,
      "loss": 0.6266,
      "step": 106910
    },
    {
      "epoch": 780.4379562043796,
      "grad_norm": 1.5740967988967896,
      "learning_rate": 1.0978102189781023e-05,
      "loss": 0.7411,
      "step": 106920
    },
    {
      "epoch": 780.5109489051094,
      "grad_norm": 0.06416942924261093,
      "learning_rate": 1.0974452554744525e-05,
      "loss": 0.6593,
      "step": 106930
    },
    {
      "epoch": 780.5839416058394,
      "grad_norm": 10.120035171508789,
      "learning_rate": 1.097080291970803e-05,
      "loss": 0.9787,
      "step": 106940
    },
    {
      "epoch": 780.6569343065694,
      "grad_norm": 1.9116129875183105,
      "learning_rate": 1.0967153284671533e-05,
      "loss": 0.8036,
      "step": 106950
    },
    {
      "epoch": 780.7299270072992,
      "grad_norm": 0.07556778937578201,
      "learning_rate": 1.0963503649635037e-05,
      "loss": 0.8084,
      "step": 106960
    },
    {
      "epoch": 780.8029197080292,
      "grad_norm": 0.10210071504116058,
      "learning_rate": 1.095985401459854e-05,
      "loss": 0.9757,
      "step": 106970
    },
    {
      "epoch": 780.8759124087592,
      "grad_norm": 8.92707347869873,
      "learning_rate": 1.0956204379562045e-05,
      "loss": 0.9659,
      "step": 106980
    },
    {
      "epoch": 780.948905109489,
      "grad_norm": 9.04405403137207,
      "learning_rate": 1.0952554744525548e-05,
      "loss": 1.4199,
      "step": 106990
    },
    {
      "epoch": 781.021897810219,
      "grad_norm": 8.834665298461914,
      "learning_rate": 1.0948905109489052e-05,
      "loss": 0.8735,
      "step": 107000
    },
    {
      "epoch": 781.0948905109489,
      "grad_norm": 11.354394912719727,
      "learning_rate": 1.0945255474452555e-05,
      "loss": 1.1015,
      "step": 107010
    },
    {
      "epoch": 781.1678832116788,
      "grad_norm": 6.944737434387207,
      "learning_rate": 1.094160583941606e-05,
      "loss": 1.5188,
      "step": 107020
    },
    {
      "epoch": 781.2408759124088,
      "grad_norm": 8.714011192321777,
      "learning_rate": 1.0937956204379562e-05,
      "loss": 0.8862,
      "step": 107030
    },
    {
      "epoch": 781.3138686131387,
      "grad_norm": 15.310388565063477,
      "learning_rate": 1.0934306569343066e-05,
      "loss": 0.815,
      "step": 107040
    },
    {
      "epoch": 781.3868613138686,
      "grad_norm": 8.432409286499023,
      "learning_rate": 1.093065693430657e-05,
      "loss": 0.9291,
      "step": 107050
    },
    {
      "epoch": 781.4598540145986,
      "grad_norm": 12.373998641967773,
      "learning_rate": 1.0927007299270072e-05,
      "loss": 0.7412,
      "step": 107060
    },
    {
      "epoch": 781.5328467153284,
      "grad_norm": 15.494531631469727,
      "learning_rate": 1.0923357664233578e-05,
      "loss": 1.2838,
      "step": 107070
    },
    {
      "epoch": 781.6058394160584,
      "grad_norm": 7.206666469573975,
      "learning_rate": 1.091970802919708e-05,
      "loss": 0.4826,
      "step": 107080
    },
    {
      "epoch": 781.6788321167883,
      "grad_norm": 10.848188400268555,
      "learning_rate": 1.0916058394160584e-05,
      "loss": 0.8906,
      "step": 107090
    },
    {
      "epoch": 781.7518248175182,
      "grad_norm": 14.982124328613281,
      "learning_rate": 1.0912408759124088e-05,
      "loss": 0.6187,
      "step": 107100
    },
    {
      "epoch": 781.8248175182482,
      "grad_norm": 7.756038188934326,
      "learning_rate": 1.0908759124087592e-05,
      "loss": 0.8731,
      "step": 107110
    },
    {
      "epoch": 781.8978102189781,
      "grad_norm": 7.919498920440674,
      "learning_rate": 1.0905109489051096e-05,
      "loss": 0.8621,
      "step": 107120
    },
    {
      "epoch": 781.970802919708,
      "grad_norm": 18.53868865966797,
      "learning_rate": 1.09014598540146e-05,
      "loss": 0.9362,
      "step": 107130
    },
    {
      "epoch": 782.043795620438,
      "grad_norm": 11.64013385772705,
      "learning_rate": 1.0897810218978102e-05,
      "loss": 0.8664,
      "step": 107140
    },
    {
      "epoch": 782.1167883211679,
      "grad_norm": 4.882964134216309,
      "learning_rate": 1.0894160583941607e-05,
      "loss": 1.0759,
      "step": 107150
    },
    {
      "epoch": 782.1897810218978,
      "grad_norm": 7.76812744140625,
      "learning_rate": 1.089051094890511e-05,
      "loss": 0.8166,
      "step": 107160
    },
    {
      "epoch": 782.2627737226277,
      "grad_norm": 0.1256934404373169,
      "learning_rate": 1.0886861313868613e-05,
      "loss": 0.6746,
      "step": 107170
    },
    {
      "epoch": 782.3357664233577,
      "grad_norm": 13.020270347595215,
      "learning_rate": 1.0883211678832117e-05,
      "loss": 1.0392,
      "step": 107180
    },
    {
      "epoch": 782.4087591240876,
      "grad_norm": 5.650679111480713,
      "learning_rate": 1.0879562043795621e-05,
      "loss": 0.8435,
      "step": 107190
    },
    {
      "epoch": 782.4817518248175,
      "grad_norm": 9.191902160644531,
      "learning_rate": 1.0875912408759125e-05,
      "loss": 0.8706,
      "step": 107200
    },
    {
      "epoch": 782.5547445255474,
      "grad_norm": 9.216686248779297,
      "learning_rate": 1.0872262773722629e-05,
      "loss": 0.3115,
      "step": 107210
    },
    {
      "epoch": 782.6277372262774,
      "grad_norm": 8.888772964477539,
      "learning_rate": 1.0868613138686131e-05,
      "loss": 0.8298,
      "step": 107220
    },
    {
      "epoch": 782.7007299270073,
      "grad_norm": 11.712388038635254,
      "learning_rate": 1.0864963503649637e-05,
      "loss": 1.3316,
      "step": 107230
    },
    {
      "epoch": 782.7737226277372,
      "grad_norm": 15.416539192199707,
      "learning_rate": 1.0861313868613139e-05,
      "loss": 1.0334,
      "step": 107240
    },
    {
      "epoch": 782.8467153284671,
      "grad_norm": 12.293017387390137,
      "learning_rate": 1.0857664233576643e-05,
      "loss": 0.7526,
      "step": 107250
    },
    {
      "epoch": 782.9197080291971,
      "grad_norm": 0.031363338232040405,
      "learning_rate": 1.0854014598540147e-05,
      "loss": 0.4614,
      "step": 107260
    },
    {
      "epoch": 782.992700729927,
      "grad_norm": 5.613211154937744,
      "learning_rate": 1.085036496350365e-05,
      "loss": 1.1925,
      "step": 107270
    },
    {
      "epoch": 783.0656934306569,
      "grad_norm": 9.02199649810791,
      "learning_rate": 1.0846715328467154e-05,
      "loss": 0.6734,
      "step": 107280
    },
    {
      "epoch": 783.1386861313869,
      "grad_norm": 11.828753471374512,
      "learning_rate": 1.0843065693430656e-05,
      "loss": 1.1747,
      "step": 107290
    },
    {
      "epoch": 783.2116788321168,
      "grad_norm": 8.956523895263672,
      "learning_rate": 1.083941605839416e-05,
      "loss": 0.6327,
      "step": 107300
    },
    {
      "epoch": 783.2846715328467,
      "grad_norm": 10.927184104919434,
      "learning_rate": 1.0835766423357664e-05,
      "loss": 0.9009,
      "step": 107310
    },
    {
      "epoch": 783.3576642335767,
      "grad_norm": 13.417187690734863,
      "learning_rate": 1.0832116788321168e-05,
      "loss": 1.3978,
      "step": 107320
    },
    {
      "epoch": 783.4306569343066,
      "grad_norm": 9.238790512084961,
      "learning_rate": 1.0828467153284672e-05,
      "loss": 0.6801,
      "step": 107330
    },
    {
      "epoch": 783.5036496350365,
      "grad_norm": 12.295079231262207,
      "learning_rate": 1.0824817518248176e-05,
      "loss": 0.5322,
      "step": 107340
    },
    {
      "epoch": 783.5766423357665,
      "grad_norm": 11.393770217895508,
      "learning_rate": 1.0821167883211678e-05,
      "loss": 1.0211,
      "step": 107350
    },
    {
      "epoch": 783.6496350364963,
      "grad_norm": 13.194109916687012,
      "learning_rate": 1.0817518248175184e-05,
      "loss": 0.923,
      "step": 107360
    },
    {
      "epoch": 783.7226277372263,
      "grad_norm": 10.397025108337402,
      "learning_rate": 1.0813868613138686e-05,
      "loss": 0.7706,
      "step": 107370
    },
    {
      "epoch": 783.7956204379562,
      "grad_norm": 6.833831787109375,
      "learning_rate": 1.0810218978102191e-05,
      "loss": 0.9341,
      "step": 107380
    },
    {
      "epoch": 783.8686131386861,
      "grad_norm": 7.882462024688721,
      "learning_rate": 1.0806569343065694e-05,
      "loss": 0.9733,
      "step": 107390
    },
    {
      "epoch": 783.9416058394161,
      "grad_norm": 12.424546241760254,
      "learning_rate": 1.0802919708029198e-05,
      "loss": 0.9824,
      "step": 107400
    },
    {
      "epoch": 784.014598540146,
      "grad_norm": 6.359083652496338,
      "learning_rate": 1.0799270072992701e-05,
      "loss": 1.1619,
      "step": 107410
    },
    {
      "epoch": 784.0875912408759,
      "grad_norm": 9.463067054748535,
      "learning_rate": 1.0795620437956205e-05,
      "loss": 0.6528,
      "step": 107420
    },
    {
      "epoch": 784.1605839416059,
      "grad_norm": 14.073813438415527,
      "learning_rate": 1.079197080291971e-05,
      "loss": 1.0166,
      "step": 107430
    },
    {
      "epoch": 784.2335766423357,
      "grad_norm": 10.462102890014648,
      "learning_rate": 1.0788321167883213e-05,
      "loss": 0.9731,
      "step": 107440
    },
    {
      "epoch": 784.3065693430657,
      "grad_norm": 16.381690979003906,
      "learning_rate": 1.0784671532846715e-05,
      "loss": 0.9658,
      "step": 107450
    },
    {
      "epoch": 784.3795620437957,
      "grad_norm": 5.932417869567871,
      "learning_rate": 1.078102189781022e-05,
      "loss": 0.4774,
      "step": 107460
    },
    {
      "epoch": 784.4525547445255,
      "grad_norm": 11.884306907653809,
      "learning_rate": 1.0777372262773723e-05,
      "loss": 0.8133,
      "step": 107470
    },
    {
      "epoch": 784.5255474452555,
      "grad_norm": 9.764695167541504,
      "learning_rate": 1.0773722627737227e-05,
      "loss": 1.2544,
      "step": 107480
    },
    {
      "epoch": 784.5985401459855,
      "grad_norm": 17.09992790222168,
      "learning_rate": 1.077007299270073e-05,
      "loss": 0.9654,
      "step": 107490
    },
    {
      "epoch": 784.6715328467153,
      "grad_norm": 12.173334121704102,
      "learning_rate": 1.0766423357664233e-05,
      "loss": 0.7861,
      "step": 107500
    },
    {
      "epoch": 784.7445255474453,
      "grad_norm": 15.466840744018555,
      "learning_rate": 1.0762773722627739e-05,
      "loss": 0.9066,
      "step": 107510
    },
    {
      "epoch": 784.8175182481751,
      "grad_norm": 9.705028533935547,
      "learning_rate": 1.075912408759124e-05,
      "loss": 0.7669,
      "step": 107520
    },
    {
      "epoch": 784.8905109489051,
      "grad_norm": 6.874539852142334,
      "learning_rate": 1.0755474452554745e-05,
      "loss": 1.0591,
      "step": 107530
    },
    {
      "epoch": 784.9635036496351,
      "grad_norm": 19.964448928833008,
      "learning_rate": 1.0751824817518248e-05,
      "loss": 0.8856,
      "step": 107540
    },
    {
      "epoch": 785.0364963503649,
      "grad_norm": 6.279201507568359,
      "learning_rate": 1.0748175182481752e-05,
      "loss": 0.5569,
      "step": 107550
    },
    {
      "epoch": 785.1094890510949,
      "grad_norm": 5.958984851837158,
      "learning_rate": 1.0744525547445256e-05,
      "loss": 0.5852,
      "step": 107560
    },
    {
      "epoch": 785.1824817518249,
      "grad_norm": 14.264052391052246,
      "learning_rate": 1.074087591240876e-05,
      "loss": 0.8475,
      "step": 107570
    },
    {
      "epoch": 785.2554744525547,
      "grad_norm": 8.348653793334961,
      "learning_rate": 1.0737226277372262e-05,
      "loss": 0.8265,
      "step": 107580
    },
    {
      "epoch": 785.3284671532847,
      "grad_norm": 7.446000099182129,
      "learning_rate": 1.0733576642335768e-05,
      "loss": 0.879,
      "step": 107590
    },
    {
      "epoch": 785.4014598540145,
      "grad_norm": 7.249825954437256,
      "learning_rate": 1.072992700729927e-05,
      "loss": 1.102,
      "step": 107600
    },
    {
      "epoch": 785.4744525547445,
      "grad_norm": 7.20109748840332,
      "learning_rate": 1.0726277372262774e-05,
      "loss": 0.6119,
      "step": 107610
    },
    {
      "epoch": 785.5474452554745,
      "grad_norm": 12.75157356262207,
      "learning_rate": 1.0722627737226278e-05,
      "loss": 1.1314,
      "step": 107620
    },
    {
      "epoch": 785.6204379562043,
      "grad_norm": 11.343392372131348,
      "learning_rate": 1.0718978102189782e-05,
      "loss": 0.9111,
      "step": 107630
    },
    {
      "epoch": 785.6934306569343,
      "grad_norm": 7.670937538146973,
      "learning_rate": 1.0715328467153286e-05,
      "loss": 1.0974,
      "step": 107640
    },
    {
      "epoch": 785.7664233576643,
      "grad_norm": 21.40930938720703,
      "learning_rate": 1.071167883211679e-05,
      "loss": 0.9515,
      "step": 107650
    },
    {
      "epoch": 785.8394160583941,
      "grad_norm": 15.534014701843262,
      "learning_rate": 1.0708029197080292e-05,
      "loss": 0.7743,
      "step": 107660
    },
    {
      "epoch": 785.9124087591241,
      "grad_norm": 10.976653099060059,
      "learning_rate": 1.0704379562043797e-05,
      "loss": 1.1454,
      "step": 107670
    },
    {
      "epoch": 785.985401459854,
      "grad_norm": 14.868656158447266,
      "learning_rate": 1.07007299270073e-05,
      "loss": 0.7874,
      "step": 107680
    },
    {
      "epoch": 786.0583941605839,
      "grad_norm": 13.17934513092041,
      "learning_rate": 1.0697080291970803e-05,
      "loss": 0.8873,
      "step": 107690
    },
    {
      "epoch": 786.1313868613139,
      "grad_norm": 10.107864379882812,
      "learning_rate": 1.0693430656934307e-05,
      "loss": 0.6214,
      "step": 107700
    },
    {
      "epoch": 786.2043795620438,
      "grad_norm": 8.25605297088623,
      "learning_rate": 1.068978102189781e-05,
      "loss": 0.8385,
      "step": 107710
    },
    {
      "epoch": 786.2773722627737,
      "grad_norm": 6.487135410308838,
      "learning_rate": 1.0686131386861315e-05,
      "loss": 0.6248,
      "step": 107720
    },
    {
      "epoch": 786.3503649635037,
      "grad_norm": 17.278839111328125,
      "learning_rate": 1.0682481751824817e-05,
      "loss": 0.7143,
      "step": 107730
    },
    {
      "epoch": 786.4233576642335,
      "grad_norm": 11.346504211425781,
      "learning_rate": 1.0678832116788321e-05,
      "loss": 0.9474,
      "step": 107740
    },
    {
      "epoch": 786.4963503649635,
      "grad_norm": 11.027871131896973,
      "learning_rate": 1.0675182481751825e-05,
      "loss": 1.3374,
      "step": 107750
    },
    {
      "epoch": 786.5693430656934,
      "grad_norm": 15.213071823120117,
      "learning_rate": 1.0671532846715329e-05,
      "loss": 1.2004,
      "step": 107760
    },
    {
      "epoch": 786.6423357664233,
      "grad_norm": 9.764037132263184,
      "learning_rate": 1.0667883211678833e-05,
      "loss": 0.6498,
      "step": 107770
    },
    {
      "epoch": 786.7153284671533,
      "grad_norm": 8.337871551513672,
      "learning_rate": 1.0664233576642337e-05,
      "loss": 1.0469,
      "step": 107780
    },
    {
      "epoch": 786.7883211678832,
      "grad_norm": 0.02484946884214878,
      "learning_rate": 1.0660583941605839e-05,
      "loss": 0.3668,
      "step": 107790
    },
    {
      "epoch": 786.8613138686131,
      "grad_norm": 6.838311195373535,
      "learning_rate": 1.0656934306569344e-05,
      "loss": 0.6949,
      "step": 107800
    },
    {
      "epoch": 786.9343065693431,
      "grad_norm": 16.557706832885742,
      "learning_rate": 1.0653284671532847e-05,
      "loss": 1.4671,
      "step": 107810
    },
    {
      "epoch": 787.007299270073,
      "grad_norm": 8.637115478515625,
      "learning_rate": 1.064963503649635e-05,
      "loss": 1.3172,
      "step": 107820
    },
    {
      "epoch": 787.0802919708029,
      "grad_norm": 6.963536739349365,
      "learning_rate": 1.0645985401459854e-05,
      "loss": 0.8543,
      "step": 107830
    },
    {
      "epoch": 787.1532846715329,
      "grad_norm": 10.635180473327637,
      "learning_rate": 1.0642335766423358e-05,
      "loss": 0.683,
      "step": 107840
    },
    {
      "epoch": 787.2262773722628,
      "grad_norm": 0.13149984180927277,
      "learning_rate": 1.0638686131386862e-05,
      "loss": 0.8077,
      "step": 107850
    },
    {
      "epoch": 787.2992700729927,
      "grad_norm": 0.065484419465065,
      "learning_rate": 1.0635036496350366e-05,
      "loss": 0.643,
      "step": 107860
    },
    {
      "epoch": 787.3722627737226,
      "grad_norm": 5.789909362792969,
      "learning_rate": 1.0631386861313868e-05,
      "loss": 1.0851,
      "step": 107870
    },
    {
      "epoch": 787.4452554744526,
      "grad_norm": 8.011905670166016,
      "learning_rate": 1.0627737226277374e-05,
      "loss": 0.4792,
      "step": 107880
    },
    {
      "epoch": 787.5182481751825,
      "grad_norm": 18.641788482666016,
      "learning_rate": 1.0624087591240876e-05,
      "loss": 0.8526,
      "step": 107890
    },
    {
      "epoch": 787.5912408759124,
      "grad_norm": 16.623641967773438,
      "learning_rate": 1.0620437956204381e-05,
      "loss": 1.0953,
      "step": 107900
    },
    {
      "epoch": 787.6642335766423,
      "grad_norm": 8.895923614501953,
      "learning_rate": 1.0616788321167884e-05,
      "loss": 0.8754,
      "step": 107910
    },
    {
      "epoch": 787.7372262773723,
      "grad_norm": 5.024848461151123,
      "learning_rate": 1.0613138686131386e-05,
      "loss": 1.1431,
      "step": 107920
    },
    {
      "epoch": 787.8102189781022,
      "grad_norm": 7.397121906280518,
      "learning_rate": 1.0609489051094891e-05,
      "loss": 1.2743,
      "step": 107930
    },
    {
      "epoch": 787.8832116788321,
      "grad_norm": 9.969122886657715,
      "learning_rate": 1.0605839416058394e-05,
      "loss": 0.8949,
      "step": 107940
    },
    {
      "epoch": 787.956204379562,
      "grad_norm": 14.34145450592041,
      "learning_rate": 1.06021897810219e-05,
      "loss": 0.7804,
      "step": 107950
    },
    {
      "epoch": 788.029197080292,
      "grad_norm": 8.01416301727295,
      "learning_rate": 1.0598540145985401e-05,
      "loss": 1.1479,
      "step": 107960
    },
    {
      "epoch": 788.1021897810219,
      "grad_norm": 17.349252700805664,
      "learning_rate": 1.0594890510948905e-05,
      "loss": 1.1143,
      "step": 107970
    },
    {
      "epoch": 788.1751824817518,
      "grad_norm": 7.05905818939209,
      "learning_rate": 1.0591240875912409e-05,
      "loss": 1.0394,
      "step": 107980
    },
    {
      "epoch": 788.2481751824818,
      "grad_norm": 7.2539849281311035,
      "learning_rate": 1.0587591240875913e-05,
      "loss": 0.3557,
      "step": 107990
    },
    {
      "epoch": 788.3211678832117,
      "grad_norm": 0.12193343788385391,
      "learning_rate": 1.0583941605839417e-05,
      "loss": 1.1549,
      "step": 108000
    },
    {
      "epoch": 788.3941605839416,
      "grad_norm": 7.058525562286377,
      "learning_rate": 1.058029197080292e-05,
      "loss": 0.8409,
      "step": 108010
    },
    {
      "epoch": 788.4671532846716,
      "grad_norm": 7.325376510620117,
      "learning_rate": 1.0576642335766423e-05,
      "loss": 0.7813,
      "step": 108020
    },
    {
      "epoch": 788.5401459854014,
      "grad_norm": 6.191468715667725,
      "learning_rate": 1.0572992700729929e-05,
      "loss": 0.7364,
      "step": 108030
    },
    {
      "epoch": 788.6131386861314,
      "grad_norm": 0.010115780867636204,
      "learning_rate": 1.056934306569343e-05,
      "loss": 0.7694,
      "step": 108040
    },
    {
      "epoch": 788.6861313868613,
      "grad_norm": 0.04115575924515724,
      "learning_rate": 1.0565693430656935e-05,
      "loss": 0.7693,
      "step": 108050
    },
    {
      "epoch": 788.7591240875912,
      "grad_norm": 14.742568016052246,
      "learning_rate": 1.0562043795620438e-05,
      "loss": 0.7571,
      "step": 108060
    },
    {
      "epoch": 788.8321167883212,
      "grad_norm": 5.326193332672119,
      "learning_rate": 1.0558394160583942e-05,
      "loss": 0.8434,
      "step": 108070
    },
    {
      "epoch": 788.9051094890511,
      "grad_norm": 0.03724578022956848,
      "learning_rate": 1.0554744525547446e-05,
      "loss": 0.8503,
      "step": 108080
    },
    {
      "epoch": 788.978102189781,
      "grad_norm": 15.017754554748535,
      "learning_rate": 1.055109489051095e-05,
      "loss": 1.1002,
      "step": 108090
    },
    {
      "epoch": 789.051094890511,
      "grad_norm": 12.041672706604004,
      "learning_rate": 1.0547445255474452e-05,
      "loss": 0.6979,
      "step": 108100
    },
    {
      "epoch": 789.1240875912408,
      "grad_norm": 0.04552309960126877,
      "learning_rate": 1.0543795620437958e-05,
      "loss": 0.989,
      "step": 108110
    },
    {
      "epoch": 789.1970802919708,
      "grad_norm": 7.112337112426758,
      "learning_rate": 1.054014598540146e-05,
      "loss": 0.626,
      "step": 108120
    },
    {
      "epoch": 789.2700729927008,
      "grad_norm": 15.278972625732422,
      "learning_rate": 1.0536496350364964e-05,
      "loss": 0.8521,
      "step": 108130
    },
    {
      "epoch": 789.3430656934306,
      "grad_norm": 10.70693588256836,
      "learning_rate": 1.0532846715328468e-05,
      "loss": 0.7888,
      "step": 108140
    },
    {
      "epoch": 789.4160583941606,
      "grad_norm": 7.167882919311523,
      "learning_rate": 1.052919708029197e-05,
      "loss": 0.949,
      "step": 108150
    },
    {
      "epoch": 789.4890510948906,
      "grad_norm": 12.267723083496094,
      "learning_rate": 1.0525547445255476e-05,
      "loss": 0.563,
      "step": 108160
    },
    {
      "epoch": 789.5620437956204,
      "grad_norm": 16.14583396911621,
      "learning_rate": 1.0521897810218978e-05,
      "loss": 0.9309,
      "step": 108170
    },
    {
      "epoch": 789.6350364963504,
      "grad_norm": 7.274283409118652,
      "learning_rate": 1.0518248175182482e-05,
      "loss": 0.7659,
      "step": 108180
    },
    {
      "epoch": 789.7080291970802,
      "grad_norm": 0.03020683117210865,
      "learning_rate": 1.0514598540145986e-05,
      "loss": 1.194,
      "step": 108190
    },
    {
      "epoch": 789.7810218978102,
      "grad_norm": 5.33313512802124,
      "learning_rate": 1.051094890510949e-05,
      "loss": 1.3459,
      "step": 108200
    },
    {
      "epoch": 789.8540145985402,
      "grad_norm": 0.023335278034210205,
      "learning_rate": 1.0507299270072993e-05,
      "loss": 0.7815,
      "step": 108210
    },
    {
      "epoch": 789.92700729927,
      "grad_norm": 0.04434974491596222,
      "learning_rate": 1.0503649635036497e-05,
      "loss": 1.2516,
      "step": 108220
    },
    {
      "epoch": 790.0,
      "grad_norm": 13.706792831420898,
      "learning_rate": 1.05e-05,
      "loss": 0.5271,
      "step": 108230
    },
    {
      "epoch": 790.07299270073,
      "grad_norm": 20.806859970092773,
      "learning_rate": 1.0496350364963505e-05,
      "loss": 1.5519,
      "step": 108240
    },
    {
      "epoch": 790.1459854014598,
      "grad_norm": 16.597646713256836,
      "learning_rate": 1.0492700729927007e-05,
      "loss": 0.8345,
      "step": 108250
    },
    {
      "epoch": 790.2189781021898,
      "grad_norm": 10.81096363067627,
      "learning_rate": 1.0489051094890511e-05,
      "loss": 1.3646,
      "step": 108260
    },
    {
      "epoch": 790.2919708029198,
      "grad_norm": 14.770923614501953,
      "learning_rate": 1.0485401459854015e-05,
      "loss": 0.9356,
      "step": 108270
    },
    {
      "epoch": 790.3649635036496,
      "grad_norm": 0.08893435448408127,
      "learning_rate": 1.0481751824817519e-05,
      "loss": 0.7472,
      "step": 108280
    },
    {
      "epoch": 790.4379562043796,
      "grad_norm": 12.02590274810791,
      "learning_rate": 1.0478102189781023e-05,
      "loss": 0.8587,
      "step": 108290
    },
    {
      "epoch": 790.5109489051094,
      "grad_norm": 8.124615669250488,
      "learning_rate": 1.0474452554744527e-05,
      "loss": 0.8898,
      "step": 108300
    },
    {
      "epoch": 790.5839416058394,
      "grad_norm": 12.402571678161621,
      "learning_rate": 1.0470802919708029e-05,
      "loss": 0.9233,
      "step": 108310
    },
    {
      "epoch": 790.6569343065694,
      "grad_norm": 19.10910415649414,
      "learning_rate": 1.0467153284671534e-05,
      "loss": 0.9071,
      "step": 108320
    },
    {
      "epoch": 790.7299270072992,
      "grad_norm": 0.7903462648391724,
      "learning_rate": 1.0463503649635037e-05,
      "loss": 0.4472,
      "step": 108330
    },
    {
      "epoch": 790.8029197080292,
      "grad_norm": 10.3608980178833,
      "learning_rate": 1.0459854014598542e-05,
      "loss": 0.7701,
      "step": 108340
    },
    {
      "epoch": 790.8759124087592,
      "grad_norm": 4.963726043701172,
      "learning_rate": 1.0456204379562044e-05,
      "loss": 0.7423,
      "step": 108350
    },
    {
      "epoch": 790.948905109489,
      "grad_norm": 10.004903793334961,
      "learning_rate": 1.0452554744525547e-05,
      "loss": 0.7382,
      "step": 108360
    },
    {
      "epoch": 791.021897810219,
      "grad_norm": 9.167455673217773,
      "learning_rate": 1.0448905109489052e-05,
      "loss": 0.8417,
      "step": 108370
    },
    {
      "epoch": 791.0948905109489,
      "grad_norm": 0.11764781922101974,
      "learning_rate": 1.0445255474452554e-05,
      "loss": 0.6429,
      "step": 108380
    },
    {
      "epoch": 791.1678832116788,
      "grad_norm": 0.05913161858916283,
      "learning_rate": 1.044160583941606e-05,
      "loss": 1.0341,
      "step": 108390
    },
    {
      "epoch": 791.2408759124088,
      "grad_norm": 7.92570686340332,
      "learning_rate": 1.0437956204379562e-05,
      "loss": 1.0554,
      "step": 108400
    },
    {
      "epoch": 791.3138686131387,
      "grad_norm": 8.108253479003906,
      "learning_rate": 1.0434306569343066e-05,
      "loss": 0.678,
      "step": 108410
    },
    {
      "epoch": 791.3868613138686,
      "grad_norm": 17.9959716796875,
      "learning_rate": 1.043065693430657e-05,
      "loss": 0.8179,
      "step": 108420
    },
    {
      "epoch": 791.4598540145986,
      "grad_norm": 10.342256546020508,
      "learning_rate": 1.0427007299270074e-05,
      "loss": 0.9604,
      "step": 108430
    },
    {
      "epoch": 791.5328467153284,
      "grad_norm": 1.582177996635437,
      "learning_rate": 1.0423357664233578e-05,
      "loss": 0.7482,
      "step": 108440
    },
    {
      "epoch": 791.6058394160584,
      "grad_norm": 9.61956787109375,
      "learning_rate": 1.0419708029197081e-05,
      "loss": 0.8778,
      "step": 108450
    },
    {
      "epoch": 791.6788321167883,
      "grad_norm": 17.513700485229492,
      "learning_rate": 1.0416058394160584e-05,
      "loss": 1.1012,
      "step": 108460
    },
    {
      "epoch": 791.7518248175182,
      "grad_norm": 13.500672340393066,
      "learning_rate": 1.041240875912409e-05,
      "loss": 0.6451,
      "step": 108470
    },
    {
      "epoch": 791.8248175182482,
      "grad_norm": 9.336256980895996,
      "learning_rate": 1.0408759124087591e-05,
      "loss": 0.9441,
      "step": 108480
    },
    {
      "epoch": 791.8978102189781,
      "grad_norm": 7.726866245269775,
      "learning_rate": 1.0405109489051095e-05,
      "loss": 0.9728,
      "step": 108490
    },
    {
      "epoch": 791.970802919708,
      "grad_norm": 7.243250846862793,
      "learning_rate": 1.04014598540146e-05,
      "loss": 0.781,
      "step": 108500
    },
    {
      "epoch": 792.043795620438,
      "grad_norm": 0.05158701539039612,
      "learning_rate": 1.0397810218978103e-05,
      "loss": 0.7129,
      "step": 108510
    },
    {
      "epoch": 792.1167883211679,
      "grad_norm": 8.182190895080566,
      "learning_rate": 1.0394160583941607e-05,
      "loss": 1.0151,
      "step": 108520
    },
    {
      "epoch": 792.1897810218978,
      "grad_norm": 11.814709663391113,
      "learning_rate": 1.039051094890511e-05,
      "loss": 0.9436,
      "step": 108530
    },
    {
      "epoch": 792.2627737226277,
      "grad_norm": 17.2934627532959,
      "learning_rate": 1.0386861313868613e-05,
      "loss": 1.2518,
      "step": 108540
    },
    {
      "epoch": 792.3357664233577,
      "grad_norm": 10.539846420288086,
      "learning_rate": 1.0383211678832119e-05,
      "loss": 1.0631,
      "step": 108550
    },
    {
      "epoch": 792.4087591240876,
      "grad_norm": 0.02738967537879944,
      "learning_rate": 1.037956204379562e-05,
      "loss": 0.6915,
      "step": 108560
    },
    {
      "epoch": 792.4817518248175,
      "grad_norm": 7.731965065002441,
      "learning_rate": 1.0375912408759125e-05,
      "loss": 0.9439,
      "step": 108570
    },
    {
      "epoch": 792.5547445255474,
      "grad_norm": 9.09718132019043,
      "learning_rate": 1.0372262773722629e-05,
      "loss": 0.956,
      "step": 108580
    },
    {
      "epoch": 792.6277372262774,
      "grad_norm": 2.861259698867798,
      "learning_rate": 1.036861313868613e-05,
      "loss": 0.9768,
      "step": 108590
    },
    {
      "epoch": 792.7007299270073,
      "grad_norm": 10.546147346496582,
      "learning_rate": 1.0364963503649636e-05,
      "loss": 0.881,
      "step": 108600
    },
    {
      "epoch": 792.7737226277372,
      "grad_norm": 11.656665802001953,
      "learning_rate": 1.0361313868613138e-05,
      "loss": 0.6075,
      "step": 108610
    },
    {
      "epoch": 792.8467153284671,
      "grad_norm": 7.796600818634033,
      "learning_rate": 1.0357664233576642e-05,
      "loss": 0.9075,
      "step": 108620
    },
    {
      "epoch": 792.9197080291971,
      "grad_norm": 11.41260051727295,
      "learning_rate": 1.0354014598540146e-05,
      "loss": 0.9117,
      "step": 108630
    },
    {
      "epoch": 792.992700729927,
      "grad_norm": 11.59974479675293,
      "learning_rate": 1.035036496350365e-05,
      "loss": 0.6325,
      "step": 108640
    },
    {
      "epoch": 793.0656934306569,
      "grad_norm": 13.573593139648438,
      "learning_rate": 1.0346715328467154e-05,
      "loss": 1.0071,
      "step": 108650
    },
    {
      "epoch": 793.1386861313869,
      "grad_norm": 0.05082980543375015,
      "learning_rate": 1.0343065693430658e-05,
      "loss": 0.4835,
      "step": 108660
    },
    {
      "epoch": 793.2116788321168,
      "grad_norm": 0.047372277826070786,
      "learning_rate": 1.033941605839416e-05,
      "loss": 1.0776,
      "step": 108670
    },
    {
      "epoch": 793.2846715328467,
      "grad_norm": 0.03498557209968567,
      "learning_rate": 1.0335766423357666e-05,
      "loss": 0.6558,
      "step": 108680
    },
    {
      "epoch": 793.3576642335767,
      "grad_norm": 7.8282341957092285,
      "learning_rate": 1.0332116788321168e-05,
      "loss": 0.6156,
      "step": 108690
    },
    {
      "epoch": 793.4306569343066,
      "grad_norm": 9.39963436126709,
      "learning_rate": 1.0328467153284672e-05,
      "loss": 1.0111,
      "step": 108700
    },
    {
      "epoch": 793.5036496350365,
      "grad_norm": 12.56335163116455,
      "learning_rate": 1.0324817518248176e-05,
      "loss": 0.9293,
      "step": 108710
    },
    {
      "epoch": 793.5766423357665,
      "grad_norm": 6.477377891540527,
      "learning_rate": 1.032116788321168e-05,
      "loss": 0.5837,
      "step": 108720
    },
    {
      "epoch": 793.6496350364963,
      "grad_norm": 25.427940368652344,
      "learning_rate": 1.0317518248175183e-05,
      "loss": 1.1653,
      "step": 108730
    },
    {
      "epoch": 793.7226277372263,
      "grad_norm": 14.267889976501465,
      "learning_rate": 1.0313868613138687e-05,
      "loss": 1.1506,
      "step": 108740
    },
    {
      "epoch": 793.7956204379562,
      "grad_norm": 11.337682723999023,
      "learning_rate": 1.031021897810219e-05,
      "loss": 1.0061,
      "step": 108750
    },
    {
      "epoch": 793.8686131386861,
      "grad_norm": 17.784992218017578,
      "learning_rate": 1.0306569343065695e-05,
      "loss": 0.6736,
      "step": 108760
    },
    {
      "epoch": 793.9416058394161,
      "grad_norm": 12.6765775680542,
      "learning_rate": 1.0302919708029197e-05,
      "loss": 1.0742,
      "step": 108770
    },
    {
      "epoch": 794.014598540146,
      "grad_norm": 18.746501922607422,
      "learning_rate": 1.0299270072992701e-05,
      "loss": 0.8361,
      "step": 108780
    },
    {
      "epoch": 794.0875912408759,
      "grad_norm": 0.6935179233551025,
      "learning_rate": 1.0295620437956205e-05,
      "loss": 0.7115,
      "step": 108790
    },
    {
      "epoch": 794.1605839416059,
      "grad_norm": 11.728246688842773,
      "learning_rate": 1.0291970802919707e-05,
      "loss": 0.8793,
      "step": 108800
    },
    {
      "epoch": 794.2335766423357,
      "grad_norm": 9.138851165771484,
      "learning_rate": 1.0288321167883213e-05,
      "loss": 0.6694,
      "step": 108810
    },
    {
      "epoch": 794.3065693430657,
      "grad_norm": 11.059637069702148,
      "learning_rate": 1.0284671532846715e-05,
      "loss": 0.8828,
      "step": 108820
    },
    {
      "epoch": 794.3795620437957,
      "grad_norm": 9.836008071899414,
      "learning_rate": 1.0281021897810219e-05,
      "loss": 0.7907,
      "step": 108830
    },
    {
      "epoch": 794.4525547445255,
      "grad_norm": 7.338359355926514,
      "learning_rate": 1.0277372262773723e-05,
      "loss": 0.8378,
      "step": 108840
    },
    {
      "epoch": 794.5255474452555,
      "grad_norm": 10.114075660705566,
      "learning_rate": 1.0273722627737227e-05,
      "loss": 0.8149,
      "step": 108850
    },
    {
      "epoch": 794.5985401459855,
      "grad_norm": 7.827880859375,
      "learning_rate": 1.027007299270073e-05,
      "loss": 1.1418,
      "step": 108860
    },
    {
      "epoch": 794.6715328467153,
      "grad_norm": 8.91321849822998,
      "learning_rate": 1.0266423357664234e-05,
      "loss": 1.0671,
      "step": 108870
    },
    {
      "epoch": 794.7445255474453,
      "grad_norm": 11.409337997436523,
      "learning_rate": 1.0262773722627737e-05,
      "loss": 1.0071,
      "step": 108880
    },
    {
      "epoch": 794.8175182481751,
      "grad_norm": 7.698253154754639,
      "learning_rate": 1.0259124087591242e-05,
      "loss": 0.8176,
      "step": 108890
    },
    {
      "epoch": 794.8905109489051,
      "grad_norm": 9.396537780761719,
      "learning_rate": 1.0255474452554744e-05,
      "loss": 0.8426,
      "step": 108900
    },
    {
      "epoch": 794.9635036496351,
      "grad_norm": 0.07979371398687363,
      "learning_rate": 1.025182481751825e-05,
      "loss": 0.8355,
      "step": 108910
    },
    {
      "epoch": 795.0364963503649,
      "grad_norm": 14.632486343383789,
      "learning_rate": 1.0248175182481752e-05,
      "loss": 1.0278,
      "step": 108920
    },
    {
      "epoch": 795.1094890510949,
      "grad_norm": 10.82344913482666,
      "learning_rate": 1.0244525547445256e-05,
      "loss": 0.9257,
      "step": 108930
    },
    {
      "epoch": 795.1824817518249,
      "grad_norm": 8.071540832519531,
      "learning_rate": 1.024087591240876e-05,
      "loss": 0.9879,
      "step": 108940
    },
    {
      "epoch": 795.2554744525547,
      "grad_norm": 6.692251205444336,
      "learning_rate": 1.0237226277372264e-05,
      "loss": 0.8801,
      "step": 108950
    },
    {
      "epoch": 795.3284671532847,
      "grad_norm": 14.034529685974121,
      "learning_rate": 1.0233576642335768e-05,
      "loss": 1.1468,
      "step": 108960
    },
    {
      "epoch": 795.4014598540145,
      "grad_norm": 19.35974884033203,
      "learning_rate": 1.0229927007299271e-05,
      "loss": 0.9122,
      "step": 108970
    },
    {
      "epoch": 795.4744525547445,
      "grad_norm": 8.288641929626465,
      "learning_rate": 1.0226277372262774e-05,
      "loss": 0.8066,
      "step": 108980
    },
    {
      "epoch": 795.5474452554745,
      "grad_norm": 0.09635455161333084,
      "learning_rate": 1.022262773722628e-05,
      "loss": 0.8708,
      "step": 108990
    },
    {
      "epoch": 795.6204379562043,
      "grad_norm": 6.50654411315918,
      "learning_rate": 1.0218978102189781e-05,
      "loss": 0.6922,
      "step": 109000
    },
    {
      "epoch": 795.6934306569343,
      "grad_norm": 7.7972235679626465,
      "learning_rate": 1.0215328467153285e-05,
      "loss": 0.7543,
      "step": 109010
    },
    {
      "epoch": 795.7664233576643,
      "grad_norm": 18.143552780151367,
      "learning_rate": 1.021167883211679e-05,
      "loss": 1.1556,
      "step": 109020
    },
    {
      "epoch": 795.8394160583941,
      "grad_norm": 7.7590742111206055,
      "learning_rate": 1.0208029197080291e-05,
      "loss": 0.7445,
      "step": 109030
    },
    {
      "epoch": 795.9124087591241,
      "grad_norm": 0.013936654664576054,
      "learning_rate": 1.0204379562043797e-05,
      "loss": 0.8194,
      "step": 109040
    },
    {
      "epoch": 795.985401459854,
      "grad_norm": 10.126646041870117,
      "learning_rate": 1.0200729927007299e-05,
      "loss": 0.7606,
      "step": 109050
    },
    {
      "epoch": 796.0583941605839,
      "grad_norm": 7.96720552444458,
      "learning_rate": 1.0197080291970803e-05,
      "loss": 0.7498,
      "step": 109060
    },
    {
      "epoch": 796.1313868613139,
      "grad_norm": 0.041473690420389175,
      "learning_rate": 1.0193430656934307e-05,
      "loss": 0.7742,
      "step": 109070
    },
    {
      "epoch": 796.2043795620438,
      "grad_norm": 8.978446006774902,
      "learning_rate": 1.018978102189781e-05,
      "loss": 0.8731,
      "step": 109080
    },
    {
      "epoch": 796.2773722627737,
      "grad_norm": 14.106221199035645,
      "learning_rate": 1.0186131386861315e-05,
      "loss": 0.9952,
      "step": 109090
    },
    {
      "epoch": 796.3503649635037,
      "grad_norm": 6.871679306030273,
      "learning_rate": 1.0182481751824819e-05,
      "loss": 0.5974,
      "step": 109100
    },
    {
      "epoch": 796.4233576642335,
      "grad_norm": 0.09824611991643906,
      "learning_rate": 1.017883211678832e-05,
      "loss": 0.8238,
      "step": 109110
    },
    {
      "epoch": 796.4963503649635,
      "grad_norm": 16.814645767211914,
      "learning_rate": 1.0175182481751826e-05,
      "loss": 1.231,
      "step": 109120
    },
    {
      "epoch": 796.5693430656934,
      "grad_norm": 10.089582443237305,
      "learning_rate": 1.0171532846715329e-05,
      "loss": 0.9412,
      "step": 109130
    },
    {
      "epoch": 796.6423357664233,
      "grad_norm": 20.2041015625,
      "learning_rate": 1.0167883211678832e-05,
      "loss": 1.3188,
      "step": 109140
    },
    {
      "epoch": 796.7153284671533,
      "grad_norm": 9.195981979370117,
      "learning_rate": 1.0164233576642336e-05,
      "loss": 1.0437,
      "step": 109150
    },
    {
      "epoch": 796.7883211678832,
      "grad_norm": 13.971213340759277,
      "learning_rate": 1.016058394160584e-05,
      "loss": 0.7452,
      "step": 109160
    },
    {
      "epoch": 796.8613138686131,
      "grad_norm": 0.020318903028964996,
      "learning_rate": 1.0156934306569344e-05,
      "loss": 0.5056,
      "step": 109170
    },
    {
      "epoch": 796.9343065693431,
      "grad_norm": 11.392193794250488,
      "learning_rate": 1.0153284671532848e-05,
      "loss": 1.1532,
      "step": 109180
    },
    {
      "epoch": 797.007299270073,
      "grad_norm": 7.802159786224365,
      "learning_rate": 1.014963503649635e-05,
      "loss": 0.7772,
      "step": 109190
    },
    {
      "epoch": 797.0802919708029,
      "grad_norm": 13.811619758605957,
      "learning_rate": 1.0145985401459856e-05,
      "loss": 0.8571,
      "step": 109200
    },
    {
      "epoch": 797.1532846715329,
      "grad_norm": 14.902827262878418,
      "learning_rate": 1.0142335766423358e-05,
      "loss": 1.1677,
      "step": 109210
    },
    {
      "epoch": 797.2262773722628,
      "grad_norm": 0.02652602456510067,
      "learning_rate": 1.0138686131386862e-05,
      "loss": 1.1046,
      "step": 109220
    },
    {
      "epoch": 797.2992700729927,
      "grad_norm": 7.340966701507568,
      "learning_rate": 1.0135036496350366e-05,
      "loss": 0.5683,
      "step": 109230
    },
    {
      "epoch": 797.3722627737226,
      "grad_norm": 0.08215969055891037,
      "learning_rate": 1.0131386861313868e-05,
      "loss": 1.0291,
      "step": 109240
    },
    {
      "epoch": 797.4452554744526,
      "grad_norm": 7.505634784698486,
      "learning_rate": 1.0127737226277373e-05,
      "loss": 1.1191,
      "step": 109250
    },
    {
      "epoch": 797.5182481751825,
      "grad_norm": 0.04749007150530815,
      "learning_rate": 1.0124087591240876e-05,
      "loss": 0.7816,
      "step": 109260
    },
    {
      "epoch": 797.5912408759124,
      "grad_norm": 9.417363166809082,
      "learning_rate": 1.012043795620438e-05,
      "loss": 0.6402,
      "step": 109270
    },
    {
      "epoch": 797.6642335766423,
      "grad_norm": 0.011402467265725136,
      "learning_rate": 1.0116788321167883e-05,
      "loss": 0.7739,
      "step": 109280
    },
    {
      "epoch": 797.7372262773723,
      "grad_norm": 7.516753673553467,
      "learning_rate": 1.0113138686131387e-05,
      "loss": 1.0761,
      "step": 109290
    },
    {
      "epoch": 797.8102189781022,
      "grad_norm": 12.353826522827148,
      "learning_rate": 1.0109489051094891e-05,
      "loss": 0.9259,
      "step": 109300
    },
    {
      "epoch": 797.8832116788321,
      "grad_norm": 10.889249801635742,
      "learning_rate": 1.0105839416058395e-05,
      "loss": 0.8018,
      "step": 109310
    },
    {
      "epoch": 797.956204379562,
      "grad_norm": 0.11159609258174896,
      "learning_rate": 1.0102189781021897e-05,
      "loss": 0.8247,
      "step": 109320
    },
    {
      "epoch": 798.029197080292,
      "grad_norm": 16.369094848632812,
      "learning_rate": 1.0098540145985403e-05,
      "loss": 1.061,
      "step": 109330
    },
    {
      "epoch": 798.1021897810219,
      "grad_norm": 8.177861213684082,
      "learning_rate": 1.0094890510948905e-05,
      "loss": 0.7035,
      "step": 109340
    },
    {
      "epoch": 798.1751824817518,
      "grad_norm": 0.02988562360405922,
      "learning_rate": 1.0091240875912409e-05,
      "loss": 0.8412,
      "step": 109350
    },
    {
      "epoch": 798.2481751824818,
      "grad_norm": 14.353198051452637,
      "learning_rate": 1.0087591240875913e-05,
      "loss": 0.636,
      "step": 109360
    },
    {
      "epoch": 798.3211678832117,
      "grad_norm": 7.820021629333496,
      "learning_rate": 1.0083941605839417e-05,
      "loss": 1.1752,
      "step": 109370
    },
    {
      "epoch": 798.3941605839416,
      "grad_norm": 7.6409454345703125,
      "learning_rate": 1.008029197080292e-05,
      "loss": 0.9112,
      "step": 109380
    },
    {
      "epoch": 798.4671532846716,
      "grad_norm": 10.384896278381348,
      "learning_rate": 1.0076642335766424e-05,
      "loss": 1.0457,
      "step": 109390
    },
    {
      "epoch": 798.5401459854014,
      "grad_norm": 5.171270847320557,
      "learning_rate": 1.0072992700729927e-05,
      "loss": 0.8254,
      "step": 109400
    },
    {
      "epoch": 798.6131386861314,
      "grad_norm": 14.599921226501465,
      "learning_rate": 1.0069343065693432e-05,
      "loss": 1.1126,
      "step": 109410
    },
    {
      "epoch": 798.6861313868613,
      "grad_norm": 10.841123580932617,
      "learning_rate": 1.0065693430656934e-05,
      "loss": 0.7586,
      "step": 109420
    },
    {
      "epoch": 798.7591240875912,
      "grad_norm": 13.108080863952637,
      "learning_rate": 1.0062043795620438e-05,
      "loss": 1.5232,
      "step": 109430
    },
    {
      "epoch": 798.8321167883212,
      "grad_norm": 0.03215888887643814,
      "learning_rate": 1.0058394160583942e-05,
      "loss": 0.7313,
      "step": 109440
    },
    {
      "epoch": 798.9051094890511,
      "grad_norm": 1.3458514213562012,
      "learning_rate": 1.0054744525547444e-05,
      "loss": 0.594,
      "step": 109450
    },
    {
      "epoch": 798.978102189781,
      "grad_norm": 0.07244905084371567,
      "learning_rate": 1.005109489051095e-05,
      "loss": 0.677,
      "step": 109460
    },
    {
      "epoch": 799.051094890511,
      "grad_norm": 3.9477903842926025,
      "learning_rate": 1.0047445255474452e-05,
      "loss": 0.744,
      "step": 109470
    },
    {
      "epoch": 799.1240875912408,
      "grad_norm": 13.95312213897705,
      "learning_rate": 1.0043795620437958e-05,
      "loss": 1.0684,
      "step": 109480
    },
    {
      "epoch": 799.1970802919708,
      "grad_norm": 10.399971961975098,
      "learning_rate": 1.004014598540146e-05,
      "loss": 0.9086,
      "step": 109490
    },
    {
      "epoch": 799.2700729927008,
      "grad_norm": 8.405025482177734,
      "learning_rate": 1.0036496350364964e-05,
      "loss": 1.1544,
      "step": 109500
    },
    {
      "epoch": 799.3430656934306,
      "grad_norm": 0.08259791880846024,
      "learning_rate": 1.0032846715328468e-05,
      "loss": 0.9303,
      "step": 109510
    },
    {
      "epoch": 799.4160583941606,
      "grad_norm": 0.034663550555706024,
      "learning_rate": 1.0029197080291971e-05,
      "loss": 1.0072,
      "step": 109520
    },
    {
      "epoch": 799.4890510948906,
      "grad_norm": 11.869001388549805,
      "learning_rate": 1.0025547445255475e-05,
      "loss": 0.9904,
      "step": 109530
    },
    {
      "epoch": 799.5620437956204,
      "grad_norm": 14.825867652893066,
      "learning_rate": 1.002189781021898e-05,
      "loss": 0.8515,
      "step": 109540
    },
    {
      "epoch": 799.6350364963504,
      "grad_norm": 12.500085830688477,
      "learning_rate": 1.0018248175182481e-05,
      "loss": 0.8934,
      "step": 109550
    },
    {
      "epoch": 799.7080291970802,
      "grad_norm": 6.060945510864258,
      "learning_rate": 1.0014598540145987e-05,
      "loss": 0.7592,
      "step": 109560
    },
    {
      "epoch": 799.7810218978102,
      "grad_norm": 0.9212793707847595,
      "learning_rate": 1.001094890510949e-05,
      "loss": 0.7565,
      "step": 109570
    },
    {
      "epoch": 799.8540145985402,
      "grad_norm": 0.0800621286034584,
      "learning_rate": 1.0007299270072993e-05,
      "loss": 0.8387,
      "step": 109580
    },
    {
      "epoch": 799.92700729927,
      "grad_norm": 5.554584503173828,
      "learning_rate": 1.0003649635036497e-05,
      "loss": 0.466,
      "step": 109590
    },
    {
      "epoch": 800.0,
      "grad_norm": 17.35145378112793,
      "learning_rate": 1e-05,
      "loss": 1.1261,
      "step": 109600
    },
    {
      "epoch": 800.07299270073,
      "grad_norm": 1.9094102382659912,
      "learning_rate": 9.996350364963505e-06,
      "loss": 0.8151,
      "step": 109610
    },
    {
      "epoch": 800.1459854014598,
      "grad_norm": 18.426969528198242,
      "learning_rate": 9.992700729927009e-06,
      "loss": 0.5041,
      "step": 109620
    },
    {
      "epoch": 800.2189781021898,
      "grad_norm": 0.10382992029190063,
      "learning_rate": 9.98905109489051e-06,
      "loss": 0.7304,
      "step": 109630
    },
    {
      "epoch": 800.2919708029198,
      "grad_norm": 15.482207298278809,
      "learning_rate": 9.985401459854016e-06,
      "loss": 1.0194,
      "step": 109640
    },
    {
      "epoch": 800.3649635036496,
      "grad_norm": 0.6265804171562195,
      "learning_rate": 9.981751824817519e-06,
      "loss": 0.812,
      "step": 109650
    },
    {
      "epoch": 800.4379562043796,
      "grad_norm": 5.034392356872559,
      "learning_rate": 9.978102189781022e-06,
      "loss": 0.736,
      "step": 109660
    },
    {
      "epoch": 800.5109489051094,
      "grad_norm": 19.313129425048828,
      "learning_rate": 9.974452554744526e-06,
      "loss": 1.3235,
      "step": 109670
    },
    {
      "epoch": 800.5839416058394,
      "grad_norm": 10.933815956115723,
      "learning_rate": 9.970802919708028e-06,
      "loss": 0.8785,
      "step": 109680
    },
    {
      "epoch": 800.6569343065694,
      "grad_norm": 7.1973490715026855,
      "learning_rate": 9.967153284671534e-06,
      "loss": 0.683,
      "step": 109690
    },
    {
      "epoch": 800.7299270072992,
      "grad_norm": 11.359164237976074,
      "learning_rate": 9.963503649635036e-06,
      "loss": 0.9006,
      "step": 109700
    },
    {
      "epoch": 800.8029197080292,
      "grad_norm": 8.58089828491211,
      "learning_rate": 9.95985401459854e-06,
      "loss": 0.9571,
      "step": 109710
    },
    {
      "epoch": 800.8759124087592,
      "grad_norm": 10.472423553466797,
      "learning_rate": 9.956204379562044e-06,
      "loss": 1.0034,
      "step": 109720
    },
    {
      "epoch": 800.948905109489,
      "grad_norm": 8.073038101196289,
      "learning_rate": 9.952554744525548e-06,
      "loss": 1.2007,
      "step": 109730
    },
    {
      "epoch": 801.021897810219,
      "grad_norm": 12.617059707641602,
      "learning_rate": 9.948905109489052e-06,
      "loss": 0.8759,
      "step": 109740
    },
    {
      "epoch": 801.0948905109489,
      "grad_norm": 0.10127964615821838,
      "learning_rate": 9.945255474452556e-06,
      "loss": 1.2399,
      "step": 109750
    },
    {
      "epoch": 801.1678832116788,
      "grad_norm": 0.04242182523012161,
      "learning_rate": 9.941605839416058e-06,
      "loss": 1.1517,
      "step": 109760
    },
    {
      "epoch": 801.2408759124088,
      "grad_norm": 8.218823432922363,
      "learning_rate": 9.937956204379563e-06,
      "loss": 0.8327,
      "step": 109770
    },
    {
      "epoch": 801.3138686131387,
      "grad_norm": 0.019817017018795013,
      "learning_rate": 9.934306569343066e-06,
      "loss": 1.0091,
      "step": 109780
    },
    {
      "epoch": 801.3868613138686,
      "grad_norm": 4.914229393005371,
      "learning_rate": 9.93065693430657e-06,
      "loss": 0.362,
      "step": 109790
    },
    {
      "epoch": 801.4598540145986,
      "grad_norm": 18.732847213745117,
      "learning_rate": 9.927007299270073e-06,
      "loss": 0.9506,
      "step": 109800
    },
    {
      "epoch": 801.5328467153284,
      "grad_norm": 13.483282089233398,
      "learning_rate": 9.923357664233577e-06,
      "loss": 0.7986,
      "step": 109810
    },
    {
      "epoch": 801.6058394160584,
      "grad_norm": 10.047529220581055,
      "learning_rate": 9.919708029197081e-06,
      "loss": 1.1504,
      "step": 109820
    },
    {
      "epoch": 801.6788321167883,
      "grad_norm": 7.479699611663818,
      "learning_rate": 9.916058394160585e-06,
      "loss": 0.8207,
      "step": 109830
    },
    {
      "epoch": 801.7518248175182,
      "grad_norm": 7.8676300048828125,
      "learning_rate": 9.912408759124087e-06,
      "loss": 0.9509,
      "step": 109840
    },
    {
      "epoch": 801.8248175182482,
      "grad_norm": 9.98508071899414,
      "learning_rate": 9.908759124087593e-06,
      "loss": 0.7329,
      "step": 109850
    },
    {
      "epoch": 801.8978102189781,
      "grad_norm": 0.07842077314853668,
      "learning_rate": 9.905109489051095e-06,
      "loss": 0.5343,
      "step": 109860
    },
    {
      "epoch": 801.970802919708,
      "grad_norm": 9.852670669555664,
      "learning_rate": 9.901459854014599e-06,
      "loss": 1.1622,
      "step": 109870
    },
    {
      "epoch": 802.043795620438,
      "grad_norm": 0.15041756629943848,
      "learning_rate": 9.897810218978103e-06,
      "loss": 1.0829,
      "step": 109880
    },
    {
      "epoch": 802.1167883211679,
      "grad_norm": 5.2472991943359375,
      "learning_rate": 9.894160583941605e-06,
      "loss": 1.1278,
      "step": 109890
    },
    {
      "epoch": 802.1897810218978,
      "grad_norm": 7.2348713874816895,
      "learning_rate": 9.89051094890511e-06,
      "loss": 0.9412,
      "step": 109900
    },
    {
      "epoch": 802.2627737226277,
      "grad_norm": 12.2733736038208,
      "learning_rate": 9.886861313868613e-06,
      "loss": 0.6101,
      "step": 109910
    },
    {
      "epoch": 802.3357664233577,
      "grad_norm": 8.293254852294922,
      "learning_rate": 9.883211678832118e-06,
      "loss": 0.7585,
      "step": 109920
    },
    {
      "epoch": 802.4087591240876,
      "grad_norm": 0.008241874165832996,
      "learning_rate": 9.87956204379562e-06,
      "loss": 0.6972,
      "step": 109930
    },
    {
      "epoch": 802.4817518248175,
      "grad_norm": 11.748137474060059,
      "learning_rate": 9.875912408759124e-06,
      "loss": 0.8065,
      "step": 109940
    },
    {
      "epoch": 802.5547445255474,
      "grad_norm": 17.214548110961914,
      "learning_rate": 9.872262773722628e-06,
      "loss": 1.0051,
      "step": 109950
    },
    {
      "epoch": 802.6277372262774,
      "grad_norm": 11.634774208068848,
      "learning_rate": 9.868613138686132e-06,
      "loss": 0.8538,
      "step": 109960
    },
    {
      "epoch": 802.7007299270073,
      "grad_norm": 7.313470363616943,
      "learning_rate": 9.864963503649636e-06,
      "loss": 1.0826,
      "step": 109970
    },
    {
      "epoch": 802.7737226277372,
      "grad_norm": 0.02882389910519123,
      "learning_rate": 9.86131386861314e-06,
      "loss": 0.7617,
      "step": 109980
    },
    {
      "epoch": 802.8467153284671,
      "grad_norm": 12.193489074707031,
      "learning_rate": 9.857664233576642e-06,
      "loss": 0.7785,
      "step": 109990
    },
    {
      "epoch": 802.9197080291971,
      "grad_norm": 0.014892454259097576,
      "learning_rate": 9.854014598540148e-06,
      "loss": 0.9753,
      "step": 110000
    },
    {
      "epoch": 802.992700729927,
      "grad_norm": 4.536034107208252,
      "learning_rate": 9.85036496350365e-06,
      "loss": 1.0203,
      "step": 110010
    },
    {
      "epoch": 803.0656934306569,
      "grad_norm": 9.94646167755127,
      "learning_rate": 9.846715328467154e-06,
      "loss": 0.7875,
      "step": 110020
    },
    {
      "epoch": 803.1386861313869,
      "grad_norm": 18.67836570739746,
      "learning_rate": 9.843065693430658e-06,
      "loss": 0.9009,
      "step": 110030
    },
    {
      "epoch": 803.2116788321168,
      "grad_norm": 0.0641058087348938,
      "learning_rate": 9.839416058394161e-06,
      "loss": 0.6646,
      "step": 110040
    },
    {
      "epoch": 803.2846715328467,
      "grad_norm": 17.340303421020508,
      "learning_rate": 9.835766423357665e-06,
      "loss": 0.8427,
      "step": 110050
    },
    {
      "epoch": 803.3576642335767,
      "grad_norm": 0.025951582938432693,
      "learning_rate": 9.83211678832117e-06,
      "loss": 0.9251,
      "step": 110060
    },
    {
      "epoch": 803.4306569343066,
      "grad_norm": 10.36718463897705,
      "learning_rate": 9.828467153284671e-06,
      "loss": 0.7861,
      "step": 110070
    },
    {
      "epoch": 803.5036496350365,
      "grad_norm": 12.533429145812988,
      "learning_rate": 9.824817518248175e-06,
      "loss": 0.8063,
      "step": 110080
    },
    {
      "epoch": 803.5766423357665,
      "grad_norm": 5.397364616394043,
      "learning_rate": 9.82116788321168e-06,
      "loss": 0.7322,
      "step": 110090
    },
    {
      "epoch": 803.6496350364963,
      "grad_norm": 15.022095680236816,
      "learning_rate": 9.817518248175183e-06,
      "loss": 0.8849,
      "step": 110100
    },
    {
      "epoch": 803.7226277372263,
      "grad_norm": 11.606635093688965,
      "learning_rate": 9.813868613138687e-06,
      "loss": 0.9599,
      "step": 110110
    },
    {
      "epoch": 803.7956204379562,
      "grad_norm": 13.845121383666992,
      "learning_rate": 9.810218978102189e-06,
      "loss": 1.5238,
      "step": 110120
    },
    {
      "epoch": 803.8686131386861,
      "grad_norm": 0.03253665566444397,
      "learning_rate": 9.806569343065695e-06,
      "loss": 0.9754,
      "step": 110130
    },
    {
      "epoch": 803.9416058394161,
      "grad_norm": 12.465411186218262,
      "learning_rate": 9.802919708029197e-06,
      "loss": 0.9451,
      "step": 110140
    },
    {
      "epoch": 804.014598540146,
      "grad_norm": 11.456504821777344,
      "learning_rate": 9.7992700729927e-06,
      "loss": 1.1041,
      "step": 110150
    },
    {
      "epoch": 804.0875912408759,
      "grad_norm": 17.056873321533203,
      "learning_rate": 9.795620437956205e-06,
      "loss": 0.7995,
      "step": 110160
    },
    {
      "epoch": 804.1605839416059,
      "grad_norm": 7.231696128845215,
      "learning_rate": 9.791970802919709e-06,
      "loss": 0.5435,
      "step": 110170
    },
    {
      "epoch": 804.2335766423357,
      "grad_norm": 15.49924373626709,
      "learning_rate": 9.788321167883212e-06,
      "loss": 0.987,
      "step": 110180
    },
    {
      "epoch": 804.3065693430657,
      "grad_norm": 6.730625152587891,
      "learning_rate": 9.784671532846716e-06,
      "loss": 0.5953,
      "step": 110190
    },
    {
      "epoch": 804.3795620437957,
      "grad_norm": 0.07304001599550247,
      "learning_rate": 9.781021897810219e-06,
      "loss": 0.8693,
      "step": 110200
    },
    {
      "epoch": 804.4525547445255,
      "grad_norm": 0.03097464144229889,
      "learning_rate": 9.777372262773724e-06,
      "loss": 1.2245,
      "step": 110210
    },
    {
      "epoch": 804.5255474452555,
      "grad_norm": 5.553643226623535,
      "learning_rate": 9.773722627737226e-06,
      "loss": 0.7024,
      "step": 110220
    },
    {
      "epoch": 804.5985401459855,
      "grad_norm": 7.963962078094482,
      "learning_rate": 9.77007299270073e-06,
      "loss": 1.1906,
      "step": 110230
    },
    {
      "epoch": 804.6715328467153,
      "grad_norm": 6.981229305267334,
      "learning_rate": 9.766423357664234e-06,
      "loss": 0.922,
      "step": 110240
    },
    {
      "epoch": 804.7445255474453,
      "grad_norm": 0.038479696959257126,
      "learning_rate": 9.762773722627738e-06,
      "loss": 0.6566,
      "step": 110250
    },
    {
      "epoch": 804.8175182481751,
      "grad_norm": 0.0705968588590622,
      "learning_rate": 9.759124087591242e-06,
      "loss": 0.6316,
      "step": 110260
    },
    {
      "epoch": 804.8905109489051,
      "grad_norm": 8.589914321899414,
      "learning_rate": 9.755474452554746e-06,
      "loss": 1.2521,
      "step": 110270
    },
    {
      "epoch": 804.9635036496351,
      "grad_norm": 7.719965934753418,
      "learning_rate": 9.751824817518248e-06,
      "loss": 1.2056,
      "step": 110280
    },
    {
      "epoch": 805.0364963503649,
      "grad_norm": 9.982455253601074,
      "learning_rate": 9.748175182481752e-06,
      "loss": 0.8018,
      "step": 110290
    },
    {
      "epoch": 805.1094890510949,
      "grad_norm": 0.14342862367630005,
      "learning_rate": 9.744525547445256e-06,
      "loss": 0.6276,
      "step": 110300
    },
    {
      "epoch": 805.1824817518249,
      "grad_norm": 15.33120346069336,
      "learning_rate": 9.74087591240876e-06,
      "loss": 1.2579,
      "step": 110310
    },
    {
      "epoch": 805.2554744525547,
      "grad_norm": 0.07180628180503845,
      "learning_rate": 9.737226277372263e-06,
      "loss": 0.9563,
      "step": 110320
    },
    {
      "epoch": 805.3284671532847,
      "grad_norm": 0.08049153536558151,
      "learning_rate": 9.733576642335766e-06,
      "loss": 0.6288,
      "step": 110330
    },
    {
      "epoch": 805.4014598540145,
      "grad_norm": 7.247372150421143,
      "learning_rate": 9.729927007299271e-06,
      "loss": 0.8672,
      "step": 110340
    },
    {
      "epoch": 805.4744525547445,
      "grad_norm": 0.026524461805820465,
      "learning_rate": 9.726277372262773e-06,
      "loss": 1.2377,
      "step": 110350
    },
    {
      "epoch": 805.5474452554745,
      "grad_norm": 0.04374149441719055,
      "learning_rate": 9.722627737226277e-06,
      "loss": 1.0901,
      "step": 110360
    },
    {
      "epoch": 805.6204379562043,
      "grad_norm": 9.070291519165039,
      "learning_rate": 9.718978102189781e-06,
      "loss": 0.6505,
      "step": 110370
    },
    {
      "epoch": 805.6934306569343,
      "grad_norm": 10.824625015258789,
      "learning_rate": 9.715328467153285e-06,
      "loss": 0.7141,
      "step": 110380
    },
    {
      "epoch": 805.7664233576643,
      "grad_norm": 7.289287567138672,
      "learning_rate": 9.711678832116789e-06,
      "loss": 0.9944,
      "step": 110390
    },
    {
      "epoch": 805.8394160583941,
      "grad_norm": 14.892936706542969,
      "learning_rate": 9.708029197080293e-06,
      "loss": 0.8902,
      "step": 110400
    },
    {
      "epoch": 805.9124087591241,
      "grad_norm": 9.331880569458008,
      "learning_rate": 9.704379562043795e-06,
      "loss": 0.5677,
      "step": 110410
    },
    {
      "epoch": 805.985401459854,
      "grad_norm": 8.787820816040039,
      "learning_rate": 9.7007299270073e-06,
      "loss": 0.8152,
      "step": 110420
    },
    {
      "epoch": 806.0583941605839,
      "grad_norm": 10.333430290222168,
      "learning_rate": 9.697080291970803e-06,
      "loss": 1.1266,
      "step": 110430
    },
    {
      "epoch": 806.1313868613139,
      "grad_norm": 8.88438606262207,
      "learning_rate": 9.693430656934308e-06,
      "loss": 0.8895,
      "step": 110440
    },
    {
      "epoch": 806.2043795620438,
      "grad_norm": 13.93844223022461,
      "learning_rate": 9.68978102189781e-06,
      "loss": 1.0187,
      "step": 110450
    },
    {
      "epoch": 806.2773722627737,
      "grad_norm": 9.645646095275879,
      "learning_rate": 9.686131386861314e-06,
      "loss": 0.7469,
      "step": 110460
    },
    {
      "epoch": 806.3503649635037,
      "grad_norm": 5.920847415924072,
      "learning_rate": 9.682481751824818e-06,
      "loss": 0.679,
      "step": 110470
    },
    {
      "epoch": 806.4233576642335,
      "grad_norm": 0.08345549553632736,
      "learning_rate": 9.678832116788322e-06,
      "loss": 0.6448,
      "step": 110480
    },
    {
      "epoch": 806.4963503649635,
      "grad_norm": 8.660152435302734,
      "learning_rate": 9.675182481751826e-06,
      "loss": 0.8082,
      "step": 110490
    },
    {
      "epoch": 806.5693430656934,
      "grad_norm": 0.09447532147169113,
      "learning_rate": 9.67153284671533e-06,
      "loss": 0.8484,
      "step": 110500
    },
    {
      "epoch": 806.6423357664233,
      "grad_norm": 10.617267608642578,
      "learning_rate": 9.667883211678832e-06,
      "loss": 0.9615,
      "step": 110510
    },
    {
      "epoch": 806.7153284671533,
      "grad_norm": 4.97482967376709,
      "learning_rate": 9.664233576642336e-06,
      "loss": 0.6333,
      "step": 110520
    },
    {
      "epoch": 806.7883211678832,
      "grad_norm": 6.929558277130127,
      "learning_rate": 9.66058394160584e-06,
      "loss": 1.4415,
      "step": 110530
    },
    {
      "epoch": 806.8613138686131,
      "grad_norm": 7.8352556228637695,
      "learning_rate": 9.656934306569344e-06,
      "loss": 0.8758,
      "step": 110540
    },
    {
      "epoch": 806.9343065693431,
      "grad_norm": 3.3152031898498535,
      "learning_rate": 9.653284671532848e-06,
      "loss": 0.6381,
      "step": 110550
    },
    {
      "epoch": 807.007299270073,
      "grad_norm": 7.548917293548584,
      "learning_rate": 9.64963503649635e-06,
      "loss": 0.9779,
      "step": 110560
    },
    {
      "epoch": 807.0802919708029,
      "grad_norm": 7.0933837890625,
      "learning_rate": 9.645985401459855e-06,
      "loss": 0.8195,
      "step": 110570
    },
    {
      "epoch": 807.1532846715329,
      "grad_norm": 9.903676986694336,
      "learning_rate": 9.642335766423358e-06,
      "loss": 1.2526,
      "step": 110580
    },
    {
      "epoch": 807.2262773722628,
      "grad_norm": 7.551436901092529,
      "learning_rate": 9.638686131386861e-06,
      "loss": 1.0954,
      "step": 110590
    },
    {
      "epoch": 807.2992700729927,
      "grad_norm": 13.841325759887695,
      "learning_rate": 9.635036496350365e-06,
      "loss": 0.9353,
      "step": 110600
    },
    {
      "epoch": 807.3722627737226,
      "grad_norm": 5.527343273162842,
      "learning_rate": 9.63138686131387e-06,
      "loss": 0.6774,
      "step": 110610
    },
    {
      "epoch": 807.4452554744526,
      "grad_norm": 7.335901260375977,
      "learning_rate": 9.627737226277373e-06,
      "loss": 0.6789,
      "step": 110620
    },
    {
      "epoch": 807.5182481751825,
      "grad_norm": 7.15856409072876,
      "learning_rate": 9.624087591240877e-06,
      "loss": 0.876,
      "step": 110630
    },
    {
      "epoch": 807.5912408759124,
      "grad_norm": 6.609988212585449,
      "learning_rate": 9.62043795620438e-06,
      "loss": 0.4147,
      "step": 110640
    },
    {
      "epoch": 807.6642335766423,
      "grad_norm": 6.952852249145508,
      "learning_rate": 9.616788321167885e-06,
      "loss": 0.7295,
      "step": 110650
    },
    {
      "epoch": 807.7372262773723,
      "grad_norm": 12.9064359664917,
      "learning_rate": 9.613138686131387e-06,
      "loss": 0.9939,
      "step": 110660
    },
    {
      "epoch": 807.8102189781022,
      "grad_norm": 9.512330055236816,
      "learning_rate": 9.60948905109489e-06,
      "loss": 0.9048,
      "step": 110670
    },
    {
      "epoch": 807.8832116788321,
      "grad_norm": 7.752460956573486,
      "learning_rate": 9.605839416058395e-06,
      "loss": 0.7466,
      "step": 110680
    },
    {
      "epoch": 807.956204379562,
      "grad_norm": 9.026652336120605,
      "learning_rate": 9.602189781021899e-06,
      "loss": 0.9435,
      "step": 110690
    },
    {
      "epoch": 808.029197080292,
      "grad_norm": 6.900860786437988,
      "learning_rate": 9.598540145985402e-06,
      "loss": 1.1381,
      "step": 110700
    },
    {
      "epoch": 808.1021897810219,
      "grad_norm": 10.4393310546875,
      "learning_rate": 9.594890510948906e-06,
      "loss": 0.5479,
      "step": 110710
    },
    {
      "epoch": 808.1751824817518,
      "grad_norm": 17.549238204956055,
      "learning_rate": 9.591240875912409e-06,
      "loss": 0.7307,
      "step": 110720
    },
    {
      "epoch": 808.2481751824818,
      "grad_norm": 1.0982980728149414,
      "learning_rate": 9.587591240875912e-06,
      "loss": 1.108,
      "step": 110730
    },
    {
      "epoch": 808.3211678832117,
      "grad_norm": 12.733668327331543,
      "learning_rate": 9.583941605839416e-06,
      "loss": 1.1096,
      "step": 110740
    },
    {
      "epoch": 808.3941605839416,
      "grad_norm": 12.609367370605469,
      "learning_rate": 9.58029197080292e-06,
      "loss": 1.0711,
      "step": 110750
    },
    {
      "epoch": 808.4671532846716,
      "grad_norm": 14.428156852722168,
      "learning_rate": 9.576642335766424e-06,
      "loss": 1.2838,
      "step": 110760
    },
    {
      "epoch": 808.5401459854014,
      "grad_norm": 9.984981536865234,
      "learning_rate": 9.572992700729926e-06,
      "loss": 1.0302,
      "step": 110770
    },
    {
      "epoch": 808.6131386861314,
      "grad_norm": 11.297593116760254,
      "learning_rate": 9.569343065693432e-06,
      "loss": 0.9175,
      "step": 110780
    },
    {
      "epoch": 808.6861313868613,
      "grad_norm": 15.229799270629883,
      "learning_rate": 9.565693430656934e-06,
      "loss": 0.8802,
      "step": 110790
    },
    {
      "epoch": 808.7591240875912,
      "grad_norm": 12.358187675476074,
      "learning_rate": 9.562043795620438e-06,
      "loss": 0.7445,
      "step": 110800
    },
    {
      "epoch": 808.8321167883212,
      "grad_norm": 20.065441131591797,
      "learning_rate": 9.558394160583942e-06,
      "loss": 0.7545,
      "step": 110810
    },
    {
      "epoch": 808.9051094890511,
      "grad_norm": 0.0471724234521389,
      "learning_rate": 9.554744525547446e-06,
      "loss": 0.7282,
      "step": 110820
    },
    {
      "epoch": 808.978102189781,
      "grad_norm": 18.399089813232422,
      "learning_rate": 9.55109489051095e-06,
      "loss": 0.9544,
      "step": 110830
    },
    {
      "epoch": 809.051094890511,
      "grad_norm": 8.129020690917969,
      "learning_rate": 9.547445255474453e-06,
      "loss": 1.1229,
      "step": 110840
    },
    {
      "epoch": 809.1240875912408,
      "grad_norm": 7.484013080596924,
      "learning_rate": 9.543795620437956e-06,
      "loss": 0.8208,
      "step": 110850
    },
    {
      "epoch": 809.1970802919708,
      "grad_norm": 8.793121337890625,
      "learning_rate": 9.540145985401461e-06,
      "loss": 0.5713,
      "step": 110860
    },
    {
      "epoch": 809.2700729927008,
      "grad_norm": 1.3815709352493286,
      "learning_rate": 9.536496350364963e-06,
      "loss": 0.6263,
      "step": 110870
    },
    {
      "epoch": 809.3430656934306,
      "grad_norm": 0.020141350105404854,
      "learning_rate": 9.532846715328469e-06,
      "loss": 0.5854,
      "step": 110880
    },
    {
      "epoch": 809.4160583941606,
      "grad_norm": 11.92436408996582,
      "learning_rate": 9.529197080291971e-06,
      "loss": 0.6547,
      "step": 110890
    },
    {
      "epoch": 809.4890510948906,
      "grad_norm": 12.12976360321045,
      "learning_rate": 9.525547445255475e-06,
      "loss": 1.0107,
      "step": 110900
    },
    {
      "epoch": 809.5620437956204,
      "grad_norm": 11.439980506896973,
      "learning_rate": 9.521897810218979e-06,
      "loss": 1.3073,
      "step": 110910
    },
    {
      "epoch": 809.6350364963504,
      "grad_norm": 7.566213130950928,
      "learning_rate": 9.518248175182483e-06,
      "loss": 0.788,
      "step": 110920
    },
    {
      "epoch": 809.7080291970802,
      "grad_norm": 16.67130470275879,
      "learning_rate": 9.514598540145985e-06,
      "loss": 1.1475,
      "step": 110930
    },
    {
      "epoch": 809.7810218978102,
      "grad_norm": 11.800274848937988,
      "learning_rate": 9.510948905109489e-06,
      "loss": 1.0951,
      "step": 110940
    },
    {
      "epoch": 809.8540145985402,
      "grad_norm": 19.15737533569336,
      "learning_rate": 9.507299270072993e-06,
      "loss": 1.1292,
      "step": 110950
    },
    {
      "epoch": 809.92700729927,
      "grad_norm": 5.4631876945495605,
      "learning_rate": 9.503649635036497e-06,
      "loss": 0.871,
      "step": 110960
    },
    {
      "epoch": 810.0,
      "grad_norm": 15.557831764221191,
      "learning_rate": 9.5e-06,
      "loss": 0.8778,
      "step": 110970
    },
    {
      "epoch": 810.07299270073,
      "grad_norm": 9.36575698852539,
      "learning_rate": 9.496350364963503e-06,
      "loss": 0.8534,
      "step": 110980
    },
    {
      "epoch": 810.1459854014598,
      "grad_norm": 0.12578026950359344,
      "learning_rate": 9.492700729927008e-06,
      "loss": 0.99,
      "step": 110990
    },
    {
      "epoch": 810.2189781021898,
      "grad_norm": 0.11991038918495178,
      "learning_rate": 9.48905109489051e-06,
      "loss": 0.8059,
      "step": 111000
    },
    {
      "epoch": 810.2919708029198,
      "grad_norm": 15.98337173461914,
      "learning_rate": 9.485401459854016e-06,
      "loss": 1.1647,
      "step": 111010
    },
    {
      "epoch": 810.3649635036496,
      "grad_norm": 13.645061492919922,
      "learning_rate": 9.481751824817518e-06,
      "loss": 0.6789,
      "step": 111020
    },
    {
      "epoch": 810.4379562043796,
      "grad_norm": 9.750442504882812,
      "learning_rate": 9.478102189781022e-06,
      "loss": 0.6354,
      "step": 111030
    },
    {
      "epoch": 810.5109489051094,
      "grad_norm": 10.331165313720703,
      "learning_rate": 9.474452554744526e-06,
      "loss": 0.9105,
      "step": 111040
    },
    {
      "epoch": 810.5839416058394,
      "grad_norm": 12.201094627380371,
      "learning_rate": 9.47080291970803e-06,
      "loss": 1.1566,
      "step": 111050
    },
    {
      "epoch": 810.6569343065694,
      "grad_norm": 0.060613758862018585,
      "learning_rate": 9.467153284671534e-06,
      "loss": 0.5147,
      "step": 111060
    },
    {
      "epoch": 810.7299270072992,
      "grad_norm": 15.240727424621582,
      "learning_rate": 9.463503649635038e-06,
      "loss": 1.2492,
      "step": 111070
    },
    {
      "epoch": 810.8029197080292,
      "grad_norm": 8.099311828613281,
      "learning_rate": 9.45985401459854e-06,
      "loss": 0.8884,
      "step": 111080
    },
    {
      "epoch": 810.8759124087592,
      "grad_norm": 6.780642509460449,
      "learning_rate": 9.456204379562045e-06,
      "loss": 0.9429,
      "step": 111090
    },
    {
      "epoch": 810.948905109489,
      "grad_norm": 7.341760635375977,
      "learning_rate": 9.452554744525548e-06,
      "loss": 0.7255,
      "step": 111100
    },
    {
      "epoch": 811.021897810219,
      "grad_norm": 13.952929496765137,
      "learning_rate": 9.448905109489051e-06,
      "loss": 0.8934,
      "step": 111110
    },
    {
      "epoch": 811.0948905109489,
      "grad_norm": 13.982779502868652,
      "learning_rate": 9.445255474452555e-06,
      "loss": 0.9332,
      "step": 111120
    },
    {
      "epoch": 811.1678832116788,
      "grad_norm": 0.4102144241333008,
      "learning_rate": 9.44160583941606e-06,
      "loss": 0.677,
      "step": 111130
    },
    {
      "epoch": 811.2408759124088,
      "grad_norm": 0.06057068705558777,
      "learning_rate": 9.437956204379563e-06,
      "loss": 0.7934,
      "step": 111140
    },
    {
      "epoch": 811.3138686131387,
      "grad_norm": 9.61559009552002,
      "learning_rate": 9.434306569343067e-06,
      "loss": 0.5485,
      "step": 111150
    },
    {
      "epoch": 811.3868613138686,
      "grad_norm": 13.688474655151367,
      "learning_rate": 9.43065693430657e-06,
      "loss": 1.1504,
      "step": 111160
    },
    {
      "epoch": 811.4598540145986,
      "grad_norm": 11.299995422363281,
      "learning_rate": 9.427007299270073e-06,
      "loss": 0.814,
      "step": 111170
    },
    {
      "epoch": 811.5328467153284,
      "grad_norm": 9.401607513427734,
      "learning_rate": 9.423357664233577e-06,
      "loss": 0.7492,
      "step": 111180
    },
    {
      "epoch": 811.6058394160584,
      "grad_norm": 7.122422218322754,
      "learning_rate": 9.419708029197081e-06,
      "loss": 1.2294,
      "step": 111190
    },
    {
      "epoch": 811.6788321167883,
      "grad_norm": 14.009838104248047,
      "learning_rate": 9.416058394160585e-06,
      "loss": 0.9974,
      "step": 111200
    },
    {
      "epoch": 811.7518248175182,
      "grad_norm": 0.03260551020503044,
      "learning_rate": 9.412408759124087e-06,
      "loss": 0.9759,
      "step": 111210
    },
    {
      "epoch": 811.8248175182482,
      "grad_norm": 9.839730262756348,
      "learning_rate": 9.408759124087593e-06,
      "loss": 0.9361,
      "step": 111220
    },
    {
      "epoch": 811.8978102189781,
      "grad_norm": 7.524353981018066,
      "learning_rate": 9.405109489051095e-06,
      "loss": 0.8178,
      "step": 111230
    },
    {
      "epoch": 811.970802919708,
      "grad_norm": 7.224709987640381,
      "learning_rate": 9.401459854014599e-06,
      "loss": 0.6472,
      "step": 111240
    },
    {
      "epoch": 812.043795620438,
      "grad_norm": 7.983072280883789,
      "learning_rate": 9.397810218978102e-06,
      "loss": 0.8445,
      "step": 111250
    },
    {
      "epoch": 812.1167883211679,
      "grad_norm": 12.465913772583008,
      "learning_rate": 9.394160583941606e-06,
      "loss": 1.0247,
      "step": 111260
    },
    {
      "epoch": 812.1897810218978,
      "grad_norm": 0.023546835407614708,
      "learning_rate": 9.39051094890511e-06,
      "loss": 0.8206,
      "step": 111270
    },
    {
      "epoch": 812.2627737226277,
      "grad_norm": 13.639254570007324,
      "learning_rate": 9.386861313868614e-06,
      "loss": 1.1808,
      "step": 111280
    },
    {
      "epoch": 812.3357664233577,
      "grad_norm": 22.010787963867188,
      "learning_rate": 9.383211678832116e-06,
      "loss": 0.926,
      "step": 111290
    },
    {
      "epoch": 812.4087591240876,
      "grad_norm": 7.172257423400879,
      "learning_rate": 9.379562043795622e-06,
      "loss": 1.0248,
      "step": 111300
    },
    {
      "epoch": 812.4817518248175,
      "grad_norm": 7.253335952758789,
      "learning_rate": 9.375912408759124e-06,
      "loss": 0.9951,
      "step": 111310
    },
    {
      "epoch": 812.5547445255474,
      "grad_norm": 8.628355979919434,
      "learning_rate": 9.372262773722628e-06,
      "loss": 0.6208,
      "step": 111320
    },
    {
      "epoch": 812.6277372262774,
      "grad_norm": 13.739640235900879,
      "learning_rate": 9.368613138686132e-06,
      "loss": 0.9067,
      "step": 111330
    },
    {
      "epoch": 812.7007299270073,
      "grad_norm": 0.033393409103155136,
      "learning_rate": 9.364963503649636e-06,
      "loss": 0.8724,
      "step": 111340
    },
    {
      "epoch": 812.7737226277372,
      "grad_norm": 14.845222473144531,
      "learning_rate": 9.36131386861314e-06,
      "loss": 1.0025,
      "step": 111350
    },
    {
      "epoch": 812.8467153284671,
      "grad_norm": 15.589275360107422,
      "learning_rate": 9.357664233576643e-06,
      "loss": 0.6891,
      "step": 111360
    },
    {
      "epoch": 812.9197080291971,
      "grad_norm": 0.0919337347149849,
      "learning_rate": 9.354014598540146e-06,
      "loss": 0.642,
      "step": 111370
    },
    {
      "epoch": 812.992700729927,
      "grad_norm": 7.297455787658691,
      "learning_rate": 9.35036496350365e-06,
      "loss": 0.9517,
      "step": 111380
    },
    {
      "epoch": 813.0656934306569,
      "grad_norm": 0.051690276712179184,
      "learning_rate": 9.346715328467153e-06,
      "loss": 1.1125,
      "step": 111390
    },
    {
      "epoch": 813.1386861313869,
      "grad_norm": 0.042518485337495804,
      "learning_rate": 9.343065693430657e-06,
      "loss": 1.0971,
      "step": 111400
    },
    {
      "epoch": 813.2116788321168,
      "grad_norm": 11.081232070922852,
      "learning_rate": 9.339416058394161e-06,
      "loss": 0.5185,
      "step": 111410
    },
    {
      "epoch": 813.2846715328467,
      "grad_norm": 12.2312650680542,
      "learning_rate": 9.335766423357663e-06,
      "loss": 0.7483,
      "step": 111420
    },
    {
      "epoch": 813.3576642335767,
      "grad_norm": 8.770251274108887,
      "learning_rate": 9.332116788321169e-06,
      "loss": 0.883,
      "step": 111430
    },
    {
      "epoch": 813.4306569343066,
      "grad_norm": 15.749316215515137,
      "learning_rate": 9.328467153284671e-06,
      "loss": 0.9032,
      "step": 111440
    },
    {
      "epoch": 813.5036496350365,
      "grad_norm": 10.322067260742188,
      "learning_rate": 9.324817518248177e-06,
      "loss": 0.6293,
      "step": 111450
    },
    {
      "epoch": 813.5766423357665,
      "grad_norm": 17.10533905029297,
      "learning_rate": 9.321167883211679e-06,
      "loss": 0.8615,
      "step": 111460
    },
    {
      "epoch": 813.6496350364963,
      "grad_norm": 9.908697128295898,
      "learning_rate": 9.317518248175183e-06,
      "loss": 1.1629,
      "step": 111470
    },
    {
      "epoch": 813.7226277372263,
      "grad_norm": 0.019882194697856903,
      "learning_rate": 9.313868613138687e-06,
      "loss": 0.482,
      "step": 111480
    },
    {
      "epoch": 813.7956204379562,
      "grad_norm": 10.646568298339844,
      "learning_rate": 9.31021897810219e-06,
      "loss": 1.3262,
      "step": 111490
    },
    {
      "epoch": 813.8686131386861,
      "grad_norm": 8.58608627319336,
      "learning_rate": 9.306569343065694e-06,
      "loss": 0.526,
      "step": 111500
    },
    {
      "epoch": 813.9416058394161,
      "grad_norm": 0.083440862596035,
      "learning_rate": 9.302919708029198e-06,
      "loss": 1.1997,
      "step": 111510
    },
    {
      "epoch": 814.014598540146,
      "grad_norm": 13.27440357208252,
      "learning_rate": 9.2992700729927e-06,
      "loss": 0.6454,
      "step": 111520
    },
    {
      "epoch": 814.0875912408759,
      "grad_norm": 18.698453903198242,
      "learning_rate": 9.295620437956206e-06,
      "loss": 0.664,
      "step": 111530
    },
    {
      "epoch": 814.1605839416059,
      "grad_norm": 7.22430944442749,
      "learning_rate": 9.291970802919708e-06,
      "loss": 0.5779,
      "step": 111540
    },
    {
      "epoch": 814.2335766423357,
      "grad_norm": 8.646849632263184,
      "learning_rate": 9.288321167883212e-06,
      "loss": 0.7869,
      "step": 111550
    },
    {
      "epoch": 814.3065693430657,
      "grad_norm": 12.833327293395996,
      "learning_rate": 9.284671532846716e-06,
      "loss": 0.9674,
      "step": 111560
    },
    {
      "epoch": 814.3795620437957,
      "grad_norm": 5.590660095214844,
      "learning_rate": 9.28102189781022e-06,
      "loss": 0.9688,
      "step": 111570
    },
    {
      "epoch": 814.4525547445255,
      "grad_norm": 7.2234039306640625,
      "learning_rate": 9.277372262773724e-06,
      "loss": 0.4866,
      "step": 111580
    },
    {
      "epoch": 814.5255474452555,
      "grad_norm": 0.1610259860754013,
      "learning_rate": 9.273722627737226e-06,
      "loss": 0.7191,
      "step": 111590
    },
    {
      "epoch": 814.5985401459855,
      "grad_norm": 0.06419454514980316,
      "learning_rate": 9.27007299270073e-06,
      "loss": 1.2673,
      "step": 111600
    },
    {
      "epoch": 814.6715328467153,
      "grad_norm": 6.7549333572387695,
      "learning_rate": 9.266423357664234e-06,
      "loss": 0.9916,
      "step": 111610
    },
    {
      "epoch": 814.7445255474453,
      "grad_norm": 12.401602745056152,
      "learning_rate": 9.262773722627738e-06,
      "loss": 0.9757,
      "step": 111620
    },
    {
      "epoch": 814.8175182481751,
      "grad_norm": 12.383889198303223,
      "learning_rate": 9.259124087591242e-06,
      "loss": 1.1981,
      "step": 111630
    },
    {
      "epoch": 814.8905109489051,
      "grad_norm": 10.5801420211792,
      "learning_rate": 9.255474452554745e-06,
      "loss": 1.0793,
      "step": 111640
    },
    {
      "epoch": 814.9635036496351,
      "grad_norm": 12.34274959564209,
      "learning_rate": 9.251824817518248e-06,
      "loss": 0.9809,
      "step": 111650
    },
    {
      "epoch": 815.0364963503649,
      "grad_norm": 0.010608350858092308,
      "learning_rate": 9.248175182481753e-06,
      "loss": 0.7129,
      "step": 111660
    },
    {
      "epoch": 815.1094890510949,
      "grad_norm": 13.56680679321289,
      "learning_rate": 9.244525547445255e-06,
      "loss": 0.5279,
      "step": 111670
    },
    {
      "epoch": 815.1824817518249,
      "grad_norm": 10.899420738220215,
      "learning_rate": 9.24087591240876e-06,
      "loss": 0.9092,
      "step": 111680
    },
    {
      "epoch": 815.2554744525547,
      "grad_norm": 7.676671028137207,
      "learning_rate": 9.237226277372263e-06,
      "loss": 1.3089,
      "step": 111690
    },
    {
      "epoch": 815.3284671532847,
      "grad_norm": 9.409817695617676,
      "learning_rate": 9.233576642335767e-06,
      "loss": 1.0887,
      "step": 111700
    },
    {
      "epoch": 815.4014598540145,
      "grad_norm": 0.021813154220581055,
      "learning_rate": 9.229927007299271e-06,
      "loss": 0.8146,
      "step": 111710
    },
    {
      "epoch": 815.4744525547445,
      "grad_norm": 12.237635612487793,
      "learning_rate": 9.226277372262775e-06,
      "loss": 0.7711,
      "step": 111720
    },
    {
      "epoch": 815.5474452554745,
      "grad_norm": 14.41362476348877,
      "learning_rate": 9.222627737226277e-06,
      "loss": 0.6868,
      "step": 111730
    },
    {
      "epoch": 815.6204379562043,
      "grad_norm": 10.663651466369629,
      "learning_rate": 9.218978102189783e-06,
      "loss": 0.9528,
      "step": 111740
    },
    {
      "epoch": 815.6934306569343,
      "grad_norm": 7.458006858825684,
      "learning_rate": 9.215328467153285e-06,
      "loss": 0.9314,
      "step": 111750
    },
    {
      "epoch": 815.7664233576643,
      "grad_norm": 9.422226905822754,
      "learning_rate": 9.211678832116789e-06,
      "loss": 0.6606,
      "step": 111760
    },
    {
      "epoch": 815.8394160583941,
      "grad_norm": 6.946417331695557,
      "learning_rate": 9.208029197080292e-06,
      "loss": 0.9332,
      "step": 111770
    },
    {
      "epoch": 815.9124087591241,
      "grad_norm": 8.407966613769531,
      "learning_rate": 9.204379562043796e-06,
      "loss": 1.2616,
      "step": 111780
    },
    {
      "epoch": 815.985401459854,
      "grad_norm": 0.021294327452778816,
      "learning_rate": 9.2007299270073e-06,
      "loss": 0.6721,
      "step": 111790
    },
    {
      "epoch": 816.0583941605839,
      "grad_norm": 12.87707233428955,
      "learning_rate": 9.197080291970802e-06,
      "loss": 0.9469,
      "step": 111800
    },
    {
      "epoch": 816.1313868613139,
      "grad_norm": 0.9218387603759766,
      "learning_rate": 9.193430656934306e-06,
      "loss": 0.3909,
      "step": 111810
    },
    {
      "epoch": 816.2043795620438,
      "grad_norm": 0.05155552923679352,
      "learning_rate": 9.18978102189781e-06,
      "loss": 1.1958,
      "step": 111820
    },
    {
      "epoch": 816.2773722627737,
      "grad_norm": 6.548418045043945,
      "learning_rate": 9.186131386861314e-06,
      "loss": 1.1583,
      "step": 111830
    },
    {
      "epoch": 816.3503649635037,
      "grad_norm": 13.93454647064209,
      "learning_rate": 9.182481751824818e-06,
      "loss": 0.7411,
      "step": 111840
    },
    {
      "epoch": 816.4233576642335,
      "grad_norm": 5.211879253387451,
      "learning_rate": 9.178832116788322e-06,
      "loss": 0.6083,
      "step": 111850
    },
    {
      "epoch": 816.4963503649635,
      "grad_norm": 10.428932189941406,
      "learning_rate": 9.175182481751824e-06,
      "loss": 0.8767,
      "step": 111860
    },
    {
      "epoch": 816.5693430656934,
      "grad_norm": 8.074307441711426,
      "learning_rate": 9.17153284671533e-06,
      "loss": 0.9114,
      "step": 111870
    },
    {
      "epoch": 816.6423357664233,
      "grad_norm": 17.390714645385742,
      "learning_rate": 9.167883211678832e-06,
      "loss": 0.8421,
      "step": 111880
    },
    {
      "epoch": 816.7153284671533,
      "grad_norm": 0.01463309582322836,
      "learning_rate": 9.164233576642336e-06,
      "loss": 1.2542,
      "step": 111890
    },
    {
      "epoch": 816.7883211678832,
      "grad_norm": 7.20889949798584,
      "learning_rate": 9.16058394160584e-06,
      "loss": 1.0145,
      "step": 111900
    },
    {
      "epoch": 816.8613138686131,
      "grad_norm": 9.426413536071777,
      "learning_rate": 9.156934306569343e-06,
      "loss": 0.8548,
      "step": 111910
    },
    {
      "epoch": 816.9343065693431,
      "grad_norm": 9.321130752563477,
      "learning_rate": 9.153284671532847e-06,
      "loss": 1.0869,
      "step": 111920
    },
    {
      "epoch": 817.007299270073,
      "grad_norm": 9.630980491638184,
      "learning_rate": 9.149635036496351e-06,
      "loss": 0.5425,
      "step": 111930
    },
    {
      "epoch": 817.0802919708029,
      "grad_norm": 0.06744647771120071,
      "learning_rate": 9.145985401459853e-06,
      "loss": 1.0159,
      "step": 111940
    },
    {
      "epoch": 817.1532846715329,
      "grad_norm": 17.083763122558594,
      "learning_rate": 9.142335766423359e-06,
      "loss": 0.7946,
      "step": 111950
    },
    {
      "epoch": 817.2262773722628,
      "grad_norm": 6.33311128616333,
      "learning_rate": 9.138686131386861e-06,
      "loss": 0.6929,
      "step": 111960
    },
    {
      "epoch": 817.2992700729927,
      "grad_norm": 7.7561492919921875,
      "learning_rate": 9.135036496350367e-06,
      "loss": 0.8413,
      "step": 111970
    },
    {
      "epoch": 817.3722627737226,
      "grad_norm": 4.805810928344727,
      "learning_rate": 9.131386861313869e-06,
      "loss": 0.5757,
      "step": 111980
    },
    {
      "epoch": 817.4452554744526,
      "grad_norm": 10.902848243713379,
      "learning_rate": 9.127737226277373e-06,
      "loss": 1.1952,
      "step": 111990
    },
    {
      "epoch": 817.5182481751825,
      "grad_norm": 7.717445373535156,
      "learning_rate": 9.124087591240877e-06,
      "loss": 0.7611,
      "step": 112000
    },
    {
      "epoch": 817.5912408759124,
      "grad_norm": 16.550352096557617,
      "learning_rate": 9.12043795620438e-06,
      "loss": 1.0675,
      "step": 112010
    },
    {
      "epoch": 817.6642335766423,
      "grad_norm": 8.988235473632812,
      "learning_rate": 9.116788321167884e-06,
      "loss": 1.0355,
      "step": 112020
    },
    {
      "epoch": 817.7372262773723,
      "grad_norm": 21.67530059814453,
      "learning_rate": 9.113138686131387e-06,
      "loss": 1.1167,
      "step": 112030
    },
    {
      "epoch": 817.8102189781022,
      "grad_norm": 12.124323844909668,
      "learning_rate": 9.10948905109489e-06,
      "loss": 0.7809,
      "step": 112040
    },
    {
      "epoch": 817.8832116788321,
      "grad_norm": 0.01808285526931286,
      "learning_rate": 9.105839416058394e-06,
      "loss": 0.8264,
      "step": 112050
    },
    {
      "epoch": 817.956204379562,
      "grad_norm": 0.014947593212127686,
      "learning_rate": 9.102189781021898e-06,
      "loss": 0.7161,
      "step": 112060
    },
    {
      "epoch": 818.029197080292,
      "grad_norm": 7.874227523803711,
      "learning_rate": 9.098540145985402e-06,
      "loss": 1.0582,
      "step": 112070
    },
    {
      "epoch": 818.1021897810219,
      "grad_norm": 7.668557167053223,
      "learning_rate": 9.094890510948906e-06,
      "loss": 0.9126,
      "step": 112080
    },
    {
      "epoch": 818.1751824817518,
      "grad_norm": 7.432408809661865,
      "learning_rate": 9.091240875912408e-06,
      "loss": 0.8657,
      "step": 112090
    },
    {
      "epoch": 818.2481751824818,
      "grad_norm": 0.04988967254757881,
      "learning_rate": 9.087591240875914e-06,
      "loss": 0.9961,
      "step": 112100
    },
    {
      "epoch": 818.3211678832117,
      "grad_norm": 8.207071304321289,
      "learning_rate": 9.083941605839416e-06,
      "loss": 1.1785,
      "step": 112110
    },
    {
      "epoch": 818.3941605839416,
      "grad_norm": 12.334150314331055,
      "learning_rate": 9.08029197080292e-06,
      "loss": 1.1043,
      "step": 112120
    },
    {
      "epoch": 818.4671532846716,
      "grad_norm": 18.7391300201416,
      "learning_rate": 9.076642335766424e-06,
      "loss": 0.8926,
      "step": 112130
    },
    {
      "epoch": 818.5401459854014,
      "grad_norm": 12.031201362609863,
      "learning_rate": 9.072992700729928e-06,
      "loss": 0.7194,
      "step": 112140
    },
    {
      "epoch": 818.6131386861314,
      "grad_norm": 7.496435165405273,
      "learning_rate": 9.069343065693432e-06,
      "loss": 1.1353,
      "step": 112150
    },
    {
      "epoch": 818.6861313868613,
      "grad_norm": 7.711463451385498,
      "learning_rate": 9.065693430656935e-06,
      "loss": 0.775,
      "step": 112160
    },
    {
      "epoch": 818.7591240875912,
      "grad_norm": 0.03553026542067528,
      "learning_rate": 9.062043795620438e-06,
      "loss": 0.7097,
      "step": 112170
    },
    {
      "epoch": 818.8321167883212,
      "grad_norm": 0.04651695489883423,
      "learning_rate": 9.058394160583943e-06,
      "loss": 0.6981,
      "step": 112180
    },
    {
      "epoch": 818.9051094890511,
      "grad_norm": 2.1598751544952393,
      "learning_rate": 9.054744525547445e-06,
      "loss": 1.0255,
      "step": 112190
    },
    {
      "epoch": 818.978102189781,
      "grad_norm": 3.2129204273223877,
      "learning_rate": 9.05109489051095e-06,
      "loss": 0.4938,
      "step": 112200
    },
    {
      "epoch": 819.051094890511,
      "grad_norm": 11.160270690917969,
      "learning_rate": 9.047445255474453e-06,
      "loss": 1.0273,
      "step": 112210
    },
    {
      "epoch": 819.1240875912408,
      "grad_norm": 8.937213897705078,
      "learning_rate": 9.043795620437957e-06,
      "loss": 1.0142,
      "step": 112220
    },
    {
      "epoch": 819.1970802919708,
      "grad_norm": 0.07649245858192444,
      "learning_rate": 9.040145985401461e-06,
      "loss": 1.1066,
      "step": 112230
    },
    {
      "epoch": 819.2700729927008,
      "grad_norm": 8.717999458312988,
      "learning_rate": 9.036496350364963e-06,
      "loss": 0.6838,
      "step": 112240
    },
    {
      "epoch": 819.3430656934306,
      "grad_norm": 10.922701835632324,
      "learning_rate": 9.032846715328467e-06,
      "loss": 0.7164,
      "step": 112250
    },
    {
      "epoch": 819.4160583941606,
      "grad_norm": 9.140183448791504,
      "learning_rate": 9.029197080291971e-06,
      "loss": 0.8596,
      "step": 112260
    },
    {
      "epoch": 819.4890510948906,
      "grad_norm": 14.506843566894531,
      "learning_rate": 9.025547445255475e-06,
      "loss": 0.7308,
      "step": 112270
    },
    {
      "epoch": 819.5620437956204,
      "grad_norm": 0.04078279435634613,
      "learning_rate": 9.021897810218979e-06,
      "loss": 0.7725,
      "step": 112280
    },
    {
      "epoch": 819.6350364963504,
      "grad_norm": 13.006011009216309,
      "learning_rate": 9.018248175182483e-06,
      "loss": 1.0749,
      "step": 112290
    },
    {
      "epoch": 819.7080291970802,
      "grad_norm": 9.776308059692383,
      "learning_rate": 9.014598540145985e-06,
      "loss": 0.7591,
      "step": 112300
    },
    {
      "epoch": 819.7810218978102,
      "grad_norm": 16.264123916625977,
      "learning_rate": 9.01094890510949e-06,
      "loss": 1.0929,
      "step": 112310
    },
    {
      "epoch": 819.8540145985402,
      "grad_norm": 10.392373085021973,
      "learning_rate": 9.007299270072992e-06,
      "loss": 1.017,
      "step": 112320
    },
    {
      "epoch": 819.92700729927,
      "grad_norm": 13.156723022460938,
      "learning_rate": 9.003649635036496e-06,
      "loss": 0.6586,
      "step": 112330
    },
    {
      "epoch": 820.0,
      "grad_norm": 0.3770672380924225,
      "learning_rate": 9e-06,
      "loss": 0.7747,
      "step": 112340
    },
    {
      "epoch": 820.07299270073,
      "grad_norm": 12.996160507202148,
      "learning_rate": 8.996350364963504e-06,
      "loss": 0.8317,
      "step": 112350
    },
    {
      "epoch": 820.1459854014598,
      "grad_norm": 13.244194984436035,
      "learning_rate": 8.992700729927008e-06,
      "loss": 1.2112,
      "step": 112360
    },
    {
      "epoch": 820.2189781021898,
      "grad_norm": 7.7206339836120605,
      "learning_rate": 8.989051094890512e-06,
      "loss": 0.4661,
      "step": 112370
    },
    {
      "epoch": 820.2919708029198,
      "grad_norm": 9.04003620147705,
      "learning_rate": 8.985401459854014e-06,
      "loss": 1.3073,
      "step": 112380
    },
    {
      "epoch": 820.3649635036496,
      "grad_norm": 3.5130410194396973,
      "learning_rate": 8.98175182481752e-06,
      "loss": 0.84,
      "step": 112390
    },
    {
      "epoch": 820.4379562043796,
      "grad_norm": 12.440804481506348,
      "learning_rate": 8.978102189781022e-06,
      "loss": 1.0745,
      "step": 112400
    },
    {
      "epoch": 820.5109489051094,
      "grad_norm": 12.310356140136719,
      "learning_rate": 8.974452554744527e-06,
      "loss": 0.8796,
      "step": 112410
    },
    {
      "epoch": 820.5839416058394,
      "grad_norm": 0.060522131621837616,
      "learning_rate": 8.97080291970803e-06,
      "loss": 0.8912,
      "step": 112420
    },
    {
      "epoch": 820.6569343065694,
      "grad_norm": 0.05017297342419624,
      "learning_rate": 8.967153284671533e-06,
      "loss": 1.179,
      "step": 112430
    },
    {
      "epoch": 820.7299270072992,
      "grad_norm": 11.801908493041992,
      "learning_rate": 8.963503649635037e-06,
      "loss": 1.0615,
      "step": 112440
    },
    {
      "epoch": 820.8029197080292,
      "grad_norm": 12.747896194458008,
      "learning_rate": 8.95985401459854e-06,
      "loss": 0.6762,
      "step": 112450
    },
    {
      "epoch": 820.8759124087592,
      "grad_norm": 11.650071144104004,
      "learning_rate": 8.956204379562045e-06,
      "loss": 0.6719,
      "step": 112460
    },
    {
      "epoch": 820.948905109489,
      "grad_norm": 13.736407279968262,
      "learning_rate": 8.952554744525547e-06,
      "loss": 0.6626,
      "step": 112470
    },
    {
      "epoch": 821.021897810219,
      "grad_norm": 9.916864395141602,
      "learning_rate": 8.948905109489051e-06,
      "loss": 0.5253,
      "step": 112480
    },
    {
      "epoch": 821.0948905109489,
      "grad_norm": 5.774120330810547,
      "learning_rate": 8.945255474452555e-06,
      "loss": 0.7808,
      "step": 112490
    },
    {
      "epoch": 821.1678832116788,
      "grad_norm": 6.073439598083496,
      "learning_rate": 8.941605839416059e-06,
      "loss": 1.3832,
      "step": 112500
    },
    {
      "epoch": 821.2408759124088,
      "grad_norm": 8.502946853637695,
      "learning_rate": 8.937956204379561e-06,
      "loss": 0.9377,
      "step": 112510
    },
    {
      "epoch": 821.3138686131387,
      "grad_norm": 5.452771186828613,
      "learning_rate": 8.934306569343067e-06,
      "loss": 0.9893,
      "step": 112520
    },
    {
      "epoch": 821.3868613138686,
      "grad_norm": 9.163289070129395,
      "learning_rate": 8.930656934306569e-06,
      "loss": 0.6312,
      "step": 112530
    },
    {
      "epoch": 821.4598540145986,
      "grad_norm": 5.586883068084717,
      "learning_rate": 8.927007299270074e-06,
      "loss": 0.3555,
      "step": 112540
    },
    {
      "epoch": 821.5328467153284,
      "grad_norm": 13.161327362060547,
      "learning_rate": 8.923357664233577e-06,
      "loss": 0.6938,
      "step": 112550
    },
    {
      "epoch": 821.6058394160584,
      "grad_norm": 15.476922035217285,
      "learning_rate": 8.91970802919708e-06,
      "loss": 0.9736,
      "step": 112560
    },
    {
      "epoch": 821.6788321167883,
      "grad_norm": 0.10629769414663315,
      "learning_rate": 8.916058394160584e-06,
      "loss": 0.7687,
      "step": 112570
    },
    {
      "epoch": 821.7518248175182,
      "grad_norm": 7.886153697967529,
      "learning_rate": 8.912408759124088e-06,
      "loss": 1.0246,
      "step": 112580
    },
    {
      "epoch": 821.8248175182482,
      "grad_norm": 17.650907516479492,
      "learning_rate": 8.908759124087592e-06,
      "loss": 1.1868,
      "step": 112590
    },
    {
      "epoch": 821.8978102189781,
      "grad_norm": 14.105562210083008,
      "learning_rate": 8.905109489051096e-06,
      "loss": 1.0096,
      "step": 112600
    },
    {
      "epoch": 821.970802919708,
      "grad_norm": 0.057541318237781525,
      "learning_rate": 8.901459854014598e-06,
      "loss": 0.521,
      "step": 112610
    },
    {
      "epoch": 822.043795620438,
      "grad_norm": 14.314759254455566,
      "learning_rate": 8.897810218978104e-06,
      "loss": 1.0781,
      "step": 112620
    },
    {
      "epoch": 822.1167883211679,
      "grad_norm": 0.06462869793176651,
      "learning_rate": 8.894160583941606e-06,
      "loss": 0.574,
      "step": 112630
    },
    {
      "epoch": 822.1897810218978,
      "grad_norm": 13.741621017456055,
      "learning_rate": 8.89051094890511e-06,
      "loss": 0.8305,
      "step": 112640
    },
    {
      "epoch": 822.2627737226277,
      "grad_norm": 15.818649291992188,
      "learning_rate": 8.886861313868614e-06,
      "loss": 0.8964,
      "step": 112650
    },
    {
      "epoch": 822.3357664233577,
      "grad_norm": 12.528481483459473,
      "learning_rate": 8.883211678832116e-06,
      "loss": 1.021,
      "step": 112660
    },
    {
      "epoch": 822.4087591240876,
      "grad_norm": 13.501341819763184,
      "learning_rate": 8.879562043795622e-06,
      "loss": 0.7986,
      "step": 112670
    },
    {
      "epoch": 822.4817518248175,
      "grad_norm": 10.407350540161133,
      "learning_rate": 8.875912408759124e-06,
      "loss": 1.5035,
      "step": 112680
    },
    {
      "epoch": 822.5547445255474,
      "grad_norm": 7.904796600341797,
      "learning_rate": 8.872262773722628e-06,
      "loss": 0.7183,
      "step": 112690
    },
    {
      "epoch": 822.6277372262774,
      "grad_norm": 7.1968584060668945,
      "learning_rate": 8.868613138686132e-06,
      "loss": 0.8423,
      "step": 112700
    },
    {
      "epoch": 822.7007299270073,
      "grad_norm": 11.852107048034668,
      "learning_rate": 8.864963503649635e-06,
      "loss": 0.8774,
      "step": 112710
    },
    {
      "epoch": 822.7737226277372,
      "grad_norm": 14.305962562561035,
      "learning_rate": 8.86131386861314e-06,
      "loss": 1.0894,
      "step": 112720
    },
    {
      "epoch": 822.8467153284671,
      "grad_norm": 9.672773361206055,
      "learning_rate": 8.857664233576643e-06,
      "loss": 0.7753,
      "step": 112730
    },
    {
      "epoch": 822.9197080291971,
      "grad_norm": 10.087162971496582,
      "learning_rate": 8.854014598540145e-06,
      "loss": 0.5787,
      "step": 112740
    },
    {
      "epoch": 822.992700729927,
      "grad_norm": 11.551423072814941,
      "learning_rate": 8.850364963503651e-06,
      "loss": 0.8392,
      "step": 112750
    },
    {
      "epoch": 823.0656934306569,
      "grad_norm": 9.858024597167969,
      "learning_rate": 8.846715328467153e-06,
      "loss": 0.7885,
      "step": 112760
    },
    {
      "epoch": 823.1386861313869,
      "grad_norm": 11.057914733886719,
      "learning_rate": 8.843065693430657e-06,
      "loss": 1.1327,
      "step": 112770
    },
    {
      "epoch": 823.2116788321168,
      "grad_norm": 14.486625671386719,
      "learning_rate": 8.839416058394161e-06,
      "loss": 0.7957,
      "step": 112780
    },
    {
      "epoch": 823.2846715328467,
      "grad_norm": 7.1124982833862305,
      "learning_rate": 8.835766423357665e-06,
      "loss": 0.8443,
      "step": 112790
    },
    {
      "epoch": 823.3576642335767,
      "grad_norm": 13.728196144104004,
      "learning_rate": 8.832116788321169e-06,
      "loss": 1.1562,
      "step": 112800
    },
    {
      "epoch": 823.4306569343066,
      "grad_norm": 5.3985595703125,
      "learning_rate": 8.828467153284673e-06,
      "loss": 0.7443,
      "step": 112810
    },
    {
      "epoch": 823.5036496350365,
      "grad_norm": 0.06161321699619293,
      "learning_rate": 8.824817518248175e-06,
      "loss": 0.9464,
      "step": 112820
    },
    {
      "epoch": 823.5766423357665,
      "grad_norm": 9.176517486572266,
      "learning_rate": 8.82116788321168e-06,
      "loss": 1.0107,
      "step": 112830
    },
    {
      "epoch": 823.6496350364963,
      "grad_norm": 11.348610877990723,
      "learning_rate": 8.817518248175182e-06,
      "loss": 0.8945,
      "step": 112840
    },
    {
      "epoch": 823.7226277372263,
      "grad_norm": 8.95827579498291,
      "learning_rate": 8.813868613138686e-06,
      "loss": 0.6562,
      "step": 112850
    },
    {
      "epoch": 823.7956204379562,
      "grad_norm": 0.07812771201133728,
      "learning_rate": 8.81021897810219e-06,
      "loss": 0.7504,
      "step": 112860
    },
    {
      "epoch": 823.8686131386861,
      "grad_norm": 10.116666793823242,
      "learning_rate": 8.806569343065694e-06,
      "loss": 1.1811,
      "step": 112870
    },
    {
      "epoch": 823.9416058394161,
      "grad_norm": 0.05452367663383484,
      "learning_rate": 8.802919708029198e-06,
      "loss": 0.7669,
      "step": 112880
    },
    {
      "epoch": 824.014598540146,
      "grad_norm": 7.750523567199707,
      "learning_rate": 8.7992700729927e-06,
      "loss": 1.1102,
      "step": 112890
    },
    {
      "epoch": 824.0875912408759,
      "grad_norm": 7.252569675445557,
      "learning_rate": 8.795620437956204e-06,
      "loss": 0.8456,
      "step": 112900
    },
    {
      "epoch": 824.1605839416059,
      "grad_norm": 7.431115627288818,
      "learning_rate": 8.791970802919708e-06,
      "loss": 0.7266,
      "step": 112910
    },
    {
      "epoch": 824.2335766423357,
      "grad_norm": 18.259021759033203,
      "learning_rate": 8.788321167883212e-06,
      "loss": 0.3029,
      "step": 112920
    },
    {
      "epoch": 824.3065693430657,
      "grad_norm": 14.178031921386719,
      "learning_rate": 8.784671532846716e-06,
      "loss": 0.9655,
      "step": 112930
    },
    {
      "epoch": 824.3795620437957,
      "grad_norm": 9.296096801757812,
      "learning_rate": 8.78102189781022e-06,
      "loss": 0.8127,
      "step": 112940
    },
    {
      "epoch": 824.4525547445255,
      "grad_norm": 0.06636494398117065,
      "learning_rate": 8.777372262773722e-06,
      "loss": 0.4334,
      "step": 112950
    },
    {
      "epoch": 824.5255474452555,
      "grad_norm": 9.820615768432617,
      "learning_rate": 8.773722627737227e-06,
      "loss": 1.4746,
      "step": 112960
    },
    {
      "epoch": 824.5985401459855,
      "grad_norm": 0.08717282116413116,
      "learning_rate": 8.77007299270073e-06,
      "loss": 1.0486,
      "step": 112970
    },
    {
      "epoch": 824.6715328467153,
      "grad_norm": 15.920625686645508,
      "learning_rate": 8.766423357664235e-06,
      "loss": 0.9397,
      "step": 112980
    },
    {
      "epoch": 824.7445255474453,
      "grad_norm": 12.757197380065918,
      "learning_rate": 8.762773722627737e-06,
      "loss": 1.2378,
      "step": 112990
    },
    {
      "epoch": 824.8175182481751,
      "grad_norm": 8.336420059204102,
      "learning_rate": 8.759124087591241e-06,
      "loss": 0.705,
      "step": 113000
    },
    {
      "epoch": 824.8905109489051,
      "grad_norm": 10.440043449401855,
      "learning_rate": 8.755474452554745e-06,
      "loss": 0.8377,
      "step": 113010
    },
    {
      "epoch": 824.9635036496351,
      "grad_norm": 11.733270645141602,
      "learning_rate": 8.751824817518249e-06,
      "loss": 0.9997,
      "step": 113020
    },
    {
      "epoch": 825.0364963503649,
      "grad_norm": 6.645016670227051,
      "learning_rate": 8.748175182481753e-06,
      "loss": 0.8568,
      "step": 113030
    },
    {
      "epoch": 825.1094890510949,
      "grad_norm": 0.0340435728430748,
      "learning_rate": 8.744525547445257e-06,
      "loss": 0.9284,
      "step": 113040
    },
    {
      "epoch": 825.1824817518249,
      "grad_norm": 15.739775657653809,
      "learning_rate": 8.740875912408759e-06,
      "loss": 0.895,
      "step": 113050
    },
    {
      "epoch": 825.2554744525547,
      "grad_norm": 7.092933177947998,
      "learning_rate": 8.737226277372265e-06,
      "loss": 0.7739,
      "step": 113060
    },
    {
      "epoch": 825.3284671532847,
      "grad_norm": 15.026650428771973,
      "learning_rate": 8.733576642335767e-06,
      "loss": 1.101,
      "step": 113070
    },
    {
      "epoch": 825.4014598540145,
      "grad_norm": 7.774159908294678,
      "learning_rate": 8.72992700729927e-06,
      "loss": 0.7596,
      "step": 113080
    },
    {
      "epoch": 825.4744525547445,
      "grad_norm": 11.913374900817871,
      "learning_rate": 8.726277372262774e-06,
      "loss": 0.6206,
      "step": 113090
    },
    {
      "epoch": 825.5474452554745,
      "grad_norm": 12.285001754760742,
      "learning_rate": 8.722627737226277e-06,
      "loss": 0.6453,
      "step": 113100
    },
    {
      "epoch": 825.6204379562043,
      "grad_norm": 12.748510360717773,
      "learning_rate": 8.718978102189782e-06,
      "loss": 1.0122,
      "step": 113110
    },
    {
      "epoch": 825.6934306569343,
      "grad_norm": 16.2110595703125,
      "learning_rate": 8.715328467153284e-06,
      "loss": 0.9957,
      "step": 113120
    },
    {
      "epoch": 825.7664233576643,
      "grad_norm": 12.630589485168457,
      "learning_rate": 8.711678832116788e-06,
      "loss": 1.0645,
      "step": 113130
    },
    {
      "epoch": 825.8394160583941,
      "grad_norm": 12.56911563873291,
      "learning_rate": 8.708029197080292e-06,
      "loss": 0.728,
      "step": 113140
    },
    {
      "epoch": 825.9124087591241,
      "grad_norm": 0.1671711653470993,
      "learning_rate": 8.704379562043796e-06,
      "loss": 0.6594,
      "step": 113150
    },
    {
      "epoch": 825.985401459854,
      "grad_norm": 5.550100803375244,
      "learning_rate": 8.7007299270073e-06,
      "loss": 1.1295,
      "step": 113160
    },
    {
      "epoch": 826.0583941605839,
      "grad_norm": 12.568727493286133,
      "learning_rate": 8.697080291970804e-06,
      "loss": 1.0273,
      "step": 113170
    },
    {
      "epoch": 826.1313868613139,
      "grad_norm": 10.755951881408691,
      "learning_rate": 8.693430656934306e-06,
      "loss": 0.6474,
      "step": 113180
    },
    {
      "epoch": 826.2043795620438,
      "grad_norm": 8.027189254760742,
      "learning_rate": 8.689781021897812e-06,
      "loss": 1.126,
      "step": 113190
    },
    {
      "epoch": 826.2773722627737,
      "grad_norm": 13.070975303649902,
      "learning_rate": 8.686131386861314e-06,
      "loss": 1.2021,
      "step": 113200
    },
    {
      "epoch": 826.3503649635037,
      "grad_norm": 0.27327799797058105,
      "learning_rate": 8.682481751824818e-06,
      "loss": 0.8176,
      "step": 113210
    },
    {
      "epoch": 826.4233576642335,
      "grad_norm": 8.741605758666992,
      "learning_rate": 8.678832116788322e-06,
      "loss": 0.9481,
      "step": 113220
    },
    {
      "epoch": 826.4963503649635,
      "grad_norm": 6.027706623077393,
      "learning_rate": 8.675182481751825e-06,
      "loss": 0.5482,
      "step": 113230
    },
    {
      "epoch": 826.5693430656934,
      "grad_norm": 0.031707558780908585,
      "learning_rate": 8.67153284671533e-06,
      "loss": 0.8272,
      "step": 113240
    },
    {
      "epoch": 826.6423357664233,
      "grad_norm": 10.361654281616211,
      "learning_rate": 8.667883211678833e-06,
      "loss": 1.2285,
      "step": 113250
    },
    {
      "epoch": 826.7153284671533,
      "grad_norm": 15.92982006072998,
      "learning_rate": 8.664233576642335e-06,
      "loss": 0.9512,
      "step": 113260
    },
    {
      "epoch": 826.7883211678832,
      "grad_norm": 17.156885147094727,
      "learning_rate": 8.660583941605841e-06,
      "loss": 0.726,
      "step": 113270
    },
    {
      "epoch": 826.8613138686131,
      "grad_norm": 0.018335701897740364,
      "learning_rate": 8.656934306569343e-06,
      "loss": 0.8612,
      "step": 113280
    },
    {
      "epoch": 826.9343065693431,
      "grad_norm": 0.020431354641914368,
      "learning_rate": 8.653284671532847e-06,
      "loss": 1.1031,
      "step": 113290
    },
    {
      "epoch": 827.007299270073,
      "grad_norm": 6.724194049835205,
      "learning_rate": 8.649635036496351e-06,
      "loss": 0.8392,
      "step": 113300
    },
    {
      "epoch": 827.0802919708029,
      "grad_norm": 13.90066909790039,
      "learning_rate": 8.645985401459853e-06,
      "loss": 1.1349,
      "step": 113310
    },
    {
      "epoch": 827.1532846715329,
      "grad_norm": 7.829832077026367,
      "learning_rate": 8.642335766423359e-06,
      "loss": 0.7829,
      "step": 113320
    },
    {
      "epoch": 827.2262773722628,
      "grad_norm": 5.198497295379639,
      "learning_rate": 8.638686131386861e-06,
      "loss": 1.005,
      "step": 113330
    },
    {
      "epoch": 827.2992700729927,
      "grad_norm": 9.820063591003418,
      "learning_rate": 8.635036496350365e-06,
      "loss": 0.8838,
      "step": 113340
    },
    {
      "epoch": 827.3722627737226,
      "grad_norm": 10.471386909484863,
      "learning_rate": 8.631386861313869e-06,
      "loss": 1.2568,
      "step": 113350
    },
    {
      "epoch": 827.4452554744526,
      "grad_norm": 0.17306603491306305,
      "learning_rate": 8.627737226277373e-06,
      "loss": 0.9394,
      "step": 113360
    },
    {
      "epoch": 827.5182481751825,
      "grad_norm": 10.981914520263672,
      "learning_rate": 8.624087591240876e-06,
      "loss": 0.9418,
      "step": 113370
    },
    {
      "epoch": 827.5912408759124,
      "grad_norm": 13.725455284118652,
      "learning_rate": 8.62043795620438e-06,
      "loss": 0.7926,
      "step": 113380
    },
    {
      "epoch": 827.6642335766423,
      "grad_norm": 0.27063167095184326,
      "learning_rate": 8.616788321167882e-06,
      "loss": 0.837,
      "step": 113390
    },
    {
      "epoch": 827.7372262773723,
      "grad_norm": 15.935009956359863,
      "learning_rate": 8.613138686131388e-06,
      "loss": 0.8083,
      "step": 113400
    },
    {
      "epoch": 827.8102189781022,
      "grad_norm": 9.436139106750488,
      "learning_rate": 8.60948905109489e-06,
      "loss": 0.5568,
      "step": 113410
    },
    {
      "epoch": 827.8832116788321,
      "grad_norm": 13.745386123657227,
      "learning_rate": 8.605839416058394e-06,
      "loss": 0.8605,
      "step": 113420
    },
    {
      "epoch": 827.956204379562,
      "grad_norm": 7.858036518096924,
      "learning_rate": 8.602189781021898e-06,
      "loss": 1.081,
      "step": 113430
    },
    {
      "epoch": 828.029197080292,
      "grad_norm": 8.856619834899902,
      "learning_rate": 8.598540145985402e-06,
      "loss": 0.9193,
      "step": 113440
    },
    {
      "epoch": 828.1021897810219,
      "grad_norm": 1.311536431312561,
      "learning_rate": 8.594890510948906e-06,
      "loss": 0.6781,
      "step": 113450
    },
    {
      "epoch": 828.1751824817518,
      "grad_norm": 0.0578620508313179,
      "learning_rate": 8.59124087591241e-06,
      "loss": 0.8973,
      "step": 113460
    },
    {
      "epoch": 828.2481751824818,
      "grad_norm": 0.012556509114801884,
      "learning_rate": 8.587591240875912e-06,
      "loss": 0.636,
      "step": 113470
    },
    {
      "epoch": 828.3211678832117,
      "grad_norm": 6.658450126647949,
      "learning_rate": 8.583941605839417e-06,
      "loss": 1.0161,
      "step": 113480
    },
    {
      "epoch": 828.3941605839416,
      "grad_norm": 14.627881050109863,
      "learning_rate": 8.58029197080292e-06,
      "loss": 1.0551,
      "step": 113490
    },
    {
      "epoch": 828.4671532846716,
      "grad_norm": 0.05381562188267708,
      "learning_rate": 8.576642335766425e-06,
      "loss": 0.7332,
      "step": 113500
    },
    {
      "epoch": 828.5401459854014,
      "grad_norm": 0.02058192901313305,
      "learning_rate": 8.572992700729927e-06,
      "loss": 0.9554,
      "step": 113510
    },
    {
      "epoch": 828.6131386861314,
      "grad_norm": 9.304522514343262,
      "learning_rate": 8.569343065693431e-06,
      "loss": 1.2858,
      "step": 113520
    },
    {
      "epoch": 828.6861313868613,
      "grad_norm": 12.889349937438965,
      "learning_rate": 8.565693430656935e-06,
      "loss": 0.7458,
      "step": 113530
    },
    {
      "epoch": 828.7591240875912,
      "grad_norm": 7.305940628051758,
      "learning_rate": 8.562043795620437e-06,
      "loss": 0.8976,
      "step": 113540
    },
    {
      "epoch": 828.8321167883212,
      "grad_norm": 0.02240617759525776,
      "learning_rate": 8.558394160583943e-06,
      "loss": 0.3641,
      "step": 113550
    },
    {
      "epoch": 828.9051094890511,
      "grad_norm": 6.046110153198242,
      "learning_rate": 8.554744525547445e-06,
      "loss": 1.2149,
      "step": 113560
    },
    {
      "epoch": 828.978102189781,
      "grad_norm": 10.181578636169434,
      "learning_rate": 8.551094890510949e-06,
      "loss": 0.982,
      "step": 113570
    },
    {
      "epoch": 829.051094890511,
      "grad_norm": 10.301641464233398,
      "learning_rate": 8.547445255474453e-06,
      "loss": 1.0285,
      "step": 113580
    },
    {
      "epoch": 829.1240875912408,
      "grad_norm": 7.860485076904297,
      "learning_rate": 8.543795620437957e-06,
      "loss": 0.507,
      "step": 113590
    },
    {
      "epoch": 829.1970802919708,
      "grad_norm": 11.404369354248047,
      "learning_rate": 8.54014598540146e-06,
      "loss": 0.6459,
      "step": 113600
    },
    {
      "epoch": 829.2700729927008,
      "grad_norm": 11.334577560424805,
      "learning_rate": 8.536496350364964e-06,
      "loss": 1.1737,
      "step": 113610
    },
    {
      "epoch": 829.3430656934306,
      "grad_norm": 3.4596829414367676,
      "learning_rate": 8.532846715328467e-06,
      "loss": 0.5704,
      "step": 113620
    },
    {
      "epoch": 829.4160583941606,
      "grad_norm": 5.413675785064697,
      "learning_rate": 8.529197080291972e-06,
      "loss": 0.9764,
      "step": 113630
    },
    {
      "epoch": 829.4890510948906,
      "grad_norm": 7.284287452697754,
      "learning_rate": 8.525547445255474e-06,
      "loss": 1.1261,
      "step": 113640
    },
    {
      "epoch": 829.5620437956204,
      "grad_norm": 12.170522689819336,
      "learning_rate": 8.521897810218978e-06,
      "loss": 0.9931,
      "step": 113650
    },
    {
      "epoch": 829.6350364963504,
      "grad_norm": 13.926916122436523,
      "learning_rate": 8.518248175182482e-06,
      "loss": 1.2493,
      "step": 113660
    },
    {
      "epoch": 829.7080291970802,
      "grad_norm": 5.736902236938477,
      "learning_rate": 8.514598540145986e-06,
      "loss": 0.4625,
      "step": 113670
    },
    {
      "epoch": 829.7810218978102,
      "grad_norm": 10.589921951293945,
      "learning_rate": 8.51094890510949e-06,
      "loss": 1.0274,
      "step": 113680
    },
    {
      "epoch": 829.8540145985402,
      "grad_norm": 9.918990135192871,
      "learning_rate": 8.507299270072994e-06,
      "loss": 0.6677,
      "step": 113690
    },
    {
      "epoch": 829.92700729927,
      "grad_norm": 11.789617538452148,
      "learning_rate": 8.503649635036496e-06,
      "loss": 0.7457,
      "step": 113700
    },
    {
      "epoch": 830.0,
      "grad_norm": 34.890071868896484,
      "learning_rate": 8.500000000000002e-06,
      "loss": 1.3571,
      "step": 113710
    },
    {
      "epoch": 830.07299270073,
      "grad_norm": 11.059596061706543,
      "learning_rate": 8.496350364963504e-06,
      "loss": 0.7637,
      "step": 113720
    },
    {
      "epoch": 830.1459854014598,
      "grad_norm": 10.357230186462402,
      "learning_rate": 8.492700729927008e-06,
      "loss": 1.1756,
      "step": 113730
    },
    {
      "epoch": 830.2189781021898,
      "grad_norm": 15.374530792236328,
      "learning_rate": 8.489051094890512e-06,
      "loss": 0.8709,
      "step": 113740
    },
    {
      "epoch": 830.2919708029198,
      "grad_norm": 1.8896335363388062,
      "learning_rate": 8.485401459854014e-06,
      "loss": 0.936,
      "step": 113750
    },
    {
      "epoch": 830.3649635036496,
      "grad_norm": 14.60588550567627,
      "learning_rate": 8.48175182481752e-06,
      "loss": 1.1367,
      "step": 113760
    },
    {
      "epoch": 830.4379562043796,
      "grad_norm": 8.877355575561523,
      "learning_rate": 8.478102189781022e-06,
      "loss": 0.542,
      "step": 113770
    },
    {
      "epoch": 830.5109489051094,
      "grad_norm": 4.459740161895752,
      "learning_rate": 8.474452554744525e-06,
      "loss": 0.8631,
      "step": 113780
    },
    {
      "epoch": 830.5839416058394,
      "grad_norm": 11.738394737243652,
      "learning_rate": 8.47080291970803e-06,
      "loss": 0.9416,
      "step": 113790
    },
    {
      "epoch": 830.6569343065694,
      "grad_norm": 6.618130683898926,
      "learning_rate": 8.467153284671533e-06,
      "loss": 0.9531,
      "step": 113800
    },
    {
      "epoch": 830.7299270072992,
      "grad_norm": 13.47377872467041,
      "learning_rate": 8.463503649635037e-06,
      "loss": 1.227,
      "step": 113810
    },
    {
      "epoch": 830.8029197080292,
      "grad_norm": 9.118064880371094,
      "learning_rate": 8.459854014598541e-06,
      "loss": 0.8215,
      "step": 113820
    },
    {
      "epoch": 830.8759124087592,
      "grad_norm": 11.923340797424316,
      "learning_rate": 8.456204379562043e-06,
      "loss": 0.7612,
      "step": 113830
    },
    {
      "epoch": 830.948905109489,
      "grad_norm": 0.0361153818666935,
      "learning_rate": 8.452554744525549e-06,
      "loss": 0.8755,
      "step": 113840
    },
    {
      "epoch": 831.021897810219,
      "grad_norm": 10.164311408996582,
      "learning_rate": 8.448905109489051e-06,
      "loss": 0.8421,
      "step": 113850
    },
    {
      "epoch": 831.0948905109489,
      "grad_norm": 8.280881881713867,
      "learning_rate": 8.445255474452555e-06,
      "loss": 0.7495,
      "step": 113860
    },
    {
      "epoch": 831.1678832116788,
      "grad_norm": 8.732049942016602,
      "learning_rate": 8.441605839416059e-06,
      "loss": 0.6793,
      "step": 113870
    },
    {
      "epoch": 831.2408759124088,
      "grad_norm": 3.943725109100342,
      "learning_rate": 8.437956204379563e-06,
      "loss": 0.5647,
      "step": 113880
    },
    {
      "epoch": 831.3138686131387,
      "grad_norm": 13.947680473327637,
      "learning_rate": 8.434306569343066e-06,
      "loss": 1.1413,
      "step": 113890
    },
    {
      "epoch": 831.3868613138686,
      "grad_norm": 13.747353553771973,
      "learning_rate": 8.43065693430657e-06,
      "loss": 0.6866,
      "step": 113900
    },
    {
      "epoch": 831.4598540145986,
      "grad_norm": 16.80494499206543,
      "learning_rate": 8.427007299270072e-06,
      "loss": 0.9764,
      "step": 113910
    },
    {
      "epoch": 831.5328467153284,
      "grad_norm": 8.298784255981445,
      "learning_rate": 8.423357664233578e-06,
      "loss": 1.1113,
      "step": 113920
    },
    {
      "epoch": 831.6058394160584,
      "grad_norm": 0.03024110570549965,
      "learning_rate": 8.41970802919708e-06,
      "loss": 0.5449,
      "step": 113930
    },
    {
      "epoch": 831.6788321167883,
      "grad_norm": 12.190189361572266,
      "learning_rate": 8.416058394160586e-06,
      "loss": 1.1388,
      "step": 113940
    },
    {
      "epoch": 831.7518248175182,
      "grad_norm": 10.025856018066406,
      "learning_rate": 8.412408759124088e-06,
      "loss": 0.8507,
      "step": 113950
    },
    {
      "epoch": 831.8248175182482,
      "grad_norm": 5.859533309936523,
      "learning_rate": 8.40875912408759e-06,
      "loss": 0.6293,
      "step": 113960
    },
    {
      "epoch": 831.8978102189781,
      "grad_norm": 16.713516235351562,
      "learning_rate": 8.405109489051096e-06,
      "loss": 1.151,
      "step": 113970
    },
    {
      "epoch": 831.970802919708,
      "grad_norm": 13.046937942504883,
      "learning_rate": 8.401459854014598e-06,
      "loss": 1.1978,
      "step": 113980
    },
    {
      "epoch": 832.043795620438,
      "grad_norm": 8.308372497558594,
      "learning_rate": 8.397810218978104e-06,
      "loss": 0.4633,
      "step": 113990
    },
    {
      "epoch": 832.1167883211679,
      "grad_norm": 10.633161544799805,
      "learning_rate": 8.394160583941606e-06,
      "loss": 1.0201,
      "step": 114000
    },
    {
      "epoch": 832.1897810218978,
      "grad_norm": 6.929266929626465,
      "learning_rate": 8.39051094890511e-06,
      "loss": 0.3747,
      "step": 114010
    },
    {
      "epoch": 832.2627737226277,
      "grad_norm": 6.337162494659424,
      "learning_rate": 8.386861313868614e-06,
      "loss": 1.142,
      "step": 114020
    },
    {
      "epoch": 832.3357664233577,
      "grad_norm": 13.013229370117188,
      "learning_rate": 8.383211678832117e-06,
      "loss": 0.8773,
      "step": 114030
    },
    {
      "epoch": 832.4087591240876,
      "grad_norm": 0.07724954187870026,
      "learning_rate": 8.379562043795621e-06,
      "loss": 0.9071,
      "step": 114040
    },
    {
      "epoch": 832.4817518248175,
      "grad_norm": 12.435280799865723,
      "learning_rate": 8.375912408759125e-06,
      "loss": 0.5454,
      "step": 114050
    },
    {
      "epoch": 832.5547445255474,
      "grad_norm": 0.01425815187394619,
      "learning_rate": 8.372262773722627e-06,
      "loss": 0.6352,
      "step": 114060
    },
    {
      "epoch": 832.6277372262774,
      "grad_norm": 12.189192771911621,
      "learning_rate": 8.368613138686133e-06,
      "loss": 1.1139,
      "step": 114070
    },
    {
      "epoch": 832.7007299270073,
      "grad_norm": 8.323592185974121,
      "learning_rate": 8.364963503649635e-06,
      "loss": 1.1032,
      "step": 114080
    },
    {
      "epoch": 832.7737226277372,
      "grad_norm": 5.383484840393066,
      "learning_rate": 8.361313868613139e-06,
      "loss": 0.7323,
      "step": 114090
    },
    {
      "epoch": 832.8467153284671,
      "grad_norm": 12.740693092346191,
      "learning_rate": 8.357664233576643e-06,
      "loss": 1.3042,
      "step": 114100
    },
    {
      "epoch": 832.9197080291971,
      "grad_norm": 7.050715446472168,
      "learning_rate": 8.354014598540147e-06,
      "loss": 0.73,
      "step": 114110
    },
    {
      "epoch": 832.992700729927,
      "grad_norm": 7.859582901000977,
      "learning_rate": 8.35036496350365e-06,
      "loss": 1.0374,
      "step": 114120
    },
    {
      "epoch": 833.0656934306569,
      "grad_norm": 12.014132499694824,
      "learning_rate": 8.346715328467155e-06,
      "loss": 1.0573,
      "step": 114130
    },
    {
      "epoch": 833.1386861313869,
      "grad_norm": 7.223046779632568,
      "learning_rate": 8.343065693430657e-06,
      "loss": 0.7575,
      "step": 114140
    },
    {
      "epoch": 833.2116788321168,
      "grad_norm": 14.51696491241455,
      "learning_rate": 8.339416058394162e-06,
      "loss": 0.6827,
      "step": 114150
    },
    {
      "epoch": 833.2846715328467,
      "grad_norm": 0.045470114797353745,
      "learning_rate": 8.335766423357664e-06,
      "loss": 0.8017,
      "step": 114160
    },
    {
      "epoch": 833.3576642335767,
      "grad_norm": 10.33737564086914,
      "learning_rate": 8.332116788321168e-06,
      "loss": 0.6099,
      "step": 114170
    },
    {
      "epoch": 833.4306569343066,
      "grad_norm": 7.376652240753174,
      "learning_rate": 8.328467153284672e-06,
      "loss": 1.0336,
      "step": 114180
    },
    {
      "epoch": 833.5036496350365,
      "grad_norm": 14.137630462646484,
      "learning_rate": 8.324817518248174e-06,
      "loss": 1.3624,
      "step": 114190
    },
    {
      "epoch": 833.5766423357665,
      "grad_norm": 7.3488359451293945,
      "learning_rate": 8.32116788321168e-06,
      "loss": 0.6839,
      "step": 114200
    },
    {
      "epoch": 833.6496350364963,
      "grad_norm": 11.203781127929688,
      "learning_rate": 8.317518248175182e-06,
      "loss": 0.9875,
      "step": 114210
    },
    {
      "epoch": 833.7226277372263,
      "grad_norm": 0.05224139988422394,
      "learning_rate": 8.313868613138686e-06,
      "loss": 1.0826,
      "step": 114220
    },
    {
      "epoch": 833.7956204379562,
      "grad_norm": 9.75673770904541,
      "learning_rate": 8.31021897810219e-06,
      "loss": 1.0358,
      "step": 114230
    },
    {
      "epoch": 833.8686131386861,
      "grad_norm": 3.6875617504119873,
      "learning_rate": 8.306569343065694e-06,
      "loss": 0.8905,
      "step": 114240
    },
    {
      "epoch": 833.9416058394161,
      "grad_norm": 5.742641448974609,
      "learning_rate": 8.302919708029198e-06,
      "loss": 0.7308,
      "step": 114250
    },
    {
      "epoch": 834.014598540146,
      "grad_norm": 13.1996431350708,
      "learning_rate": 8.299270072992702e-06,
      "loss": 0.818,
      "step": 114260
    },
    {
      "epoch": 834.0875912408759,
      "grad_norm": 13.596416473388672,
      "learning_rate": 8.295620437956204e-06,
      "loss": 1.0336,
      "step": 114270
    },
    {
      "epoch": 834.1605839416059,
      "grad_norm": 8.734395980834961,
      "learning_rate": 8.29197080291971e-06,
      "loss": 1.0096,
      "step": 114280
    },
    {
      "epoch": 834.2335766423357,
      "grad_norm": 12.745736122131348,
      "learning_rate": 8.288321167883212e-06,
      "loss": 1.0916,
      "step": 114290
    },
    {
      "epoch": 834.3065693430657,
      "grad_norm": 6.546846866607666,
      "learning_rate": 8.284671532846715e-06,
      "loss": 0.7916,
      "step": 114300
    },
    {
      "epoch": 834.3795620437957,
      "grad_norm": 13.698986053466797,
      "learning_rate": 8.28102189781022e-06,
      "loss": 1.0338,
      "step": 114310
    },
    {
      "epoch": 834.4525547445255,
      "grad_norm": 9.814021110534668,
      "learning_rate": 8.277372262773723e-06,
      "loss": 1.2249,
      "step": 114320
    },
    {
      "epoch": 834.5255474452555,
      "grad_norm": 8.273648262023926,
      "learning_rate": 8.273722627737227e-06,
      "loss": 0.4117,
      "step": 114330
    },
    {
      "epoch": 834.5985401459855,
      "grad_norm": 8.925524711608887,
      "learning_rate": 8.270072992700731e-06,
      "loss": 0.6378,
      "step": 114340
    },
    {
      "epoch": 834.6715328467153,
      "grad_norm": 8.231501579284668,
      "learning_rate": 8.266423357664233e-06,
      "loss": 1.028,
      "step": 114350
    },
    {
      "epoch": 834.7445255474453,
      "grad_norm": 7.264704704284668,
      "learning_rate": 8.262773722627739e-06,
      "loss": 0.8121,
      "step": 114360
    },
    {
      "epoch": 834.8175182481751,
      "grad_norm": 15.137128829956055,
      "learning_rate": 8.259124087591241e-06,
      "loss": 0.8289,
      "step": 114370
    },
    {
      "epoch": 834.8905109489051,
      "grad_norm": 0.06351696699857712,
      "learning_rate": 8.255474452554745e-06,
      "loss": 0.6777,
      "step": 114380
    },
    {
      "epoch": 834.9635036496351,
      "grad_norm": 11.319432258605957,
      "learning_rate": 8.251824817518249e-06,
      "loss": 0.7301,
      "step": 114390
    },
    {
      "epoch": 835.0364963503649,
      "grad_norm": 8.359053611755371,
      "learning_rate": 8.248175182481751e-06,
      "loss": 0.7846,
      "step": 114400
    },
    {
      "epoch": 835.1094890510949,
      "grad_norm": 5.890974521636963,
      "learning_rate": 8.244525547445256e-06,
      "loss": 0.81,
      "step": 114410
    },
    {
      "epoch": 835.1824817518249,
      "grad_norm": 8.15207290649414,
      "learning_rate": 8.240875912408759e-06,
      "loss": 1.021,
      "step": 114420
    },
    {
      "epoch": 835.2554744525547,
      "grad_norm": 0.08069614320993423,
      "learning_rate": 8.237226277372263e-06,
      "loss": 0.9792,
      "step": 114430
    },
    {
      "epoch": 835.3284671532847,
      "grad_norm": 21.085546493530273,
      "learning_rate": 8.233576642335766e-06,
      "loss": 0.9031,
      "step": 114440
    },
    {
      "epoch": 835.4014598540145,
      "grad_norm": 15.191219329833984,
      "learning_rate": 8.22992700729927e-06,
      "loss": 1.1184,
      "step": 114450
    },
    {
      "epoch": 835.4744525547445,
      "grad_norm": 14.70824909210205,
      "learning_rate": 8.226277372262774e-06,
      "loss": 0.7053,
      "step": 114460
    },
    {
      "epoch": 835.5474452554745,
      "grad_norm": 14.392322540283203,
      "learning_rate": 8.222627737226278e-06,
      "loss": 1.0287,
      "step": 114470
    },
    {
      "epoch": 835.6204379562043,
      "grad_norm": 0.019378984346985817,
      "learning_rate": 8.21897810218978e-06,
      "loss": 0.6847,
      "step": 114480
    },
    {
      "epoch": 835.6934306569343,
      "grad_norm": 0.04466203600168228,
      "learning_rate": 8.215328467153286e-06,
      "loss": 1.3009,
      "step": 114490
    },
    {
      "epoch": 835.7664233576643,
      "grad_norm": 4.894906997680664,
      "learning_rate": 8.211678832116788e-06,
      "loss": 0.7074,
      "step": 114500
    },
    {
      "epoch": 835.8394160583941,
      "grad_norm": 16.933073043823242,
      "learning_rate": 8.208029197080294e-06,
      "loss": 0.8131,
      "step": 114510
    },
    {
      "epoch": 835.9124087591241,
      "grad_norm": 14.804373741149902,
      "learning_rate": 8.204379562043796e-06,
      "loss": 0.6973,
      "step": 114520
    },
    {
      "epoch": 835.985401459854,
      "grad_norm": 18.36349868774414,
      "learning_rate": 8.2007299270073e-06,
      "loss": 1.066,
      "step": 114530
    },
    {
      "epoch": 836.0583941605839,
      "grad_norm": 0.05667705088853836,
      "learning_rate": 8.197080291970804e-06,
      "loss": 0.81,
      "step": 114540
    },
    {
      "epoch": 836.1313868613139,
      "grad_norm": 15.921749114990234,
      "learning_rate": 8.193430656934307e-06,
      "loss": 1.298,
      "step": 114550
    },
    {
      "epoch": 836.2043795620438,
      "grad_norm": 0.010309912264347076,
      "learning_rate": 8.189781021897811e-06,
      "loss": 0.869,
      "step": 114560
    },
    {
      "epoch": 836.2773722627737,
      "grad_norm": 17.58629035949707,
      "learning_rate": 8.186131386861315e-06,
      "loss": 1.1005,
      "step": 114570
    },
    {
      "epoch": 836.3503649635037,
      "grad_norm": 12.331396102905273,
      "learning_rate": 8.182481751824817e-06,
      "loss": 1.1095,
      "step": 114580
    },
    {
      "epoch": 836.4233576642335,
      "grad_norm": 0.029523978009819984,
      "learning_rate": 8.178832116788323e-06,
      "loss": 0.8477,
      "step": 114590
    },
    {
      "epoch": 836.4963503649635,
      "grad_norm": 11.169387817382812,
      "learning_rate": 8.175182481751825e-06,
      "loss": 0.6836,
      "step": 114600
    },
    {
      "epoch": 836.5693430656934,
      "grad_norm": 10.865872383117676,
      "learning_rate": 8.171532846715329e-06,
      "loss": 1.0434,
      "step": 114610
    },
    {
      "epoch": 836.6423357664233,
      "grad_norm": 20.013469696044922,
      "learning_rate": 8.167883211678833e-06,
      "loss": 0.8161,
      "step": 114620
    },
    {
      "epoch": 836.7153284671533,
      "grad_norm": 1.744417667388916,
      "learning_rate": 8.164233576642335e-06,
      "loss": 0.6867,
      "step": 114630
    },
    {
      "epoch": 836.7883211678832,
      "grad_norm": 0.0283201951533556,
      "learning_rate": 8.16058394160584e-06,
      "loss": 1.0594,
      "step": 114640
    },
    {
      "epoch": 836.8613138686131,
      "grad_norm": 16.574634552001953,
      "learning_rate": 8.156934306569343e-06,
      "loss": 0.8587,
      "step": 114650
    },
    {
      "epoch": 836.9343065693431,
      "grad_norm": 6.919865608215332,
      "learning_rate": 8.153284671532847e-06,
      "loss": 0.8356,
      "step": 114660
    },
    {
      "epoch": 837.007299270073,
      "grad_norm": 0.11366509646177292,
      "learning_rate": 8.14963503649635e-06,
      "loss": 0.6915,
      "step": 114670
    },
    {
      "epoch": 837.0802919708029,
      "grad_norm": 13.390275955200195,
      "learning_rate": 8.145985401459855e-06,
      "loss": 0.7975,
      "step": 114680
    },
    {
      "epoch": 837.1532846715329,
      "grad_norm": 14.16175651550293,
      "learning_rate": 8.142335766423358e-06,
      "loss": 0.5413,
      "step": 114690
    },
    {
      "epoch": 837.2262773722628,
      "grad_norm": 9.598642349243164,
      "learning_rate": 8.138686131386862e-06,
      "loss": 1.1135,
      "step": 114700
    },
    {
      "epoch": 837.2992700729927,
      "grad_norm": 0.08257029205560684,
      "learning_rate": 8.135036496350364e-06,
      "loss": 0.7711,
      "step": 114710
    },
    {
      "epoch": 837.3722627737226,
      "grad_norm": 7.659451484680176,
      "learning_rate": 8.13138686131387e-06,
      "loss": 0.7065,
      "step": 114720
    },
    {
      "epoch": 837.4452554744526,
      "grad_norm": 0.025170138105750084,
      "learning_rate": 8.127737226277372e-06,
      "loss": 0.7251,
      "step": 114730
    },
    {
      "epoch": 837.5182481751825,
      "grad_norm": 19.18924331665039,
      "learning_rate": 8.124087591240876e-06,
      "loss": 1.2456,
      "step": 114740
    },
    {
      "epoch": 837.5912408759124,
      "grad_norm": 7.478421211242676,
      "learning_rate": 8.12043795620438e-06,
      "loss": 0.8205,
      "step": 114750
    },
    {
      "epoch": 837.6642335766423,
      "grad_norm": 18.022621154785156,
      "learning_rate": 8.116788321167884e-06,
      "loss": 1.0733,
      "step": 114760
    },
    {
      "epoch": 837.7372262773723,
      "grad_norm": 15.118351936340332,
      "learning_rate": 8.113138686131388e-06,
      "loss": 0.9928,
      "step": 114770
    },
    {
      "epoch": 837.8102189781022,
      "grad_norm": 0.023046039044857025,
      "learning_rate": 8.109489051094892e-06,
      "loss": 0.6562,
      "step": 114780
    },
    {
      "epoch": 837.8832116788321,
      "grad_norm": 9.019200325012207,
      "learning_rate": 8.105839416058394e-06,
      "loss": 0.8798,
      "step": 114790
    },
    {
      "epoch": 837.956204379562,
      "grad_norm": 5.296182632446289,
      "learning_rate": 8.1021897810219e-06,
      "loss": 0.687,
      "step": 114800
    },
    {
      "epoch": 838.029197080292,
      "grad_norm": 5.603652000427246,
      "learning_rate": 8.098540145985402e-06,
      "loss": 1.0099,
      "step": 114810
    },
    {
      "epoch": 838.1021897810219,
      "grad_norm": 13.107072830200195,
      "learning_rate": 8.094890510948905e-06,
      "loss": 1.0288,
      "step": 114820
    },
    {
      "epoch": 838.1751824817518,
      "grad_norm": 0.02242058515548706,
      "learning_rate": 8.09124087591241e-06,
      "loss": 0.7137,
      "step": 114830
    },
    {
      "epoch": 838.2481751824818,
      "grad_norm": 9.879290580749512,
      "learning_rate": 8.087591240875912e-06,
      "loss": 1.0082,
      "step": 114840
    },
    {
      "epoch": 838.3211678832117,
      "grad_norm": 8.284038543701172,
      "learning_rate": 8.083941605839417e-06,
      "loss": 1.2222,
      "step": 114850
    },
    {
      "epoch": 838.3941605839416,
      "grad_norm": 9.253138542175293,
      "learning_rate": 8.08029197080292e-06,
      "loss": 0.6742,
      "step": 114860
    },
    {
      "epoch": 838.4671532846716,
      "grad_norm": 7.70160436630249,
      "learning_rate": 8.076642335766423e-06,
      "loss": 0.7809,
      "step": 114870
    },
    {
      "epoch": 838.5401459854014,
      "grad_norm": 8.203770637512207,
      "learning_rate": 8.072992700729927e-06,
      "loss": 0.8415,
      "step": 114880
    },
    {
      "epoch": 838.6131386861314,
      "grad_norm": 11.910520553588867,
      "learning_rate": 8.069343065693431e-06,
      "loss": 1.2423,
      "step": 114890
    },
    {
      "epoch": 838.6861313868613,
      "grad_norm": 7.12200927734375,
      "learning_rate": 8.065693430656935e-06,
      "loss": 0.69,
      "step": 114900
    },
    {
      "epoch": 838.7591240875912,
      "grad_norm": 0.030717970803380013,
      "learning_rate": 8.062043795620439e-06,
      "loss": 0.8439,
      "step": 114910
    },
    {
      "epoch": 838.8321167883212,
      "grad_norm": 7.896255016326904,
      "learning_rate": 8.058394160583941e-06,
      "loss": 0.6495,
      "step": 114920
    },
    {
      "epoch": 838.9051094890511,
      "grad_norm": 9.953322410583496,
      "learning_rate": 8.054744525547446e-06,
      "loss": 1.1913,
      "step": 114930
    },
    {
      "epoch": 838.978102189781,
      "grad_norm": 8.951912879943848,
      "learning_rate": 8.051094890510949e-06,
      "loss": 0.915,
      "step": 114940
    },
    {
      "epoch": 839.051094890511,
      "grad_norm": 11.852831840515137,
      "learning_rate": 8.047445255474453e-06,
      "loss": 0.8581,
      "step": 114950
    },
    {
      "epoch": 839.1240875912408,
      "grad_norm": 5.190783977508545,
      "learning_rate": 8.043795620437956e-06,
      "loss": 1.1308,
      "step": 114960
    },
    {
      "epoch": 839.1970802919708,
      "grad_norm": 14.48788070678711,
      "learning_rate": 8.04014598540146e-06,
      "loss": 1.0111,
      "step": 114970
    },
    {
      "epoch": 839.2700729927008,
      "grad_norm": 5.671236515045166,
      "learning_rate": 8.036496350364964e-06,
      "loss": 0.8057,
      "step": 114980
    },
    {
      "epoch": 839.3430656934306,
      "grad_norm": 16.48225212097168,
      "learning_rate": 8.032846715328468e-06,
      "loss": 1.0317,
      "step": 114990
    },
    {
      "epoch": 839.4160583941606,
      "grad_norm": 9.053750991821289,
      "learning_rate": 8.02919708029197e-06,
      "loss": 0.721,
      "step": 115000
    },
    {
      "epoch": 839.4890510948906,
      "grad_norm": 10.715280532836914,
      "learning_rate": 8.025547445255476e-06,
      "loss": 1.0038,
      "step": 115010
    },
    {
      "epoch": 839.5620437956204,
      "grad_norm": 7.743950366973877,
      "learning_rate": 8.021897810218978e-06,
      "loss": 0.6558,
      "step": 115020
    },
    {
      "epoch": 839.6350364963504,
      "grad_norm": 11.752845764160156,
      "learning_rate": 8.018248175182482e-06,
      "loss": 1.1554,
      "step": 115030
    },
    {
      "epoch": 839.7080291970802,
      "grad_norm": 4.3984456062316895,
      "learning_rate": 8.014598540145986e-06,
      "loss": 0.6338,
      "step": 115040
    },
    {
      "epoch": 839.7810218978102,
      "grad_norm": 8.910198211669922,
      "learning_rate": 8.010948905109488e-06,
      "loss": 0.7574,
      "step": 115050
    },
    {
      "epoch": 839.8540145985402,
      "grad_norm": 9.881856918334961,
      "learning_rate": 8.007299270072994e-06,
      "loss": 0.817,
      "step": 115060
    },
    {
      "epoch": 839.92700729927,
      "grad_norm": 15.288666725158691,
      "learning_rate": 8.003649635036496e-06,
      "loss": 0.9053,
      "step": 115070
    },
    {
      "epoch": 840.0,
      "grad_norm": 2.936856508255005,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.8961,
      "step": 115080
    },
    {
      "epoch": 840.07299270073,
      "grad_norm": 6.667881011962891,
      "learning_rate": 7.996350364963504e-06,
      "loss": 0.5903,
      "step": 115090
    },
    {
      "epoch": 840.1459854014598,
      "grad_norm": 13.030296325683594,
      "learning_rate": 7.992700729927007e-06,
      "loss": 0.9547,
      "step": 115100
    },
    {
      "epoch": 840.2189781021898,
      "grad_norm": 10.476239204406738,
      "learning_rate": 7.989051094890511e-06,
      "loss": 1.2114,
      "step": 115110
    },
    {
      "epoch": 840.2919708029198,
      "grad_norm": 11.284351348876953,
      "learning_rate": 7.985401459854015e-06,
      "loss": 0.835,
      "step": 115120
    },
    {
      "epoch": 840.3649635036496,
      "grad_norm": 19.81720542907715,
      "learning_rate": 7.981751824817519e-06,
      "loss": 0.7864,
      "step": 115130
    },
    {
      "epoch": 840.4379562043796,
      "grad_norm": 17.769071578979492,
      "learning_rate": 7.978102189781023e-06,
      "loss": 1.1436,
      "step": 115140
    },
    {
      "epoch": 840.5109489051094,
      "grad_norm": 13.806524276733398,
      "learning_rate": 7.974452554744525e-06,
      "loss": 1.2393,
      "step": 115150
    },
    {
      "epoch": 840.5839416058394,
      "grad_norm": 7.093770503997803,
      "learning_rate": 7.97080291970803e-06,
      "loss": 0.8724,
      "step": 115160
    },
    {
      "epoch": 840.6569343065694,
      "grad_norm": 7.529989242553711,
      "learning_rate": 7.967153284671533e-06,
      "loss": 0.9377,
      "step": 115170
    },
    {
      "epoch": 840.7299270072992,
      "grad_norm": 12.00415325164795,
      "learning_rate": 7.963503649635037e-06,
      "loss": 0.9113,
      "step": 115180
    },
    {
      "epoch": 840.8029197080292,
      "grad_norm": 5.066011905670166,
      "learning_rate": 7.95985401459854e-06,
      "loss": 0.884,
      "step": 115190
    },
    {
      "epoch": 840.8759124087592,
      "grad_norm": 9.581707000732422,
      "learning_rate": 7.956204379562045e-06,
      "loss": 0.7375,
      "step": 115200
    },
    {
      "epoch": 840.948905109489,
      "grad_norm": 7.700491905212402,
      "learning_rate": 7.952554744525548e-06,
      "loss": 0.7273,
      "step": 115210
    },
    {
      "epoch": 841.021897810219,
      "grad_norm": 11.315330505371094,
      "learning_rate": 7.948905109489052e-06,
      "loss": 0.8974,
      "step": 115220
    },
    {
      "epoch": 841.0948905109489,
      "grad_norm": 5.616428852081299,
      "learning_rate": 7.945255474452554e-06,
      "loss": 0.9812,
      "step": 115230
    },
    {
      "epoch": 841.1678832116788,
      "grad_norm": 6.480992794036865,
      "learning_rate": 7.94160583941606e-06,
      "loss": 0.9854,
      "step": 115240
    },
    {
      "epoch": 841.2408759124088,
      "grad_norm": 9.781682968139648,
      "learning_rate": 7.937956204379562e-06,
      "loss": 1.1013,
      "step": 115250
    },
    {
      "epoch": 841.3138686131387,
      "grad_norm": 0.05435295030474663,
      "learning_rate": 7.934306569343066e-06,
      "loss": 0.9121,
      "step": 115260
    },
    {
      "epoch": 841.3868613138686,
      "grad_norm": 17.326570510864258,
      "learning_rate": 7.93065693430657e-06,
      "loss": 0.9008,
      "step": 115270
    },
    {
      "epoch": 841.4598540145986,
      "grad_norm": 7.345602989196777,
      "learning_rate": 7.927007299270072e-06,
      "loss": 0.6937,
      "step": 115280
    },
    {
      "epoch": 841.5328467153284,
      "grad_norm": 15.009953498840332,
      "learning_rate": 7.923357664233578e-06,
      "loss": 1.0186,
      "step": 115290
    },
    {
      "epoch": 841.6058394160584,
      "grad_norm": 0.08071130514144897,
      "learning_rate": 7.91970802919708e-06,
      "loss": 0.8312,
      "step": 115300
    },
    {
      "epoch": 841.6788321167883,
      "grad_norm": 9.21197509765625,
      "learning_rate": 7.916058394160584e-06,
      "loss": 0.6118,
      "step": 115310
    },
    {
      "epoch": 841.7518248175182,
      "grad_norm": 13.804830551147461,
      "learning_rate": 7.912408759124088e-06,
      "loss": 0.7826,
      "step": 115320
    },
    {
      "epoch": 841.8248175182482,
      "grad_norm": 7.82002067565918,
      "learning_rate": 7.908759124087592e-06,
      "loss": 0.6214,
      "step": 115330
    },
    {
      "epoch": 841.8978102189781,
      "grad_norm": 18.247949600219727,
      "learning_rate": 7.905109489051095e-06,
      "loss": 0.9301,
      "step": 115340
    },
    {
      "epoch": 841.970802919708,
      "grad_norm": 6.838807582855225,
      "learning_rate": 7.9014598540146e-06,
      "loss": 1.0396,
      "step": 115350
    },
    {
      "epoch": 842.043795620438,
      "grad_norm": 12.743155479431152,
      "learning_rate": 7.897810218978102e-06,
      "loss": 0.9451,
      "step": 115360
    },
    {
      "epoch": 842.1167883211679,
      "grad_norm": 6.292281150817871,
      "learning_rate": 7.894160583941607e-06,
      "loss": 1.3386,
      "step": 115370
    },
    {
      "epoch": 842.1897810218978,
      "grad_norm": 7.801581382751465,
      "learning_rate": 7.89051094890511e-06,
      "loss": 0.809,
      "step": 115380
    },
    {
      "epoch": 842.2627737226277,
      "grad_norm": 6.232397556304932,
      "learning_rate": 7.886861313868613e-06,
      "loss": 0.6236,
      "step": 115390
    },
    {
      "epoch": 842.3357664233577,
      "grad_norm": 0.025452783331274986,
      "learning_rate": 7.883211678832117e-06,
      "loss": 1.0283,
      "step": 115400
    },
    {
      "epoch": 842.4087591240876,
      "grad_norm": 5.186251640319824,
      "learning_rate": 7.879562043795621e-06,
      "loss": 0.534,
      "step": 115410
    },
    {
      "epoch": 842.4817518248175,
      "grad_norm": 0.06132727116346359,
      "learning_rate": 7.875912408759125e-06,
      "loss": 0.5967,
      "step": 115420
    },
    {
      "epoch": 842.5547445255474,
      "grad_norm": 12.985995292663574,
      "learning_rate": 7.872262773722629e-06,
      "loss": 1.3621,
      "step": 115430
    },
    {
      "epoch": 842.6277372262774,
      "grad_norm": 0.01947612501680851,
      "learning_rate": 7.868613138686131e-06,
      "loss": 0.7142,
      "step": 115440
    },
    {
      "epoch": 842.7007299270073,
      "grad_norm": 14.004322052001953,
      "learning_rate": 7.864963503649637e-06,
      "loss": 1.2386,
      "step": 115450
    },
    {
      "epoch": 842.7737226277372,
      "grad_norm": 7.857177734375,
      "learning_rate": 7.861313868613139e-06,
      "loss": 0.9111,
      "step": 115460
    },
    {
      "epoch": 842.8467153284671,
      "grad_norm": 5.831774711608887,
      "learning_rate": 7.857664233576643e-06,
      "loss": 0.6552,
      "step": 115470
    },
    {
      "epoch": 842.9197080291971,
      "grad_norm": 8.490013122558594,
      "learning_rate": 7.854014598540146e-06,
      "loss": 1.1102,
      "step": 115480
    },
    {
      "epoch": 842.992700729927,
      "grad_norm": 14.003613471984863,
      "learning_rate": 7.850364963503649e-06,
      "loss": 1.0693,
      "step": 115490
    },
    {
      "epoch": 843.0656934306569,
      "grad_norm": 16.382848739624023,
      "learning_rate": 7.846715328467154e-06,
      "loss": 1.0794,
      "step": 115500
    },
    {
      "epoch": 843.1386861313869,
      "grad_norm": 13.1185884475708,
      "learning_rate": 7.843065693430656e-06,
      "loss": 0.5369,
      "step": 115510
    },
    {
      "epoch": 843.2116788321168,
      "grad_norm": 9.296364784240723,
      "learning_rate": 7.839416058394162e-06,
      "loss": 1.0076,
      "step": 115520
    },
    {
      "epoch": 843.2846715328467,
      "grad_norm": 9.134953498840332,
      "learning_rate": 7.835766423357664e-06,
      "loss": 0.664,
      "step": 115530
    },
    {
      "epoch": 843.3576642335767,
      "grad_norm": 12.520661354064941,
      "learning_rate": 7.832116788321168e-06,
      "loss": 1.1572,
      "step": 115540
    },
    {
      "epoch": 843.4306569343066,
      "grad_norm": 0.01959586702287197,
      "learning_rate": 7.828467153284672e-06,
      "loss": 0.8295,
      "step": 115550
    },
    {
      "epoch": 843.5036496350365,
      "grad_norm": 13.33056640625,
      "learning_rate": 7.824817518248176e-06,
      "loss": 0.7868,
      "step": 115560
    },
    {
      "epoch": 843.5766423357665,
      "grad_norm": 10.058208465576172,
      "learning_rate": 7.82116788321168e-06,
      "loss": 0.8763,
      "step": 115570
    },
    {
      "epoch": 843.6496350364963,
      "grad_norm": 6.481346130371094,
      "learning_rate": 7.817518248175184e-06,
      "loss": 0.6545,
      "step": 115580
    },
    {
      "epoch": 843.7226277372263,
      "grad_norm": 13.856770515441895,
      "learning_rate": 7.813868613138686e-06,
      "loss": 0.8563,
      "step": 115590
    },
    {
      "epoch": 843.7956204379562,
      "grad_norm": 2.608166456222534,
      "learning_rate": 7.810218978102191e-06,
      "loss": 1.0789,
      "step": 115600
    },
    {
      "epoch": 843.8686131386861,
      "grad_norm": 9.918998718261719,
      "learning_rate": 7.806569343065694e-06,
      "loss": 0.8002,
      "step": 115610
    },
    {
      "epoch": 843.9416058394161,
      "grad_norm": 0.5101690292358398,
      "learning_rate": 7.802919708029197e-06,
      "loss": 0.9275,
      "step": 115620
    },
    {
      "epoch": 844.014598540146,
      "grad_norm": 0.08901344239711761,
      "learning_rate": 7.799270072992701e-06,
      "loss": 0.6941,
      "step": 115630
    },
    {
      "epoch": 844.0875912408759,
      "grad_norm": 10.170462608337402,
      "learning_rate": 7.795620437956205e-06,
      "loss": 0.8605,
      "step": 115640
    },
    {
      "epoch": 844.1605839416059,
      "grad_norm": 8.287871360778809,
      "learning_rate": 7.791970802919709e-06,
      "loss": 0.8003,
      "step": 115650
    },
    {
      "epoch": 844.2335766423357,
      "grad_norm": 7.1894330978393555,
      "learning_rate": 7.788321167883213e-06,
      "loss": 1.0346,
      "step": 115660
    },
    {
      "epoch": 844.3065693430657,
      "grad_norm": 9.78623104095459,
      "learning_rate": 7.784671532846715e-06,
      "loss": 1.2802,
      "step": 115670
    },
    {
      "epoch": 844.3795620437957,
      "grad_norm": 5.186701774597168,
      "learning_rate": 7.781021897810219e-06,
      "loss": 0.9053,
      "step": 115680
    },
    {
      "epoch": 844.4525547445255,
      "grad_norm": 9.622989654541016,
      "learning_rate": 7.777372262773723e-06,
      "loss": 0.9991,
      "step": 115690
    },
    {
      "epoch": 844.5255474452555,
      "grad_norm": 6.9947309494018555,
      "learning_rate": 7.773722627737227e-06,
      "loss": 1.0408,
      "step": 115700
    },
    {
      "epoch": 844.5985401459855,
      "grad_norm": 0.07064401358366013,
      "learning_rate": 7.77007299270073e-06,
      "loss": 0.7583,
      "step": 115710
    },
    {
      "epoch": 844.6715328467153,
      "grad_norm": 0.05869613215327263,
      "learning_rate": 7.766423357664233e-06,
      "loss": 0.3844,
      "step": 115720
    },
    {
      "epoch": 844.7445255474453,
      "grad_norm": 17.780288696289062,
      "learning_rate": 7.762773722627738e-06,
      "loss": 1.1628,
      "step": 115730
    },
    {
      "epoch": 844.8175182481751,
      "grad_norm": 0.030015110969543457,
      "learning_rate": 7.75912408759124e-06,
      "loss": 0.9542,
      "step": 115740
    },
    {
      "epoch": 844.8905109489051,
      "grad_norm": 8.173421859741211,
      "learning_rate": 7.755474452554745e-06,
      "loss": 0.8182,
      "step": 115750
    },
    {
      "epoch": 844.9635036496351,
      "grad_norm": 8.624451637268066,
      "learning_rate": 7.751824817518248e-06,
      "loss": 0.6553,
      "step": 115760
    },
    {
      "epoch": 845.0364963503649,
      "grad_norm": 8.061424255371094,
      "learning_rate": 7.748175182481752e-06,
      "loss": 0.7469,
      "step": 115770
    },
    {
      "epoch": 845.1094890510949,
      "grad_norm": 7.816514492034912,
      "learning_rate": 7.744525547445256e-06,
      "loss": 0.627,
      "step": 115780
    },
    {
      "epoch": 845.1824817518249,
      "grad_norm": 7.089133262634277,
      "learning_rate": 7.74087591240876e-06,
      "loss": 0.8402,
      "step": 115790
    },
    {
      "epoch": 845.2554744525547,
      "grad_norm": 10.993902206420898,
      "learning_rate": 7.737226277372262e-06,
      "loss": 1.39,
      "step": 115800
    },
    {
      "epoch": 845.3284671532847,
      "grad_norm": 8.235511779785156,
      "learning_rate": 7.733576642335768e-06,
      "loss": 0.8631,
      "step": 115810
    },
    {
      "epoch": 845.4014598540145,
      "grad_norm": 14.903596878051758,
      "learning_rate": 7.72992700729927e-06,
      "loss": 1.0401,
      "step": 115820
    },
    {
      "epoch": 845.4744525547445,
      "grad_norm": 0.01975865103304386,
      "learning_rate": 7.726277372262774e-06,
      "loss": 0.6508,
      "step": 115830
    },
    {
      "epoch": 845.5474452554745,
      "grad_norm": 0.006468319334089756,
      "learning_rate": 7.722627737226278e-06,
      "loss": 0.9277,
      "step": 115840
    },
    {
      "epoch": 845.6204379562043,
      "grad_norm": 8.12679386138916,
      "learning_rate": 7.718978102189782e-06,
      "loss": 0.7197,
      "step": 115850
    },
    {
      "epoch": 845.6934306569343,
      "grad_norm": 9.479849815368652,
      "learning_rate": 7.715328467153286e-06,
      "loss": 1.0213,
      "step": 115860
    },
    {
      "epoch": 845.7664233576643,
      "grad_norm": 8.02752685546875,
      "learning_rate": 7.71167883211679e-06,
      "loss": 0.8951,
      "step": 115870
    },
    {
      "epoch": 845.8394160583941,
      "grad_norm": 10.763978958129883,
      "learning_rate": 7.708029197080292e-06,
      "loss": 1.2561,
      "step": 115880
    },
    {
      "epoch": 845.9124087591241,
      "grad_norm": 7.683365345001221,
      "learning_rate": 7.704379562043795e-06,
      "loss": 0.6527,
      "step": 115890
    },
    {
      "epoch": 845.985401459854,
      "grad_norm": 0.048327185213565826,
      "learning_rate": 7.7007299270073e-06,
      "loss": 0.6902,
      "step": 115900
    },
    {
      "epoch": 846.0583941605839,
      "grad_norm": 7.246248722076416,
      "learning_rate": 7.697080291970803e-06,
      "loss": 1.105,
      "step": 115910
    },
    {
      "epoch": 846.1313868613139,
      "grad_norm": 0.14202147722244263,
      "learning_rate": 7.693430656934307e-06,
      "loss": 0.9387,
      "step": 115920
    },
    {
      "epoch": 846.2043795620438,
      "grad_norm": 8.329740524291992,
      "learning_rate": 7.68978102189781e-06,
      "loss": 0.9219,
      "step": 115930
    },
    {
      "epoch": 846.2773722627737,
      "grad_norm": 10.95075511932373,
      "learning_rate": 7.686131386861315e-06,
      "loss": 0.6049,
      "step": 115940
    },
    {
      "epoch": 846.3503649635037,
      "grad_norm": 7.654411315917969,
      "learning_rate": 7.682481751824817e-06,
      "loss": 0.8528,
      "step": 115950
    },
    {
      "epoch": 846.4233576642335,
      "grad_norm": 3.6893386840820312,
      "learning_rate": 7.678832116788321e-06,
      "loss": 0.9592,
      "step": 115960
    },
    {
      "epoch": 846.4963503649635,
      "grad_norm": 8.410658836364746,
      "learning_rate": 7.675182481751825e-06,
      "loss": 0.7578,
      "step": 115970
    },
    {
      "epoch": 846.5693430656934,
      "grad_norm": 4.101653575897217,
      "learning_rate": 7.671532846715329e-06,
      "loss": 0.8563,
      "step": 115980
    },
    {
      "epoch": 846.6423357664233,
      "grad_norm": 0.02693933993577957,
      "learning_rate": 7.667883211678833e-06,
      "loss": 0.7136,
      "step": 115990
    },
    {
      "epoch": 846.7153284671533,
      "grad_norm": 9.066803932189941,
      "learning_rate": 7.664233576642336e-06,
      "loss": 0.9831,
      "step": 116000
    },
    {
      "epoch": 846.7883211678832,
      "grad_norm": 7.659029006958008,
      "learning_rate": 7.660583941605839e-06,
      "loss": 0.9792,
      "step": 116010
    },
    {
      "epoch": 846.8613138686131,
      "grad_norm": 9.670389175415039,
      "learning_rate": 7.656934306569344e-06,
      "loss": 0.8089,
      "step": 116020
    },
    {
      "epoch": 846.9343065693431,
      "grad_norm": 16.543071746826172,
      "learning_rate": 7.653284671532846e-06,
      "loss": 0.7913,
      "step": 116030
    },
    {
      "epoch": 847.007299270073,
      "grad_norm": 4.611100196838379,
      "learning_rate": 7.649635036496352e-06,
      "loss": 0.9914,
      "step": 116040
    },
    {
      "epoch": 847.0802919708029,
      "grad_norm": 7.740407466888428,
      "learning_rate": 7.645985401459854e-06,
      "loss": 0.7576,
      "step": 116050
    },
    {
      "epoch": 847.1532846715329,
      "grad_norm": 9.674415588378906,
      "learning_rate": 7.642335766423358e-06,
      "loss": 0.8147,
      "step": 116060
    },
    {
      "epoch": 847.2262773722628,
      "grad_norm": 4.983302593231201,
      "learning_rate": 7.638686131386862e-06,
      "loss": 0.6597,
      "step": 116070
    },
    {
      "epoch": 847.2992700729927,
      "grad_norm": 8.071943283081055,
      "learning_rate": 7.635036496350366e-06,
      "loss": 1.1123,
      "step": 116080
    },
    {
      "epoch": 847.3722627737226,
      "grad_norm": 0.05871330946683884,
      "learning_rate": 7.63138686131387e-06,
      "loss": 1.0668,
      "step": 116090
    },
    {
      "epoch": 847.4452554744526,
      "grad_norm": 9.250951766967773,
      "learning_rate": 7.627737226277374e-06,
      "loss": 0.7298,
      "step": 116100
    },
    {
      "epoch": 847.5182481751825,
      "grad_norm": 0.012053647078573704,
      "learning_rate": 7.624087591240877e-06,
      "loss": 0.6573,
      "step": 116110
    },
    {
      "epoch": 847.5912408759124,
      "grad_norm": 12.08928394317627,
      "learning_rate": 7.62043795620438e-06,
      "loss": 1.0899,
      "step": 116120
    },
    {
      "epoch": 847.6642335766423,
      "grad_norm": 16.272077560424805,
      "learning_rate": 7.6167883211678836e-06,
      "loss": 0.6941,
      "step": 116130
    },
    {
      "epoch": 847.7372262773723,
      "grad_norm": 0.040675096213817596,
      "learning_rate": 7.613138686131387e-06,
      "loss": 0.7127,
      "step": 116140
    },
    {
      "epoch": 847.8102189781022,
      "grad_norm": 10.415338516235352,
      "learning_rate": 7.609489051094891e-06,
      "loss": 0.973,
      "step": 116150
    },
    {
      "epoch": 847.8832116788321,
      "grad_norm": 11.01945972442627,
      "learning_rate": 7.605839416058394e-06,
      "loss": 1.103,
      "step": 116160
    },
    {
      "epoch": 847.956204379562,
      "grad_norm": 8.30556583404541,
      "learning_rate": 7.602189781021898e-06,
      "loss": 1.3045,
      "step": 116170
    },
    {
      "epoch": 848.029197080292,
      "grad_norm": 6.593398094177246,
      "learning_rate": 7.598540145985401e-06,
      "loss": 0.6784,
      "step": 116180
    },
    {
      "epoch": 848.1021897810219,
      "grad_norm": 7.783246994018555,
      "learning_rate": 7.594890510948906e-06,
      "loss": 1.1305,
      "step": 116190
    },
    {
      "epoch": 848.1751824817518,
      "grad_norm": 0.0111979516223073,
      "learning_rate": 7.591240875912409e-06,
      "loss": 0.7807,
      "step": 116200
    },
    {
      "epoch": 848.2481751824818,
      "grad_norm": 11.982748985290527,
      "learning_rate": 7.587591240875913e-06,
      "loss": 0.9958,
      "step": 116210
    },
    {
      "epoch": 848.3211678832117,
      "grad_norm": 0.06323151290416718,
      "learning_rate": 7.583941605839416e-06,
      "loss": 1.1133,
      "step": 116220
    },
    {
      "epoch": 848.3941605839416,
      "grad_norm": 10.110942840576172,
      "learning_rate": 7.580291970802921e-06,
      "loss": 0.7565,
      "step": 116230
    },
    {
      "epoch": 848.4671532846716,
      "grad_norm": 0.020328637212514877,
      "learning_rate": 7.576642335766424e-06,
      "loss": 0.8404,
      "step": 116240
    },
    {
      "epoch": 848.5401459854014,
      "grad_norm": 7.958823204040527,
      "learning_rate": 7.572992700729928e-06,
      "loss": 1.2324,
      "step": 116250
    },
    {
      "epoch": 848.6131386861314,
      "grad_norm": 12.366406440734863,
      "learning_rate": 7.569343065693431e-06,
      "loss": 0.4765,
      "step": 116260
    },
    {
      "epoch": 848.6861313868613,
      "grad_norm": 10.926628112792969,
      "learning_rate": 7.565693430656935e-06,
      "loss": 0.5472,
      "step": 116270
    },
    {
      "epoch": 848.7591240875912,
      "grad_norm": 8.852203369140625,
      "learning_rate": 7.5620437956204384e-06,
      "loss": 0.5878,
      "step": 116280
    },
    {
      "epoch": 848.8321167883212,
      "grad_norm": 12.50948429107666,
      "learning_rate": 7.558394160583942e-06,
      "loss": 0.9218,
      "step": 116290
    },
    {
      "epoch": 848.9051094890511,
      "grad_norm": 17.27522087097168,
      "learning_rate": 7.554744525547445e-06,
      "loss": 1.2504,
      "step": 116300
    },
    {
      "epoch": 848.978102189781,
      "grad_norm": 16.15993309020996,
      "learning_rate": 7.55109489051095e-06,
      "loss": 1.3553,
      "step": 116310
    },
    {
      "epoch": 849.051094890511,
      "grad_norm": 20.512163162231445,
      "learning_rate": 7.547445255474453e-06,
      "loss": 0.6162,
      "step": 116320
    },
    {
      "epoch": 849.1240875912408,
      "grad_norm": 11.952219009399414,
      "learning_rate": 7.543795620437956e-06,
      "loss": 0.7933,
      "step": 116330
    },
    {
      "epoch": 849.1970802919708,
      "grad_norm": 16.34710121154785,
      "learning_rate": 7.54014598540146e-06,
      "loss": 1.0223,
      "step": 116340
    },
    {
      "epoch": 849.2700729927008,
      "grad_norm": 9.135356903076172,
      "learning_rate": 7.536496350364963e-06,
      "loss": 0.9074,
      "step": 116350
    },
    {
      "epoch": 849.3430656934306,
      "grad_norm": 11.561450004577637,
      "learning_rate": 7.532846715328468e-06,
      "loss": 0.8005,
      "step": 116360
    },
    {
      "epoch": 849.4160583941606,
      "grad_norm": 0.9785761833190918,
      "learning_rate": 7.529197080291971e-06,
      "loss": 0.7357,
      "step": 116370
    },
    {
      "epoch": 849.4890510948906,
      "grad_norm": 11.121295928955078,
      "learning_rate": 7.525547445255475e-06,
      "loss": 0.9748,
      "step": 116380
    },
    {
      "epoch": 849.5620437956204,
      "grad_norm": 0.07685784250497818,
      "learning_rate": 7.521897810218978e-06,
      "loss": 0.8997,
      "step": 116390
    },
    {
      "epoch": 849.6350364963504,
      "grad_norm": 14.039875030517578,
      "learning_rate": 7.5182481751824825e-06,
      "loss": 1.3246,
      "step": 116400
    },
    {
      "epoch": 849.7080291970802,
      "grad_norm": 9.97642707824707,
      "learning_rate": 7.5145985401459855e-06,
      "loss": 0.956,
      "step": 116410
    },
    {
      "epoch": 849.7810218978102,
      "grad_norm": 0.03493760898709297,
      "learning_rate": 7.510948905109489e-06,
      "loss": 0.7636,
      "step": 116420
    },
    {
      "epoch": 849.8540145985402,
      "grad_norm": 15.368143081665039,
      "learning_rate": 7.5072992700729924e-06,
      "loss": 0.8936,
      "step": 116430
    },
    {
      "epoch": 849.92700729927,
      "grad_norm": 0.08245106041431427,
      "learning_rate": 7.503649635036497e-06,
      "loss": 0.5365,
      "step": 116440
    },
    {
      "epoch": 850.0,
      "grad_norm": 1.8916749954223633,
      "learning_rate": 7.5e-06,
      "loss": 1.0733,
      "step": 116450
    },
    {
      "epoch": 850.07299270073,
      "grad_norm": 1.6042838096618652,
      "learning_rate": 7.496350364963504e-06,
      "loss": 1.1639,
      "step": 116460
    },
    {
      "epoch": 850.1459854014598,
      "grad_norm": 9.416810989379883,
      "learning_rate": 7.492700729927007e-06,
      "loss": 0.5566,
      "step": 116470
    },
    {
      "epoch": 850.2189781021898,
      "grad_norm": 10.0907564163208,
      "learning_rate": 7.489051094890512e-06,
      "loss": 0.7012,
      "step": 116480
    },
    {
      "epoch": 850.2919708029198,
      "grad_norm": 19.683961868286133,
      "learning_rate": 7.485401459854015e-06,
      "loss": 1.0419,
      "step": 116490
    },
    {
      "epoch": 850.3649635036496,
      "grad_norm": 0.596377968788147,
      "learning_rate": 7.48175182481752e-06,
      "loss": 1.0484,
      "step": 116500
    },
    {
      "epoch": 850.4379562043796,
      "grad_norm": 0.07612024247646332,
      "learning_rate": 7.478102189781022e-06,
      "loss": 0.6985,
      "step": 116510
    },
    {
      "epoch": 850.5109489051094,
      "grad_norm": 7.693037033081055,
      "learning_rate": 7.4744525547445265e-06,
      "loss": 0.7127,
      "step": 116520
    },
    {
      "epoch": 850.5839416058394,
      "grad_norm": 7.975799083709717,
      "learning_rate": 7.4708029197080296e-06,
      "loss": 0.6967,
      "step": 116530
    },
    {
      "epoch": 850.6569343065694,
      "grad_norm": 9.178895950317383,
      "learning_rate": 7.467153284671533e-06,
      "loss": 0.7659,
      "step": 116540
    },
    {
      "epoch": 850.7299270072992,
      "grad_norm": 14.509793281555176,
      "learning_rate": 7.463503649635037e-06,
      "loss": 0.7886,
      "step": 116550
    },
    {
      "epoch": 850.8029197080292,
      "grad_norm": 7.354746341705322,
      "learning_rate": 7.4598540145985395e-06,
      "loss": 0.966,
      "step": 116560
    },
    {
      "epoch": 850.8759124087592,
      "grad_norm": 10.516798973083496,
      "learning_rate": 7.456204379562044e-06,
      "loss": 1.0846,
      "step": 116570
    },
    {
      "epoch": 850.948905109489,
      "grad_norm": 8.734942436218262,
      "learning_rate": 7.452554744525547e-06,
      "loss": 1.3175,
      "step": 116580
    },
    {
      "epoch": 851.021897810219,
      "grad_norm": 11.107503890991211,
      "learning_rate": 7.448905109489052e-06,
      "loss": 1.0036,
      "step": 116590
    },
    {
      "epoch": 851.0948905109489,
      "grad_norm": 11.430773735046387,
      "learning_rate": 7.445255474452554e-06,
      "loss": 1.0048,
      "step": 116600
    },
    {
      "epoch": 851.1678832116788,
      "grad_norm": 7.751433372497559,
      "learning_rate": 7.441605839416059e-06,
      "loss": 0.675,
      "step": 116610
    },
    {
      "epoch": 851.2408759124088,
      "grad_norm": 5.936666488647461,
      "learning_rate": 7.437956204379562e-06,
      "loss": 1.1489,
      "step": 116620
    },
    {
      "epoch": 851.3138686131387,
      "grad_norm": 6.438419818878174,
      "learning_rate": 7.434306569343067e-06,
      "loss": 0.7134,
      "step": 116630
    },
    {
      "epoch": 851.3868613138686,
      "grad_norm": 12.311779022216797,
      "learning_rate": 7.43065693430657e-06,
      "loss": 0.8832,
      "step": 116640
    },
    {
      "epoch": 851.4598540145986,
      "grad_norm": 12.03458023071289,
      "learning_rate": 7.427007299270074e-06,
      "loss": 1.001,
      "step": 116650
    },
    {
      "epoch": 851.5328467153284,
      "grad_norm": 3.7399489879608154,
      "learning_rate": 7.423357664233577e-06,
      "loss": 0.7842,
      "step": 116660
    },
    {
      "epoch": 851.6058394160584,
      "grad_norm": 9.529937744140625,
      "learning_rate": 7.419708029197081e-06,
      "loss": 0.6401,
      "step": 116670
    },
    {
      "epoch": 851.6788321167883,
      "grad_norm": 7.17702054977417,
      "learning_rate": 7.416058394160584e-06,
      "loss": 0.7566,
      "step": 116680
    },
    {
      "epoch": 851.7518248175182,
      "grad_norm": 8.220693588256836,
      "learning_rate": 7.412408759124088e-06,
      "loss": 1.4093,
      "step": 116690
    },
    {
      "epoch": 851.8248175182482,
      "grad_norm": 0.03043556958436966,
      "learning_rate": 7.408759124087591e-06,
      "loss": 0.6926,
      "step": 116700
    },
    {
      "epoch": 851.8978102189781,
      "grad_norm": 11.631091117858887,
      "learning_rate": 7.405109489051096e-06,
      "loss": 0.9335,
      "step": 116710
    },
    {
      "epoch": 851.970802919708,
      "grad_norm": 7.770172119140625,
      "learning_rate": 7.401459854014599e-06,
      "loss": 0.7643,
      "step": 116720
    },
    {
      "epoch": 852.043795620438,
      "grad_norm": 16.46229362487793,
      "learning_rate": 7.397810218978103e-06,
      "loss": 1.0792,
      "step": 116730
    },
    {
      "epoch": 852.1167883211679,
      "grad_norm": 0.057900939136743546,
      "learning_rate": 7.394160583941606e-06,
      "loss": 0.5739,
      "step": 116740
    },
    {
      "epoch": 852.1897810218978,
      "grad_norm": 6.601588249206543,
      "learning_rate": 7.390510948905111e-06,
      "loss": 0.8731,
      "step": 116750
    },
    {
      "epoch": 852.2627737226277,
      "grad_norm": 9.519638061523438,
      "learning_rate": 7.386861313868614e-06,
      "loss": 0.6872,
      "step": 116760
    },
    {
      "epoch": 852.3357664233577,
      "grad_norm": 8.18428897857666,
      "learning_rate": 7.383211678832117e-06,
      "loss": 0.736,
      "step": 116770
    },
    {
      "epoch": 852.4087591240876,
      "grad_norm": 7.219114303588867,
      "learning_rate": 7.379562043795621e-06,
      "loss": 0.9819,
      "step": 116780
    },
    {
      "epoch": 852.4817518248175,
      "grad_norm": 15.341120719909668,
      "learning_rate": 7.375912408759124e-06,
      "loss": 0.9241,
      "step": 116790
    },
    {
      "epoch": 852.5547445255474,
      "grad_norm": 6.615355491638184,
      "learning_rate": 7.3722627737226285e-06,
      "loss": 0.8904,
      "step": 116800
    },
    {
      "epoch": 852.6277372262774,
      "grad_norm": 13.887463569641113,
      "learning_rate": 7.3686131386861315e-06,
      "loss": 1.1459,
      "step": 116810
    },
    {
      "epoch": 852.7007299270073,
      "grad_norm": 0.48581650853157043,
      "learning_rate": 7.364963503649635e-06,
      "loss": 1.2295,
      "step": 116820
    },
    {
      "epoch": 852.7737226277372,
      "grad_norm": 11.70194149017334,
      "learning_rate": 7.361313868613138e-06,
      "loss": 0.8524,
      "step": 116830
    },
    {
      "epoch": 852.8467153284671,
      "grad_norm": 13.543684005737305,
      "learning_rate": 7.357664233576643e-06,
      "loss": 0.5835,
      "step": 116840
    },
    {
      "epoch": 852.9197080291971,
      "grad_norm": 8.624600410461426,
      "learning_rate": 7.354014598540146e-06,
      "loss": 0.6526,
      "step": 116850
    },
    {
      "epoch": 852.992700729927,
      "grad_norm": 7.956599235534668,
      "learning_rate": 7.35036496350365e-06,
      "loss": 0.9957,
      "step": 116860
    },
    {
      "epoch": 853.0656934306569,
      "grad_norm": 0.03654684126377106,
      "learning_rate": 7.346715328467153e-06,
      "loss": 0.4745,
      "step": 116870
    },
    {
      "epoch": 853.1386861313869,
      "grad_norm": 0.05229794979095459,
      "learning_rate": 7.343065693430658e-06,
      "loss": 0.6337,
      "step": 116880
    },
    {
      "epoch": 853.2116788321168,
      "grad_norm": 4.458669662475586,
      "learning_rate": 7.339416058394161e-06,
      "loss": 0.7874,
      "step": 116890
    },
    {
      "epoch": 853.2846715328467,
      "grad_norm": 7.608589172363281,
      "learning_rate": 7.335766423357665e-06,
      "loss": 1.1956,
      "step": 116900
    },
    {
      "epoch": 853.3576642335767,
      "grad_norm": 8.105937004089355,
      "learning_rate": 7.332116788321168e-06,
      "loss": 0.9032,
      "step": 116910
    },
    {
      "epoch": 853.4306569343066,
      "grad_norm": 5.349018096923828,
      "learning_rate": 7.3284671532846725e-06,
      "loss": 0.9104,
      "step": 116920
    },
    {
      "epoch": 853.5036496350365,
      "grad_norm": 8.041027069091797,
      "learning_rate": 7.3248175182481755e-06,
      "loss": 1.0255,
      "step": 116930
    },
    {
      "epoch": 853.5766423357665,
      "grad_norm": 8.368589401245117,
      "learning_rate": 7.321167883211679e-06,
      "loss": 0.5449,
      "step": 116940
    },
    {
      "epoch": 853.6496350364963,
      "grad_norm": 9.162720680236816,
      "learning_rate": 7.3175182481751825e-06,
      "loss": 0.694,
      "step": 116950
    },
    {
      "epoch": 853.7226277372263,
      "grad_norm": 15.165163040161133,
      "learning_rate": 7.313868613138687e-06,
      "loss": 0.9357,
      "step": 116960
    },
    {
      "epoch": 853.7956204379562,
      "grad_norm": 12.85975456237793,
      "learning_rate": 7.31021897810219e-06,
      "loss": 0.9908,
      "step": 116970
    },
    {
      "epoch": 853.8686131386861,
      "grad_norm": 17.8301944732666,
      "learning_rate": 7.306569343065693e-06,
      "loss": 0.9699,
      "step": 116980
    },
    {
      "epoch": 853.9416058394161,
      "grad_norm": 12.73166275024414,
      "learning_rate": 7.302919708029197e-06,
      "loss": 0.9776,
      "step": 116990
    },
    {
      "epoch": 854.014598540146,
      "grad_norm": 11.472143173217773,
      "learning_rate": 7.2992700729927e-06,
      "loss": 1.0418,
      "step": 117000
    },
    {
      "epoch": 854.0875912408759,
      "grad_norm": 17.289064407348633,
      "learning_rate": 7.295620437956205e-06,
      "loss": 1.0331,
      "step": 117010
    },
    {
      "epoch": 854.1605839416059,
      "grad_norm": 0.03405999764800072,
      "learning_rate": 7.291970802919708e-06,
      "loss": 0.556,
      "step": 117020
    },
    {
      "epoch": 854.2335766423357,
      "grad_norm": 9.045066833496094,
      "learning_rate": 7.288321167883212e-06,
      "loss": 0.706,
      "step": 117030
    },
    {
      "epoch": 854.3065693430657,
      "grad_norm": 0.03417909890413284,
      "learning_rate": 7.284671532846715e-06,
      "loss": 0.9918,
      "step": 117040
    },
    {
      "epoch": 854.3795620437957,
      "grad_norm": 10.06166934967041,
      "learning_rate": 7.28102189781022e-06,
      "loss": 0.829,
      "step": 117050
    },
    {
      "epoch": 854.4525547445255,
      "grad_norm": 13.457202911376953,
      "learning_rate": 7.277372262773723e-06,
      "loss": 0.957,
      "step": 117060
    },
    {
      "epoch": 854.5255474452555,
      "grad_norm": 13.132153511047363,
      "learning_rate": 7.273722627737227e-06,
      "loss": 0.8555,
      "step": 117070
    },
    {
      "epoch": 854.5985401459855,
      "grad_norm": 0.05619281530380249,
      "learning_rate": 7.2700729927007295e-06,
      "loss": 0.7691,
      "step": 117080
    },
    {
      "epoch": 854.6715328467153,
      "grad_norm": 12.502955436706543,
      "learning_rate": 7.266423357664234e-06,
      "loss": 0.7355,
      "step": 117090
    },
    {
      "epoch": 854.7445255474453,
      "grad_norm": 2.2666432857513428,
      "learning_rate": 7.262773722627737e-06,
      "loss": 1.0982,
      "step": 117100
    },
    {
      "epoch": 854.8175182481751,
      "grad_norm": 20.484399795532227,
      "learning_rate": 7.259124087591242e-06,
      "loss": 0.9948,
      "step": 117110
    },
    {
      "epoch": 854.8905109489051,
      "grad_norm": 15.149260520935059,
      "learning_rate": 7.255474452554745e-06,
      "loss": 0.8673,
      "step": 117120
    },
    {
      "epoch": 854.9635036496351,
      "grad_norm": 0.055577944964170456,
      "learning_rate": 7.251824817518249e-06,
      "loss": 0.6172,
      "step": 117130
    },
    {
      "epoch": 855.0364963503649,
      "grad_norm": 16.17525291442871,
      "learning_rate": 7.248175182481752e-06,
      "loss": 1.1804,
      "step": 117140
    },
    {
      "epoch": 855.1094890510949,
      "grad_norm": 6.194034576416016,
      "learning_rate": 7.244525547445257e-06,
      "loss": 1.0762,
      "step": 117150
    },
    {
      "epoch": 855.1824817518249,
      "grad_norm": 1.003995656967163,
      "learning_rate": 7.24087591240876e-06,
      "loss": 0.7713,
      "step": 117160
    },
    {
      "epoch": 855.2554744525547,
      "grad_norm": 5.588256359100342,
      "learning_rate": 7.237226277372264e-06,
      "loss": 0.993,
      "step": 117170
    },
    {
      "epoch": 855.3284671532847,
      "grad_norm": 9.402531623840332,
      "learning_rate": 7.233576642335767e-06,
      "loss": 0.6896,
      "step": 117180
    },
    {
      "epoch": 855.4014598540145,
      "grad_norm": 9.235547065734863,
      "learning_rate": 7.22992700729927e-06,
      "loss": 1.1229,
      "step": 117190
    },
    {
      "epoch": 855.4744525547445,
      "grad_norm": 13.764280319213867,
      "learning_rate": 7.2262773722627744e-06,
      "loss": 0.8693,
      "step": 117200
    },
    {
      "epoch": 855.5474452554745,
      "grad_norm": 0.15635843575000763,
      "learning_rate": 7.2226277372262775e-06,
      "loss": 0.8563,
      "step": 117210
    },
    {
      "epoch": 855.6204379562043,
      "grad_norm": 7.706189155578613,
      "learning_rate": 7.218978102189781e-06,
      "loss": 0.8572,
      "step": 117220
    },
    {
      "epoch": 855.6934306569343,
      "grad_norm": 16.132362365722656,
      "learning_rate": 7.215328467153284e-06,
      "loss": 0.9198,
      "step": 117230
    },
    {
      "epoch": 855.7664233576643,
      "grad_norm": 0.04106684401631355,
      "learning_rate": 7.211678832116789e-06,
      "loss": 0.7861,
      "step": 117240
    },
    {
      "epoch": 855.8394160583941,
      "grad_norm": 14.185548782348633,
      "learning_rate": 7.208029197080292e-06,
      "loss": 0.8375,
      "step": 117250
    },
    {
      "epoch": 855.9124087591241,
      "grad_norm": 15.317281723022461,
      "learning_rate": 7.204379562043796e-06,
      "loss": 1.13,
      "step": 117260
    },
    {
      "epoch": 855.985401459854,
      "grad_norm": 0.046325765550136566,
      "learning_rate": 7.200729927007299e-06,
      "loss": 0.6562,
      "step": 117270
    },
    {
      "epoch": 856.0583941605839,
      "grad_norm": 9.706928253173828,
      "learning_rate": 7.197080291970804e-06,
      "loss": 0.9252,
      "step": 117280
    },
    {
      "epoch": 856.1313868613139,
      "grad_norm": 6.820965766906738,
      "learning_rate": 7.193430656934307e-06,
      "loss": 1.1719,
      "step": 117290
    },
    {
      "epoch": 856.2043795620438,
      "grad_norm": 9.02872085571289,
      "learning_rate": 7.189781021897811e-06,
      "loss": 0.6385,
      "step": 117300
    },
    {
      "epoch": 856.2773722627737,
      "grad_norm": 10.540407180786133,
      "learning_rate": 7.186131386861314e-06,
      "loss": 0.809,
      "step": 117310
    },
    {
      "epoch": 856.3503649635037,
      "grad_norm": 6.79636812210083,
      "learning_rate": 7.1824817518248185e-06,
      "loss": 1.1722,
      "step": 117320
    },
    {
      "epoch": 856.4233576642335,
      "grad_norm": 6.939676761627197,
      "learning_rate": 7.1788321167883215e-06,
      "loss": 0.6925,
      "step": 117330
    },
    {
      "epoch": 856.4963503649635,
      "grad_norm": 10.483561515808105,
      "learning_rate": 7.175182481751825e-06,
      "loss": 0.8819,
      "step": 117340
    },
    {
      "epoch": 856.5693430656934,
      "grad_norm": 18.87523078918457,
      "learning_rate": 7.1715328467153284e-06,
      "loss": 1.2185,
      "step": 117350
    },
    {
      "epoch": 856.6423357664233,
      "grad_norm": 7.056513786315918,
      "learning_rate": 7.167883211678833e-06,
      "loss": 1.0535,
      "step": 117360
    },
    {
      "epoch": 856.7153284671533,
      "grad_norm": 9.134424209594727,
      "learning_rate": 7.164233576642336e-06,
      "loss": 0.9083,
      "step": 117370
    },
    {
      "epoch": 856.7883211678832,
      "grad_norm": 6.3404316902160645,
      "learning_rate": 7.16058394160584e-06,
      "loss": 0.9002,
      "step": 117380
    },
    {
      "epoch": 856.8613138686131,
      "grad_norm": 9.528668403625488,
      "learning_rate": 7.156934306569343e-06,
      "loss": 0.8357,
      "step": 117390
    },
    {
      "epoch": 856.9343065693431,
      "grad_norm": 7.369312286376953,
      "learning_rate": 7.153284671532846e-06,
      "loss": 0.3835,
      "step": 117400
    },
    {
      "epoch": 857.007299270073,
      "grad_norm": 0.09191148728132248,
      "learning_rate": 7.149635036496351e-06,
      "loss": 0.9093,
      "step": 117410
    },
    {
      "epoch": 857.0802919708029,
      "grad_norm": 16.116519927978516,
      "learning_rate": 7.145985401459854e-06,
      "loss": 1.0589,
      "step": 117420
    },
    {
      "epoch": 857.1532846715329,
      "grad_norm": 7.945804119110107,
      "learning_rate": 7.142335766423358e-06,
      "loss": 1.0026,
      "step": 117430
    },
    {
      "epoch": 857.2262773722628,
      "grad_norm": 1.6320397853851318,
      "learning_rate": 7.138686131386861e-06,
      "loss": 0.6379,
      "step": 117440
    },
    {
      "epoch": 857.2992700729927,
      "grad_norm": 7.545466899871826,
      "learning_rate": 7.1350364963503656e-06,
      "loss": 0.9629,
      "step": 117450
    },
    {
      "epoch": 857.3722627737226,
      "grad_norm": 0.01370801031589508,
      "learning_rate": 7.131386861313869e-06,
      "loss": 0.8875,
      "step": 117460
    },
    {
      "epoch": 857.4452554744526,
      "grad_norm": 10.674983978271484,
      "learning_rate": 7.1277372262773725e-06,
      "loss": 1.0197,
      "step": 117470
    },
    {
      "epoch": 857.5182481751825,
      "grad_norm": 12.705281257629395,
      "learning_rate": 7.1240875912408755e-06,
      "loss": 1.0691,
      "step": 117480
    },
    {
      "epoch": 857.5912408759124,
      "grad_norm": 9.953264236450195,
      "learning_rate": 7.12043795620438e-06,
      "loss": 0.9208,
      "step": 117490
    },
    {
      "epoch": 857.6642335766423,
      "grad_norm": 14.275629997253418,
      "learning_rate": 7.116788321167883e-06,
      "loss": 1.2694,
      "step": 117500
    },
    {
      "epoch": 857.7372262773723,
      "grad_norm": 8.783675193786621,
      "learning_rate": 7.113138686131387e-06,
      "loss": 0.6667,
      "step": 117510
    },
    {
      "epoch": 857.8102189781022,
      "grad_norm": 16.234874725341797,
      "learning_rate": 7.10948905109489e-06,
      "loss": 0.9802,
      "step": 117520
    },
    {
      "epoch": 857.8832116788321,
      "grad_norm": 4.080787181854248,
      "learning_rate": 7.105839416058395e-06,
      "loss": 0.7266,
      "step": 117530
    },
    {
      "epoch": 857.956204379562,
      "grad_norm": 10.91936206817627,
      "learning_rate": 7.102189781021898e-06,
      "loss": 0.8744,
      "step": 117540
    },
    {
      "epoch": 858.029197080292,
      "grad_norm": 12.509730339050293,
      "learning_rate": 7.098540145985403e-06,
      "loss": 0.7027,
      "step": 117550
    },
    {
      "epoch": 858.1021897810219,
      "grad_norm": 17.734052658081055,
      "learning_rate": 7.094890510948905e-06,
      "loss": 0.7293,
      "step": 117560
    },
    {
      "epoch": 858.1751824817518,
      "grad_norm": 19.3726806640625,
      "learning_rate": 7.09124087591241e-06,
      "loss": 0.7486,
      "step": 117570
    },
    {
      "epoch": 858.2481751824818,
      "grad_norm": 8.289531707763672,
      "learning_rate": 7.087591240875913e-06,
      "loss": 0.6701,
      "step": 117580
    },
    {
      "epoch": 858.3211678832117,
      "grad_norm": 12.77011775970459,
      "learning_rate": 7.083941605839417e-06,
      "loss": 1.2385,
      "step": 117590
    },
    {
      "epoch": 858.3941605839416,
      "grad_norm": 7.740964889526367,
      "learning_rate": 7.08029197080292e-06,
      "loss": 0.838,
      "step": 117600
    },
    {
      "epoch": 858.4671532846716,
      "grad_norm": 0.034522950649261475,
      "learning_rate": 7.076642335766424e-06,
      "loss": 0.6104,
      "step": 117610
    },
    {
      "epoch": 858.5401459854014,
      "grad_norm": 7.469590187072754,
      "learning_rate": 7.072992700729927e-06,
      "loss": 1.1943,
      "step": 117620
    },
    {
      "epoch": 858.6131386861314,
      "grad_norm": 0.03630803897976875,
      "learning_rate": 7.06934306569343e-06,
      "loss": 0.7942,
      "step": 117630
    },
    {
      "epoch": 858.6861313868613,
      "grad_norm": 0.053553346544504166,
      "learning_rate": 7.065693430656935e-06,
      "loss": 1.0745,
      "step": 117640
    },
    {
      "epoch": 858.7591240875912,
      "grad_norm": 9.81174373626709,
      "learning_rate": 7.062043795620438e-06,
      "loss": 0.7842,
      "step": 117650
    },
    {
      "epoch": 858.8321167883212,
      "grad_norm": 9.784652709960938,
      "learning_rate": 7.058394160583942e-06,
      "loss": 0.9189,
      "step": 117660
    },
    {
      "epoch": 858.9051094890511,
      "grad_norm": 5.440174102783203,
      "learning_rate": 7.054744525547445e-06,
      "loss": 0.8612,
      "step": 117670
    },
    {
      "epoch": 858.978102189781,
      "grad_norm": 9.43969440460205,
      "learning_rate": 7.05109489051095e-06,
      "loss": 0.8131,
      "step": 117680
    },
    {
      "epoch": 859.051094890511,
      "grad_norm": 0.05648336932063103,
      "learning_rate": 7.047445255474453e-06,
      "loss": 0.9977,
      "step": 117690
    },
    {
      "epoch": 859.1240875912408,
      "grad_norm": 0.045974843204021454,
      "learning_rate": 7.043795620437957e-06,
      "loss": 0.9991,
      "step": 117700
    },
    {
      "epoch": 859.1970802919708,
      "grad_norm": 14.110464096069336,
      "learning_rate": 7.04014598540146e-06,
      "loss": 1.098,
      "step": 117710
    },
    {
      "epoch": 859.2700729927008,
      "grad_norm": 0.018347367644309998,
      "learning_rate": 7.0364963503649645e-06,
      "loss": 1.1309,
      "step": 117720
    },
    {
      "epoch": 859.3430656934306,
      "grad_norm": 1.3168810606002808,
      "learning_rate": 7.0328467153284675e-06,
      "loss": 0.4346,
      "step": 117730
    },
    {
      "epoch": 859.4160583941606,
      "grad_norm": 7.65214204788208,
      "learning_rate": 7.029197080291971e-06,
      "loss": 0.9834,
      "step": 117740
    },
    {
      "epoch": 859.4890510948906,
      "grad_norm": 12.375632286071777,
      "learning_rate": 7.025547445255474e-06,
      "loss": 1.0133,
      "step": 117750
    },
    {
      "epoch": 859.5620437956204,
      "grad_norm": 11.792946815490723,
      "learning_rate": 7.021897810218979e-06,
      "loss": 0.4289,
      "step": 117760
    },
    {
      "epoch": 859.6350364963504,
      "grad_norm": 10.838789939880371,
      "learning_rate": 7.018248175182482e-06,
      "loss": 0.7942,
      "step": 117770
    },
    {
      "epoch": 859.7080291970802,
      "grad_norm": 10.154190063476562,
      "learning_rate": 7.014598540145986e-06,
      "loss": 0.4959,
      "step": 117780
    },
    {
      "epoch": 859.7810218978102,
      "grad_norm": 7.585804462432861,
      "learning_rate": 7.010948905109489e-06,
      "loss": 1.1192,
      "step": 117790
    },
    {
      "epoch": 859.8540145985402,
      "grad_norm": 0.042931973934173584,
      "learning_rate": 7.007299270072994e-06,
      "loss": 1.1673,
      "step": 117800
    },
    {
      "epoch": 859.92700729927,
      "grad_norm": 11.256805419921875,
      "learning_rate": 7.003649635036497e-06,
      "loss": 0.7299,
      "step": 117810
    },
    {
      "epoch": 860.0,
      "grad_norm": 0.09473364055156708,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.9612,
      "step": 117820
    },
    {
      "epoch": 860.07299270073,
      "grad_norm": 10.856084823608398,
      "learning_rate": 6.996350364963504e-06,
      "loss": 0.5275,
      "step": 117830
    },
    {
      "epoch": 860.1459854014598,
      "grad_norm": 0.05363183468580246,
      "learning_rate": 6.992700729927007e-06,
      "loss": 0.8016,
      "step": 117840
    },
    {
      "epoch": 860.2189781021898,
      "grad_norm": 0.03689482808113098,
      "learning_rate": 6.9890510948905115e-06,
      "loss": 0.7207,
      "step": 117850
    },
    {
      "epoch": 860.2919708029198,
      "grad_norm": 12.19275188446045,
      "learning_rate": 6.985401459854015e-06,
      "loss": 1.1193,
      "step": 117860
    },
    {
      "epoch": 860.3649635036496,
      "grad_norm": 12.799311637878418,
      "learning_rate": 6.9817518248175185e-06,
      "loss": 1.0859,
      "step": 117870
    },
    {
      "epoch": 860.4379562043796,
      "grad_norm": 0.041077062487602234,
      "learning_rate": 6.9781021897810215e-06,
      "loss": 0.9326,
      "step": 117880
    },
    {
      "epoch": 860.5109489051094,
      "grad_norm": 9.524436950683594,
      "learning_rate": 6.974452554744526e-06,
      "loss": 0.9272,
      "step": 117890
    },
    {
      "epoch": 860.5839416058394,
      "grad_norm": 0.02422751486301422,
      "learning_rate": 6.970802919708029e-06,
      "loss": 0.7386,
      "step": 117900
    },
    {
      "epoch": 860.6569343065694,
      "grad_norm": 8.905745506286621,
      "learning_rate": 6.967153284671533e-06,
      "loss": 1.1017,
      "step": 117910
    },
    {
      "epoch": 860.7299270072992,
      "grad_norm": 6.616739749908447,
      "learning_rate": 6.963503649635036e-06,
      "loss": 0.935,
      "step": 117920
    },
    {
      "epoch": 860.8029197080292,
      "grad_norm": 9.643144607543945,
      "learning_rate": 6.959854014598541e-06,
      "loss": 1.0823,
      "step": 117930
    },
    {
      "epoch": 860.8759124087592,
      "grad_norm": 9.169013023376465,
      "learning_rate": 6.956204379562044e-06,
      "loss": 0.6664,
      "step": 117940
    },
    {
      "epoch": 860.948905109489,
      "grad_norm": 9.159414291381836,
      "learning_rate": 6.952554744525548e-06,
      "loss": 0.7572,
      "step": 117950
    },
    {
      "epoch": 861.021897810219,
      "grad_norm": 13.037918090820312,
      "learning_rate": 6.948905109489051e-06,
      "loss": 1.2113,
      "step": 117960
    },
    {
      "epoch": 861.0948905109489,
      "grad_norm": 0.11954785138368607,
      "learning_rate": 6.945255474452556e-06,
      "loss": 1.0519,
      "step": 117970
    },
    {
      "epoch": 861.1678832116788,
      "grad_norm": 20.439496994018555,
      "learning_rate": 6.941605839416059e-06,
      "loss": 0.8357,
      "step": 117980
    },
    {
      "epoch": 861.2408759124088,
      "grad_norm": 14.955695152282715,
      "learning_rate": 6.9379562043795625e-06,
      "loss": 1.0317,
      "step": 117990
    },
    {
      "epoch": 861.3138686131387,
      "grad_norm": 18.03053092956543,
      "learning_rate": 6.9343065693430655e-06,
      "loss": 0.8043,
      "step": 118000
    },
    {
      "epoch": 861.3868613138686,
      "grad_norm": 16.537109375,
      "learning_rate": 6.93065693430657e-06,
      "loss": 0.7057,
      "step": 118010
    },
    {
      "epoch": 861.4598540145986,
      "grad_norm": 12.764894485473633,
      "learning_rate": 6.927007299270073e-06,
      "loss": 1.1782,
      "step": 118020
    },
    {
      "epoch": 861.5328467153284,
      "grad_norm": 6.93806791305542,
      "learning_rate": 6.923357664233578e-06,
      "loss": 0.7549,
      "step": 118030
    },
    {
      "epoch": 861.6058394160584,
      "grad_norm": 4.683284282684326,
      "learning_rate": 6.91970802919708e-06,
      "loss": 1.0566,
      "step": 118040
    },
    {
      "epoch": 861.6788321167883,
      "grad_norm": 1.7766392230987549,
      "learning_rate": 6.916058394160583e-06,
      "loss": 0.4886,
      "step": 118050
    },
    {
      "epoch": 861.7518248175182,
      "grad_norm": 0.015346757136285305,
      "learning_rate": 6.912408759124088e-06,
      "loss": 0.8504,
      "step": 118060
    },
    {
      "epoch": 861.8248175182482,
      "grad_norm": 0.09740832448005676,
      "learning_rate": 6.908759124087591e-06,
      "loss": 0.4087,
      "step": 118070
    },
    {
      "epoch": 861.8978102189781,
      "grad_norm": 8.0993070602417,
      "learning_rate": 6.905109489051096e-06,
      "loss": 0.6317,
      "step": 118080
    },
    {
      "epoch": 861.970802919708,
      "grad_norm": 12.416190147399902,
      "learning_rate": 6.901459854014598e-06,
      "loss": 1.1619,
      "step": 118090
    },
    {
      "epoch": 862.043795620438,
      "grad_norm": 0.011227494105696678,
      "learning_rate": 6.897810218978103e-06,
      "loss": 1.2014,
      "step": 118100
    },
    {
      "epoch": 862.1167883211679,
      "grad_norm": 18.602031707763672,
      "learning_rate": 6.894160583941606e-06,
      "loss": 1.4403,
      "step": 118110
    },
    {
      "epoch": 862.1897810218978,
      "grad_norm": 14.005257606506348,
      "learning_rate": 6.8905109489051104e-06,
      "loss": 0.8341,
      "step": 118120
    },
    {
      "epoch": 862.2627737226277,
      "grad_norm": 8.313267707824707,
      "learning_rate": 6.8868613138686135e-06,
      "loss": 0.9068,
      "step": 118130
    },
    {
      "epoch": 862.3357664233577,
      "grad_norm": 7.867526531219482,
      "learning_rate": 6.883211678832117e-06,
      "loss": 0.72,
      "step": 118140
    },
    {
      "epoch": 862.4087591240876,
      "grad_norm": 10.679948806762695,
      "learning_rate": 6.87956204379562e-06,
      "loss": 0.7903,
      "step": 118150
    },
    {
      "epoch": 862.4817518248175,
      "grad_norm": 6.731922626495361,
      "learning_rate": 6.875912408759125e-06,
      "loss": 0.9774,
      "step": 118160
    },
    {
      "epoch": 862.5547445255474,
      "grad_norm": 13.374309539794922,
      "learning_rate": 6.872262773722628e-06,
      "loss": 0.7637,
      "step": 118170
    },
    {
      "epoch": 862.6277372262774,
      "grad_norm": 11.22154426574707,
      "learning_rate": 6.868613138686132e-06,
      "loss": 0.8892,
      "step": 118180
    },
    {
      "epoch": 862.7007299270073,
      "grad_norm": 12.041971206665039,
      "learning_rate": 6.864963503649635e-06,
      "loss": 0.81,
      "step": 118190
    },
    {
      "epoch": 862.7737226277372,
      "grad_norm": 13.035183906555176,
      "learning_rate": 6.86131386861314e-06,
      "loss": 0.5785,
      "step": 118200
    },
    {
      "epoch": 862.8467153284671,
      "grad_norm": 0.012337173335254192,
      "learning_rate": 6.857664233576643e-06,
      "loss": 0.6437,
      "step": 118210
    },
    {
      "epoch": 862.9197080291971,
      "grad_norm": 12.662226676940918,
      "learning_rate": 6.854014598540147e-06,
      "loss": 0.9357,
      "step": 118220
    },
    {
      "epoch": 862.992700729927,
      "grad_norm": 10.828088760375977,
      "learning_rate": 6.85036496350365e-06,
      "loss": 0.9471,
      "step": 118230
    },
    {
      "epoch": 863.0656934306569,
      "grad_norm": 9.80863094329834,
      "learning_rate": 6.8467153284671545e-06,
      "loss": 0.8068,
      "step": 118240
    },
    {
      "epoch": 863.1386861313869,
      "grad_norm": 13.218018531799316,
      "learning_rate": 6.8430656934306575e-06,
      "loss": 0.5529,
      "step": 118250
    },
    {
      "epoch": 863.2116788321168,
      "grad_norm": 9.34282398223877,
      "learning_rate": 6.8394160583941606e-06,
      "loss": 0.9837,
      "step": 118260
    },
    {
      "epoch": 863.2846715328467,
      "grad_norm": 10.033285140991211,
      "learning_rate": 6.8357664233576644e-06,
      "loss": 0.7656,
      "step": 118270
    },
    {
      "epoch": 863.3576642335767,
      "grad_norm": 8.122559547424316,
      "learning_rate": 6.8321167883211675e-06,
      "loss": 0.9002,
      "step": 118280
    },
    {
      "epoch": 863.4306569343066,
      "grad_norm": 8.061614036560059,
      "learning_rate": 6.828467153284672e-06,
      "loss": 1.2036,
      "step": 118290
    },
    {
      "epoch": 863.5036496350365,
      "grad_norm": 13.263825416564941,
      "learning_rate": 6.824817518248175e-06,
      "loss": 1.0378,
      "step": 118300
    },
    {
      "epoch": 863.5766423357665,
      "grad_norm": 12.83749771118164,
      "learning_rate": 6.821167883211679e-06,
      "loss": 1.0456,
      "step": 118310
    },
    {
      "epoch": 863.6496350364963,
      "grad_norm": 7.464545726776123,
      "learning_rate": 6.817518248175182e-06,
      "loss": 0.9852,
      "step": 118320
    },
    {
      "epoch": 863.7226277372263,
      "grad_norm": 10.31368637084961,
      "learning_rate": 6.813868613138687e-06,
      "loss": 1.0744,
      "step": 118330
    },
    {
      "epoch": 863.7956204379562,
      "grad_norm": 15.989221572875977,
      "learning_rate": 6.81021897810219e-06,
      "loss": 0.4162,
      "step": 118340
    },
    {
      "epoch": 863.8686131386861,
      "grad_norm": 14.545548439025879,
      "learning_rate": 6.806569343065694e-06,
      "loss": 0.9349,
      "step": 118350
    },
    {
      "epoch": 863.9416058394161,
      "grad_norm": 17.002525329589844,
      "learning_rate": 6.802919708029197e-06,
      "loss": 0.8724,
      "step": 118360
    },
    {
      "epoch": 864.014598540146,
      "grad_norm": 13.998339653015137,
      "learning_rate": 6.7992700729927016e-06,
      "loss": 0.8351,
      "step": 118370
    },
    {
      "epoch": 864.0875912408759,
      "grad_norm": 18.788650512695312,
      "learning_rate": 6.795620437956205e-06,
      "loss": 1.0053,
      "step": 118380
    },
    {
      "epoch": 864.1605839416059,
      "grad_norm": 10.43912410736084,
      "learning_rate": 6.7919708029197085e-06,
      "loss": 0.6823,
      "step": 118390
    },
    {
      "epoch": 864.2335766423357,
      "grad_norm": 14.142389297485352,
      "learning_rate": 6.7883211678832115e-06,
      "loss": 0.8296,
      "step": 118400
    },
    {
      "epoch": 864.3065693430657,
      "grad_norm": 10.512001037597656,
      "learning_rate": 6.784671532846716e-06,
      "loss": 0.911,
      "step": 118410
    },
    {
      "epoch": 864.3795620437957,
      "grad_norm": 0.07234715670347214,
      "learning_rate": 6.781021897810219e-06,
      "loss": 1.0358,
      "step": 118420
    },
    {
      "epoch": 864.4525547445255,
      "grad_norm": 0.024859707802534103,
      "learning_rate": 6.777372262773723e-06,
      "loss": 0.4994,
      "step": 118430
    },
    {
      "epoch": 864.5255474452555,
      "grad_norm": 14.910436630249023,
      "learning_rate": 6.773722627737226e-06,
      "loss": 1.1049,
      "step": 118440
    },
    {
      "epoch": 864.5985401459855,
      "grad_norm": 5.930897235870361,
      "learning_rate": 6.770072992700731e-06,
      "loss": 0.9494,
      "step": 118450
    },
    {
      "epoch": 864.6715328467153,
      "grad_norm": 10.041918754577637,
      "learning_rate": 6.766423357664234e-06,
      "loss": 1.0336,
      "step": 118460
    },
    {
      "epoch": 864.7445255474453,
      "grad_norm": 2.6434624195098877,
      "learning_rate": 6.762773722627738e-06,
      "loss": 0.695,
      "step": 118470
    },
    {
      "epoch": 864.8175182481751,
      "grad_norm": 0.01664331555366516,
      "learning_rate": 6.759124087591241e-06,
      "loss": 0.533,
      "step": 118480
    },
    {
      "epoch": 864.8905109489051,
      "grad_norm": 7.766096591949463,
      "learning_rate": 6.755474452554744e-06,
      "loss": 0.8785,
      "step": 118490
    },
    {
      "epoch": 864.9635036496351,
      "grad_norm": 11.584137916564941,
      "learning_rate": 6.751824817518249e-06,
      "loss": 0.8559,
      "step": 118500
    },
    {
      "epoch": 865.0364963503649,
      "grad_norm": 6.873091220855713,
      "learning_rate": 6.748175182481752e-06,
      "loss": 1.1209,
      "step": 118510
    },
    {
      "epoch": 865.1094890510949,
      "grad_norm": 0.11769987642765045,
      "learning_rate": 6.7445255474452556e-06,
      "loss": 0.7966,
      "step": 118520
    },
    {
      "epoch": 865.1824817518249,
      "grad_norm": 16.34061622619629,
      "learning_rate": 6.740875912408759e-06,
      "loss": 0.7802,
      "step": 118530
    },
    {
      "epoch": 865.2554744525547,
      "grad_norm": 17.36888885498047,
      "learning_rate": 6.737226277372263e-06,
      "loss": 0.574,
      "step": 118540
    },
    {
      "epoch": 865.3284671532847,
      "grad_norm": 9.1724853515625,
      "learning_rate": 6.733576642335766e-06,
      "loss": 0.8587,
      "step": 118550
    },
    {
      "epoch": 865.4014598540145,
      "grad_norm": 0.05355997011065483,
      "learning_rate": 6.72992700729927e-06,
      "loss": 1.0008,
      "step": 118560
    },
    {
      "epoch": 865.4744525547445,
      "grad_norm": 8.935140609741211,
      "learning_rate": 6.726277372262773e-06,
      "loss": 1.0159,
      "step": 118570
    },
    {
      "epoch": 865.5474452554745,
      "grad_norm": 7.295223712921143,
      "learning_rate": 6.722627737226278e-06,
      "loss": 0.6796,
      "step": 118580
    },
    {
      "epoch": 865.6204379562043,
      "grad_norm": 10.643166542053223,
      "learning_rate": 6.718978102189781e-06,
      "loss": 0.769,
      "step": 118590
    },
    {
      "epoch": 865.6934306569343,
      "grad_norm": 7.802170276641846,
      "learning_rate": 6.715328467153286e-06,
      "loss": 1.3117,
      "step": 118600
    },
    {
      "epoch": 865.7664233576643,
      "grad_norm": 9.269330024719238,
      "learning_rate": 6.711678832116788e-06,
      "loss": 0.7061,
      "step": 118610
    },
    {
      "epoch": 865.8394160583941,
      "grad_norm": 9.817497253417969,
      "learning_rate": 6.708029197080293e-06,
      "loss": 0.91,
      "step": 118620
    },
    {
      "epoch": 865.9124087591241,
      "grad_norm": 12.693160057067871,
      "learning_rate": 6.704379562043796e-06,
      "loss": 0.8392,
      "step": 118630
    },
    {
      "epoch": 865.985401459854,
      "grad_norm": 8.325580596923828,
      "learning_rate": 6.7007299270073005e-06,
      "loss": 1.2376,
      "step": 118640
    },
    {
      "epoch": 866.0583941605839,
      "grad_norm": 17.305265426635742,
      "learning_rate": 6.6970802919708035e-06,
      "loss": 0.9522,
      "step": 118650
    },
    {
      "epoch": 866.1313868613139,
      "grad_norm": 18.662921905517578,
      "learning_rate": 6.693430656934307e-06,
      "loss": 1.258,
      "step": 118660
    },
    {
      "epoch": 866.2043795620438,
      "grad_norm": 16.30040168762207,
      "learning_rate": 6.68978102189781e-06,
      "loss": 1.0185,
      "step": 118670
    },
    {
      "epoch": 866.2773722627737,
      "grad_norm": 14.838890075683594,
      "learning_rate": 6.686131386861315e-06,
      "loss": 1.0584,
      "step": 118680
    },
    {
      "epoch": 866.3503649635037,
      "grad_norm": 13.798599243164062,
      "learning_rate": 6.682481751824818e-06,
      "loss": 0.8013,
      "step": 118690
    },
    {
      "epoch": 866.4233576642335,
      "grad_norm": 13.097928047180176,
      "learning_rate": 6.678832116788321e-06,
      "loss": 0.6305,
      "step": 118700
    },
    {
      "epoch": 866.4963503649635,
      "grad_norm": 0.011900109238922596,
      "learning_rate": 6.675182481751825e-06,
      "loss": 0.854,
      "step": 118710
    },
    {
      "epoch": 866.5693430656934,
      "grad_norm": 20.497798919677734,
      "learning_rate": 6.671532846715328e-06,
      "loss": 1.3098,
      "step": 118720
    },
    {
      "epoch": 866.6423357664233,
      "grad_norm": 9.773702621459961,
      "learning_rate": 6.667883211678833e-06,
      "loss": 0.7117,
      "step": 118730
    },
    {
      "epoch": 866.7153284671533,
      "grad_norm": 16.361257553100586,
      "learning_rate": 6.664233576642336e-06,
      "loss": 1.0551,
      "step": 118740
    },
    {
      "epoch": 866.7883211678832,
      "grad_norm": 7.228529453277588,
      "learning_rate": 6.66058394160584e-06,
      "loss": 0.5651,
      "step": 118750
    },
    {
      "epoch": 866.8613138686131,
      "grad_norm": 8.862131118774414,
      "learning_rate": 6.656934306569343e-06,
      "loss": 0.9756,
      "step": 118760
    },
    {
      "epoch": 866.9343065693431,
      "grad_norm": 13.807677268981934,
      "learning_rate": 6.6532846715328475e-06,
      "loss": 1.0521,
      "step": 118770
    },
    {
      "epoch": 867.007299270073,
      "grad_norm": 14.063425064086914,
      "learning_rate": 6.649635036496351e-06,
      "loss": 0.6902,
      "step": 118780
    },
    {
      "epoch": 867.0802919708029,
      "grad_norm": 9.057656288146973,
      "learning_rate": 6.6459854014598545e-06,
      "loss": 0.7215,
      "step": 118790
    },
    {
      "epoch": 867.1532846715329,
      "grad_norm": 6.0170745849609375,
      "learning_rate": 6.6423357664233575e-06,
      "loss": 0.8013,
      "step": 118800
    },
    {
      "epoch": 867.2262773722628,
      "grad_norm": 9.019304275512695,
      "learning_rate": 6.638686131386862e-06,
      "loss": 0.9473,
      "step": 118810
    },
    {
      "epoch": 867.2992700729927,
      "grad_norm": 13.572949409484863,
      "learning_rate": 6.635036496350365e-06,
      "loss": 0.7046,
      "step": 118820
    },
    {
      "epoch": 867.3722627737226,
      "grad_norm": 6.168227672576904,
      "learning_rate": 6.631386861313869e-06,
      "loss": 0.6815,
      "step": 118830
    },
    {
      "epoch": 867.4452554744526,
      "grad_norm": 6.581315040588379,
      "learning_rate": 6.627737226277372e-06,
      "loss": 1.1257,
      "step": 118840
    },
    {
      "epoch": 867.5182481751825,
      "grad_norm": 11.38211727142334,
      "learning_rate": 6.624087591240877e-06,
      "loss": 1.0047,
      "step": 118850
    },
    {
      "epoch": 867.5912408759124,
      "grad_norm": 12.04450798034668,
      "learning_rate": 6.62043795620438e-06,
      "loss": 0.7886,
      "step": 118860
    },
    {
      "epoch": 867.6642335766423,
      "grad_norm": 16.410860061645508,
      "learning_rate": 6.616788321167884e-06,
      "loss": 0.5706,
      "step": 118870
    },
    {
      "epoch": 867.7372262773723,
      "grad_norm": 15.32496166229248,
      "learning_rate": 6.613138686131387e-06,
      "loss": 1.1496,
      "step": 118880
    },
    {
      "epoch": 867.8102189781022,
      "grad_norm": 0.09146346151828766,
      "learning_rate": 6.609489051094892e-06,
      "loss": 0.9562,
      "step": 118890
    },
    {
      "epoch": 867.8832116788321,
      "grad_norm": 0.08563687652349472,
      "learning_rate": 6.605839416058395e-06,
      "loss": 0.9651,
      "step": 118900
    },
    {
      "epoch": 867.956204379562,
      "grad_norm": 17.673236846923828,
      "learning_rate": 6.602189781021898e-06,
      "loss": 1.0923,
      "step": 118910
    },
    {
      "epoch": 868.029197080292,
      "grad_norm": 14.820627212524414,
      "learning_rate": 6.5985401459854016e-06,
      "loss": 1.0252,
      "step": 118920
    },
    {
      "epoch": 868.1021897810219,
      "grad_norm": 0.01439119502902031,
      "learning_rate": 6.594890510948905e-06,
      "loss": 0.5507,
      "step": 118930
    },
    {
      "epoch": 868.1751824817518,
      "grad_norm": 7.453703880310059,
      "learning_rate": 6.591240875912409e-06,
      "loss": 0.5086,
      "step": 118940
    },
    {
      "epoch": 868.2481751824818,
      "grad_norm": 11.409459114074707,
      "learning_rate": 6.587591240875912e-06,
      "loss": 1.081,
      "step": 118950
    },
    {
      "epoch": 868.3211678832117,
      "grad_norm": 15.09813404083252,
      "learning_rate": 6.583941605839416e-06,
      "loss": 0.9332,
      "step": 118960
    },
    {
      "epoch": 868.3941605839416,
      "grad_norm": 5.768594741821289,
      "learning_rate": 6.580291970802919e-06,
      "loss": 0.7302,
      "step": 118970
    },
    {
      "epoch": 868.4671532846716,
      "grad_norm": 7.263433933258057,
      "learning_rate": 6.576642335766424e-06,
      "loss": 0.7055,
      "step": 118980
    },
    {
      "epoch": 868.5401459854014,
      "grad_norm": 6.430347919464111,
      "learning_rate": 6.572992700729927e-06,
      "loss": 0.8149,
      "step": 118990
    },
    {
      "epoch": 868.6131386861314,
      "grad_norm": 8.078141212463379,
      "learning_rate": 6.569343065693431e-06,
      "loss": 0.8951,
      "step": 119000
    },
    {
      "epoch": 868.6861313868613,
      "grad_norm": 8.72282600402832,
      "learning_rate": 6.565693430656934e-06,
      "loss": 0.7372,
      "step": 119010
    },
    {
      "epoch": 868.7591240875912,
      "grad_norm": 13.881634712219238,
      "learning_rate": 6.562043795620439e-06,
      "loss": 1.1187,
      "step": 119020
    },
    {
      "epoch": 868.8321167883212,
      "grad_norm": 8.113198280334473,
      "learning_rate": 6.558394160583942e-06,
      "loss": 1.4878,
      "step": 119030
    },
    {
      "epoch": 868.9051094890511,
      "grad_norm": 11.530723571777344,
      "learning_rate": 6.554744525547446e-06,
      "loss": 0.9581,
      "step": 119040
    },
    {
      "epoch": 868.978102189781,
      "grad_norm": 11.103764533996582,
      "learning_rate": 6.551094890510949e-06,
      "loss": 0.8063,
      "step": 119050
    },
    {
      "epoch": 869.051094890511,
      "grad_norm": 0.9454507827758789,
      "learning_rate": 6.547445255474453e-06,
      "loss": 0.8026,
      "step": 119060
    },
    {
      "epoch": 869.1240875912408,
      "grad_norm": 8.170075416564941,
      "learning_rate": 6.543795620437956e-06,
      "loss": 0.4987,
      "step": 119070
    },
    {
      "epoch": 869.1970802919708,
      "grad_norm": 12.282991409301758,
      "learning_rate": 6.540145985401461e-06,
      "loss": 1.1114,
      "step": 119080
    },
    {
      "epoch": 869.2700729927008,
      "grad_norm": 10.213693618774414,
      "learning_rate": 6.536496350364963e-06,
      "loss": 0.8765,
      "step": 119090
    },
    {
      "epoch": 869.3430656934306,
      "grad_norm": 6.182926654815674,
      "learning_rate": 6.532846715328468e-06,
      "loss": 1.0925,
      "step": 119100
    },
    {
      "epoch": 869.4160583941606,
      "grad_norm": 8.957419395446777,
      "learning_rate": 6.529197080291971e-06,
      "loss": 1.107,
      "step": 119110
    },
    {
      "epoch": 869.4890510948906,
      "grad_norm": 18.85443878173828,
      "learning_rate": 6.525547445255476e-06,
      "loss": 1.4638,
      "step": 119120
    },
    {
      "epoch": 869.5620437956204,
      "grad_norm": 6.94607400894165,
      "learning_rate": 6.521897810218979e-06,
      "loss": 0.5756,
      "step": 119130
    },
    {
      "epoch": 869.6350364963504,
      "grad_norm": 0.041205033659935,
      "learning_rate": 6.518248175182481e-06,
      "loss": 0.6218,
      "step": 119140
    },
    {
      "epoch": 869.7080291970802,
      "grad_norm": 10.068208694458008,
      "learning_rate": 6.514598540145986e-06,
      "loss": 0.7512,
      "step": 119150
    },
    {
      "epoch": 869.7810218978102,
      "grad_norm": 6.903468132019043,
      "learning_rate": 6.510948905109489e-06,
      "loss": 0.6093,
      "step": 119160
    },
    {
      "epoch": 869.8540145985402,
      "grad_norm": 7.771955490112305,
      "learning_rate": 6.5072992700729935e-06,
      "loss": 0.9035,
      "step": 119170
    },
    {
      "epoch": 869.92700729927,
      "grad_norm": 6.06334114074707,
      "learning_rate": 6.5036496350364966e-06,
      "loss": 0.5486,
      "step": 119180
    },
    {
      "epoch": 870.0,
      "grad_norm": 0.11120260506868362,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.8394,
      "step": 119190
    },
    {
      "epoch": 870.07299270073,
      "grad_norm": 15.808127403259277,
      "learning_rate": 6.4963503649635035e-06,
      "loss": 0.7124,
      "step": 119200
    },
    {
      "epoch": 870.1459854014598,
      "grad_norm": 7.965677738189697,
      "learning_rate": 6.492700729927008e-06,
      "loss": 0.5712,
      "step": 119210
    },
    {
      "epoch": 870.2189781021898,
      "grad_norm": 6.956167697906494,
      "learning_rate": 6.489051094890511e-06,
      "loss": 0.6206,
      "step": 119220
    },
    {
      "epoch": 870.2919708029198,
      "grad_norm": 6.335449695587158,
      "learning_rate": 6.485401459854015e-06,
      "loss": 1.4132,
      "step": 119230
    },
    {
      "epoch": 870.3649635036496,
      "grad_norm": 8.815398216247559,
      "learning_rate": 6.481751824817518e-06,
      "loss": 0.9479,
      "step": 119240
    },
    {
      "epoch": 870.4379562043796,
      "grad_norm": 0.03984421491622925,
      "learning_rate": 6.478102189781023e-06,
      "loss": 0.5333,
      "step": 119250
    },
    {
      "epoch": 870.5109489051094,
      "grad_norm": 0.058590009808540344,
      "learning_rate": 6.474452554744526e-06,
      "loss": 0.8922,
      "step": 119260
    },
    {
      "epoch": 870.5839416058394,
      "grad_norm": 8.059324264526367,
      "learning_rate": 6.47080291970803e-06,
      "loss": 1.051,
      "step": 119270
    },
    {
      "epoch": 870.6569343065694,
      "grad_norm": 7.3630218505859375,
      "learning_rate": 6.467153284671533e-06,
      "loss": 0.9948,
      "step": 119280
    },
    {
      "epoch": 870.7299270072992,
      "grad_norm": 5.638537883758545,
      "learning_rate": 6.463503649635038e-06,
      "loss": 0.7567,
      "step": 119290
    },
    {
      "epoch": 870.8029197080292,
      "grad_norm": 7.179797649383545,
      "learning_rate": 6.459854014598541e-06,
      "loss": 1.4019,
      "step": 119300
    },
    {
      "epoch": 870.8759124087592,
      "grad_norm": 6.113305568695068,
      "learning_rate": 6.4562043795620445e-06,
      "loss": 0.919,
      "step": 119310
    },
    {
      "epoch": 870.948905109489,
      "grad_norm": 16.673538208007812,
      "learning_rate": 6.4525547445255475e-06,
      "loss": 1.058,
      "step": 119320
    },
    {
      "epoch": 871.021897810219,
      "grad_norm": 11.796737670898438,
      "learning_rate": 6.448905109489052e-06,
      "loss": 0.6288,
      "step": 119330
    },
    {
      "epoch": 871.0948905109489,
      "grad_norm": 9.248144149780273,
      "learning_rate": 6.445255474452555e-06,
      "loss": 1.0744,
      "step": 119340
    },
    {
      "epoch": 871.1678832116788,
      "grad_norm": 0.01924961991608143,
      "learning_rate": 6.441605839416058e-06,
      "loss": 0.6902,
      "step": 119350
    },
    {
      "epoch": 871.2408759124088,
      "grad_norm": 9.771106719970703,
      "learning_rate": 6.437956204379562e-06,
      "loss": 0.8578,
      "step": 119360
    },
    {
      "epoch": 871.3138686131387,
      "grad_norm": 10.83591079711914,
      "learning_rate": 6.434306569343065e-06,
      "loss": 0.9033,
      "step": 119370
    },
    {
      "epoch": 871.3868613138686,
      "grad_norm": 6.449833393096924,
      "learning_rate": 6.43065693430657e-06,
      "loss": 0.648,
      "step": 119380
    },
    {
      "epoch": 871.4598540145986,
      "grad_norm": 6.199917793273926,
      "learning_rate": 6.427007299270073e-06,
      "loss": 1.0013,
      "step": 119390
    },
    {
      "epoch": 871.5328467153284,
      "grad_norm": 0.0525381863117218,
      "learning_rate": 6.423357664233577e-06,
      "loss": 0.6777,
      "step": 119400
    },
    {
      "epoch": 871.6058394160584,
      "grad_norm": 13.549591064453125,
      "learning_rate": 6.41970802919708e-06,
      "loss": 0.904,
      "step": 119410
    },
    {
      "epoch": 871.6788321167883,
      "grad_norm": 6.406760215759277,
      "learning_rate": 6.416058394160585e-06,
      "loss": 0.6022,
      "step": 119420
    },
    {
      "epoch": 871.7518248175182,
      "grad_norm": 15.81008529663086,
      "learning_rate": 6.412408759124088e-06,
      "loss": 1.0469,
      "step": 119430
    },
    {
      "epoch": 871.8248175182482,
      "grad_norm": 15.260578155517578,
      "learning_rate": 6.408759124087592e-06,
      "loss": 1.3067,
      "step": 119440
    },
    {
      "epoch": 871.8978102189781,
      "grad_norm": 1.724305510520935,
      "learning_rate": 6.405109489051095e-06,
      "loss": 0.7854,
      "step": 119450
    },
    {
      "epoch": 871.970802919708,
      "grad_norm": 14.331507682800293,
      "learning_rate": 6.401459854014599e-06,
      "loss": 0.8793,
      "step": 119460
    },
    {
      "epoch": 872.043795620438,
      "grad_norm": 0.05142715945839882,
      "learning_rate": 6.397810218978102e-06,
      "loss": 0.5958,
      "step": 119470
    },
    {
      "epoch": 872.1167883211679,
      "grad_norm": 0.017625266686081886,
      "learning_rate": 6.394160583941606e-06,
      "loss": 1.1328,
      "step": 119480
    },
    {
      "epoch": 872.1897810218978,
      "grad_norm": 1.349082112312317,
      "learning_rate": 6.390510948905109e-06,
      "loss": 0.4846,
      "step": 119490
    },
    {
      "epoch": 872.2627737226277,
      "grad_norm": 6.920359134674072,
      "learning_rate": 6.386861313868614e-06,
      "loss": 1.0372,
      "step": 119500
    },
    {
      "epoch": 872.3357664233577,
      "grad_norm": 10.395108222961426,
      "learning_rate": 6.383211678832117e-06,
      "loss": 0.8013,
      "step": 119510
    },
    {
      "epoch": 872.4087591240876,
      "grad_norm": 0.027021972462534904,
      "learning_rate": 6.379562043795621e-06,
      "loss": 0.9269,
      "step": 119520
    },
    {
      "epoch": 872.4817518248175,
      "grad_norm": 7.304652214050293,
      "learning_rate": 6.375912408759124e-06,
      "loss": 1.2399,
      "step": 119530
    },
    {
      "epoch": 872.5547445255474,
      "grad_norm": 0.018048280850052834,
      "learning_rate": 6.372262773722629e-06,
      "loss": 0.505,
      "step": 119540
    },
    {
      "epoch": 872.6277372262774,
      "grad_norm": 17.349960327148438,
      "learning_rate": 6.368613138686132e-06,
      "loss": 1.0857,
      "step": 119550
    },
    {
      "epoch": 872.7007299270073,
      "grad_norm": 7.5035552978515625,
      "learning_rate": 6.364963503649635e-06,
      "loss": 1.1463,
      "step": 119560
    },
    {
      "epoch": 872.7737226277372,
      "grad_norm": 8.904406547546387,
      "learning_rate": 6.361313868613139e-06,
      "loss": 0.7622,
      "step": 119570
    },
    {
      "epoch": 872.8467153284671,
      "grad_norm": 12.014274597167969,
      "learning_rate": 6.357664233576642e-06,
      "loss": 0.8003,
      "step": 119580
    },
    {
      "epoch": 872.9197080291971,
      "grad_norm": 11.93830680847168,
      "learning_rate": 6.3540145985401464e-06,
      "loss": 0.7408,
      "step": 119590
    },
    {
      "epoch": 872.992700729927,
      "grad_norm": 10.590109825134277,
      "learning_rate": 6.3503649635036495e-06,
      "loss": 0.9369,
      "step": 119600
    },
    {
      "epoch": 873.0656934306569,
      "grad_norm": 0.016446096822619438,
      "learning_rate": 6.346715328467154e-06,
      "loss": 0.9097,
      "step": 119610
    },
    {
      "epoch": 873.1386861313869,
      "grad_norm": 10.1605806350708,
      "learning_rate": 6.343065693430656e-06,
      "loss": 0.6924,
      "step": 119620
    },
    {
      "epoch": 873.2116788321168,
      "grad_norm": 0.05563787743449211,
      "learning_rate": 6.339416058394161e-06,
      "loss": 1.186,
      "step": 119630
    },
    {
      "epoch": 873.2846715328467,
      "grad_norm": 9.728411674499512,
      "learning_rate": 6.335766423357664e-06,
      "loss": 0.9842,
      "step": 119640
    },
    {
      "epoch": 873.3576642335767,
      "grad_norm": 10.523262023925781,
      "learning_rate": 6.332116788321169e-06,
      "loss": 0.7882,
      "step": 119650
    },
    {
      "epoch": 873.4306569343066,
      "grad_norm": 11.507256507873535,
      "learning_rate": 6.328467153284672e-06,
      "loss": 0.6443,
      "step": 119660
    },
    {
      "epoch": 873.5036496350365,
      "grad_norm": 7.165878772735596,
      "learning_rate": 6.324817518248176e-06,
      "loss": 0.862,
      "step": 119670
    },
    {
      "epoch": 873.5766423357665,
      "grad_norm": 9.866414070129395,
      "learning_rate": 6.321167883211679e-06,
      "loss": 0.9004,
      "step": 119680
    },
    {
      "epoch": 873.6496350364963,
      "grad_norm": 15.016376495361328,
      "learning_rate": 6.3175182481751836e-06,
      "loss": 1.003,
      "step": 119690
    },
    {
      "epoch": 873.7226277372263,
      "grad_norm": 0.010677373968064785,
      "learning_rate": 6.313868613138687e-06,
      "loss": 1.3667,
      "step": 119700
    },
    {
      "epoch": 873.7956204379562,
      "grad_norm": 5.91359281539917,
      "learning_rate": 6.3102189781021905e-06,
      "loss": 0.7243,
      "step": 119710
    },
    {
      "epoch": 873.8686131386861,
      "grad_norm": 10.90884780883789,
      "learning_rate": 6.3065693430656935e-06,
      "loss": 0.7523,
      "step": 119720
    },
    {
      "epoch": 873.9416058394161,
      "grad_norm": 10.258727073669434,
      "learning_rate": 6.302919708029198e-06,
      "loss": 1.1736,
      "step": 119730
    },
    {
      "epoch": 874.014598540146,
      "grad_norm": 0.007500617764890194,
      "learning_rate": 6.299270072992701e-06,
      "loss": 0.5037,
      "step": 119740
    },
    {
      "epoch": 874.0875912408759,
      "grad_norm": 7.121884822845459,
      "learning_rate": 6.295620437956205e-06,
      "loss": 0.7418,
      "step": 119750
    },
    {
      "epoch": 874.1605839416059,
      "grad_norm": 16.801368713378906,
      "learning_rate": 6.291970802919708e-06,
      "loss": 1.0144,
      "step": 119760
    },
    {
      "epoch": 874.2335766423357,
      "grad_norm": 7.371623516082764,
      "learning_rate": 6.288321167883211e-06,
      "loss": 0.7374,
      "step": 119770
    },
    {
      "epoch": 874.3065693430657,
      "grad_norm": 10.910547256469727,
      "learning_rate": 6.284671532846716e-06,
      "loss": 0.8234,
      "step": 119780
    },
    {
      "epoch": 874.3795620437957,
      "grad_norm": 9.763803482055664,
      "learning_rate": 6.281021897810219e-06,
      "loss": 0.9289,
      "step": 119790
    },
    {
      "epoch": 874.4525547445255,
      "grad_norm": 5.908407211303711,
      "learning_rate": 6.277372262773723e-06,
      "loss": 0.8051,
      "step": 119800
    },
    {
      "epoch": 874.5255474452555,
      "grad_norm": 0.07995938509702682,
      "learning_rate": 6.273722627737226e-06,
      "loss": 0.9642,
      "step": 119810
    },
    {
      "epoch": 874.5985401459855,
      "grad_norm": 12.779661178588867,
      "learning_rate": 6.270072992700731e-06,
      "loss": 0.9394,
      "step": 119820
    },
    {
      "epoch": 874.6715328467153,
      "grad_norm": 7.8350830078125,
      "learning_rate": 6.266423357664234e-06,
      "loss": 0.9607,
      "step": 119830
    },
    {
      "epoch": 874.7445255474453,
      "grad_norm": 7.913457870483398,
      "learning_rate": 6.2627737226277376e-06,
      "loss": 1.1771,
      "step": 119840
    },
    {
      "epoch": 874.8175182481751,
      "grad_norm": 6.561548709869385,
      "learning_rate": 6.259124087591241e-06,
      "loss": 0.8631,
      "step": 119850
    },
    {
      "epoch": 874.8905109489051,
      "grad_norm": 14.52122688293457,
      "learning_rate": 6.255474452554745e-06,
      "loss": 0.8344,
      "step": 119860
    },
    {
      "epoch": 874.9635036496351,
      "grad_norm": 12.096044540405273,
      "learning_rate": 6.251824817518248e-06,
      "loss": 0.9191,
      "step": 119870
    },
    {
      "epoch": 875.0364963503649,
      "grad_norm": 9.946756362915039,
      "learning_rate": 6.248175182481751e-06,
      "loss": 1.0444,
      "step": 119880
    },
    {
      "epoch": 875.1094890510949,
      "grad_norm": 12.413274765014648,
      "learning_rate": 6.244525547445255e-06,
      "loss": 0.9302,
      "step": 119890
    },
    {
      "epoch": 875.1824817518249,
      "grad_norm": 15.448295593261719,
      "learning_rate": 6.240875912408759e-06,
      "loss": 1.423,
      "step": 119900
    },
    {
      "epoch": 875.2554744525547,
      "grad_norm": 10.52586555480957,
      "learning_rate": 6.237226277372263e-06,
      "loss": 1.3553,
      "step": 119910
    },
    {
      "epoch": 875.3284671532847,
      "grad_norm": 20.978187561035156,
      "learning_rate": 6.233576642335767e-06,
      "loss": 0.8885,
      "step": 119920
    },
    {
      "epoch": 875.4014598540145,
      "grad_norm": 4.0734028816223145,
      "learning_rate": 6.22992700729927e-06,
      "loss": 0.8772,
      "step": 119930
    },
    {
      "epoch": 875.4744525547445,
      "grad_norm": 0.08072098344564438,
      "learning_rate": 6.226277372262774e-06,
      "loss": 0.8607,
      "step": 119940
    },
    {
      "epoch": 875.5474452554745,
      "grad_norm": 0.11476358026266098,
      "learning_rate": 6.222627737226278e-06,
      "loss": 0.4792,
      "step": 119950
    },
    {
      "epoch": 875.6204379562043,
      "grad_norm": 7.148118495941162,
      "learning_rate": 6.218978102189782e-06,
      "loss": 0.7167,
      "step": 119960
    },
    {
      "epoch": 875.6934306569343,
      "grad_norm": 0.020200977101922035,
      "learning_rate": 6.215328467153285e-06,
      "loss": 0.521,
      "step": 119970
    },
    {
      "epoch": 875.7664233576643,
      "grad_norm": 9.8738431930542,
      "learning_rate": 6.2116788321167885e-06,
      "loss": 1.0336,
      "step": 119980
    },
    {
      "epoch": 875.8394160583941,
      "grad_norm": 7.871664524078369,
      "learning_rate": 6.208029197080292e-06,
      "loss": 0.7534,
      "step": 119990
    },
    {
      "epoch": 875.9124087591241,
      "grad_norm": 7.371730327606201,
      "learning_rate": 6.204379562043796e-06,
      "loss": 0.5726,
      "step": 120000
    },
    {
      "epoch": 875.985401459854,
      "grad_norm": 11.035064697265625,
      "learning_rate": 6.200729927007299e-06,
      "loss": 0.8716,
      "step": 120010
    },
    {
      "epoch": 876.0583941605839,
      "grad_norm": 7.0990495681762695,
      "learning_rate": 6.197080291970803e-06,
      "loss": 0.888,
      "step": 120020
    },
    {
      "epoch": 876.1313868613139,
      "grad_norm": 9.20058536529541,
      "learning_rate": 6.193430656934307e-06,
      "loss": 0.9612,
      "step": 120030
    },
    {
      "epoch": 876.2043795620438,
      "grad_norm": 9.71389102935791,
      "learning_rate": 6.189781021897811e-06,
      "loss": 0.8336,
      "step": 120040
    },
    {
      "epoch": 876.2773722627737,
      "grad_norm": 0.0806669071316719,
      "learning_rate": 6.186131386861314e-06,
      "loss": 0.9053,
      "step": 120050
    },
    {
      "epoch": 876.3503649635037,
      "grad_norm": 12.996048927307129,
      "learning_rate": 6.182481751824818e-06,
      "loss": 1.2402,
      "step": 120060
    },
    {
      "epoch": 876.4233576642335,
      "grad_norm": 21.505664825439453,
      "learning_rate": 6.178832116788322e-06,
      "loss": 0.9517,
      "step": 120070
    },
    {
      "epoch": 876.4963503649635,
      "grad_norm": 10.391587257385254,
      "learning_rate": 6.175182481751826e-06,
      "loss": 0.9639,
      "step": 120080
    },
    {
      "epoch": 876.5693430656934,
      "grad_norm": 13.907458305358887,
      "learning_rate": 6.171532846715329e-06,
      "loss": 1.1262,
      "step": 120090
    },
    {
      "epoch": 876.6423357664233,
      "grad_norm": 10.899892807006836,
      "learning_rate": 6.167883211678832e-06,
      "loss": 1.2205,
      "step": 120100
    },
    {
      "epoch": 876.7153284671533,
      "grad_norm": 0.017419688403606415,
      "learning_rate": 6.164233576642336e-06,
      "loss": 0.7773,
      "step": 120110
    },
    {
      "epoch": 876.7883211678832,
      "grad_norm": 10.17041015625,
      "learning_rate": 6.1605839416058395e-06,
      "loss": 0.6686,
      "step": 120120
    },
    {
      "epoch": 876.8613138686131,
      "grad_norm": 10.927600860595703,
      "learning_rate": 6.156934306569343e-06,
      "loss": 0.8303,
      "step": 120130
    },
    {
      "epoch": 876.9343065693431,
      "grad_norm": 10.525547981262207,
      "learning_rate": 6.153284671532846e-06,
      "loss": 0.916,
      "step": 120140
    },
    {
      "epoch": 877.007299270073,
      "grad_norm": 7.463832855224609,
      "learning_rate": 6.14963503649635e-06,
      "loss": 0.4321,
      "step": 120150
    },
    {
      "epoch": 877.0802919708029,
      "grad_norm": 0.03900391608476639,
      "learning_rate": 6.145985401459854e-06,
      "loss": 1.0251,
      "step": 120160
    },
    {
      "epoch": 877.1532846715329,
      "grad_norm": 15.69053840637207,
      "learning_rate": 6.142335766423358e-06,
      "loss": 1.0442,
      "step": 120170
    },
    {
      "epoch": 877.2262773722628,
      "grad_norm": 0.627959668636322,
      "learning_rate": 6.138686131386862e-06,
      "loss": 0.7649,
      "step": 120180
    },
    {
      "epoch": 877.2992700729927,
      "grad_norm": 6.820981979370117,
      "learning_rate": 6.135036496350365e-06,
      "loss": 0.8502,
      "step": 120190
    },
    {
      "epoch": 877.3722627737226,
      "grad_norm": 0.02440202236175537,
      "learning_rate": 6.131386861313869e-06,
      "loss": 0.536,
      "step": 120200
    },
    {
      "epoch": 877.4452554744526,
      "grad_norm": 8.27400016784668,
      "learning_rate": 6.127737226277373e-06,
      "loss": 0.7223,
      "step": 120210
    },
    {
      "epoch": 877.5182481751825,
      "grad_norm": 7.413669109344482,
      "learning_rate": 6.124087591240877e-06,
      "loss": 1.3918,
      "step": 120220
    },
    {
      "epoch": 877.5912408759124,
      "grad_norm": 10.104815483093262,
      "learning_rate": 6.12043795620438e-06,
      "loss": 0.8487,
      "step": 120230
    },
    {
      "epoch": 877.6642335766423,
      "grad_norm": 11.084803581237793,
      "learning_rate": 6.1167883211678835e-06,
      "loss": 1.2246,
      "step": 120240
    },
    {
      "epoch": 877.7372262773723,
      "grad_norm": 20.297096252441406,
      "learning_rate": 6.113138686131387e-06,
      "loss": 1.013,
      "step": 120250
    },
    {
      "epoch": 877.8102189781022,
      "grad_norm": 6.765441417694092,
      "learning_rate": 6.109489051094891e-06,
      "loss": 0.4243,
      "step": 120260
    },
    {
      "epoch": 877.8832116788321,
      "grad_norm": 10.887839317321777,
      "learning_rate": 6.105839416058394e-06,
      "loss": 0.9188,
      "step": 120270
    },
    {
      "epoch": 877.956204379562,
      "grad_norm": 14.822356224060059,
      "learning_rate": 6.102189781021898e-06,
      "loss": 1.0007,
      "step": 120280
    },
    {
      "epoch": 878.029197080292,
      "grad_norm": 6.454437732696533,
      "learning_rate": 6.098540145985402e-06,
      "loss": 0.673,
      "step": 120290
    },
    {
      "epoch": 878.1021897810219,
      "grad_norm": 14.485149383544922,
      "learning_rate": 6.094890510948906e-06,
      "loss": 1.0047,
      "step": 120300
    },
    {
      "epoch": 878.1751824817518,
      "grad_norm": 7.304704189300537,
      "learning_rate": 6.091240875912409e-06,
      "loss": 1.1384,
      "step": 120310
    },
    {
      "epoch": 878.2481751824818,
      "grad_norm": 6.76957893371582,
      "learning_rate": 6.087591240875912e-06,
      "loss": 0.6499,
      "step": 120320
    },
    {
      "epoch": 878.3211678832117,
      "grad_norm": 7.223079681396484,
      "learning_rate": 6.083941605839416e-06,
      "loss": 1.4118,
      "step": 120330
    },
    {
      "epoch": 878.3941605839416,
      "grad_norm": 7.729803085327148,
      "learning_rate": 6.08029197080292e-06,
      "loss": 0.6876,
      "step": 120340
    },
    {
      "epoch": 878.4671532846716,
      "grad_norm": 0.03346798196434975,
      "learning_rate": 6.076642335766424e-06,
      "loss": 0.7073,
      "step": 120350
    },
    {
      "epoch": 878.5401459854014,
      "grad_norm": 10.478532791137695,
      "learning_rate": 6.072992700729927e-06,
      "loss": 0.6775,
      "step": 120360
    },
    {
      "epoch": 878.6131386861314,
      "grad_norm": 11.730952262878418,
      "learning_rate": 6.069343065693431e-06,
      "loss": 1.1696,
      "step": 120370
    },
    {
      "epoch": 878.6861313868613,
      "grad_norm": 18.593067169189453,
      "learning_rate": 6.0656934306569345e-06,
      "loss": 1.0364,
      "step": 120380
    },
    {
      "epoch": 878.7591240875912,
      "grad_norm": 10.738637924194336,
      "learning_rate": 6.062043795620438e-06,
      "loss": 0.416,
      "step": 120390
    },
    {
      "epoch": 878.8321167883212,
      "grad_norm": 8.08981704711914,
      "learning_rate": 6.058394160583942e-06,
      "loss": 0.6865,
      "step": 120400
    },
    {
      "epoch": 878.9051094890511,
      "grad_norm": 27.581958770751953,
      "learning_rate": 6.054744525547445e-06,
      "loss": 0.795,
      "step": 120410
    },
    {
      "epoch": 878.978102189781,
      "grad_norm": 7.179932594299316,
      "learning_rate": 6.051094890510949e-06,
      "loss": 1.3002,
      "step": 120420
    },
    {
      "epoch": 879.051094890511,
      "grad_norm": 10.614542961120605,
      "learning_rate": 6.047445255474453e-06,
      "loss": 0.678,
      "step": 120430
    },
    {
      "epoch": 879.1240875912408,
      "grad_norm": 12.899388313293457,
      "learning_rate": 6.043795620437957e-06,
      "loss": 1.0693,
      "step": 120440
    },
    {
      "epoch": 879.1970802919708,
      "grad_norm": 0.03227828070521355,
      "learning_rate": 6.04014598540146e-06,
      "loss": 0.6777,
      "step": 120450
    },
    {
      "epoch": 879.2700729927008,
      "grad_norm": 10.46733570098877,
      "learning_rate": 6.036496350364964e-06,
      "loss": 0.6628,
      "step": 120460
    },
    {
      "epoch": 879.3430656934306,
      "grad_norm": 6.861841678619385,
      "learning_rate": 6.032846715328468e-06,
      "loss": 0.8748,
      "step": 120470
    },
    {
      "epoch": 879.4160583941606,
      "grad_norm": 7.0731282234191895,
      "learning_rate": 6.029197080291972e-06,
      "loss": 0.6135,
      "step": 120480
    },
    {
      "epoch": 879.4890510948906,
      "grad_norm": 7.297980308532715,
      "learning_rate": 6.025547445255475e-06,
      "loss": 0.9229,
      "step": 120490
    },
    {
      "epoch": 879.5620437956204,
      "grad_norm": 8.132218360900879,
      "learning_rate": 6.0218978102189786e-06,
      "loss": 1.0157,
      "step": 120500
    },
    {
      "epoch": 879.6350364963504,
      "grad_norm": 0.06360843032598495,
      "learning_rate": 6.0182481751824824e-06,
      "loss": 0.7878,
      "step": 120510
    },
    {
      "epoch": 879.7080291970802,
      "grad_norm": 14.166943550109863,
      "learning_rate": 6.014598540145986e-06,
      "loss": 1.0879,
      "step": 120520
    },
    {
      "epoch": 879.7810218978102,
      "grad_norm": 11.525848388671875,
      "learning_rate": 6.010948905109489e-06,
      "loss": 0.9832,
      "step": 120530
    },
    {
      "epoch": 879.8540145985402,
      "grad_norm": 13.73151683807373,
      "learning_rate": 6.007299270072992e-06,
      "loss": 0.8755,
      "step": 120540
    },
    {
      "epoch": 879.92700729927,
      "grad_norm": 0.034693196415901184,
      "learning_rate": 6.003649635036496e-06,
      "loss": 0.8823,
      "step": 120550
    },
    {
      "epoch": 880.0,
      "grad_norm": 20.34486961364746,
      "learning_rate": 6e-06,
      "loss": 1.0057,
      "step": 120560
    },
    {
      "epoch": 880.07299270073,
      "grad_norm": 13.23159122467041,
      "learning_rate": 5.996350364963504e-06,
      "loss": 1.0527,
      "step": 120570
    },
    {
      "epoch": 880.1459854014598,
      "grad_norm": 8.174500465393066,
      "learning_rate": 5.992700729927007e-06,
      "loss": 0.8926,
      "step": 120580
    },
    {
      "epoch": 880.2189781021898,
      "grad_norm": 7.046168327331543,
      "learning_rate": 5.989051094890511e-06,
      "loss": 0.9367,
      "step": 120590
    },
    {
      "epoch": 880.2919708029198,
      "grad_norm": 6.897391319274902,
      "learning_rate": 5.985401459854015e-06,
      "loss": 0.623,
      "step": 120600
    },
    {
      "epoch": 880.3649635036496,
      "grad_norm": 8.879371643066406,
      "learning_rate": 5.981751824817519e-06,
      "loss": 1.0202,
      "step": 120610
    },
    {
      "epoch": 880.4379562043796,
      "grad_norm": 11.074362754821777,
      "learning_rate": 5.978102189781022e-06,
      "loss": 0.4921,
      "step": 120620
    },
    {
      "epoch": 880.5109489051094,
      "grad_norm": 7.943736553192139,
      "learning_rate": 5.974452554744526e-06,
      "loss": 0.9238,
      "step": 120630
    },
    {
      "epoch": 880.5839416058394,
      "grad_norm": 13.36426830291748,
      "learning_rate": 5.9708029197080295e-06,
      "loss": 0.8736,
      "step": 120640
    },
    {
      "epoch": 880.6569343065694,
      "grad_norm": 4.458006858825684,
      "learning_rate": 5.967153284671533e-06,
      "loss": 0.7433,
      "step": 120650
    },
    {
      "epoch": 880.7299270072992,
      "grad_norm": 0.10188703238964081,
      "learning_rate": 5.963503649635037e-06,
      "loss": 0.7443,
      "step": 120660
    },
    {
      "epoch": 880.8029197080292,
      "grad_norm": 12.96896743774414,
      "learning_rate": 5.95985401459854e-06,
      "loss": 0.7885,
      "step": 120670
    },
    {
      "epoch": 880.8759124087592,
      "grad_norm": 16.32956314086914,
      "learning_rate": 5.956204379562044e-06,
      "loss": 1.3753,
      "step": 120680
    },
    {
      "epoch": 880.948905109489,
      "grad_norm": 9.360776901245117,
      "learning_rate": 5.952554744525548e-06,
      "loss": 1.2919,
      "step": 120690
    },
    {
      "epoch": 881.021897810219,
      "grad_norm": 14.55764102935791,
      "learning_rate": 5.948905109489052e-06,
      "loss": 0.952,
      "step": 120700
    },
    {
      "epoch": 881.0948905109489,
      "grad_norm": 0.021746469661593437,
      "learning_rate": 5.945255474452555e-06,
      "loss": 0.6449,
      "step": 120710
    },
    {
      "epoch": 881.1678832116788,
      "grad_norm": 9.002726554870605,
      "learning_rate": 5.941605839416059e-06,
      "loss": 1.02,
      "step": 120720
    },
    {
      "epoch": 881.2408759124088,
      "grad_norm": 8.680994033813477,
      "learning_rate": 5.937956204379563e-06,
      "loss": 0.9129,
      "step": 120730
    },
    {
      "epoch": 881.3138686131387,
      "grad_norm": 7.433944225311279,
      "learning_rate": 5.934306569343066e-06,
      "loss": 1.2372,
      "step": 120740
    },
    {
      "epoch": 881.3868613138686,
      "grad_norm": 15.283632278442383,
      "learning_rate": 5.93065693430657e-06,
      "loss": 0.5625,
      "step": 120750
    },
    {
      "epoch": 881.4598540145986,
      "grad_norm": 0.05925248563289642,
      "learning_rate": 5.927007299270073e-06,
      "loss": 0.5069,
      "step": 120760
    },
    {
      "epoch": 881.5328467153284,
      "grad_norm": 0.026339486241340637,
      "learning_rate": 5.923357664233577e-06,
      "loss": 0.7784,
      "step": 120770
    },
    {
      "epoch": 881.6058394160584,
      "grad_norm": 13.55062484741211,
      "learning_rate": 5.9197080291970805e-06,
      "loss": 0.7133,
      "step": 120780
    },
    {
      "epoch": 881.6788321167883,
      "grad_norm": 9.501982688903809,
      "learning_rate": 5.916058394160584e-06,
      "loss": 1.1536,
      "step": 120790
    },
    {
      "epoch": 881.7518248175182,
      "grad_norm": 14.529696464538574,
      "learning_rate": 5.912408759124087e-06,
      "loss": 1.0218,
      "step": 120800
    },
    {
      "epoch": 881.8248175182482,
      "grad_norm": 9.547887802124023,
      "learning_rate": 5.908759124087591e-06,
      "loss": 0.9502,
      "step": 120810
    },
    {
      "epoch": 881.8978102189781,
      "grad_norm": 6.552285671234131,
      "learning_rate": 5.905109489051095e-06,
      "loss": 0.5862,
      "step": 120820
    },
    {
      "epoch": 881.970802919708,
      "grad_norm": 5.731201648712158,
      "learning_rate": 5.901459854014599e-06,
      "loss": 1.0269,
      "step": 120830
    },
    {
      "epoch": 882.043795620438,
      "grad_norm": 5.45811653137207,
      "learning_rate": 5.897810218978102e-06,
      "loss": 0.5591,
      "step": 120840
    },
    {
      "epoch": 882.1167883211679,
      "grad_norm": 7.261786937713623,
      "learning_rate": 5.894160583941606e-06,
      "loss": 0.9727,
      "step": 120850
    },
    {
      "epoch": 882.1897810218978,
      "grad_norm": 12.685860633850098,
      "learning_rate": 5.89051094890511e-06,
      "loss": 1.0753,
      "step": 120860
    },
    {
      "epoch": 882.2627737226277,
      "grad_norm": 0.030766531825065613,
      "learning_rate": 5.886861313868614e-06,
      "loss": 0.5985,
      "step": 120870
    },
    {
      "epoch": 882.3357664233577,
      "grad_norm": 9.54419994354248,
      "learning_rate": 5.883211678832117e-06,
      "loss": 1.3597,
      "step": 120880
    },
    {
      "epoch": 882.4087591240876,
      "grad_norm": 6.76749849319458,
      "learning_rate": 5.879562043795621e-06,
      "loss": 0.7145,
      "step": 120890
    },
    {
      "epoch": 882.4817518248175,
      "grad_norm": 18.312894821166992,
      "learning_rate": 5.8759124087591245e-06,
      "loss": 0.8477,
      "step": 120900
    },
    {
      "epoch": 882.5547445255474,
      "grad_norm": 14.651140213012695,
      "learning_rate": 5.872262773722628e-06,
      "loss": 0.7319,
      "step": 120910
    },
    {
      "epoch": 882.6277372262774,
      "grad_norm": 12.088375091552734,
      "learning_rate": 5.868613138686132e-06,
      "loss": 0.7629,
      "step": 120920
    },
    {
      "epoch": 882.7007299270073,
      "grad_norm": 7.537093639373779,
      "learning_rate": 5.864963503649635e-06,
      "loss": 0.8306,
      "step": 120930
    },
    {
      "epoch": 882.7737226277372,
      "grad_norm": 11.11067008972168,
      "learning_rate": 5.861313868613139e-06,
      "loss": 0.9445,
      "step": 120940
    },
    {
      "epoch": 882.8467153284671,
      "grad_norm": 8.762858390808105,
      "learning_rate": 5.857664233576643e-06,
      "loss": 1.0616,
      "step": 120950
    },
    {
      "epoch": 882.9197080291971,
      "grad_norm": 12.763779640197754,
      "learning_rate": 5.854014598540146e-06,
      "loss": 0.9125,
      "step": 120960
    },
    {
      "epoch": 882.992700729927,
      "grad_norm": 10.526817321777344,
      "learning_rate": 5.85036496350365e-06,
      "loss": 1.0622,
      "step": 120970
    },
    {
      "epoch": 883.0656934306569,
      "grad_norm": 0.047318097203969955,
      "learning_rate": 5.846715328467153e-06,
      "loss": 0.9533,
      "step": 120980
    },
    {
      "epoch": 883.1386861313869,
      "grad_norm": 11.387762069702148,
      "learning_rate": 5.843065693430657e-06,
      "loss": 0.8292,
      "step": 120990
    },
    {
      "epoch": 883.2116788321168,
      "grad_norm": 8.341429710388184,
      "learning_rate": 5.839416058394161e-06,
      "loss": 0.8631,
      "step": 121000
    },
    {
      "epoch": 883.2846715328467,
      "grad_norm": 7.9598565101623535,
      "learning_rate": 5.835766423357665e-06,
      "loss": 0.8373,
      "step": 121010
    },
    {
      "epoch": 883.3576642335767,
      "grad_norm": 0.04267270117998123,
      "learning_rate": 5.832116788321168e-06,
      "loss": 0.9161,
      "step": 121020
    },
    {
      "epoch": 883.4306569343066,
      "grad_norm": 11.270844459533691,
      "learning_rate": 5.828467153284672e-06,
      "loss": 0.787,
      "step": 121030
    },
    {
      "epoch": 883.5036496350365,
      "grad_norm": 0.01497333962470293,
      "learning_rate": 5.8248175182481755e-06,
      "loss": 0.7904,
      "step": 121040
    },
    {
      "epoch": 883.5766423357665,
      "grad_norm": 14.652507781982422,
      "learning_rate": 5.821167883211679e-06,
      "loss": 1.0047,
      "step": 121050
    },
    {
      "epoch": 883.6496350364963,
      "grad_norm": 0.19424334168434143,
      "learning_rate": 5.817518248175182e-06,
      "loss": 1.2504,
      "step": 121060
    },
    {
      "epoch": 883.7226277372263,
      "grad_norm": 9.49824333190918,
      "learning_rate": 5.813868613138686e-06,
      "loss": 0.9861,
      "step": 121070
    },
    {
      "epoch": 883.7956204379562,
      "grad_norm": 5.236675262451172,
      "learning_rate": 5.81021897810219e-06,
      "loss": 0.9326,
      "step": 121080
    },
    {
      "epoch": 883.8686131386861,
      "grad_norm": 0.0750112533569336,
      "learning_rate": 5.806569343065694e-06,
      "loss": 0.7049,
      "step": 121090
    },
    {
      "epoch": 883.9416058394161,
      "grad_norm": 8.696224212646484,
      "learning_rate": 5.802919708029197e-06,
      "loss": 0.6465,
      "step": 121100
    },
    {
      "epoch": 884.014598540146,
      "grad_norm": 9.355049133300781,
      "learning_rate": 5.799270072992701e-06,
      "loss": 1.1388,
      "step": 121110
    },
    {
      "epoch": 884.0875912408759,
      "grad_norm": 9.304097175598145,
      "learning_rate": 5.795620437956205e-06,
      "loss": 0.9531,
      "step": 121120
    },
    {
      "epoch": 884.1605839416059,
      "grad_norm": 9.220129013061523,
      "learning_rate": 5.791970802919709e-06,
      "loss": 1.1658,
      "step": 121130
    },
    {
      "epoch": 884.2335766423357,
      "grad_norm": 10.115738868713379,
      "learning_rate": 5.788321167883213e-06,
      "loss": 1.5104,
      "step": 121140
    },
    {
      "epoch": 884.3065693430657,
      "grad_norm": 0.4128209948539734,
      "learning_rate": 5.784671532846716e-06,
      "loss": 0.839,
      "step": 121150
    },
    {
      "epoch": 884.3795620437957,
      "grad_norm": 10.354545593261719,
      "learning_rate": 5.7810218978102195e-06,
      "loss": 0.3934,
      "step": 121160
    },
    {
      "epoch": 884.4525547445255,
      "grad_norm": 8.190485000610352,
      "learning_rate": 5.777372262773723e-06,
      "loss": 1.129,
      "step": 121170
    },
    {
      "epoch": 884.5255474452555,
      "grad_norm": 11.907246589660645,
      "learning_rate": 5.7737226277372265e-06,
      "loss": 0.7043,
      "step": 121180
    },
    {
      "epoch": 884.5985401459855,
      "grad_norm": 10.71234130859375,
      "learning_rate": 5.77007299270073e-06,
      "loss": 0.6949,
      "step": 121190
    },
    {
      "epoch": 884.6715328467153,
      "grad_norm": 0.019621169194579124,
      "learning_rate": 5.766423357664233e-06,
      "loss": 0.5253,
      "step": 121200
    },
    {
      "epoch": 884.7445255474453,
      "grad_norm": 5.069501876831055,
      "learning_rate": 5.762773722627737e-06,
      "loss": 0.8146,
      "step": 121210
    },
    {
      "epoch": 884.8175182481751,
      "grad_norm": 12.753634452819824,
      "learning_rate": 5.759124087591241e-06,
      "loss": 0.9593,
      "step": 121220
    },
    {
      "epoch": 884.8905109489051,
      "grad_norm": 11.144552230834961,
      "learning_rate": 5.755474452554745e-06,
      "loss": 0.895,
      "step": 121230
    },
    {
      "epoch": 884.9635036496351,
      "grad_norm": 17.326091766357422,
      "learning_rate": 5.751824817518248e-06,
      "loss": 0.9603,
      "step": 121240
    },
    {
      "epoch": 885.0364963503649,
      "grad_norm": 7.139634609222412,
      "learning_rate": 5.748175182481752e-06,
      "loss": 0.7638,
      "step": 121250
    },
    {
      "epoch": 885.1094890510949,
      "grad_norm": 8.932639122009277,
      "learning_rate": 5.744525547445256e-06,
      "loss": 0.5728,
      "step": 121260
    },
    {
      "epoch": 885.1824817518249,
      "grad_norm": 9.734136581420898,
      "learning_rate": 5.74087591240876e-06,
      "loss": 0.8709,
      "step": 121270
    },
    {
      "epoch": 885.2554744525547,
      "grad_norm": 5.304225444793701,
      "learning_rate": 5.737226277372263e-06,
      "loss": 0.8524,
      "step": 121280
    },
    {
      "epoch": 885.3284671532847,
      "grad_norm": 7.568299770355225,
      "learning_rate": 5.733576642335767e-06,
      "loss": 0.7173,
      "step": 121290
    },
    {
      "epoch": 885.4014598540145,
      "grad_norm": 13.262971878051758,
      "learning_rate": 5.7299270072992705e-06,
      "loss": 1.4401,
      "step": 121300
    },
    {
      "epoch": 885.4744525547445,
      "grad_norm": 5.104483604431152,
      "learning_rate": 5.726277372262774e-06,
      "loss": 0.7111,
      "step": 121310
    },
    {
      "epoch": 885.5474452554745,
      "grad_norm": 11.868173599243164,
      "learning_rate": 5.7226277372262774e-06,
      "loss": 0.8322,
      "step": 121320
    },
    {
      "epoch": 885.6204379562043,
      "grad_norm": 4.757722854614258,
      "learning_rate": 5.718978102189781e-06,
      "loss": 0.6767,
      "step": 121330
    },
    {
      "epoch": 885.6934306569343,
      "grad_norm": 10.420380592346191,
      "learning_rate": 5.715328467153285e-06,
      "loss": 0.9335,
      "step": 121340
    },
    {
      "epoch": 885.7664233576643,
      "grad_norm": 0.04779001325368881,
      "learning_rate": 5.711678832116789e-06,
      "loss": 0.8725,
      "step": 121350
    },
    {
      "epoch": 885.8394160583941,
      "grad_norm": 11.969548225402832,
      "learning_rate": 5.708029197080292e-06,
      "loss": 1.0913,
      "step": 121360
    },
    {
      "epoch": 885.9124087591241,
      "grad_norm": 14.73355484008789,
      "learning_rate": 5.704379562043796e-06,
      "loss": 1.0501,
      "step": 121370
    },
    {
      "epoch": 885.985401459854,
      "grad_norm": 20.99821662902832,
      "learning_rate": 5.7007299270073e-06,
      "loss": 1.1303,
      "step": 121380
    },
    {
      "epoch": 886.0583941605839,
      "grad_norm": 7.053927421569824,
      "learning_rate": 5.697080291970803e-06,
      "loss": 0.8907,
      "step": 121390
    },
    {
      "epoch": 886.1313868613139,
      "grad_norm": 2.2747883796691895,
      "learning_rate": 5.693430656934307e-06,
      "loss": 0.6748,
      "step": 121400
    },
    {
      "epoch": 886.2043795620438,
      "grad_norm": 9.479033470153809,
      "learning_rate": 5.68978102189781e-06,
      "loss": 0.9066,
      "step": 121410
    },
    {
      "epoch": 886.2773722627737,
      "grad_norm": 0.09025602787733078,
      "learning_rate": 5.686131386861314e-06,
      "loss": 0.7647,
      "step": 121420
    },
    {
      "epoch": 886.3503649635037,
      "grad_norm": 5.901110649108887,
      "learning_rate": 5.682481751824818e-06,
      "loss": 1.0909,
      "step": 121430
    },
    {
      "epoch": 886.4233576642335,
      "grad_norm": 6.937658309936523,
      "learning_rate": 5.6788321167883215e-06,
      "loss": 0.7548,
      "step": 121440
    },
    {
      "epoch": 886.4963503649635,
      "grad_norm": 10.583932876586914,
      "learning_rate": 5.675182481751825e-06,
      "loss": 1.1774,
      "step": 121450
    },
    {
      "epoch": 886.5693430656934,
      "grad_norm": 20.53533172607422,
      "learning_rate": 5.671532846715328e-06,
      "loss": 0.7714,
      "step": 121460
    },
    {
      "epoch": 886.6423357664233,
      "grad_norm": 12.933122634887695,
      "learning_rate": 5.667883211678832e-06,
      "loss": 0.9347,
      "step": 121470
    },
    {
      "epoch": 886.7153284671533,
      "grad_norm": 16.901025772094727,
      "learning_rate": 5.664233576642336e-06,
      "loss": 0.592,
      "step": 121480
    },
    {
      "epoch": 886.7883211678832,
      "grad_norm": 0.062091439962387085,
      "learning_rate": 5.66058394160584e-06,
      "loss": 1.0914,
      "step": 121490
    },
    {
      "epoch": 886.8613138686131,
      "grad_norm": 10.411433219909668,
      "learning_rate": 5.656934306569343e-06,
      "loss": 1.0092,
      "step": 121500
    },
    {
      "epoch": 886.9343065693431,
      "grad_norm": 9.565569877624512,
      "learning_rate": 5.653284671532847e-06,
      "loss": 1.0019,
      "step": 121510
    },
    {
      "epoch": 887.007299270073,
      "grad_norm": 16.10431671142578,
      "learning_rate": 5.649635036496351e-06,
      "loss": 0.894,
      "step": 121520
    },
    {
      "epoch": 887.0802919708029,
      "grad_norm": 8.145441055297852,
      "learning_rate": 5.645985401459855e-06,
      "loss": 0.9137,
      "step": 121530
    },
    {
      "epoch": 887.1532846715329,
      "grad_norm": 6.216732978820801,
      "learning_rate": 5.642335766423358e-06,
      "loss": 0.4823,
      "step": 121540
    },
    {
      "epoch": 887.2262773722628,
      "grad_norm": 10.479214668273926,
      "learning_rate": 5.638686131386862e-06,
      "loss": 1.0559,
      "step": 121550
    },
    {
      "epoch": 887.2992700729927,
      "grad_norm": 13.68901252746582,
      "learning_rate": 5.6350364963503655e-06,
      "loss": 0.7677,
      "step": 121560
    },
    {
      "epoch": 887.3722627737226,
      "grad_norm": 11.50106143951416,
      "learning_rate": 5.631386861313869e-06,
      "loss": 0.592,
      "step": 121570
    },
    {
      "epoch": 887.4452554744526,
      "grad_norm": 7.327057838439941,
      "learning_rate": 5.6277372262773724e-06,
      "loss": 0.8128,
      "step": 121580
    },
    {
      "epoch": 887.5182481751825,
      "grad_norm": 12.749428749084473,
      "learning_rate": 5.624087591240876e-06,
      "loss": 1.325,
      "step": 121590
    },
    {
      "epoch": 887.5912408759124,
      "grad_norm": 12.4976806640625,
      "learning_rate": 5.620437956204379e-06,
      "loss": 1.4132,
      "step": 121600
    },
    {
      "epoch": 887.6642335766423,
      "grad_norm": 11.436840057373047,
      "learning_rate": 5.616788321167883e-06,
      "loss": 0.5706,
      "step": 121610
    },
    {
      "epoch": 887.7372262773723,
      "grad_norm": 11.76298999786377,
      "learning_rate": 5.613138686131387e-06,
      "loss": 0.7763,
      "step": 121620
    },
    {
      "epoch": 887.8102189781022,
      "grad_norm": 0.04433845356106758,
      "learning_rate": 5.60948905109489e-06,
      "loss": 0.6407,
      "step": 121630
    },
    {
      "epoch": 887.8832116788321,
      "grad_norm": 6.567314147949219,
      "learning_rate": 5.605839416058394e-06,
      "loss": 1.3696,
      "step": 121640
    },
    {
      "epoch": 887.956204379562,
      "grad_norm": 10.483525276184082,
      "learning_rate": 5.602189781021898e-06,
      "loss": 0.7759,
      "step": 121650
    },
    {
      "epoch": 888.029197080292,
      "grad_norm": 10.11129379272461,
      "learning_rate": 5.598540145985402e-06,
      "loss": 1.2047,
      "step": 121660
    },
    {
      "epoch": 888.1021897810219,
      "grad_norm": 7.473496913909912,
      "learning_rate": 5.594890510948905e-06,
      "loss": 0.7871,
      "step": 121670
    },
    {
      "epoch": 888.1751824817518,
      "grad_norm": 6.143967151641846,
      "learning_rate": 5.591240875912409e-06,
      "loss": 0.5608,
      "step": 121680
    },
    {
      "epoch": 888.2481751824818,
      "grad_norm": 15.737632751464844,
      "learning_rate": 5.587591240875913e-06,
      "loss": 0.8116,
      "step": 121690
    },
    {
      "epoch": 888.3211678832117,
      "grad_norm": 14.715238571166992,
      "learning_rate": 5.5839416058394165e-06,
      "loss": 1.0793,
      "step": 121700
    },
    {
      "epoch": 888.3941605839416,
      "grad_norm": 8.58220386505127,
      "learning_rate": 5.58029197080292e-06,
      "loss": 0.698,
      "step": 121710
    },
    {
      "epoch": 888.4671532846716,
      "grad_norm": 0.09167512506246567,
      "learning_rate": 5.576642335766423e-06,
      "loss": 0.9434,
      "step": 121720
    },
    {
      "epoch": 888.5401459854014,
      "grad_norm": 6.9202561378479,
      "learning_rate": 5.572992700729927e-06,
      "loss": 1.108,
      "step": 121730
    },
    {
      "epoch": 888.6131386861314,
      "grad_norm": 13.654677391052246,
      "learning_rate": 5.569343065693431e-06,
      "loss": 0.9401,
      "step": 121740
    },
    {
      "epoch": 888.6861313868613,
      "grad_norm": 15.638442039489746,
      "learning_rate": 5.565693430656935e-06,
      "loss": 0.8718,
      "step": 121750
    },
    {
      "epoch": 888.7591240875912,
      "grad_norm": 10.642437934875488,
      "learning_rate": 5.562043795620438e-06,
      "loss": 0.9433,
      "step": 121760
    },
    {
      "epoch": 888.8321167883212,
      "grad_norm": 11.594819068908691,
      "learning_rate": 5.558394160583942e-06,
      "loss": 0.9416,
      "step": 121770
    },
    {
      "epoch": 888.9051094890511,
      "grad_norm": 17.151992797851562,
      "learning_rate": 5.554744525547446e-06,
      "loss": 0.8512,
      "step": 121780
    },
    {
      "epoch": 888.978102189781,
      "grad_norm": 7.9288129806518555,
      "learning_rate": 5.55109489051095e-06,
      "loss": 0.9587,
      "step": 121790
    },
    {
      "epoch": 889.051094890511,
      "grad_norm": 15.766349792480469,
      "learning_rate": 5.547445255474453e-06,
      "loss": 0.8107,
      "step": 121800
    },
    {
      "epoch": 889.1240875912408,
      "grad_norm": 0.11103267967700958,
      "learning_rate": 5.543795620437957e-06,
      "loss": 0.8248,
      "step": 121810
    },
    {
      "epoch": 889.1970802919708,
      "grad_norm": 12.266657829284668,
      "learning_rate": 5.54014598540146e-06,
      "loss": 0.6291,
      "step": 121820
    },
    {
      "epoch": 889.2700729927008,
      "grad_norm": 9.01781177520752,
      "learning_rate": 5.5364963503649636e-06,
      "loss": 0.9648,
      "step": 121830
    },
    {
      "epoch": 889.3430656934306,
      "grad_norm": 18.809772491455078,
      "learning_rate": 5.5328467153284675e-06,
      "loss": 1.0495,
      "step": 121840
    },
    {
      "epoch": 889.4160583941606,
      "grad_norm": 8.1864652633667,
      "learning_rate": 5.5291970802919705e-06,
      "loss": 1.2704,
      "step": 121850
    },
    {
      "epoch": 889.4890510948906,
      "grad_norm": 7.783308029174805,
      "learning_rate": 5.525547445255474e-06,
      "loss": 0.5466,
      "step": 121860
    },
    {
      "epoch": 889.5620437956204,
      "grad_norm": 10.154852867126465,
      "learning_rate": 5.521897810218978e-06,
      "loss": 0.8763,
      "step": 121870
    },
    {
      "epoch": 889.6350364963504,
      "grad_norm": 0.10080575197935104,
      "learning_rate": 5.518248175182482e-06,
      "loss": 0.659,
      "step": 121880
    },
    {
      "epoch": 889.7080291970802,
      "grad_norm": 0.12375660240650177,
      "learning_rate": 5.514598540145985e-06,
      "loss": 0.9423,
      "step": 121890
    },
    {
      "epoch": 889.7810218978102,
      "grad_norm": 0.06311286240816116,
      "learning_rate": 5.510948905109489e-06,
      "loss": 0.8869,
      "step": 121900
    },
    {
      "epoch": 889.8540145985402,
      "grad_norm": 0.06151129677891731,
      "learning_rate": 5.507299270072993e-06,
      "loss": 0.6484,
      "step": 121910
    },
    {
      "epoch": 889.92700729927,
      "grad_norm": 7.639858722686768,
      "learning_rate": 5.503649635036497e-06,
      "loss": 0.9508,
      "step": 121920
    },
    {
      "epoch": 890.0,
      "grad_norm": 0.03726595640182495,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.0669,
      "step": 121930
    },
    {
      "epoch": 890.07299270073,
      "grad_norm": 10.827583312988281,
      "learning_rate": 5.496350364963504e-06,
      "loss": 0.7449,
      "step": 121940
    },
    {
      "epoch": 890.1459854014598,
      "grad_norm": 0.05518829822540283,
      "learning_rate": 5.492700729927008e-06,
      "loss": 1.0045,
      "step": 121950
    },
    {
      "epoch": 890.2189781021898,
      "grad_norm": 15.392720222473145,
      "learning_rate": 5.4890510948905115e-06,
      "loss": 1.3085,
      "step": 121960
    },
    {
      "epoch": 890.2919708029198,
      "grad_norm": 11.699389457702637,
      "learning_rate": 5.485401459854015e-06,
      "loss": 1.15,
      "step": 121970
    },
    {
      "epoch": 890.3649635036496,
      "grad_norm": 6.142621994018555,
      "learning_rate": 5.481751824817518e-06,
      "loss": 0.7707,
      "step": 121980
    },
    {
      "epoch": 890.4379562043796,
      "grad_norm": 0.016302961856126785,
      "learning_rate": 5.478102189781022e-06,
      "loss": 0.6465,
      "step": 121990
    },
    {
      "epoch": 890.5109489051094,
      "grad_norm": 7.555207252502441,
      "learning_rate": 5.474452554744526e-06,
      "loss": 0.6308,
      "step": 122000
    },
    {
      "epoch": 890.5839416058394,
      "grad_norm": 0.05397002398967743,
      "learning_rate": 5.47080291970803e-06,
      "loss": 0.8819,
      "step": 122010
    },
    {
      "epoch": 890.6569343065694,
      "grad_norm": 9.275533676147461,
      "learning_rate": 5.467153284671533e-06,
      "loss": 1.2435,
      "step": 122020
    },
    {
      "epoch": 890.7299270072992,
      "grad_norm": 11.143062591552734,
      "learning_rate": 5.463503649635036e-06,
      "loss": 0.8464,
      "step": 122030
    },
    {
      "epoch": 890.8029197080292,
      "grad_norm": 10.286255836486816,
      "learning_rate": 5.45985401459854e-06,
      "loss": 0.8271,
      "step": 122040
    },
    {
      "epoch": 890.8759124087592,
      "grad_norm": 0.04275300353765488,
      "learning_rate": 5.456204379562044e-06,
      "loss": 0.5951,
      "step": 122050
    },
    {
      "epoch": 890.948905109489,
      "grad_norm": 0.06793199479579926,
      "learning_rate": 5.452554744525548e-06,
      "loss": 0.8762,
      "step": 122060
    },
    {
      "epoch": 891.021897810219,
      "grad_norm": 6.733689308166504,
      "learning_rate": 5.448905109489051e-06,
      "loss": 0.9112,
      "step": 122070
    },
    {
      "epoch": 891.0948905109489,
      "grad_norm": 0.03127370402216911,
      "learning_rate": 5.445255474452555e-06,
      "loss": 1.0261,
      "step": 122080
    },
    {
      "epoch": 891.1678832116788,
      "grad_norm": 6.755554676055908,
      "learning_rate": 5.441605839416059e-06,
      "loss": 1.0889,
      "step": 122090
    },
    {
      "epoch": 891.2408759124088,
      "grad_norm": 8.754278182983398,
      "learning_rate": 5.4379562043795625e-06,
      "loss": 0.8614,
      "step": 122100
    },
    {
      "epoch": 891.3138686131387,
      "grad_norm": 6.487420082092285,
      "learning_rate": 5.4343065693430655e-06,
      "loss": 0.7369,
      "step": 122110
    },
    {
      "epoch": 891.3868613138686,
      "grad_norm": 7.852011680603027,
      "learning_rate": 5.430656934306569e-06,
      "loss": 0.8293,
      "step": 122120
    },
    {
      "epoch": 891.4598540145986,
      "grad_norm": 13.611224174499512,
      "learning_rate": 5.427007299270073e-06,
      "loss": 1.1109,
      "step": 122130
    },
    {
      "epoch": 891.5328467153284,
      "grad_norm": 7.729373455047607,
      "learning_rate": 5.423357664233577e-06,
      "loss": 0.8386,
      "step": 122140
    },
    {
      "epoch": 891.6058394160584,
      "grad_norm": 5.671829700469971,
      "learning_rate": 5.41970802919708e-06,
      "loss": 1.0231,
      "step": 122150
    },
    {
      "epoch": 891.6788321167883,
      "grad_norm": 0.05183807387948036,
      "learning_rate": 5.416058394160584e-06,
      "loss": 0.6509,
      "step": 122160
    },
    {
      "epoch": 891.7518248175182,
      "grad_norm": 11.389848709106445,
      "learning_rate": 5.412408759124088e-06,
      "loss": 0.7869,
      "step": 122170
    },
    {
      "epoch": 891.8248175182482,
      "grad_norm": 10.815824508666992,
      "learning_rate": 5.408759124087592e-06,
      "loss": 1.0356,
      "step": 122180
    },
    {
      "epoch": 891.8978102189781,
      "grad_norm": 10.963367462158203,
      "learning_rate": 5.405109489051096e-06,
      "loss": 1.0078,
      "step": 122190
    },
    {
      "epoch": 891.970802919708,
      "grad_norm": 4.9889726638793945,
      "learning_rate": 5.401459854014599e-06,
      "loss": 0.5051,
      "step": 122200
    },
    {
      "epoch": 892.043795620438,
      "grad_norm": 0.01781008392572403,
      "learning_rate": 5.397810218978103e-06,
      "loss": 1.1841,
      "step": 122210
    },
    {
      "epoch": 892.1167883211679,
      "grad_norm": 10.882170677185059,
      "learning_rate": 5.3941605839416065e-06,
      "loss": 0.8767,
      "step": 122220
    },
    {
      "epoch": 892.1897810218978,
      "grad_norm": 11.03796672821045,
      "learning_rate": 5.39051094890511e-06,
      "loss": 0.8066,
      "step": 122230
    },
    {
      "epoch": 892.2627737226277,
      "grad_norm": 11.525358200073242,
      "learning_rate": 5.3868613138686134e-06,
      "loss": 0.7792,
      "step": 122240
    },
    {
      "epoch": 892.3357664233577,
      "grad_norm": 7.265815258026123,
      "learning_rate": 5.3832116788321165e-06,
      "loss": 0.5373,
      "step": 122250
    },
    {
      "epoch": 892.4087591240876,
      "grad_norm": 5.952640056610107,
      "learning_rate": 5.37956204379562e-06,
      "loss": 0.9835,
      "step": 122260
    },
    {
      "epoch": 892.4817518248175,
      "grad_norm": 16.97203826904297,
      "learning_rate": 5.375912408759124e-06,
      "loss": 0.8276,
      "step": 122270
    },
    {
      "epoch": 892.5547445255474,
      "grad_norm": 9.494342803955078,
      "learning_rate": 5.372262773722628e-06,
      "loss": 0.8387,
      "step": 122280
    },
    {
      "epoch": 892.6277372262774,
      "grad_norm": 7.477221965789795,
      "learning_rate": 5.368613138686131e-06,
      "loss": 0.5888,
      "step": 122290
    },
    {
      "epoch": 892.7007299270073,
      "grad_norm": 8.385987281799316,
      "learning_rate": 5.364963503649635e-06,
      "loss": 0.8307,
      "step": 122300
    },
    {
      "epoch": 892.7737226277372,
      "grad_norm": 13.60940170288086,
      "learning_rate": 5.361313868613139e-06,
      "loss": 1.4216,
      "step": 122310
    },
    {
      "epoch": 892.8467153284671,
      "grad_norm": 13.851045608520508,
      "learning_rate": 5.357664233576643e-06,
      "loss": 0.7771,
      "step": 122320
    },
    {
      "epoch": 892.9197080291971,
      "grad_norm": 11.217613220214844,
      "learning_rate": 5.354014598540146e-06,
      "loss": 0.8044,
      "step": 122330
    },
    {
      "epoch": 892.992700729927,
      "grad_norm": 8.198309898376465,
      "learning_rate": 5.35036496350365e-06,
      "loss": 1.0445,
      "step": 122340
    },
    {
      "epoch": 893.0656934306569,
      "grad_norm": 8.782782554626465,
      "learning_rate": 5.346715328467154e-06,
      "loss": 0.9126,
      "step": 122350
    },
    {
      "epoch": 893.1386861313869,
      "grad_norm": 0.16365644335746765,
      "learning_rate": 5.3430656934306575e-06,
      "loss": 0.6405,
      "step": 122360
    },
    {
      "epoch": 893.2116788321168,
      "grad_norm": 6.212843418121338,
      "learning_rate": 5.3394160583941605e-06,
      "loss": 1.051,
      "step": 122370
    },
    {
      "epoch": 893.2846715328467,
      "grad_norm": 7.586451530456543,
      "learning_rate": 5.335766423357664e-06,
      "loss": 0.6861,
      "step": 122380
    },
    {
      "epoch": 893.3576642335767,
      "grad_norm": 7.779687404632568,
      "learning_rate": 5.332116788321168e-06,
      "loss": 0.8901,
      "step": 122390
    },
    {
      "epoch": 893.4306569343066,
      "grad_norm": 0.02756365016102791,
      "learning_rate": 5.328467153284672e-06,
      "loss": 0.7323,
      "step": 122400
    },
    {
      "epoch": 893.5036496350365,
      "grad_norm": 11.76361083984375,
      "learning_rate": 5.324817518248175e-06,
      "loss": 0.6307,
      "step": 122410
    },
    {
      "epoch": 893.5766423357665,
      "grad_norm": 8.40112590789795,
      "learning_rate": 5.321167883211679e-06,
      "loss": 1.1202,
      "step": 122420
    },
    {
      "epoch": 893.6496350364963,
      "grad_norm": 0.018541693687438965,
      "learning_rate": 5.317518248175183e-06,
      "loss": 0.9116,
      "step": 122430
    },
    {
      "epoch": 893.7226277372263,
      "grad_norm": 10.33588695526123,
      "learning_rate": 5.313868613138687e-06,
      "loss": 0.6175,
      "step": 122440
    },
    {
      "epoch": 893.7956204379562,
      "grad_norm": 12.584189414978027,
      "learning_rate": 5.310218978102191e-06,
      "loss": 1.1895,
      "step": 122450
    },
    {
      "epoch": 893.8686131386861,
      "grad_norm": 9.180326461791992,
      "learning_rate": 5.306569343065693e-06,
      "loss": 1.0639,
      "step": 122460
    },
    {
      "epoch": 893.9416058394161,
      "grad_norm": 0.024402234703302383,
      "learning_rate": 5.302919708029197e-06,
      "loss": 0.9508,
      "step": 122470
    },
    {
      "epoch": 894.014598540146,
      "grad_norm": 9.43233871459961,
      "learning_rate": 5.299270072992701e-06,
      "loss": 0.9105,
      "step": 122480
    },
    {
      "epoch": 894.0875912408759,
      "grad_norm": 7.580503463745117,
      "learning_rate": 5.2956204379562046e-06,
      "loss": 0.7452,
      "step": 122490
    },
    {
      "epoch": 894.1605839416059,
      "grad_norm": 1.2679682970046997,
      "learning_rate": 5.2919708029197084e-06,
      "loss": 0.8534,
      "step": 122500
    },
    {
      "epoch": 894.2335766423357,
      "grad_norm": 5.686723232269287,
      "learning_rate": 5.2883211678832115e-06,
      "loss": 0.941,
      "step": 122510
    },
    {
      "epoch": 894.3065693430657,
      "grad_norm": 9.957046508789062,
      "learning_rate": 5.284671532846715e-06,
      "loss": 0.9252,
      "step": 122520
    },
    {
      "epoch": 894.3795620437957,
      "grad_norm": 15.577642440795898,
      "learning_rate": 5.281021897810219e-06,
      "loss": 0.9647,
      "step": 122530
    },
    {
      "epoch": 894.4525547445255,
      "grad_norm": 11.719904899597168,
      "learning_rate": 5.277372262773723e-06,
      "loss": 0.8757,
      "step": 122540
    },
    {
      "epoch": 894.5255474452555,
      "grad_norm": 7.304050922393799,
      "learning_rate": 5.273722627737226e-06,
      "loss": 1.2981,
      "step": 122550
    },
    {
      "epoch": 894.5985401459855,
      "grad_norm": 10.488180160522461,
      "learning_rate": 5.27007299270073e-06,
      "loss": 0.8197,
      "step": 122560
    },
    {
      "epoch": 894.6715328467153,
      "grad_norm": 10.832721710205078,
      "learning_rate": 5.266423357664234e-06,
      "loss": 0.8085,
      "step": 122570
    },
    {
      "epoch": 894.7445255474453,
      "grad_norm": 14.634207725524902,
      "learning_rate": 5.262773722627738e-06,
      "loss": 0.766,
      "step": 122580
    },
    {
      "epoch": 894.8175182481751,
      "grad_norm": 8.666703224182129,
      "learning_rate": 5.259124087591241e-06,
      "loss": 0.74,
      "step": 122590
    },
    {
      "epoch": 894.8905109489051,
      "grad_norm": 7.17069673538208,
      "learning_rate": 5.255474452554745e-06,
      "loss": 0.9529,
      "step": 122600
    },
    {
      "epoch": 894.9635036496351,
      "grad_norm": 11.591145515441895,
      "learning_rate": 5.251824817518249e-06,
      "loss": 0.9305,
      "step": 122610
    },
    {
      "epoch": 895.0364963503649,
      "grad_norm": 7.433196067810059,
      "learning_rate": 5.2481751824817525e-06,
      "loss": 0.8936,
      "step": 122620
    },
    {
      "epoch": 895.1094890510949,
      "grad_norm": 9.716678619384766,
      "learning_rate": 5.2445255474452555e-06,
      "loss": 0.5111,
      "step": 122630
    },
    {
      "epoch": 895.1824817518249,
      "grad_norm": 7.425649166107178,
      "learning_rate": 5.240875912408759e-06,
      "loss": 1.3166,
      "step": 122640
    },
    {
      "epoch": 895.2554744525547,
      "grad_norm": 0.009267491288483143,
      "learning_rate": 5.237226277372263e-06,
      "loss": 0.8295,
      "step": 122650
    },
    {
      "epoch": 895.3284671532847,
      "grad_norm": 9.597224235534668,
      "learning_rate": 5.233576642335767e-06,
      "loss": 0.7862,
      "step": 122660
    },
    {
      "epoch": 895.4014598540145,
      "grad_norm": 0.02442430891096592,
      "learning_rate": 5.229927007299271e-06,
      "loss": 0.9739,
      "step": 122670
    },
    {
      "epoch": 895.4744525547445,
      "grad_norm": 9.537189483642578,
      "learning_rate": 5.226277372262773e-06,
      "loss": 1.052,
      "step": 122680
    },
    {
      "epoch": 895.5474452554745,
      "grad_norm": 16.589550018310547,
      "learning_rate": 5.222627737226277e-06,
      "loss": 0.9298,
      "step": 122690
    },
    {
      "epoch": 895.6204379562043,
      "grad_norm": 1.4432836771011353,
      "learning_rate": 5.218978102189781e-06,
      "loss": 0.985,
      "step": 122700
    },
    {
      "epoch": 895.6934306569343,
      "grad_norm": 21.957544326782227,
      "learning_rate": 5.215328467153285e-06,
      "loss": 0.9551,
      "step": 122710
    },
    {
      "epoch": 895.7664233576643,
      "grad_norm": 12.943984031677246,
      "learning_rate": 5.211678832116789e-06,
      "loss": 0.8324,
      "step": 122720
    },
    {
      "epoch": 895.8394160583941,
      "grad_norm": 15.86030387878418,
      "learning_rate": 5.208029197080292e-06,
      "loss": 0.6895,
      "step": 122730
    },
    {
      "epoch": 895.9124087591241,
      "grad_norm": 10.197606086730957,
      "learning_rate": 5.204379562043796e-06,
      "loss": 0.7775,
      "step": 122740
    },
    {
      "epoch": 895.985401459854,
      "grad_norm": 13.55351734161377,
      "learning_rate": 5.2007299270073e-06,
      "loss": 0.7733,
      "step": 122750
    },
    {
      "epoch": 896.0583941605839,
      "grad_norm": 12.935224533081055,
      "learning_rate": 5.1970802919708035e-06,
      "loss": 0.7047,
      "step": 122760
    },
    {
      "epoch": 896.1313868613139,
      "grad_norm": 9.357721328735352,
      "learning_rate": 5.1934306569343065e-06,
      "loss": 1.1897,
      "step": 122770
    },
    {
      "epoch": 896.2043795620438,
      "grad_norm": 10.90561294555664,
      "learning_rate": 5.18978102189781e-06,
      "loss": 0.9112,
      "step": 122780
    },
    {
      "epoch": 896.2773722627737,
      "grad_norm": 6.119724273681641,
      "learning_rate": 5.186131386861314e-06,
      "loss": 0.8332,
      "step": 122790
    },
    {
      "epoch": 896.3503649635037,
      "grad_norm": 20.706459045410156,
      "learning_rate": 5.182481751824818e-06,
      "loss": 1.1769,
      "step": 122800
    },
    {
      "epoch": 896.4233576642335,
      "grad_norm": 16.674821853637695,
      "learning_rate": 5.178832116788321e-06,
      "loss": 0.7446,
      "step": 122810
    },
    {
      "epoch": 896.4963503649635,
      "grad_norm": 11.765270233154297,
      "learning_rate": 5.175182481751825e-06,
      "loss": 0.5698,
      "step": 122820
    },
    {
      "epoch": 896.5693430656934,
      "grad_norm": 7.515198707580566,
      "learning_rate": 5.171532846715329e-06,
      "loss": 0.7661,
      "step": 122830
    },
    {
      "epoch": 896.6423357664233,
      "grad_norm": 0.07839919626712799,
      "learning_rate": 5.167883211678833e-06,
      "loss": 0.8998,
      "step": 122840
    },
    {
      "epoch": 896.7153284671533,
      "grad_norm": 5.971027851104736,
      "learning_rate": 5.164233576642336e-06,
      "loss": 1.1709,
      "step": 122850
    },
    {
      "epoch": 896.7883211678832,
      "grad_norm": 11.42332935333252,
      "learning_rate": 5.16058394160584e-06,
      "loss": 0.7741,
      "step": 122860
    },
    {
      "epoch": 896.8613138686131,
      "grad_norm": 5.71228551864624,
      "learning_rate": 5.156934306569344e-06,
      "loss": 0.8942,
      "step": 122870
    },
    {
      "epoch": 896.9343065693431,
      "grad_norm": 11.059385299682617,
      "learning_rate": 5.1532846715328475e-06,
      "loss": 0.8557,
      "step": 122880
    },
    {
      "epoch": 897.007299270073,
      "grad_norm": 8.759148597717285,
      "learning_rate": 5.1496350364963505e-06,
      "loss": 1.0081,
      "step": 122890
    },
    {
      "epoch": 897.0802919708029,
      "grad_norm": 11.621360778808594,
      "learning_rate": 5.145985401459854e-06,
      "loss": 0.8456,
      "step": 122900
    },
    {
      "epoch": 897.1532846715329,
      "grad_norm": 13.360451698303223,
      "learning_rate": 5.1423357664233575e-06,
      "loss": 0.9504,
      "step": 122910
    },
    {
      "epoch": 897.2262773722628,
      "grad_norm": 7.232234477996826,
      "learning_rate": 5.138686131386861e-06,
      "loss": 0.8945,
      "step": 122920
    },
    {
      "epoch": 897.2992700729927,
      "grad_norm": 8.472895622253418,
      "learning_rate": 5.135036496350365e-06,
      "loss": 0.5662,
      "step": 122930
    },
    {
      "epoch": 897.3722627737226,
      "grad_norm": 8.711047172546387,
      "learning_rate": 5.131386861313868e-06,
      "loss": 0.8332,
      "step": 122940
    },
    {
      "epoch": 897.4452554744526,
      "grad_norm": 8.87010669708252,
      "learning_rate": 5.127737226277372e-06,
      "loss": 0.8491,
      "step": 122950
    },
    {
      "epoch": 897.5182481751825,
      "grad_norm": 7.290646553039551,
      "learning_rate": 5.124087591240876e-06,
      "loss": 1.1521,
      "step": 122960
    },
    {
      "epoch": 897.5912408759124,
      "grad_norm": 0.16971668601036072,
      "learning_rate": 5.12043795620438e-06,
      "loss": 0.5326,
      "step": 122970
    },
    {
      "epoch": 897.6642335766423,
      "grad_norm": 11.287592887878418,
      "learning_rate": 5.116788321167884e-06,
      "loss": 1.1522,
      "step": 122980
    },
    {
      "epoch": 897.7372262773723,
      "grad_norm": 0.08225743472576141,
      "learning_rate": 5.113138686131387e-06,
      "loss": 1.0662,
      "step": 122990
    },
    {
      "epoch": 897.8102189781022,
      "grad_norm": 10.915743827819824,
      "learning_rate": 5.109489051094891e-06,
      "loss": 0.7492,
      "step": 123000
    },
    {
      "epoch": 897.8832116788321,
      "grad_norm": 11.115606307983398,
      "learning_rate": 5.105839416058395e-06,
      "loss": 1.3053,
      "step": 123010
    },
    {
      "epoch": 897.956204379562,
      "grad_norm": 9.467081069946289,
      "learning_rate": 5.1021897810218985e-06,
      "loss": 0.594,
      "step": 123020
    },
    {
      "epoch": 898.029197080292,
      "grad_norm": 5.822203159332275,
      "learning_rate": 5.0985401459854015e-06,
      "loss": 0.5179,
      "step": 123030
    },
    {
      "epoch": 898.1021897810219,
      "grad_norm": 13.452250480651855,
      "learning_rate": 5.094890510948905e-06,
      "loss": 0.9611,
      "step": 123040
    },
    {
      "epoch": 898.1751824817518,
      "grad_norm": 7.5036163330078125,
      "learning_rate": 5.091240875912409e-06,
      "loss": 0.6212,
      "step": 123050
    },
    {
      "epoch": 898.2481751824818,
      "grad_norm": 7.644865989685059,
      "learning_rate": 5.087591240875913e-06,
      "loss": 0.8231,
      "step": 123060
    },
    {
      "epoch": 898.3211678832117,
      "grad_norm": 15.206270217895508,
      "learning_rate": 5.083941605839416e-06,
      "loss": 0.958,
      "step": 123070
    },
    {
      "epoch": 898.3941605839416,
      "grad_norm": 0.04205181822180748,
      "learning_rate": 5.08029197080292e-06,
      "loss": 0.6823,
      "step": 123080
    },
    {
      "epoch": 898.4671532846716,
      "grad_norm": 13.261775970458984,
      "learning_rate": 5.076642335766424e-06,
      "loss": 1.2319,
      "step": 123090
    },
    {
      "epoch": 898.5401459854014,
      "grad_norm": 16.95144271850586,
      "learning_rate": 5.072992700729928e-06,
      "loss": 1.046,
      "step": 123100
    },
    {
      "epoch": 898.6131386861314,
      "grad_norm": 10.508577346801758,
      "learning_rate": 5.069343065693431e-06,
      "loss": 0.8165,
      "step": 123110
    },
    {
      "epoch": 898.6861313868613,
      "grad_norm": 9.541501998901367,
      "learning_rate": 5.065693430656934e-06,
      "loss": 0.7564,
      "step": 123120
    },
    {
      "epoch": 898.7591240875912,
      "grad_norm": 7.690008163452148,
      "learning_rate": 5.062043795620438e-06,
      "loss": 0.6579,
      "step": 123130
    },
    {
      "epoch": 898.8321167883212,
      "grad_norm": 8.679036140441895,
      "learning_rate": 5.058394160583942e-06,
      "loss": 0.8611,
      "step": 123140
    },
    {
      "epoch": 898.9051094890511,
      "grad_norm": 15.872662544250488,
      "learning_rate": 5.0547445255474456e-06,
      "loss": 1.0241,
      "step": 123150
    },
    {
      "epoch": 898.978102189781,
      "grad_norm": 0.09278565645217896,
      "learning_rate": 5.051094890510949e-06,
      "loss": 0.7685,
      "step": 123160
    },
    {
      "epoch": 899.051094890511,
      "grad_norm": 4.049553394317627,
      "learning_rate": 5.0474452554744525e-06,
      "loss": 1.3025,
      "step": 123170
    },
    {
      "epoch": 899.1240875912408,
      "grad_norm": 1.8300145864486694,
      "learning_rate": 5.043795620437956e-06,
      "loss": 0.6842,
      "step": 123180
    },
    {
      "epoch": 899.1970802919708,
      "grad_norm": 8.390904426574707,
      "learning_rate": 5.04014598540146e-06,
      "loss": 0.7715,
      "step": 123190
    },
    {
      "epoch": 899.2700729927008,
      "grad_norm": 13.82380199432373,
      "learning_rate": 5.036496350364963e-06,
      "loss": 0.8256,
      "step": 123200
    },
    {
      "epoch": 899.3430656934306,
      "grad_norm": 0.0674695074558258,
      "learning_rate": 5.032846715328467e-06,
      "loss": 0.7145,
      "step": 123210
    },
    {
      "epoch": 899.4160583941606,
      "grad_norm": 9.031742095947266,
      "learning_rate": 5.029197080291971e-06,
      "loss": 0.9328,
      "step": 123220
    },
    {
      "epoch": 899.4890510948906,
      "grad_norm": 6.134068489074707,
      "learning_rate": 5.025547445255475e-06,
      "loss": 0.7702,
      "step": 123230
    },
    {
      "epoch": 899.5620437956204,
      "grad_norm": 6.986095905303955,
      "learning_rate": 5.021897810218979e-06,
      "loss": 0.711,
      "step": 123240
    },
    {
      "epoch": 899.6350364963504,
      "grad_norm": 13.096538543701172,
      "learning_rate": 5.018248175182482e-06,
      "loss": 0.9675,
      "step": 123250
    },
    {
      "epoch": 899.7080291970802,
      "grad_norm": 0.09729419648647308,
      "learning_rate": 5.014598540145986e-06,
      "loss": 0.629,
      "step": 123260
    },
    {
      "epoch": 899.7810218978102,
      "grad_norm": 10.39065170288086,
      "learning_rate": 5.01094890510949e-06,
      "loss": 1.0162,
      "step": 123270
    },
    {
      "epoch": 899.8540145985402,
      "grad_norm": 18.40441131591797,
      "learning_rate": 5.0072992700729935e-06,
      "loss": 1.027,
      "step": 123280
    },
    {
      "epoch": 899.92700729927,
      "grad_norm": 8.276453971862793,
      "learning_rate": 5.0036496350364965e-06,
      "loss": 1.0397,
      "step": 123290
    },
    {
      "epoch": 900.0,
      "grad_norm": 0.4230465590953827,
      "learning_rate": 5e-06,
      "loss": 0.8704,
      "step": 123300
    },
    {
      "epoch": 900.07299270073,
      "grad_norm": 12.875714302062988,
      "learning_rate": 4.996350364963504e-06,
      "loss": 0.8645,
      "step": 123310
    },
    {
      "epoch": 900.1459854014598,
      "grad_norm": 9.062385559082031,
      "learning_rate": 4.992700729927008e-06,
      "loss": 0.8364,
      "step": 123320
    },
    {
      "epoch": 900.2189781021898,
      "grad_norm": 6.974803447723389,
      "learning_rate": 4.989051094890511e-06,
      "loss": 1.0391,
      "step": 123330
    },
    {
      "epoch": 900.2919708029198,
      "grad_norm": 0.027771802619099617,
      "learning_rate": 4.985401459854014e-06,
      "loss": 0.7751,
      "step": 123340
    },
    {
      "epoch": 900.3649635036496,
      "grad_norm": 0.017658334225416183,
      "learning_rate": 4.981751824817518e-06,
      "loss": 0.9783,
      "step": 123350
    },
    {
      "epoch": 900.4379562043796,
      "grad_norm": 7.480463981628418,
      "learning_rate": 4.978102189781022e-06,
      "loss": 1.3604,
      "step": 123360
    },
    {
      "epoch": 900.5109489051094,
      "grad_norm": 10.596259117126465,
      "learning_rate": 4.974452554744526e-06,
      "loss": 0.7483,
      "step": 123370
    },
    {
      "epoch": 900.5839416058394,
      "grad_norm": 9.312302589416504,
      "learning_rate": 4.970802919708029e-06,
      "loss": 0.8934,
      "step": 123380
    },
    {
      "epoch": 900.6569343065694,
      "grad_norm": 19.34078025817871,
      "learning_rate": 4.967153284671533e-06,
      "loss": 1.1135,
      "step": 123390
    },
    {
      "epoch": 900.7299270072992,
      "grad_norm": 14.701845169067383,
      "learning_rate": 4.963503649635037e-06,
      "loss": 0.7609,
      "step": 123400
    },
    {
      "epoch": 900.8029197080292,
      "grad_norm": 0.029610760509967804,
      "learning_rate": 4.9598540145985406e-06,
      "loss": 0.6021,
      "step": 123410
    },
    {
      "epoch": 900.8759124087592,
      "grad_norm": 9.382901191711426,
      "learning_rate": 4.956204379562044e-06,
      "loss": 0.9304,
      "step": 123420
    },
    {
      "epoch": 900.948905109489,
      "grad_norm": 5.034158706665039,
      "learning_rate": 4.9525547445255475e-06,
      "loss": 1.0456,
      "step": 123430
    },
    {
      "epoch": 901.021897810219,
      "grad_norm": 16.94898223876953,
      "learning_rate": 4.948905109489051e-06,
      "loss": 0.8703,
      "step": 123440
    },
    {
      "epoch": 901.0948905109489,
      "grad_norm": 9.208710670471191,
      "learning_rate": 4.945255474452555e-06,
      "loss": 0.752,
      "step": 123450
    },
    {
      "epoch": 901.1678832116788,
      "grad_norm": 11.880094528198242,
      "learning_rate": 4.941605839416059e-06,
      "loss": 0.6923,
      "step": 123460
    },
    {
      "epoch": 901.2408759124088,
      "grad_norm": 9.294354438781738,
      "learning_rate": 4.937956204379562e-06,
      "loss": 1.0289,
      "step": 123470
    },
    {
      "epoch": 901.3138686131387,
      "grad_norm": 5.928793430328369,
      "learning_rate": 4.934306569343066e-06,
      "loss": 0.8394,
      "step": 123480
    },
    {
      "epoch": 901.3868613138686,
      "grad_norm": 5.209543228149414,
      "learning_rate": 4.93065693430657e-06,
      "loss": 0.7471,
      "step": 123490
    },
    {
      "epoch": 901.4598540145986,
      "grad_norm": 8.37895393371582,
      "learning_rate": 4.927007299270074e-06,
      "loss": 0.6007,
      "step": 123500
    },
    {
      "epoch": 901.5328467153284,
      "grad_norm": 8.174633979797363,
      "learning_rate": 4.923357664233577e-06,
      "loss": 0.9891,
      "step": 123510
    },
    {
      "epoch": 901.6058394160584,
      "grad_norm": 0.046391721814870834,
      "learning_rate": 4.919708029197081e-06,
      "loss": 0.8119,
      "step": 123520
    },
    {
      "epoch": 901.6788321167883,
      "grad_norm": 0.036622680723667145,
      "learning_rate": 4.916058394160585e-06,
      "loss": 0.839,
      "step": 123530
    },
    {
      "epoch": 901.7518248175182,
      "grad_norm": 10.010139465332031,
      "learning_rate": 4.912408759124088e-06,
      "loss": 0.8566,
      "step": 123540
    },
    {
      "epoch": 901.8248175182482,
      "grad_norm": 0.06420253217220306,
      "learning_rate": 4.9087591240875915e-06,
      "loss": 1.0289,
      "step": 123550
    },
    {
      "epoch": 901.8978102189781,
      "grad_norm": 10.544780731201172,
      "learning_rate": 4.9051094890510946e-06,
      "loss": 0.8642,
      "step": 123560
    },
    {
      "epoch": 901.970802919708,
      "grad_norm": 12.1575927734375,
      "learning_rate": 4.9014598540145985e-06,
      "loss": 0.9357,
      "step": 123570
    },
    {
      "epoch": 902.043795620438,
      "grad_norm": 13.97365665435791,
      "learning_rate": 4.897810218978102e-06,
      "loss": 0.8324,
      "step": 123580
    },
    {
      "epoch": 902.1167883211679,
      "grad_norm": 12.222519874572754,
      "learning_rate": 4.894160583941606e-06,
      "loss": 0.9655,
      "step": 123590
    },
    {
      "epoch": 902.1897810218978,
      "grad_norm": 19.105690002441406,
      "learning_rate": 4.890510948905109e-06,
      "loss": 0.9166,
      "step": 123600
    },
    {
      "epoch": 902.2627737226277,
      "grad_norm": 8.062074661254883,
      "learning_rate": 4.886861313868613e-06,
      "loss": 1.1075,
      "step": 123610
    },
    {
      "epoch": 902.3357664233577,
      "grad_norm": 8.911078453063965,
      "learning_rate": 4.883211678832117e-06,
      "loss": 0.8891,
      "step": 123620
    },
    {
      "epoch": 902.4087591240876,
      "grad_norm": 10.440649032592773,
      "learning_rate": 4.879562043795621e-06,
      "loss": 0.7543,
      "step": 123630
    },
    {
      "epoch": 902.4817518248175,
      "grad_norm": 10.9827880859375,
      "learning_rate": 4.875912408759124e-06,
      "loss": 0.6037,
      "step": 123640
    },
    {
      "epoch": 902.5547445255474,
      "grad_norm": 12.169702529907227,
      "learning_rate": 4.872262773722628e-06,
      "loss": 0.8176,
      "step": 123650
    },
    {
      "epoch": 902.6277372262774,
      "grad_norm": 5.8000054359436035,
      "learning_rate": 4.868613138686132e-06,
      "loss": 0.6893,
      "step": 123660
    },
    {
      "epoch": 902.7007299270073,
      "grad_norm": 8.22379207611084,
      "learning_rate": 4.864963503649636e-06,
      "loss": 1.1014,
      "step": 123670
    },
    {
      "epoch": 902.7737226277372,
      "grad_norm": 6.667815208435059,
      "learning_rate": 4.861313868613139e-06,
      "loss": 1.1754,
      "step": 123680
    },
    {
      "epoch": 902.8467153284671,
      "grad_norm": 10.297369003295898,
      "learning_rate": 4.8576642335766425e-06,
      "loss": 0.8238,
      "step": 123690
    },
    {
      "epoch": 902.9197080291971,
      "grad_norm": 14.735492706298828,
      "learning_rate": 4.854014598540146e-06,
      "loss": 0.5989,
      "step": 123700
    },
    {
      "epoch": 902.992700729927,
      "grad_norm": 0.036896128207445145,
      "learning_rate": 4.85036496350365e-06,
      "loss": 0.8643,
      "step": 123710
    },
    {
      "epoch": 903.0656934306569,
      "grad_norm": 6.663036346435547,
      "learning_rate": 4.846715328467154e-06,
      "loss": 0.677,
      "step": 123720
    },
    {
      "epoch": 903.1386861313869,
      "grad_norm": 0.010349233634769917,
      "learning_rate": 4.843065693430657e-06,
      "loss": 0.8886,
      "step": 123730
    },
    {
      "epoch": 903.2116788321168,
      "grad_norm": 10.423279762268066,
      "learning_rate": 4.839416058394161e-06,
      "loss": 0.6254,
      "step": 123740
    },
    {
      "epoch": 903.2846715328467,
      "grad_norm": 9.83016300201416,
      "learning_rate": 4.835766423357665e-06,
      "loss": 0.9204,
      "step": 123750
    },
    {
      "epoch": 903.3576642335767,
      "grad_norm": 12.846811294555664,
      "learning_rate": 4.832116788321168e-06,
      "loss": 1.0248,
      "step": 123760
    },
    {
      "epoch": 903.4306569343066,
      "grad_norm": 7.903259754180908,
      "learning_rate": 4.828467153284672e-06,
      "loss": 0.9305,
      "step": 123770
    },
    {
      "epoch": 903.5036496350365,
      "grad_norm": 9.674921989440918,
      "learning_rate": 4.824817518248175e-06,
      "loss": 1.4541,
      "step": 123780
    },
    {
      "epoch": 903.5766423357665,
      "grad_norm": 18.491233825683594,
      "learning_rate": 4.821167883211679e-06,
      "loss": 1.0163,
      "step": 123790
    },
    {
      "epoch": 903.6496350364963,
      "grad_norm": 12.267095565795898,
      "learning_rate": 4.817518248175183e-06,
      "loss": 0.91,
      "step": 123800
    },
    {
      "epoch": 903.7226277372263,
      "grad_norm": 14.276745796203613,
      "learning_rate": 4.8138686131386866e-06,
      "loss": 0.8151,
      "step": 123810
    },
    {
      "epoch": 903.7956204379562,
      "grad_norm": 0.11189716309309006,
      "learning_rate": 4.81021897810219e-06,
      "loss": 0.7725,
      "step": 123820
    },
    {
      "epoch": 903.8686131386861,
      "grad_norm": 13.117182731628418,
      "learning_rate": 4.8065693430656935e-06,
      "loss": 0.8288,
      "step": 123830
    },
    {
      "epoch": 903.9416058394161,
      "grad_norm": 11.600927352905273,
      "learning_rate": 4.802919708029197e-06,
      "loss": 0.8853,
      "step": 123840
    },
    {
      "epoch": 904.014598540146,
      "grad_norm": 8.37664794921875,
      "learning_rate": 4.799270072992701e-06,
      "loss": 0.9599,
      "step": 123850
    },
    {
      "epoch": 904.0875912408759,
      "grad_norm": 9.903078079223633,
      "learning_rate": 4.795620437956204e-06,
      "loss": 0.9,
      "step": 123860
    },
    {
      "epoch": 904.1605839416059,
      "grad_norm": 6.838455677032471,
      "learning_rate": 4.791970802919708e-06,
      "loss": 1.1636,
      "step": 123870
    },
    {
      "epoch": 904.2335766423357,
      "grad_norm": 0.44301798939704895,
      "learning_rate": 4.788321167883212e-06,
      "loss": 1.004,
      "step": 123880
    },
    {
      "epoch": 904.3065693430657,
      "grad_norm": 0.00937693566083908,
      "learning_rate": 4.784671532846716e-06,
      "loss": 0.496,
      "step": 123890
    },
    {
      "epoch": 904.3795620437957,
      "grad_norm": 4.74626350402832,
      "learning_rate": 4.781021897810219e-06,
      "loss": 0.2656,
      "step": 123900
    },
    {
      "epoch": 904.4525547445255,
      "grad_norm": 9.974924087524414,
      "learning_rate": 4.777372262773723e-06,
      "loss": 0.6945,
      "step": 123910
    },
    {
      "epoch": 904.5255474452555,
      "grad_norm": 18.29120445251465,
      "learning_rate": 4.773722627737227e-06,
      "loss": 1.1163,
      "step": 123920
    },
    {
      "epoch": 904.5985401459855,
      "grad_norm": 0.02999960258603096,
      "learning_rate": 4.770072992700731e-06,
      "loss": 1.0172,
      "step": 123930
    },
    {
      "epoch": 904.6715328467153,
      "grad_norm": 8.55569076538086,
      "learning_rate": 4.7664233576642345e-06,
      "loss": 0.8951,
      "step": 123940
    },
    {
      "epoch": 904.7445255474453,
      "grad_norm": 10.48017692565918,
      "learning_rate": 4.7627737226277375e-06,
      "loss": 0.8285,
      "step": 123950
    },
    {
      "epoch": 904.8175182481751,
      "grad_norm": 7.926461219787598,
      "learning_rate": 4.759124087591241e-06,
      "loss": 0.8774,
      "step": 123960
    },
    {
      "epoch": 904.8905109489051,
      "grad_norm": 9.711430549621582,
      "learning_rate": 4.7554744525547444e-06,
      "loss": 1.131,
      "step": 123970
    },
    {
      "epoch": 904.9635036496351,
      "grad_norm": 9.81132984161377,
      "learning_rate": 4.751824817518248e-06,
      "loss": 0.8319,
      "step": 123980
    },
    {
      "epoch": 905.0364963503649,
      "grad_norm": 15.199525833129883,
      "learning_rate": 4.748175182481751e-06,
      "loss": 0.8117,
      "step": 123990
    },
    {
      "epoch": 905.1094890510949,
      "grad_norm": 7.825545310974121,
      "learning_rate": 4.744525547445255e-06,
      "loss": 0.756,
      "step": 124000
    },
    {
      "epoch": 905.1824817518249,
      "grad_norm": 0.06542479246854782,
      "learning_rate": 4.740875912408759e-06,
      "loss": 0.5478,
      "step": 124010
    },
    {
      "epoch": 905.2554744525547,
      "grad_norm": 8.029465675354004,
      "learning_rate": 4.737226277372263e-06,
      "loss": 0.8858,
      "step": 124020
    },
    {
      "epoch": 905.3284671532847,
      "grad_norm": 5.108583450317383,
      "learning_rate": 4.733576642335767e-06,
      "loss": 0.8378,
      "step": 124030
    },
    {
      "epoch": 905.4014598540145,
      "grad_norm": 7.085460662841797,
      "learning_rate": 4.72992700729927e-06,
      "loss": 0.6547,
      "step": 124040
    },
    {
      "epoch": 905.4744525547445,
      "grad_norm": 7.1529221534729,
      "learning_rate": 4.726277372262774e-06,
      "loss": 1.0944,
      "step": 124050
    },
    {
      "epoch": 905.5474452554745,
      "grad_norm": 10.36564826965332,
      "learning_rate": 4.722627737226278e-06,
      "loss": 0.9477,
      "step": 124060
    },
    {
      "epoch": 905.6204379562043,
      "grad_norm": 10.42206859588623,
      "learning_rate": 4.7189781021897816e-06,
      "loss": 0.8084,
      "step": 124070
    },
    {
      "epoch": 905.6934306569343,
      "grad_norm": 0.09718967229127884,
      "learning_rate": 4.715328467153285e-06,
      "loss": 0.9453,
      "step": 124080
    },
    {
      "epoch": 905.7664233576643,
      "grad_norm": 8.427252769470215,
      "learning_rate": 4.7116788321167885e-06,
      "loss": 1.2239,
      "step": 124090
    },
    {
      "epoch": 905.8394160583941,
      "grad_norm": 0.31271156668663025,
      "learning_rate": 4.708029197080292e-06,
      "loss": 0.8786,
      "step": 124100
    },
    {
      "epoch": 905.9124087591241,
      "grad_norm": 8.02462387084961,
      "learning_rate": 4.704379562043796e-06,
      "loss": 1.2747,
      "step": 124110
    },
    {
      "epoch": 905.985401459854,
      "grad_norm": 6.10392427444458,
      "learning_rate": 4.700729927007299e-06,
      "loss": 0.653,
      "step": 124120
    },
    {
      "epoch": 906.0583941605839,
      "grad_norm": 10.557685852050781,
      "learning_rate": 4.697080291970803e-06,
      "loss": 0.6725,
      "step": 124130
    },
    {
      "epoch": 906.1313868613139,
      "grad_norm": 7.636073112487793,
      "learning_rate": 4.693430656934307e-06,
      "loss": 1.2146,
      "step": 124140
    },
    {
      "epoch": 906.2043795620438,
      "grad_norm": 7.557954788208008,
      "learning_rate": 4.689781021897811e-06,
      "loss": 1.0158,
      "step": 124150
    },
    {
      "epoch": 906.2773722627737,
      "grad_norm": 6.935406684875488,
      "learning_rate": 4.686131386861314e-06,
      "loss": 0.6864,
      "step": 124160
    },
    {
      "epoch": 906.3503649635037,
      "grad_norm": 11.966238021850586,
      "learning_rate": 4.682481751824818e-06,
      "loss": 1.198,
      "step": 124170
    },
    {
      "epoch": 906.4233576642335,
      "grad_norm": 5.565207004547119,
      "learning_rate": 4.678832116788322e-06,
      "loss": 0.8389,
      "step": 124180
    },
    {
      "epoch": 906.4963503649635,
      "grad_norm": 0.1237858384847641,
      "learning_rate": 4.675182481751825e-06,
      "loss": 1.0334,
      "step": 124190
    },
    {
      "epoch": 906.5693430656934,
      "grad_norm": 10.89566421508789,
      "learning_rate": 4.671532846715329e-06,
      "loss": 0.8884,
      "step": 124200
    },
    {
      "epoch": 906.6423357664233,
      "grad_norm": 0.05802159383893013,
      "learning_rate": 4.667883211678832e-06,
      "loss": 0.7035,
      "step": 124210
    },
    {
      "epoch": 906.7153284671533,
      "grad_norm": 0.043620530515909195,
      "learning_rate": 4.6642335766423356e-06,
      "loss": 0.4854,
      "step": 124220
    },
    {
      "epoch": 906.7883211678832,
      "grad_norm": 10.552061080932617,
      "learning_rate": 4.6605839416058395e-06,
      "loss": 1.0679,
      "step": 124230
    },
    {
      "epoch": 906.8613138686131,
      "grad_norm": 0.03130872920155525,
      "learning_rate": 4.656934306569343e-06,
      "loss": 0.9555,
      "step": 124240
    },
    {
      "epoch": 906.9343065693431,
      "grad_norm": 10.571870803833008,
      "learning_rate": 4.653284671532847e-06,
      "loss": 0.7045,
      "step": 124250
    },
    {
      "epoch": 907.007299270073,
      "grad_norm": 13.406704902648926,
      "learning_rate": 4.64963503649635e-06,
      "loss": 0.6274,
      "step": 124260
    },
    {
      "epoch": 907.0802919708029,
      "grad_norm": 0.009923174977302551,
      "learning_rate": 4.645985401459854e-06,
      "loss": 0.7814,
      "step": 124270
    },
    {
      "epoch": 907.1532846715329,
      "grad_norm": 12.470492362976074,
      "learning_rate": 4.642335766423358e-06,
      "loss": 0.8673,
      "step": 124280
    },
    {
      "epoch": 907.2262773722628,
      "grad_norm": 0.03606036305427551,
      "learning_rate": 4.638686131386862e-06,
      "loss": 0.778,
      "step": 124290
    },
    {
      "epoch": 907.2992700729927,
      "grad_norm": 7.683180332183838,
      "learning_rate": 4.635036496350365e-06,
      "loss": 1.0944,
      "step": 124300
    },
    {
      "epoch": 907.3722627737226,
      "grad_norm": 9.706151008605957,
      "learning_rate": 4.631386861313869e-06,
      "loss": 0.6389,
      "step": 124310
    },
    {
      "epoch": 907.4452554744526,
      "grad_norm": 8.939752578735352,
      "learning_rate": 4.627737226277373e-06,
      "loss": 0.7422,
      "step": 124320
    },
    {
      "epoch": 907.5182481751825,
      "grad_norm": 13.704832077026367,
      "learning_rate": 4.624087591240877e-06,
      "loss": 1.3734,
      "step": 124330
    },
    {
      "epoch": 907.5912408759124,
      "grad_norm": 9.572762489318848,
      "learning_rate": 4.62043795620438e-06,
      "loss": 0.6895,
      "step": 124340
    },
    {
      "epoch": 907.6642335766423,
      "grad_norm": 7.795063018798828,
      "learning_rate": 4.6167883211678835e-06,
      "loss": 0.9949,
      "step": 124350
    },
    {
      "epoch": 907.7372262773723,
      "grad_norm": 7.058838367462158,
      "learning_rate": 4.613138686131387e-06,
      "loss": 1.0925,
      "step": 124360
    },
    {
      "epoch": 907.8102189781022,
      "grad_norm": 14.087507247924805,
      "learning_rate": 4.609489051094891e-06,
      "loss": 0.7901,
      "step": 124370
    },
    {
      "epoch": 907.8832116788321,
      "grad_norm": 11.655329704284668,
      "learning_rate": 4.605839416058394e-06,
      "loss": 0.785,
      "step": 124380
    },
    {
      "epoch": 907.956204379562,
      "grad_norm": 7.037757873535156,
      "learning_rate": 4.602189781021898e-06,
      "loss": 1.2743,
      "step": 124390
    },
    {
      "epoch": 908.029197080292,
      "grad_norm": 12.95067310333252,
      "learning_rate": 4.598540145985401e-06,
      "loss": 0.5277,
      "step": 124400
    },
    {
      "epoch": 908.1021897810219,
      "grad_norm": 0.019945811480283737,
      "learning_rate": 4.594890510948905e-06,
      "loss": 0.5954,
      "step": 124410
    },
    {
      "epoch": 908.1751824817518,
      "grad_norm": 13.602383613586426,
      "learning_rate": 4.591240875912409e-06,
      "loss": 0.9283,
      "step": 124420
    },
    {
      "epoch": 908.2481751824818,
      "grad_norm": 0.0211953017860651,
      "learning_rate": 4.587591240875912e-06,
      "loss": 0.6883,
      "step": 124430
    },
    {
      "epoch": 908.3211678832117,
      "grad_norm": 12.73377799987793,
      "learning_rate": 4.583941605839416e-06,
      "loss": 1.4174,
      "step": 124440
    },
    {
      "epoch": 908.3941605839416,
      "grad_norm": 5.369272232055664,
      "learning_rate": 4.58029197080292e-06,
      "loss": 0.8399,
      "step": 124450
    },
    {
      "epoch": 908.4671532846716,
      "grad_norm": 6.593064308166504,
      "learning_rate": 4.576642335766424e-06,
      "loss": 0.8603,
      "step": 124460
    },
    {
      "epoch": 908.5401459854014,
      "grad_norm": 6.891779899597168,
      "learning_rate": 4.572992700729927e-06,
      "loss": 0.838,
      "step": 124470
    },
    {
      "epoch": 908.6131386861314,
      "grad_norm": 7.042219638824463,
      "learning_rate": 4.569343065693431e-06,
      "loss": 1.1063,
      "step": 124480
    },
    {
      "epoch": 908.6861313868613,
      "grad_norm": 12.531576156616211,
      "learning_rate": 4.5656934306569345e-06,
      "loss": 0.6061,
      "step": 124490
    },
    {
      "epoch": 908.7591240875912,
      "grad_norm": 7.165863037109375,
      "learning_rate": 4.562043795620438e-06,
      "loss": 0.9012,
      "step": 124500
    },
    {
      "epoch": 908.8321167883212,
      "grad_norm": 0.10556873679161072,
      "learning_rate": 4.558394160583942e-06,
      "loss": 0.7594,
      "step": 124510
    },
    {
      "epoch": 908.9051094890511,
      "grad_norm": 17.363611221313477,
      "learning_rate": 4.554744525547445e-06,
      "loss": 1.1613,
      "step": 124520
    },
    {
      "epoch": 908.978102189781,
      "grad_norm": 7.9330291748046875,
      "learning_rate": 4.551094890510949e-06,
      "loss": 0.9651,
      "step": 124530
    },
    {
      "epoch": 909.051094890511,
      "grad_norm": 13.210419654846191,
      "learning_rate": 4.547445255474453e-06,
      "loss": 0.6613,
      "step": 124540
    },
    {
      "epoch": 909.1240875912408,
      "grad_norm": 9.680912971496582,
      "learning_rate": 4.543795620437957e-06,
      "loss": 0.7639,
      "step": 124550
    },
    {
      "epoch": 909.1970802919708,
      "grad_norm": 0.09524491429328918,
      "learning_rate": 4.54014598540146e-06,
      "loss": 0.7768,
      "step": 124560
    },
    {
      "epoch": 909.2700729927008,
      "grad_norm": 16.681682586669922,
      "learning_rate": 4.536496350364964e-06,
      "loss": 1.0251,
      "step": 124570
    },
    {
      "epoch": 909.3430656934306,
      "grad_norm": 0.5416595339775085,
      "learning_rate": 4.532846715328468e-06,
      "loss": 0.8035,
      "step": 124580
    },
    {
      "epoch": 909.4160583941606,
      "grad_norm": 10.58258056640625,
      "learning_rate": 4.529197080291972e-06,
      "loss": 1.1579,
      "step": 124590
    },
    {
      "epoch": 909.4890510948906,
      "grad_norm": 11.550712585449219,
      "learning_rate": 4.525547445255475e-06,
      "loss": 1.0024,
      "step": 124600
    },
    {
      "epoch": 909.5620437956204,
      "grad_norm": 7.242093086242676,
      "learning_rate": 4.5218978102189785e-06,
      "loss": 0.8615,
      "step": 124610
    },
    {
      "epoch": 909.6350364963504,
      "grad_norm": 11.286555290222168,
      "learning_rate": 4.5182481751824815e-06,
      "loss": 1.011,
      "step": 124620
    },
    {
      "epoch": 909.7080291970802,
      "grad_norm": 7.3338189125061035,
      "learning_rate": 4.5145985401459854e-06,
      "loss": 0.8787,
      "step": 124630
    },
    {
      "epoch": 909.7810218978102,
      "grad_norm": 9.509695053100586,
      "learning_rate": 4.510948905109489e-06,
      "loss": 0.943,
      "step": 124640
    },
    {
      "epoch": 909.8540145985402,
      "grad_norm": 9.285982131958008,
      "learning_rate": 4.507299270072992e-06,
      "loss": 0.8104,
      "step": 124650
    },
    {
      "epoch": 909.92700729927,
      "grad_norm": 13.034185409545898,
      "learning_rate": 4.503649635036496e-06,
      "loss": 0.8052,
      "step": 124660
    },
    {
      "epoch": 910.0,
      "grad_norm": 17.414138793945312,
      "learning_rate": 4.5e-06,
      "loss": 0.9041,
      "step": 124670
    },
    {
      "epoch": 910.07299270073,
      "grad_norm": 0.04243846610188484,
      "learning_rate": 4.496350364963504e-06,
      "loss": 1.219,
      "step": 124680
    },
    {
      "epoch": 910.1459854014598,
      "grad_norm": 7.653507709503174,
      "learning_rate": 4.492700729927007e-06,
      "loss": 0.7494,
      "step": 124690
    },
    {
      "epoch": 910.2189781021898,
      "grad_norm": 5.609699249267578,
      "learning_rate": 4.489051094890511e-06,
      "loss": 0.9821,
      "step": 124700
    },
    {
      "epoch": 910.2919708029198,
      "grad_norm": 6.582382678985596,
      "learning_rate": 4.485401459854015e-06,
      "loss": 0.6814,
      "step": 124710
    },
    {
      "epoch": 910.3649635036496,
      "grad_norm": 11.868760108947754,
      "learning_rate": 4.481751824817519e-06,
      "loss": 1.01,
      "step": 124720
    },
    {
      "epoch": 910.4379562043796,
      "grad_norm": 7.578723430633545,
      "learning_rate": 4.4781021897810226e-06,
      "loss": 1.1031,
      "step": 124730
    },
    {
      "epoch": 910.5109489051094,
      "grad_norm": 6.882120609283447,
      "learning_rate": 4.474452554744526e-06,
      "loss": 0.5915,
      "step": 124740
    },
    {
      "epoch": 910.5839416058394,
      "grad_norm": 12.732358932495117,
      "learning_rate": 4.4708029197080295e-06,
      "loss": 1.2565,
      "step": 124750
    },
    {
      "epoch": 910.6569343065694,
      "grad_norm": 20.556516647338867,
      "learning_rate": 4.467153284671533e-06,
      "loss": 0.808,
      "step": 124760
    },
    {
      "epoch": 910.7299270072992,
      "grad_norm": 10.626321792602539,
      "learning_rate": 4.463503649635037e-06,
      "loss": 0.8011,
      "step": 124770
    },
    {
      "epoch": 910.8029197080292,
      "grad_norm": 19.988101959228516,
      "learning_rate": 4.45985401459854e-06,
      "loss": 0.902,
      "step": 124780
    },
    {
      "epoch": 910.8759124087592,
      "grad_norm": 7.693734169006348,
      "learning_rate": 4.456204379562044e-06,
      "loss": 0.7179,
      "step": 124790
    },
    {
      "epoch": 910.948905109489,
      "grad_norm": 8.741729736328125,
      "learning_rate": 4.452554744525548e-06,
      "loss": 0.8534,
      "step": 124800
    },
    {
      "epoch": 911.021897810219,
      "grad_norm": 5.975025653839111,
      "learning_rate": 4.448905109489052e-06,
      "loss": 0.7338,
      "step": 124810
    },
    {
      "epoch": 911.0948905109489,
      "grad_norm": 7.138916492462158,
      "learning_rate": 4.445255474452555e-06,
      "loss": 1.2014,
      "step": 124820
    },
    {
      "epoch": 911.1678832116788,
      "grad_norm": 0.042659517377614975,
      "learning_rate": 4.441605839416058e-06,
      "loss": 1.0067,
      "step": 124830
    },
    {
      "epoch": 911.2408759124088,
      "grad_norm": 0.029376361519098282,
      "learning_rate": 4.437956204379562e-06,
      "loss": 0.9434,
      "step": 124840
    },
    {
      "epoch": 911.3138686131387,
      "grad_norm": 11.81600570678711,
      "learning_rate": 4.434306569343066e-06,
      "loss": 0.9352,
      "step": 124850
    },
    {
      "epoch": 911.3868613138686,
      "grad_norm": 8.455228805541992,
      "learning_rate": 4.43065693430657e-06,
      "loss": 0.8034,
      "step": 124860
    },
    {
      "epoch": 911.4598540145986,
      "grad_norm": 17.76529312133789,
      "learning_rate": 4.427007299270073e-06,
      "loss": 0.7583,
      "step": 124870
    },
    {
      "epoch": 911.5328467153284,
      "grad_norm": 8.693399429321289,
      "learning_rate": 4.4233576642335766e-06,
      "loss": 0.5857,
      "step": 124880
    },
    {
      "epoch": 911.6058394160584,
      "grad_norm": 14.924294471740723,
      "learning_rate": 4.4197080291970804e-06,
      "loss": 1.2997,
      "step": 124890
    },
    {
      "epoch": 911.6788321167883,
      "grad_norm": 7.2992939949035645,
      "learning_rate": 4.416058394160584e-06,
      "loss": 1.2061,
      "step": 124900
    },
    {
      "epoch": 911.7518248175182,
      "grad_norm": 17.134971618652344,
      "learning_rate": 4.412408759124087e-06,
      "loss": 0.6497,
      "step": 124910
    },
    {
      "epoch": 911.8248175182482,
      "grad_norm": 1.1012624502182007,
      "learning_rate": 4.408759124087591e-06,
      "loss": 0.9197,
      "step": 124920
    },
    {
      "epoch": 911.8978102189781,
      "grad_norm": 17.164913177490234,
      "learning_rate": 4.405109489051095e-06,
      "loss": 0.664,
      "step": 124930
    },
    {
      "epoch": 911.970802919708,
      "grad_norm": 7.037471771240234,
      "learning_rate": 4.401459854014599e-06,
      "loss": 0.5428,
      "step": 124940
    },
    {
      "epoch": 912.043795620438,
      "grad_norm": 7.994973659515381,
      "learning_rate": 4.397810218978102e-06,
      "loss": 0.8453,
      "step": 124950
    },
    {
      "epoch": 912.1167883211679,
      "grad_norm": 10.794903755187988,
      "learning_rate": 4.394160583941606e-06,
      "loss": 0.7423,
      "step": 124960
    },
    {
      "epoch": 912.1897810218978,
      "grad_norm": 7.246308326721191,
      "learning_rate": 4.39051094890511e-06,
      "loss": 0.7379,
      "step": 124970
    },
    {
      "epoch": 912.2627737226277,
      "grad_norm": 10.570234298706055,
      "learning_rate": 4.386861313868614e-06,
      "loss": 0.8594,
      "step": 124980
    },
    {
      "epoch": 912.3357664233577,
      "grad_norm": 7.710292339324951,
      "learning_rate": 4.3832116788321176e-06,
      "loss": 0.8307,
      "step": 124990
    },
    {
      "epoch": 912.4087591240876,
      "grad_norm": 11.785015106201172,
      "learning_rate": 4.379562043795621e-06,
      "loss": 1.0286,
      "step": 125000
    },
    {
      "epoch": 912.4817518248175,
      "grad_norm": 7.344629287719727,
      "learning_rate": 4.3759124087591245e-06,
      "loss": 1.2912,
      "step": 125010
    },
    {
      "epoch": 912.5547445255474,
      "grad_norm": 12.039402961730957,
      "learning_rate": 4.372262773722628e-06,
      "loss": 1.0975,
      "step": 125020
    },
    {
      "epoch": 912.6277372262774,
      "grad_norm": 8.135141372680664,
      "learning_rate": 4.368613138686132e-06,
      "loss": 0.9389,
      "step": 125030
    },
    {
      "epoch": 912.7007299270073,
      "grad_norm": 13.302433013916016,
      "learning_rate": 4.364963503649635e-06,
      "loss": 0.7963,
      "step": 125040
    },
    {
      "epoch": 912.7737226277372,
      "grad_norm": 6.091385841369629,
      "learning_rate": 4.361313868613138e-06,
      "loss": 0.6651,
      "step": 125050
    },
    {
      "epoch": 912.8467153284671,
      "grad_norm": 16.774372100830078,
      "learning_rate": 4.357664233576642e-06,
      "loss": 0.663,
      "step": 125060
    },
    {
      "epoch": 912.9197080291971,
      "grad_norm": 12.879125595092773,
      "learning_rate": 4.354014598540146e-06,
      "loss": 0.9026,
      "step": 125070
    },
    {
      "epoch": 912.992700729927,
      "grad_norm": 9.947256088256836,
      "learning_rate": 4.35036496350365e-06,
      "loss": 0.8232,
      "step": 125080
    },
    {
      "epoch": 913.0656934306569,
      "grad_norm": 8.032669067382812,
      "learning_rate": 4.346715328467153e-06,
      "loss": 1.184,
      "step": 125090
    },
    {
      "epoch": 913.1386861313869,
      "grad_norm": 8.396024703979492,
      "learning_rate": 4.343065693430657e-06,
      "loss": 0.8326,
      "step": 125100
    },
    {
      "epoch": 913.2116788321168,
      "grad_norm": 17.897247314453125,
      "learning_rate": 4.339416058394161e-06,
      "loss": 0.8397,
      "step": 125110
    },
    {
      "epoch": 913.2846715328467,
      "grad_norm": 9.42906665802002,
      "learning_rate": 4.335766423357665e-06,
      "loss": 0.7463,
      "step": 125120
    },
    {
      "epoch": 913.3576642335767,
      "grad_norm": 14.523639678955078,
      "learning_rate": 4.332116788321168e-06,
      "loss": 0.5123,
      "step": 125130
    },
    {
      "epoch": 913.4306569343066,
      "grad_norm": 13.140498161315918,
      "learning_rate": 4.328467153284672e-06,
      "loss": 0.8918,
      "step": 125140
    },
    {
      "epoch": 913.5036496350365,
      "grad_norm": 8.591249465942383,
      "learning_rate": 4.3248175182481755e-06,
      "loss": 0.5682,
      "step": 125150
    },
    {
      "epoch": 913.5766423357665,
      "grad_norm": 6.733658313751221,
      "learning_rate": 4.321167883211679e-06,
      "loss": 0.8431,
      "step": 125160
    },
    {
      "epoch": 913.6496350364963,
      "grad_norm": 0.0354318805038929,
      "learning_rate": 4.317518248175182e-06,
      "loss": 0.9211,
      "step": 125170
    },
    {
      "epoch": 913.7226277372263,
      "grad_norm": 15.803991317749023,
      "learning_rate": 4.313868613138686e-06,
      "loss": 1.0488,
      "step": 125180
    },
    {
      "epoch": 913.7956204379562,
      "grad_norm": 11.601861953735352,
      "learning_rate": 4.31021897810219e-06,
      "loss": 1.0363,
      "step": 125190
    },
    {
      "epoch": 913.8686131386861,
      "grad_norm": 11.744518280029297,
      "learning_rate": 4.306569343065694e-06,
      "loss": 1.0902,
      "step": 125200
    },
    {
      "epoch": 913.9416058394161,
      "grad_norm": 13.618350982666016,
      "learning_rate": 4.302919708029197e-06,
      "loss": 0.8863,
      "step": 125210
    },
    {
      "epoch": 914.014598540146,
      "grad_norm": 7.986272811889648,
      "learning_rate": 4.299270072992701e-06,
      "loss": 0.9191,
      "step": 125220
    },
    {
      "epoch": 914.0875912408759,
      "grad_norm": 2.0397632122039795,
      "learning_rate": 4.295620437956205e-06,
      "loss": 0.5437,
      "step": 125230
    },
    {
      "epoch": 914.1605839416059,
      "grad_norm": 17.557727813720703,
      "learning_rate": 4.291970802919709e-06,
      "loss": 1.2946,
      "step": 125240
    },
    {
      "epoch": 914.2335766423357,
      "grad_norm": 17.263166427612305,
      "learning_rate": 4.288321167883213e-06,
      "loss": 0.9587,
      "step": 125250
    },
    {
      "epoch": 914.3065693430657,
      "grad_norm": 0.656180739402771,
      "learning_rate": 4.284671532846716e-06,
      "loss": 0.6693,
      "step": 125260
    },
    {
      "epoch": 914.3795620437957,
      "grad_norm": 12.916749954223633,
      "learning_rate": 4.281021897810219e-06,
      "loss": 0.7173,
      "step": 125270
    },
    {
      "epoch": 914.4525547445255,
      "grad_norm": 16.06473731994629,
      "learning_rate": 4.2773722627737225e-06,
      "loss": 1.0451,
      "step": 125280
    },
    {
      "epoch": 914.5255474452555,
      "grad_norm": 13.067821502685547,
      "learning_rate": 4.2737226277372264e-06,
      "loss": 1.0442,
      "step": 125290
    },
    {
      "epoch": 914.5985401459855,
      "grad_norm": 6.205847263336182,
      "learning_rate": 4.27007299270073e-06,
      "loss": 1.0392,
      "step": 125300
    },
    {
      "epoch": 914.6715328467153,
      "grad_norm": 0.0690431296825409,
      "learning_rate": 4.266423357664233e-06,
      "loss": 1.1081,
      "step": 125310
    },
    {
      "epoch": 914.7445255474453,
      "grad_norm": 11.253633499145508,
      "learning_rate": 4.262773722627737e-06,
      "loss": 0.6793,
      "step": 125320
    },
    {
      "epoch": 914.8175182481751,
      "grad_norm": 0.036844637244939804,
      "learning_rate": 4.259124087591241e-06,
      "loss": 0.9393,
      "step": 125330
    },
    {
      "epoch": 914.8905109489051,
      "grad_norm": 0.03699414059519768,
      "learning_rate": 4.255474452554745e-06,
      "loss": 0.7517,
      "step": 125340
    },
    {
      "epoch": 914.9635036496351,
      "grad_norm": 7.877211093902588,
      "learning_rate": 4.251824817518248e-06,
      "loss": 0.7235,
      "step": 125350
    },
    {
      "epoch": 915.0364963503649,
      "grad_norm": 4.570391654968262,
      "learning_rate": 4.248175182481752e-06,
      "loss": 0.5376,
      "step": 125360
    },
    {
      "epoch": 915.1094890510949,
      "grad_norm": 14.774422645568848,
      "learning_rate": 4.244525547445256e-06,
      "loss": 0.836,
      "step": 125370
    },
    {
      "epoch": 915.1824817518249,
      "grad_norm": 6.617311000823975,
      "learning_rate": 4.24087591240876e-06,
      "loss": 0.8324,
      "step": 125380
    },
    {
      "epoch": 915.2554744525547,
      "grad_norm": 9.644651412963867,
      "learning_rate": 4.237226277372263e-06,
      "loss": 0.6054,
      "step": 125390
    },
    {
      "epoch": 915.3284671532847,
      "grad_norm": 9.33304500579834,
      "learning_rate": 4.233576642335767e-06,
      "loss": 0.4764,
      "step": 125400
    },
    {
      "epoch": 915.4014598540145,
      "grad_norm": 10.327287673950195,
      "learning_rate": 4.2299270072992705e-06,
      "loss": 1.1582,
      "step": 125410
    },
    {
      "epoch": 915.4744525547445,
      "grad_norm": 10.249149322509766,
      "learning_rate": 4.226277372262774e-06,
      "loss": 0.7988,
      "step": 125420
    },
    {
      "epoch": 915.5474452554745,
      "grad_norm": 0.04628437012434006,
      "learning_rate": 4.222627737226277e-06,
      "loss": 0.8238,
      "step": 125430
    },
    {
      "epoch": 915.6204379562043,
      "grad_norm": 6.391685485839844,
      "learning_rate": 4.218978102189781e-06,
      "loss": 0.982,
      "step": 125440
    },
    {
      "epoch": 915.6934306569343,
      "grad_norm": 0.0511544831097126,
      "learning_rate": 4.215328467153285e-06,
      "loss": 1.258,
      "step": 125450
    },
    {
      "epoch": 915.7664233576643,
      "grad_norm": 16.618791580200195,
      "learning_rate": 4.211678832116789e-06,
      "loss": 1.2599,
      "step": 125460
    },
    {
      "epoch": 915.8394160583941,
      "grad_norm": 8.236932754516602,
      "learning_rate": 4.208029197080293e-06,
      "loss": 0.8788,
      "step": 125470
    },
    {
      "epoch": 915.9124087591241,
      "grad_norm": 0.05122114717960358,
      "learning_rate": 4.204379562043795e-06,
      "loss": 0.7898,
      "step": 125480
    },
    {
      "epoch": 915.985401459854,
      "grad_norm": 0.6139529943466187,
      "learning_rate": 4.200729927007299e-06,
      "loss": 0.8963,
      "step": 125490
    },
    {
      "epoch": 916.0583941605839,
      "grad_norm": 1.6073952913284302,
      "learning_rate": 4.197080291970803e-06,
      "loss": 0.3874,
      "step": 125500
    },
    {
      "epoch": 916.1313868613139,
      "grad_norm": 12.43488883972168,
      "learning_rate": 4.193430656934307e-06,
      "loss": 1.0239,
      "step": 125510
    },
    {
      "epoch": 916.2043795620438,
      "grad_norm": 0.025528861209750175,
      "learning_rate": 4.189781021897811e-06,
      "loss": 0.6809,
      "step": 125520
    },
    {
      "epoch": 916.2773722627737,
      "grad_norm": 0.15789900720119476,
      "learning_rate": 4.186131386861314e-06,
      "loss": 0.8616,
      "step": 125530
    },
    {
      "epoch": 916.3503649635037,
      "grad_norm": 9.938347816467285,
      "learning_rate": 4.1824817518248176e-06,
      "loss": 0.8242,
      "step": 125540
    },
    {
      "epoch": 916.4233576642335,
      "grad_norm": 0.7211076021194458,
      "learning_rate": 4.1788321167883214e-06,
      "loss": 0.7323,
      "step": 125550
    },
    {
      "epoch": 916.4963503649635,
      "grad_norm": 0.030887627974152565,
      "learning_rate": 4.175182481751825e-06,
      "loss": 1.365,
      "step": 125560
    },
    {
      "epoch": 916.5693430656934,
      "grad_norm": 0.04859604686498642,
      "learning_rate": 4.171532846715328e-06,
      "loss": 0.7376,
      "step": 125570
    },
    {
      "epoch": 916.6423357664233,
      "grad_norm": 0.0280672125518322,
      "learning_rate": 4.167883211678832e-06,
      "loss": 1.079,
      "step": 125580
    },
    {
      "epoch": 916.7153284671533,
      "grad_norm": 18.19813346862793,
      "learning_rate": 4.164233576642336e-06,
      "loss": 1.0169,
      "step": 125590
    },
    {
      "epoch": 916.7883211678832,
      "grad_norm": 9.33458423614502,
      "learning_rate": 4.16058394160584e-06,
      "loss": 1.3312,
      "step": 125600
    },
    {
      "epoch": 916.8613138686131,
      "grad_norm": 6.1070685386657715,
      "learning_rate": 4.156934306569343e-06,
      "loss": 0.5591,
      "step": 125610
    },
    {
      "epoch": 916.9343065693431,
      "grad_norm": 16.681190490722656,
      "learning_rate": 4.153284671532847e-06,
      "loss": 0.7689,
      "step": 125620
    },
    {
      "epoch": 917.007299270073,
      "grad_norm": 11.09042739868164,
      "learning_rate": 4.149635036496351e-06,
      "loss": 1.0228,
      "step": 125630
    },
    {
      "epoch": 917.0802919708029,
      "grad_norm": 7.479345798492432,
      "learning_rate": 4.145985401459855e-06,
      "loss": 1.2452,
      "step": 125640
    },
    {
      "epoch": 917.1532846715329,
      "grad_norm": 6.176203727722168,
      "learning_rate": 4.142335766423358e-06,
      "loss": 0.9774,
      "step": 125650
    },
    {
      "epoch": 917.2262773722628,
      "grad_norm": 18.078365325927734,
      "learning_rate": 4.138686131386862e-06,
      "loss": 0.7648,
      "step": 125660
    },
    {
      "epoch": 917.2992700729927,
      "grad_norm": 12.855777740478516,
      "learning_rate": 4.1350364963503655e-06,
      "loss": 0.9889,
      "step": 125670
    },
    {
      "epoch": 917.3722627737226,
      "grad_norm": 9.409945487976074,
      "learning_rate": 4.131386861313869e-06,
      "loss": 0.6855,
      "step": 125680
    },
    {
      "epoch": 917.4452554744526,
      "grad_norm": 9.02735424041748,
      "learning_rate": 4.127737226277372e-06,
      "loss": 1.3351,
      "step": 125690
    },
    {
      "epoch": 917.5182481751825,
      "grad_norm": 0.013889791443943977,
      "learning_rate": 4.1240875912408754e-06,
      "loss": 0.8676,
      "step": 125700
    },
    {
      "epoch": 917.5912408759124,
      "grad_norm": 0.037180934101343155,
      "learning_rate": 4.120437956204379e-06,
      "loss": 1.0033,
      "step": 125710
    },
    {
      "epoch": 917.6642335766423,
      "grad_norm": 0.042017798870801926,
      "learning_rate": 4.116788321167883e-06,
      "loss": 0.2839,
      "step": 125720
    },
    {
      "epoch": 917.7372262773723,
      "grad_norm": 11.45149040222168,
      "learning_rate": 4.113138686131387e-06,
      "loss": 0.8828,
      "step": 125730
    },
    {
      "epoch": 917.8102189781022,
      "grad_norm": 17.82061767578125,
      "learning_rate": 4.10948905109489e-06,
      "loss": 1.0853,
      "step": 125740
    },
    {
      "epoch": 917.8832116788321,
      "grad_norm": 6.896871089935303,
      "learning_rate": 4.105839416058394e-06,
      "loss": 0.8165,
      "step": 125750
    },
    {
      "epoch": 917.956204379562,
      "grad_norm": 8.403883934020996,
      "learning_rate": 4.102189781021898e-06,
      "loss": 0.9943,
      "step": 125760
    },
    {
      "epoch": 918.029197080292,
      "grad_norm": 15.782979011535645,
      "learning_rate": 4.098540145985402e-06,
      "loss": 0.5847,
      "step": 125770
    },
    {
      "epoch": 918.1021897810219,
      "grad_norm": 11.001056671142578,
      "learning_rate": 4.094890510948906e-06,
      "loss": 0.9522,
      "step": 125780
    },
    {
      "epoch": 918.1751824817518,
      "grad_norm": 0.15449662506580353,
      "learning_rate": 4.091240875912409e-06,
      "loss": 0.6065,
      "step": 125790
    },
    {
      "epoch": 918.2481751824818,
      "grad_norm": 15.169751167297363,
      "learning_rate": 4.0875912408759126e-06,
      "loss": 0.9722,
      "step": 125800
    },
    {
      "epoch": 918.3211678832117,
      "grad_norm": 7.137053489685059,
      "learning_rate": 4.0839416058394165e-06,
      "loss": 0.6906,
      "step": 125810
    },
    {
      "epoch": 918.3941605839416,
      "grad_norm": 6.790584087371826,
      "learning_rate": 4.08029197080292e-06,
      "loss": 0.8818,
      "step": 125820
    },
    {
      "epoch": 918.4671532846716,
      "grad_norm": 0.05124237760901451,
      "learning_rate": 4.076642335766423e-06,
      "loss": 0.5503,
      "step": 125830
    },
    {
      "epoch": 918.5401459854014,
      "grad_norm": 6.0663251876831055,
      "learning_rate": 4.072992700729927e-06,
      "loss": 0.8799,
      "step": 125840
    },
    {
      "epoch": 918.6131386861314,
      "grad_norm": 0.21609944105148315,
      "learning_rate": 4.069343065693431e-06,
      "loss": 0.6998,
      "step": 125850
    },
    {
      "epoch": 918.6861313868613,
      "grad_norm": 10.979693412780762,
      "learning_rate": 4.065693430656935e-06,
      "loss": 0.9355,
      "step": 125860
    },
    {
      "epoch": 918.7591240875912,
      "grad_norm": 12.287307739257812,
      "learning_rate": 4.062043795620438e-06,
      "loss": 0.9754,
      "step": 125870
    },
    {
      "epoch": 918.8321167883212,
      "grad_norm": 8.642189979553223,
      "learning_rate": 4.058394160583942e-06,
      "loss": 1.1917,
      "step": 125880
    },
    {
      "epoch": 918.9051094890511,
      "grad_norm": 10.804214477539062,
      "learning_rate": 4.054744525547446e-06,
      "loss": 0.8407,
      "step": 125890
    },
    {
      "epoch": 918.978102189781,
      "grad_norm": 18.04253578186035,
      "learning_rate": 4.05109489051095e-06,
      "loss": 1.192,
      "step": 125900
    },
    {
      "epoch": 919.051094890511,
      "grad_norm": 7.794682502746582,
      "learning_rate": 4.047445255474453e-06,
      "loss": 1.0735,
      "step": 125910
    },
    {
      "epoch": 919.1240875912408,
      "grad_norm": 9.694276809692383,
      "learning_rate": 4.043795620437956e-06,
      "loss": 0.9568,
      "step": 125920
    },
    {
      "epoch": 919.1970802919708,
      "grad_norm": 17.088428497314453,
      "learning_rate": 4.04014598540146e-06,
      "loss": 0.6342,
      "step": 125930
    },
    {
      "epoch": 919.2700729927008,
      "grad_norm": 8.295388221740723,
      "learning_rate": 4.0364963503649635e-06,
      "loss": 0.5883,
      "step": 125940
    },
    {
      "epoch": 919.3430656934306,
      "grad_norm": 8.728919982910156,
      "learning_rate": 4.032846715328467e-06,
      "loss": 0.8585,
      "step": 125950
    },
    {
      "epoch": 919.4160583941606,
      "grad_norm": 6.8125176429748535,
      "learning_rate": 4.0291970802919705e-06,
      "loss": 1.102,
      "step": 125960
    },
    {
      "epoch": 919.4890510948906,
      "grad_norm": 5.167634963989258,
      "learning_rate": 4.025547445255474e-06,
      "loss": 0.8694,
      "step": 125970
    },
    {
      "epoch": 919.5620437956204,
      "grad_norm": 15.38231086730957,
      "learning_rate": 4.021897810218978e-06,
      "loss": 0.9834,
      "step": 125980
    },
    {
      "epoch": 919.6350364963504,
      "grad_norm": 7.726703643798828,
      "learning_rate": 4.018248175182482e-06,
      "loss": 0.993,
      "step": 125990
    },
    {
      "epoch": 919.7080291970802,
      "grad_norm": 7.962123870849609,
      "learning_rate": 4.014598540145985e-06,
      "loss": 0.8179,
      "step": 126000
    },
    {
      "epoch": 919.7810218978102,
      "grad_norm": 8.579257011413574,
      "learning_rate": 4.010948905109489e-06,
      "loss": 0.6989,
      "step": 126010
    },
    {
      "epoch": 919.8540145985402,
      "grad_norm": 15.426154136657715,
      "learning_rate": 4.007299270072993e-06,
      "loss": 1.1676,
      "step": 126020
    },
    {
      "epoch": 919.92700729927,
      "grad_norm": 7.582255840301514,
      "learning_rate": 4.003649635036497e-06,
      "loss": 0.5925,
      "step": 126030
    },
    {
      "epoch": 920.0,
      "grad_norm": 14.898172378540039,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.2287,
      "step": 126040
    },
    {
      "epoch": 920.07299270073,
      "grad_norm": 14.902826309204102,
      "learning_rate": 3.996350364963504e-06,
      "loss": 0.855,
      "step": 126050
    },
    {
      "epoch": 920.1459854014598,
      "grad_norm": 5.1164350509643555,
      "learning_rate": 3.992700729927008e-06,
      "loss": 0.7686,
      "step": 126060
    },
    {
      "epoch": 920.2189781021898,
      "grad_norm": 10.692529678344727,
      "learning_rate": 3.9890510948905115e-06,
      "loss": 1.1396,
      "step": 126070
    },
    {
      "epoch": 920.2919708029198,
      "grad_norm": 5.751824378967285,
      "learning_rate": 3.985401459854015e-06,
      "loss": 0.7853,
      "step": 126080
    },
    {
      "epoch": 920.3649635036496,
      "grad_norm": 6.257477760314941,
      "learning_rate": 3.981751824817518e-06,
      "loss": 0.666,
      "step": 126090
    },
    {
      "epoch": 920.4379562043796,
      "grad_norm": 10.728475570678711,
      "learning_rate": 3.978102189781022e-06,
      "loss": 1.4665,
      "step": 126100
    },
    {
      "epoch": 920.5109489051094,
      "grad_norm": 5.491093158721924,
      "learning_rate": 3.974452554744526e-06,
      "loss": 0.7001,
      "step": 126110
    },
    {
      "epoch": 920.5839416058394,
      "grad_norm": 9.08975601196289,
      "learning_rate": 3.97080291970803e-06,
      "loss": 0.663,
      "step": 126120
    },
    {
      "epoch": 920.6569343065694,
      "grad_norm": 12.693520545959473,
      "learning_rate": 3.967153284671533e-06,
      "loss": 0.761,
      "step": 126130
    },
    {
      "epoch": 920.7299270072992,
      "grad_norm": 0.47474759817123413,
      "learning_rate": 3.963503649635036e-06,
      "loss": 0.7381,
      "step": 126140
    },
    {
      "epoch": 920.8029197080292,
      "grad_norm": 6.315579891204834,
      "learning_rate": 3.95985401459854e-06,
      "loss": 0.5279,
      "step": 126150
    },
    {
      "epoch": 920.8759124087592,
      "grad_norm": 11.39632797241211,
      "learning_rate": 3.956204379562044e-06,
      "loss": 1.2023,
      "step": 126160
    },
    {
      "epoch": 920.948905109489,
      "grad_norm": 5.320874214172363,
      "learning_rate": 3.952554744525548e-06,
      "loss": 0.7957,
      "step": 126170
    },
    {
      "epoch": 921.021897810219,
      "grad_norm": 9.008832931518555,
      "learning_rate": 3.948905109489051e-06,
      "loss": 1.2838,
      "step": 126180
    },
    {
      "epoch": 921.0948905109489,
      "grad_norm": 15.97933578491211,
      "learning_rate": 3.945255474452555e-06,
      "loss": 0.8397,
      "step": 126190
    },
    {
      "epoch": 921.1678832116788,
      "grad_norm": 3.873443603515625,
      "learning_rate": 3.9416058394160585e-06,
      "loss": 0.7906,
      "step": 126200
    },
    {
      "epoch": 921.2408759124088,
      "grad_norm": 12.294512748718262,
      "learning_rate": 3.9379562043795624e-06,
      "loss": 1.0938,
      "step": 126210
    },
    {
      "epoch": 921.3138686131387,
      "grad_norm": 10.888221740722656,
      "learning_rate": 3.9343065693430655e-06,
      "loss": 0.7772,
      "step": 126220
    },
    {
      "epoch": 921.3868613138686,
      "grad_norm": 13.8206205368042,
      "learning_rate": 3.930656934306569e-06,
      "loss": 0.829,
      "step": 126230
    },
    {
      "epoch": 921.4598540145986,
      "grad_norm": 6.916345596313477,
      "learning_rate": 3.927007299270073e-06,
      "loss": 0.5709,
      "step": 126240
    },
    {
      "epoch": 921.5328467153284,
      "grad_norm": 9.396200180053711,
      "learning_rate": 3.923357664233577e-06,
      "loss": 0.7277,
      "step": 126250
    },
    {
      "epoch": 921.6058394160584,
      "grad_norm": 10.643134117126465,
      "learning_rate": 3.919708029197081e-06,
      "loss": 0.855,
      "step": 126260
    },
    {
      "epoch": 921.6788321167883,
      "grad_norm": 13.014665603637695,
      "learning_rate": 3.916058394160584e-06,
      "loss": 1.2945,
      "step": 126270
    },
    {
      "epoch": 921.7518248175182,
      "grad_norm": 0.09567533433437347,
      "learning_rate": 3.912408759124088e-06,
      "loss": 1.0214,
      "step": 126280
    },
    {
      "epoch": 921.8248175182482,
      "grad_norm": 14.22899341583252,
      "learning_rate": 3.908759124087592e-06,
      "loss": 0.8323,
      "step": 126290
    },
    {
      "epoch": 921.8978102189781,
      "grad_norm": 9.388443946838379,
      "learning_rate": 3.905109489051096e-06,
      "loss": 0.829,
      "step": 126300
    },
    {
      "epoch": 921.970802919708,
      "grad_norm": 8.955860137939453,
      "learning_rate": 3.901459854014599e-06,
      "loss": 0.7453,
      "step": 126310
    },
    {
      "epoch": 922.043795620438,
      "grad_norm": 12.273639678955078,
      "learning_rate": 3.897810218978103e-06,
      "loss": 0.7267,
      "step": 126320
    },
    {
      "epoch": 922.1167883211679,
      "grad_norm": 14.457433700561523,
      "learning_rate": 3.8941605839416065e-06,
      "loss": 0.8113,
      "step": 126330
    },
    {
      "epoch": 922.1897810218978,
      "grad_norm": 9.619053840637207,
      "learning_rate": 3.8905109489051095e-06,
      "loss": 1.0103,
      "step": 126340
    },
    {
      "epoch": 922.2627737226277,
      "grad_norm": 11.505025863647461,
      "learning_rate": 3.886861313868613e-06,
      "loss": 0.7806,
      "step": 126350
    },
    {
      "epoch": 922.3357664233577,
      "grad_norm": 0.02653447538614273,
      "learning_rate": 3.8832116788321164e-06,
      "loss": 0.6336,
      "step": 126360
    },
    {
      "epoch": 922.4087591240876,
      "grad_norm": 2.934650182723999,
      "learning_rate": 3.87956204379562e-06,
      "loss": 0.8103,
      "step": 126370
    },
    {
      "epoch": 922.4817518248175,
      "grad_norm": 12.378854751586914,
      "learning_rate": 3.875912408759124e-06,
      "loss": 0.8657,
      "step": 126380
    },
    {
      "epoch": 922.5547445255474,
      "grad_norm": 6.852761268615723,
      "learning_rate": 3.872262773722628e-06,
      "loss": 0.7181,
      "step": 126390
    },
    {
      "epoch": 922.6277372262774,
      "grad_norm": 9.977100372314453,
      "learning_rate": 3.868613138686131e-06,
      "loss": 0.5837,
      "step": 126400
    },
    {
      "epoch": 922.7007299270073,
      "grad_norm": 10.627213478088379,
      "learning_rate": 3.864963503649635e-06,
      "loss": 1.2174,
      "step": 126410
    },
    {
      "epoch": 922.7737226277372,
      "grad_norm": 18.650131225585938,
      "learning_rate": 3.861313868613139e-06,
      "loss": 0.9933,
      "step": 126420
    },
    {
      "epoch": 922.8467153284671,
      "grad_norm": 5.025528430938721,
      "learning_rate": 3.857664233576643e-06,
      "loss": 0.7586,
      "step": 126430
    },
    {
      "epoch": 922.9197080291971,
      "grad_norm": 12.870073318481445,
      "learning_rate": 3.854014598540146e-06,
      "loss": 1.1814,
      "step": 126440
    },
    {
      "epoch": 922.992700729927,
      "grad_norm": 6.972685813903809,
      "learning_rate": 3.85036496350365e-06,
      "loss": 0.948,
      "step": 126450
    },
    {
      "epoch": 923.0656934306569,
      "grad_norm": 0.1060827374458313,
      "learning_rate": 3.8467153284671536e-06,
      "loss": 0.9176,
      "step": 126460
    },
    {
      "epoch": 923.1386861313869,
      "grad_norm": 14.924539566040039,
      "learning_rate": 3.8430656934306574e-06,
      "loss": 0.625,
      "step": 126470
    },
    {
      "epoch": 923.2116788321168,
      "grad_norm": 9.783720016479492,
      "learning_rate": 3.8394160583941605e-06,
      "loss": 1.1199,
      "step": 126480
    },
    {
      "epoch": 923.2846715328467,
      "grad_norm": 7.435920238494873,
      "learning_rate": 3.835766423357664e-06,
      "loss": 0.7892,
      "step": 126490
    },
    {
      "epoch": 923.3576642335767,
      "grad_norm": 0.014272386208176613,
      "learning_rate": 3.832116788321168e-06,
      "loss": 0.8954,
      "step": 126500
    },
    {
      "epoch": 923.4306569343066,
      "grad_norm": 4.655102729797363,
      "learning_rate": 3.828467153284672e-06,
      "loss": 0.5397,
      "step": 126510
    },
    {
      "epoch": 923.5036496350365,
      "grad_norm": 12.935394287109375,
      "learning_rate": 3.824817518248176e-06,
      "loss": 0.788,
      "step": 126520
    },
    {
      "epoch": 923.5766423357665,
      "grad_norm": 8.571688652038574,
      "learning_rate": 3.821167883211679e-06,
      "loss": 0.8918,
      "step": 126530
    },
    {
      "epoch": 923.6496350364963,
      "grad_norm": 17.16183853149414,
      "learning_rate": 3.817518248175183e-06,
      "loss": 0.9687,
      "step": 126540
    },
    {
      "epoch": 923.7226277372263,
      "grad_norm": 9.739876747131348,
      "learning_rate": 3.813868613138687e-06,
      "loss": 0.7027,
      "step": 126550
    },
    {
      "epoch": 923.7956204379562,
      "grad_norm": 9.549877166748047,
      "learning_rate": 3.81021897810219e-06,
      "loss": 1.2438,
      "step": 126560
    },
    {
      "epoch": 923.8686131386861,
      "grad_norm": 7.688474655151367,
      "learning_rate": 3.8065693430656933e-06,
      "loss": 0.7864,
      "step": 126570
    },
    {
      "epoch": 923.9416058394161,
      "grad_norm": 0.02604057639837265,
      "learning_rate": 3.802919708029197e-06,
      "loss": 0.8974,
      "step": 126580
    },
    {
      "epoch": 924.014598540146,
      "grad_norm": 18.63064956665039,
      "learning_rate": 3.7992700729927006e-06,
      "loss": 1.1023,
      "step": 126590
    },
    {
      "epoch": 924.0875912408759,
      "grad_norm": 0.024936925619840622,
      "learning_rate": 3.7956204379562045e-06,
      "loss": 1.061,
      "step": 126600
    },
    {
      "epoch": 924.1605839416059,
      "grad_norm": 9.844911575317383,
      "learning_rate": 3.791970802919708e-06,
      "loss": 1.3493,
      "step": 126610
    },
    {
      "epoch": 924.2335766423357,
      "grad_norm": 15.886475563049316,
      "learning_rate": 3.788321167883212e-06,
      "loss": 0.7817,
      "step": 126620
    },
    {
      "epoch": 924.3065693430657,
      "grad_norm": 0.013364594429731369,
      "learning_rate": 3.7846715328467153e-06,
      "loss": 0.5595,
      "step": 126630
    },
    {
      "epoch": 924.3795620437957,
      "grad_norm": 12.568258285522461,
      "learning_rate": 3.7810218978102192e-06,
      "loss": 0.7356,
      "step": 126640
    },
    {
      "epoch": 924.4525547445255,
      "grad_norm": 15.88849925994873,
      "learning_rate": 3.7773722627737227e-06,
      "loss": 0.9445,
      "step": 126650
    },
    {
      "epoch": 924.5255474452555,
      "grad_norm": 12.308895111083984,
      "learning_rate": 3.7737226277372266e-06,
      "loss": 0.9529,
      "step": 126660
    },
    {
      "epoch": 924.5985401459855,
      "grad_norm": 16.642398834228516,
      "learning_rate": 3.77007299270073e-06,
      "loss": 0.9252,
      "step": 126670
    },
    {
      "epoch": 924.6715328467153,
      "grad_norm": 7.445369243621826,
      "learning_rate": 3.766423357664234e-06,
      "loss": 0.9064,
      "step": 126680
    },
    {
      "epoch": 924.7445255474453,
      "grad_norm": 12.92542552947998,
      "learning_rate": 3.7627737226277374e-06,
      "loss": 1.1168,
      "step": 126690
    },
    {
      "epoch": 924.8175182481751,
      "grad_norm": 5.553668975830078,
      "learning_rate": 3.7591240875912412e-06,
      "loss": 0.919,
      "step": 126700
    },
    {
      "epoch": 924.8905109489051,
      "grad_norm": 9.14494514465332,
      "learning_rate": 3.7554744525547447e-06,
      "loss": 0.8337,
      "step": 126710
    },
    {
      "epoch": 924.9635036496351,
      "grad_norm": 0.015534523874521255,
      "learning_rate": 3.7518248175182486e-06,
      "loss": 0.8585,
      "step": 126720
    },
    {
      "epoch": 925.0364963503649,
      "grad_norm": 0.017800239846110344,
      "learning_rate": 3.748175182481752e-06,
      "loss": 0.6119,
      "step": 126730
    },
    {
      "epoch": 925.1094890510949,
      "grad_norm": 5.980858325958252,
      "learning_rate": 3.744525547445256e-06,
      "loss": 0.7729,
      "step": 126740
    },
    {
      "epoch": 925.1824817518249,
      "grad_norm": 17.267391204833984,
      "learning_rate": 3.74087591240876e-06,
      "loss": 0.9428,
      "step": 126750
    },
    {
      "epoch": 925.2554744525547,
      "grad_norm": 0.03378269448876381,
      "learning_rate": 3.7372262773722633e-06,
      "loss": 0.8263,
      "step": 126760
    },
    {
      "epoch": 925.3284671532847,
      "grad_norm": 6.892918109893799,
      "learning_rate": 3.7335766423357663e-06,
      "loss": 1.1419,
      "step": 126770
    },
    {
      "epoch": 925.4014598540145,
      "grad_norm": 12.99010944366455,
      "learning_rate": 3.7299270072992698e-06,
      "loss": 0.9324,
      "step": 126780
    },
    {
      "epoch": 925.4744525547445,
      "grad_norm": 8.181194305419922,
      "learning_rate": 3.7262773722627736e-06,
      "loss": 0.9554,
      "step": 126790
    },
    {
      "epoch": 925.5474452554745,
      "grad_norm": 1.4624931812286377,
      "learning_rate": 3.722627737226277e-06,
      "loss": 0.629,
      "step": 126800
    },
    {
      "epoch": 925.6204379562043,
      "grad_norm": 11.317801475524902,
      "learning_rate": 3.718978102189781e-06,
      "loss": 0.8798,
      "step": 126810
    },
    {
      "epoch": 925.6934306569343,
      "grad_norm": 10.046052932739258,
      "learning_rate": 3.715328467153285e-06,
      "loss": 0.7315,
      "step": 126820
    },
    {
      "epoch": 925.7664233576643,
      "grad_norm": 8.032247543334961,
      "learning_rate": 3.7116788321167883e-06,
      "loss": 0.9207,
      "step": 126830
    },
    {
      "epoch": 925.8394160583941,
      "grad_norm": 6.334859848022461,
      "learning_rate": 3.708029197080292e-06,
      "loss": 0.8413,
      "step": 126840
    },
    {
      "epoch": 925.9124087591241,
      "grad_norm": 6.74310302734375,
      "learning_rate": 3.7043795620437957e-06,
      "loss": 1.0368,
      "step": 126850
    },
    {
      "epoch": 925.985401459854,
      "grad_norm": 14.950775146484375,
      "learning_rate": 3.7007299270072995e-06,
      "loss": 0.995,
      "step": 126860
    },
    {
      "epoch": 926.0583941605839,
      "grad_norm": 17.974843978881836,
      "learning_rate": 3.697080291970803e-06,
      "loss": 1.0699,
      "step": 126870
    },
    {
      "epoch": 926.1313868613139,
      "grad_norm": 10.807999610900879,
      "learning_rate": 3.693430656934307e-06,
      "loss": 1.0556,
      "step": 126880
    },
    {
      "epoch": 926.2043795620438,
      "grad_norm": 11.52134895324707,
      "learning_rate": 3.6897810218978103e-06,
      "loss": 1.3563,
      "step": 126890
    },
    {
      "epoch": 926.2773722627737,
      "grad_norm": 1.1024338006973267,
      "learning_rate": 3.6861313868613142e-06,
      "loss": 0.7171,
      "step": 126900
    },
    {
      "epoch": 926.3503649635037,
      "grad_norm": 7.3900370597839355,
      "learning_rate": 3.6824817518248177e-06,
      "loss": 0.8527,
      "step": 126910
    },
    {
      "epoch": 926.4233576642335,
      "grad_norm": 0.10839930176734924,
      "learning_rate": 3.6788321167883216e-06,
      "loss": 0.9029,
      "step": 126920
    },
    {
      "epoch": 926.4963503649635,
      "grad_norm": 9.229641914367676,
      "learning_rate": 3.675182481751825e-06,
      "loss": 0.7164,
      "step": 126930
    },
    {
      "epoch": 926.5693430656934,
      "grad_norm": 8.711318969726562,
      "learning_rate": 3.671532846715329e-06,
      "loss": 0.8618,
      "step": 126940
    },
    {
      "epoch": 926.6423357664233,
      "grad_norm": 0.029286285862326622,
      "learning_rate": 3.6678832116788324e-06,
      "loss": 0.6486,
      "step": 126950
    },
    {
      "epoch": 926.7153284671533,
      "grad_norm": 6.68352746963501,
      "learning_rate": 3.6642335766423362e-06,
      "loss": 0.9284,
      "step": 126960
    },
    {
      "epoch": 926.7883211678832,
      "grad_norm": 4.983750343322754,
      "learning_rate": 3.6605839416058397e-06,
      "loss": 0.5685,
      "step": 126970
    },
    {
      "epoch": 926.8613138686131,
      "grad_norm": 0.10794516652822495,
      "learning_rate": 3.6569343065693436e-06,
      "loss": 1.0477,
      "step": 126980
    },
    {
      "epoch": 926.9343065693431,
      "grad_norm": 15.977663040161133,
      "learning_rate": 3.6532846715328466e-06,
      "loss": 1.0946,
      "step": 126990
    },
    {
      "epoch": 927.007299270073,
      "grad_norm": 13.040977478027344,
      "learning_rate": 3.64963503649635e-06,
      "loss": 0.8357,
      "step": 127000
    },
    {
      "epoch": 927.0802919708029,
      "grad_norm": 8.06118392944336,
      "learning_rate": 3.645985401459854e-06,
      "loss": 1.1932,
      "step": 127010
    },
    {
      "epoch": 927.1532846715329,
      "grad_norm": 5.3099284172058105,
      "learning_rate": 3.6423357664233574e-06,
      "loss": 1.093,
      "step": 127020
    },
    {
      "epoch": 927.2262773722628,
      "grad_norm": 0.016447575762867928,
      "learning_rate": 3.6386861313868613e-06,
      "loss": 1.0907,
      "step": 127030
    },
    {
      "epoch": 927.2992700729927,
      "grad_norm": 18.690526962280273,
      "learning_rate": 3.6350364963503648e-06,
      "loss": 0.965,
      "step": 127040
    },
    {
      "epoch": 927.3722627737226,
      "grad_norm": 12.519671440124512,
      "learning_rate": 3.6313868613138687e-06,
      "loss": 0.8871,
      "step": 127050
    },
    {
      "epoch": 927.4452554744526,
      "grad_norm": 6.597615718841553,
      "learning_rate": 3.6277372262773725e-06,
      "loss": 0.9779,
      "step": 127060
    },
    {
      "epoch": 927.5182481751825,
      "grad_norm": 11.737014770507812,
      "learning_rate": 3.624087591240876e-06,
      "loss": 0.5837,
      "step": 127070
    },
    {
      "epoch": 927.5912408759124,
      "grad_norm": 9.91640567779541,
      "learning_rate": 3.62043795620438e-06,
      "loss": 0.8272,
      "step": 127080
    },
    {
      "epoch": 927.6642335766423,
      "grad_norm": 6.722177028656006,
      "learning_rate": 3.6167883211678833e-06,
      "loss": 0.8292,
      "step": 127090
    },
    {
      "epoch": 927.7372262773723,
      "grad_norm": 8.157635688781738,
      "learning_rate": 3.6131386861313872e-06,
      "loss": 0.7701,
      "step": 127100
    },
    {
      "epoch": 927.8102189781022,
      "grad_norm": 8.836793899536133,
      "learning_rate": 3.6094890510948907e-06,
      "loss": 0.5185,
      "step": 127110
    },
    {
      "epoch": 927.8832116788321,
      "grad_norm": 10.57040786743164,
      "learning_rate": 3.6058394160583946e-06,
      "loss": 0.8983,
      "step": 127120
    },
    {
      "epoch": 927.956204379562,
      "grad_norm": 8.210894584655762,
      "learning_rate": 3.602189781021898e-06,
      "loss": 0.9296,
      "step": 127130
    },
    {
      "epoch": 928.029197080292,
      "grad_norm": 8.253399848937988,
      "learning_rate": 3.598540145985402e-06,
      "loss": 0.717,
      "step": 127140
    },
    {
      "epoch": 928.1021897810219,
      "grad_norm": 7.927940368652344,
      "learning_rate": 3.5948905109489054e-06,
      "loss": 0.6668,
      "step": 127150
    },
    {
      "epoch": 928.1751824817518,
      "grad_norm": 0.00976745504885912,
      "learning_rate": 3.5912408759124092e-06,
      "loss": 0.7207,
      "step": 127160
    },
    {
      "epoch": 928.2481751824818,
      "grad_norm": 16.95637321472168,
      "learning_rate": 3.5875912408759127e-06,
      "loss": 1.142,
      "step": 127170
    },
    {
      "epoch": 928.3211678832117,
      "grad_norm": 8.319305419921875,
      "learning_rate": 3.5839416058394166e-06,
      "loss": 0.3634,
      "step": 127180
    },
    {
      "epoch": 928.3941605839416,
      "grad_norm": 17.81662940979004,
      "learning_rate": 3.58029197080292e-06,
      "loss": 0.7954,
      "step": 127190
    },
    {
      "epoch": 928.4671532846716,
      "grad_norm": 15.09228515625,
      "learning_rate": 3.576642335766423e-06,
      "loss": 1.1373,
      "step": 127200
    },
    {
      "epoch": 928.5401459854014,
      "grad_norm": 12.924932479858398,
      "learning_rate": 3.572992700729927e-06,
      "loss": 1.1044,
      "step": 127210
    },
    {
      "epoch": 928.6131386861314,
      "grad_norm": 0.11881284415721893,
      "learning_rate": 3.5693430656934304e-06,
      "loss": 0.69,
      "step": 127220
    },
    {
      "epoch": 928.6861313868613,
      "grad_norm": 13.893527030944824,
      "learning_rate": 3.5656934306569343e-06,
      "loss": 0.984,
      "step": 127230
    },
    {
      "epoch": 928.7591240875912,
      "grad_norm": 12.555147171020508,
      "learning_rate": 3.5620437956204378e-06,
      "loss": 0.9565,
      "step": 127240
    },
    {
      "epoch": 928.8321167883212,
      "grad_norm": 8.946414947509766,
      "learning_rate": 3.5583941605839416e-06,
      "loss": 1.1249,
      "step": 127250
    },
    {
      "epoch": 928.9051094890511,
      "grad_norm": 18.070510864257812,
      "learning_rate": 3.554744525547445e-06,
      "loss": 1.2439,
      "step": 127260
    },
    {
      "epoch": 928.978102189781,
      "grad_norm": 12.952332496643066,
      "learning_rate": 3.551094890510949e-06,
      "loss": 0.88,
      "step": 127270
    },
    {
      "epoch": 929.051094890511,
      "grad_norm": 12.49507999420166,
      "learning_rate": 3.5474452554744524e-06,
      "loss": 0.9565,
      "step": 127280
    },
    {
      "epoch": 929.1240875912408,
      "grad_norm": 0.0260990709066391,
      "learning_rate": 3.5437956204379563e-06,
      "loss": 0.7146,
      "step": 127290
    },
    {
      "epoch": 929.1970802919708,
      "grad_norm": 12.026214599609375,
      "learning_rate": 3.54014598540146e-06,
      "loss": 0.7036,
      "step": 127300
    },
    {
      "epoch": 929.2700729927008,
      "grad_norm": 11.483665466308594,
      "learning_rate": 3.5364963503649637e-06,
      "loss": 0.7144,
      "step": 127310
    },
    {
      "epoch": 929.3430656934306,
      "grad_norm": 15.36821460723877,
      "learning_rate": 3.5328467153284675e-06,
      "loss": 1.2996,
      "step": 127320
    },
    {
      "epoch": 929.4160583941606,
      "grad_norm": 7.009989261627197,
      "learning_rate": 3.529197080291971e-06,
      "loss": 0.8728,
      "step": 127330
    },
    {
      "epoch": 929.4890510948906,
      "grad_norm": 19.600914001464844,
      "learning_rate": 3.525547445255475e-06,
      "loss": 1.1817,
      "step": 127340
    },
    {
      "epoch": 929.5620437956204,
      "grad_norm": 6.84523344039917,
      "learning_rate": 3.5218978102189783e-06,
      "loss": 0.7954,
      "step": 127350
    },
    {
      "epoch": 929.6350364963504,
      "grad_norm": 9.399279594421387,
      "learning_rate": 3.5182481751824822e-06,
      "loss": 1.378,
      "step": 127360
    },
    {
      "epoch": 929.7080291970802,
      "grad_norm": 9.58133316040039,
      "learning_rate": 3.5145985401459857e-06,
      "loss": 0.7583,
      "step": 127370
    },
    {
      "epoch": 929.7810218978102,
      "grad_norm": 10.57203197479248,
      "learning_rate": 3.5109489051094896e-06,
      "loss": 0.9772,
      "step": 127380
    },
    {
      "epoch": 929.8540145985402,
      "grad_norm": 12.387298583984375,
      "learning_rate": 3.507299270072993e-06,
      "loss": 0.8781,
      "step": 127390
    },
    {
      "epoch": 929.92700729927,
      "grad_norm": 7.035003662109375,
      "learning_rate": 3.503649635036497e-06,
      "loss": 0.7366,
      "step": 127400
    },
    {
      "epoch": 930.0,
      "grad_norm": 16.541759490966797,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.7262,
      "step": 127410
    },
    {
      "epoch": 930.07299270073,
      "grad_norm": 9.931766510009766,
      "learning_rate": 3.4963503649635034e-06,
      "loss": 1.0018,
      "step": 127420
    },
    {
      "epoch": 930.1459854014598,
      "grad_norm": 7.644495487213135,
      "learning_rate": 3.4927007299270073e-06,
      "loss": 1.0191,
      "step": 127430
    },
    {
      "epoch": 930.2189781021898,
      "grad_norm": 7.891960620880127,
      "learning_rate": 3.4890510948905107e-06,
      "loss": 1.0445,
      "step": 127440
    },
    {
      "epoch": 930.2919708029198,
      "grad_norm": 11.400487899780273,
      "learning_rate": 3.4854014598540146e-06,
      "loss": 1.0384,
      "step": 127450
    },
    {
      "epoch": 930.3649635036496,
      "grad_norm": 3.268416166305542,
      "learning_rate": 3.481751824817518e-06,
      "loss": 1.0373,
      "step": 127460
    },
    {
      "epoch": 930.4379562043796,
      "grad_norm": 0.023485127836465836,
      "learning_rate": 3.478102189781022e-06,
      "loss": 0.7073,
      "step": 127470
    },
    {
      "epoch": 930.5109489051094,
      "grad_norm": 13.263880729675293,
      "learning_rate": 3.4744525547445254e-06,
      "loss": 1.0857,
      "step": 127480
    },
    {
      "epoch": 930.5839416058394,
      "grad_norm": 7.694577693939209,
      "learning_rate": 3.4708029197080293e-06,
      "loss": 0.8289,
      "step": 127490
    },
    {
      "epoch": 930.6569343065694,
      "grad_norm": 10.548766136169434,
      "learning_rate": 3.4671532846715328e-06,
      "loss": 0.8294,
      "step": 127500
    },
    {
      "epoch": 930.7299270072992,
      "grad_norm": 13.914716720581055,
      "learning_rate": 3.4635036496350367e-06,
      "loss": 0.6827,
      "step": 127510
    },
    {
      "epoch": 930.8029197080292,
      "grad_norm": 11.58974838256836,
      "learning_rate": 3.45985401459854e-06,
      "loss": 1.0539,
      "step": 127520
    },
    {
      "epoch": 930.8759124087592,
      "grad_norm": 0.04930775985121727,
      "learning_rate": 3.456204379562044e-06,
      "loss": 0.5966,
      "step": 127530
    },
    {
      "epoch": 930.948905109489,
      "grad_norm": 8.947235107421875,
      "learning_rate": 3.452554744525548e-06,
      "loss": 0.757,
      "step": 127540
    },
    {
      "epoch": 931.021897810219,
      "grad_norm": 10.25163745880127,
      "learning_rate": 3.4489051094890513e-06,
      "loss": 0.8027,
      "step": 127550
    },
    {
      "epoch": 931.0948905109489,
      "grad_norm": 6.9898881912231445,
      "learning_rate": 3.4452554744525552e-06,
      "loss": 1.2213,
      "step": 127560
    },
    {
      "epoch": 931.1678832116788,
      "grad_norm": 7.630408763885498,
      "learning_rate": 3.4416058394160587e-06,
      "loss": 0.9231,
      "step": 127570
    },
    {
      "epoch": 931.2408759124088,
      "grad_norm": 11.547390937805176,
      "learning_rate": 3.4379562043795626e-06,
      "loss": 0.7552,
      "step": 127580
    },
    {
      "epoch": 931.3138686131387,
      "grad_norm": 7.459113597869873,
      "learning_rate": 3.434306569343066e-06,
      "loss": 0.7226,
      "step": 127590
    },
    {
      "epoch": 931.3868613138686,
      "grad_norm": 0.027622608467936516,
      "learning_rate": 3.43065693430657e-06,
      "loss": 0.8046,
      "step": 127600
    },
    {
      "epoch": 931.4598540145986,
      "grad_norm": 4.361431121826172,
      "learning_rate": 3.4270072992700734e-06,
      "loss": 0.9775,
      "step": 127610
    },
    {
      "epoch": 931.5328467153284,
      "grad_norm": 11.675664901733398,
      "learning_rate": 3.4233576642335772e-06,
      "loss": 0.6613,
      "step": 127620
    },
    {
      "epoch": 931.6058394160584,
      "grad_norm": 12.168251037597656,
      "learning_rate": 3.4197080291970803e-06,
      "loss": 1.2576,
      "step": 127630
    },
    {
      "epoch": 931.6788321167883,
      "grad_norm": 10.99539566040039,
      "learning_rate": 3.4160583941605837e-06,
      "loss": 0.9469,
      "step": 127640
    },
    {
      "epoch": 931.7518248175182,
      "grad_norm": 5.491716384887695,
      "learning_rate": 3.4124087591240876e-06,
      "loss": 0.8691,
      "step": 127650
    },
    {
      "epoch": 931.8248175182482,
      "grad_norm": 0.11701683700084686,
      "learning_rate": 3.408759124087591e-06,
      "loss": 0.7022,
      "step": 127660
    },
    {
      "epoch": 931.8978102189781,
      "grad_norm": 0.052082136273384094,
      "learning_rate": 3.405109489051095e-06,
      "loss": 0.8593,
      "step": 127670
    },
    {
      "epoch": 931.970802919708,
      "grad_norm": 8.005875587463379,
      "learning_rate": 3.4014598540145984e-06,
      "loss": 0.839,
      "step": 127680
    },
    {
      "epoch": 932.043795620438,
      "grad_norm": 9.835612297058105,
      "learning_rate": 3.3978102189781023e-06,
      "loss": 0.9375,
      "step": 127690
    },
    {
      "epoch": 932.1167883211679,
      "grad_norm": 9.16500473022461,
      "learning_rate": 3.3941605839416058e-06,
      "loss": 1.0133,
      "step": 127700
    },
    {
      "epoch": 932.1897810218978,
      "grad_norm": 5.925703525543213,
      "learning_rate": 3.3905109489051096e-06,
      "loss": 1.1231,
      "step": 127710
    },
    {
      "epoch": 932.2627737226277,
      "grad_norm": 12.446123123168945,
      "learning_rate": 3.386861313868613e-06,
      "loss": 0.816,
      "step": 127720
    },
    {
      "epoch": 932.3357664233577,
      "grad_norm": 9.579228401184082,
      "learning_rate": 3.383211678832117e-06,
      "loss": 1.3959,
      "step": 127730
    },
    {
      "epoch": 932.4087591240876,
      "grad_norm": 13.773362159729004,
      "learning_rate": 3.3795620437956204e-06,
      "loss": 0.7353,
      "step": 127740
    },
    {
      "epoch": 932.4817518248175,
      "grad_norm": 7.41341495513916,
      "learning_rate": 3.3759124087591243e-06,
      "loss": 0.5913,
      "step": 127750
    },
    {
      "epoch": 932.5547445255474,
      "grad_norm": 9.464567184448242,
      "learning_rate": 3.3722627737226278e-06,
      "loss": 1.1115,
      "step": 127760
    },
    {
      "epoch": 932.6277372262774,
      "grad_norm": 7.499233722686768,
      "learning_rate": 3.3686131386861317e-06,
      "loss": 0.719,
      "step": 127770
    },
    {
      "epoch": 932.7007299270073,
      "grad_norm": 7.074644088745117,
      "learning_rate": 3.364963503649635e-06,
      "loss": 0.6857,
      "step": 127780
    },
    {
      "epoch": 932.7737226277372,
      "grad_norm": 7.298714637756348,
      "learning_rate": 3.361313868613139e-06,
      "loss": 0.8617,
      "step": 127790
    },
    {
      "epoch": 932.8467153284671,
      "grad_norm": 6.808485507965088,
      "learning_rate": 3.357664233576643e-06,
      "loss": 0.6993,
      "step": 127800
    },
    {
      "epoch": 932.9197080291971,
      "grad_norm": 11.76315975189209,
      "learning_rate": 3.3540145985401464e-06,
      "loss": 1.2144,
      "step": 127810
    },
    {
      "epoch": 932.992700729927,
      "grad_norm": 9.68418025970459,
      "learning_rate": 3.3503649635036502e-06,
      "loss": 0.66,
      "step": 127820
    },
    {
      "epoch": 933.0656934306569,
      "grad_norm": 7.333197116851807,
      "learning_rate": 3.3467153284671537e-06,
      "loss": 0.8949,
      "step": 127830
    },
    {
      "epoch": 933.1386861313869,
      "grad_norm": 10.110540390014648,
      "learning_rate": 3.3430656934306576e-06,
      "loss": 0.8963,
      "step": 127840
    },
    {
      "epoch": 933.2116788321168,
      "grad_norm": 3.5954370498657227,
      "learning_rate": 3.3394160583941606e-06,
      "loss": 1.0423,
      "step": 127850
    },
    {
      "epoch": 933.2846715328467,
      "grad_norm": 12.753255844116211,
      "learning_rate": 3.335766423357664e-06,
      "loss": 1.3351,
      "step": 127860
    },
    {
      "epoch": 933.3576642335767,
      "grad_norm": 9.805451393127441,
      "learning_rate": 3.332116788321168e-06,
      "loss": 1.0598,
      "step": 127870
    },
    {
      "epoch": 933.4306569343066,
      "grad_norm": 17.164621353149414,
      "learning_rate": 3.3284671532846714e-06,
      "loss": 0.8935,
      "step": 127880
    },
    {
      "epoch": 933.5036496350365,
      "grad_norm": 5.667205333709717,
      "learning_rate": 3.3248175182481753e-06,
      "loss": 0.8664,
      "step": 127890
    },
    {
      "epoch": 933.5766423357665,
      "grad_norm": 12.116703033447266,
      "learning_rate": 3.3211678832116788e-06,
      "loss": 0.776,
      "step": 127900
    },
    {
      "epoch": 933.6496350364963,
      "grad_norm": 15.188887596130371,
      "learning_rate": 3.3175182481751826e-06,
      "loss": 0.7013,
      "step": 127910
    },
    {
      "epoch": 933.7226277372263,
      "grad_norm": 9.592048645019531,
      "learning_rate": 3.313868613138686e-06,
      "loss": 0.7841,
      "step": 127920
    },
    {
      "epoch": 933.7956204379562,
      "grad_norm": 0.09789173305034637,
      "learning_rate": 3.31021897810219e-06,
      "loss": 0.7035,
      "step": 127930
    },
    {
      "epoch": 933.8686131386861,
      "grad_norm": 12.797752380371094,
      "learning_rate": 3.3065693430656934e-06,
      "loss": 0.9761,
      "step": 127940
    },
    {
      "epoch": 933.9416058394161,
      "grad_norm": 12.39425277709961,
      "learning_rate": 3.3029197080291973e-06,
      "loss": 0.7496,
      "step": 127950
    },
    {
      "epoch": 934.014598540146,
      "grad_norm": 13.694097518920898,
      "learning_rate": 3.2992700729927008e-06,
      "loss": 0.833,
      "step": 127960
    },
    {
      "epoch": 934.0875912408759,
      "grad_norm": 10.400675773620605,
      "learning_rate": 3.2956204379562047e-06,
      "loss": 1.263,
      "step": 127970
    },
    {
      "epoch": 934.1605839416059,
      "grad_norm": 7.0305304527282715,
      "learning_rate": 3.291970802919708e-06,
      "loss": 0.9233,
      "step": 127980
    },
    {
      "epoch": 934.2335766423357,
      "grad_norm": 7.4479522705078125,
      "learning_rate": 3.288321167883212e-06,
      "loss": 0.8298,
      "step": 127990
    },
    {
      "epoch": 934.3065693430657,
      "grad_norm": 6.722994804382324,
      "learning_rate": 3.2846715328467155e-06,
      "loss": 0.7795,
      "step": 128000
    },
    {
      "epoch": 934.3795620437957,
      "grad_norm": 11.90245246887207,
      "learning_rate": 3.2810218978102193e-06,
      "loss": 0.7734,
      "step": 128010
    },
    {
      "epoch": 934.4525547445255,
      "grad_norm": 8.9512300491333,
      "learning_rate": 3.277372262773723e-06,
      "loss": 0.9634,
      "step": 128020
    },
    {
      "epoch": 934.5255474452555,
      "grad_norm": 0.006258175242692232,
      "learning_rate": 3.2737226277372267e-06,
      "loss": 0.6663,
      "step": 128030
    },
    {
      "epoch": 934.5985401459855,
      "grad_norm": 16.971500396728516,
      "learning_rate": 3.2700729927007306e-06,
      "loss": 1.304,
      "step": 128040
    },
    {
      "epoch": 934.6715328467153,
      "grad_norm": 7.883533954620361,
      "learning_rate": 3.266423357664234e-06,
      "loss": 0.6859,
      "step": 128050
    },
    {
      "epoch": 934.7445255474453,
      "grad_norm": 7.310460567474365,
      "learning_rate": 3.262773722627738e-06,
      "loss": 0.5939,
      "step": 128060
    },
    {
      "epoch": 934.8175182481751,
      "grad_norm": 1.5740571022033691,
      "learning_rate": 3.2591240875912405e-06,
      "loss": 0.7708,
      "step": 128070
    },
    {
      "epoch": 934.8905109489051,
      "grad_norm": 7.088504314422607,
      "learning_rate": 3.2554744525547444e-06,
      "loss": 0.7764,
      "step": 128080
    },
    {
      "epoch": 934.9635036496351,
      "grad_norm": 12.423995971679688,
      "learning_rate": 3.2518248175182483e-06,
      "loss": 0.6453,
      "step": 128090
    },
    {
      "epoch": 935.0364963503649,
      "grad_norm": 10.244696617126465,
      "learning_rate": 3.2481751824817517e-06,
      "loss": 0.8871,
      "step": 128100
    },
    {
      "epoch": 935.1094890510949,
      "grad_norm": 11.092121124267578,
      "learning_rate": 3.2445255474452556e-06,
      "loss": 0.8482,
      "step": 128110
    },
    {
      "epoch": 935.1824817518249,
      "grad_norm": 10.67918586730957,
      "learning_rate": 3.240875912408759e-06,
      "loss": 0.8556,
      "step": 128120
    },
    {
      "epoch": 935.2554744525547,
      "grad_norm": 13.25867748260498,
      "learning_rate": 3.237226277372263e-06,
      "loss": 0.7969,
      "step": 128130
    },
    {
      "epoch": 935.3284671532847,
      "grad_norm": 10.408672332763672,
      "learning_rate": 3.2335766423357664e-06,
      "loss": 0.9795,
      "step": 128140
    },
    {
      "epoch": 935.4014598540145,
      "grad_norm": 8.060347557067871,
      "learning_rate": 3.2299270072992703e-06,
      "loss": 0.9885,
      "step": 128150
    },
    {
      "epoch": 935.4744525547445,
      "grad_norm": 0.14288245141506195,
      "learning_rate": 3.2262773722627738e-06,
      "loss": 0.9412,
      "step": 128160
    },
    {
      "epoch": 935.5474452554745,
      "grad_norm": 8.350058555603027,
      "learning_rate": 3.2226277372262776e-06,
      "loss": 0.6716,
      "step": 128170
    },
    {
      "epoch": 935.6204379562043,
      "grad_norm": 0.012297124601900578,
      "learning_rate": 3.218978102189781e-06,
      "loss": 0.913,
      "step": 128180
    },
    {
      "epoch": 935.6934306569343,
      "grad_norm": 10.73538589477539,
      "learning_rate": 3.215328467153285e-06,
      "loss": 1.3322,
      "step": 128190
    },
    {
      "epoch": 935.7664233576643,
      "grad_norm": 15.825159072875977,
      "learning_rate": 3.2116788321167884e-06,
      "loss": 1.0491,
      "step": 128200
    },
    {
      "epoch": 935.8394160583941,
      "grad_norm": 10.519078254699707,
      "learning_rate": 3.2080291970802923e-06,
      "loss": 0.9404,
      "step": 128210
    },
    {
      "epoch": 935.9124087591241,
      "grad_norm": 7.298430442810059,
      "learning_rate": 3.204379562043796e-06,
      "loss": 0.4899,
      "step": 128220
    },
    {
      "epoch": 935.985401459854,
      "grad_norm": 4.607884407043457,
      "learning_rate": 3.2007299270072997e-06,
      "loss": 0.7912,
      "step": 128230
    },
    {
      "epoch": 936.0583941605839,
      "grad_norm": 8.248249053955078,
      "learning_rate": 3.197080291970803e-06,
      "loss": 0.4925,
      "step": 128240
    },
    {
      "epoch": 936.1313868613139,
      "grad_norm": 11.54906940460205,
      "learning_rate": 3.193430656934307e-06,
      "loss": 0.6584,
      "step": 128250
    },
    {
      "epoch": 936.2043795620438,
      "grad_norm": 6.081920146942139,
      "learning_rate": 3.1897810218978105e-06,
      "loss": 1.0266,
      "step": 128260
    },
    {
      "epoch": 936.2773722627737,
      "grad_norm": 9.625033378601074,
      "learning_rate": 3.1861313868613144e-06,
      "loss": 1.1591,
      "step": 128270
    },
    {
      "epoch": 936.3503649635037,
      "grad_norm": 0.2079019695520401,
      "learning_rate": 3.1824817518248174e-06,
      "loss": 0.7617,
      "step": 128280
    },
    {
      "epoch": 936.4233576642335,
      "grad_norm": 10.341029167175293,
      "learning_rate": 3.178832116788321e-06,
      "loss": 0.9289,
      "step": 128290
    },
    {
      "epoch": 936.4963503649635,
      "grad_norm": 7.512497901916504,
      "learning_rate": 3.1751824817518247e-06,
      "loss": 0.7853,
      "step": 128300
    },
    {
      "epoch": 936.5693430656934,
      "grad_norm": 8.76793384552002,
      "learning_rate": 3.171532846715328e-06,
      "loss": 1.448,
      "step": 128310
    },
    {
      "epoch": 936.6423357664233,
      "grad_norm": 8.881316184997559,
      "learning_rate": 3.167883211678832e-06,
      "loss": 0.8186,
      "step": 128320
    },
    {
      "epoch": 936.7153284671533,
      "grad_norm": 15.324918746948242,
      "learning_rate": 3.164233576642336e-06,
      "loss": 1.1634,
      "step": 128330
    },
    {
      "epoch": 936.7883211678832,
      "grad_norm": 13.434342384338379,
      "learning_rate": 3.1605839416058394e-06,
      "loss": 0.9777,
      "step": 128340
    },
    {
      "epoch": 936.8613138686131,
      "grad_norm": 13.454164505004883,
      "learning_rate": 3.1569343065693433e-06,
      "loss": 0.659,
      "step": 128350
    },
    {
      "epoch": 936.9343065693431,
      "grad_norm": 7.536350727081299,
      "learning_rate": 3.1532846715328468e-06,
      "loss": 0.5704,
      "step": 128360
    },
    {
      "epoch": 937.007299270073,
      "grad_norm": 0.02478400059044361,
      "learning_rate": 3.1496350364963506e-06,
      "loss": 0.8331,
      "step": 128370
    },
    {
      "epoch": 937.0802919708029,
      "grad_norm": 10.871597290039062,
      "learning_rate": 3.145985401459854e-06,
      "loss": 0.9902,
      "step": 128380
    },
    {
      "epoch": 937.1532846715329,
      "grad_norm": 7.463216781616211,
      "learning_rate": 3.142335766423358e-06,
      "loss": 0.936,
      "step": 128390
    },
    {
      "epoch": 937.2262773722628,
      "grad_norm": 13.331521034240723,
      "learning_rate": 3.1386861313868614e-06,
      "loss": 1.2576,
      "step": 128400
    },
    {
      "epoch": 937.2992700729927,
      "grad_norm": 0.01911470666527748,
      "learning_rate": 3.1350364963503653e-06,
      "loss": 0.6513,
      "step": 128410
    },
    {
      "epoch": 937.3722627737226,
      "grad_norm": 0.030675021931529045,
      "learning_rate": 3.1313868613138688e-06,
      "loss": 0.6412,
      "step": 128420
    },
    {
      "epoch": 937.4452554744526,
      "grad_norm": 6.625290393829346,
      "learning_rate": 3.1277372262773727e-06,
      "loss": 0.7884,
      "step": 128430
    },
    {
      "epoch": 937.5182481751825,
      "grad_norm": 11.771764755249023,
      "learning_rate": 3.1240875912408757e-06,
      "loss": 0.8683,
      "step": 128440
    },
    {
      "epoch": 937.5912408759124,
      "grad_norm": 6.678958415985107,
      "learning_rate": 3.1204379562043796e-06,
      "loss": 0.9352,
      "step": 128450
    },
    {
      "epoch": 937.6642335766423,
      "grad_norm": 5.349061012268066,
      "learning_rate": 3.1167883211678835e-06,
      "loss": 0.7678,
      "step": 128460
    },
    {
      "epoch": 937.7372262773723,
      "grad_norm": 5.952694892883301,
      "learning_rate": 3.113138686131387e-06,
      "loss": 0.8652,
      "step": 128470
    },
    {
      "epoch": 937.8102189781022,
      "grad_norm": 0.051211051642894745,
      "learning_rate": 3.109489051094891e-06,
      "loss": 0.7705,
      "step": 128480
    },
    {
      "epoch": 937.8832116788321,
      "grad_norm": 5.300180912017822,
      "learning_rate": 3.1058394160583943e-06,
      "loss": 1.017,
      "step": 128490
    },
    {
      "epoch": 937.956204379562,
      "grad_norm": 0.01310256589204073,
      "learning_rate": 3.102189781021898e-06,
      "loss": 0.8299,
      "step": 128500
    },
    {
      "epoch": 938.029197080292,
      "grad_norm": 13.865814208984375,
      "learning_rate": 3.0985401459854016e-06,
      "loss": 0.6163,
      "step": 128510
    },
    {
      "epoch": 938.1021897810219,
      "grad_norm": 12.336770057678223,
      "learning_rate": 3.0948905109489055e-06,
      "loss": 0.7791,
      "step": 128520
    },
    {
      "epoch": 938.1751824817518,
      "grad_norm": 5.923516750335693,
      "learning_rate": 3.091240875912409e-06,
      "loss": 1.2285,
      "step": 128530
    },
    {
      "epoch": 938.2481751824818,
      "grad_norm": 14.342596054077148,
      "learning_rate": 3.087591240875913e-06,
      "loss": 0.6823,
      "step": 128540
    },
    {
      "epoch": 938.3211678832117,
      "grad_norm": 0.11981397867202759,
      "learning_rate": 3.083941605839416e-06,
      "loss": 1.0206,
      "step": 128550
    },
    {
      "epoch": 938.3941605839416,
      "grad_norm": 15.281712532043457,
      "learning_rate": 3.0802919708029197e-06,
      "loss": 1.295,
      "step": 128560
    },
    {
      "epoch": 938.4671532846716,
      "grad_norm": 6.272029399871826,
      "learning_rate": 3.076642335766423e-06,
      "loss": 0.4792,
      "step": 128570
    },
    {
      "epoch": 938.5401459854014,
      "grad_norm": 8.51214599609375,
      "learning_rate": 3.072992700729927e-06,
      "loss": 0.6687,
      "step": 128580
    },
    {
      "epoch": 938.6131386861314,
      "grad_norm": 0.006610389333218336,
      "learning_rate": 3.069343065693431e-06,
      "loss": 0.9334,
      "step": 128590
    },
    {
      "epoch": 938.6861313868613,
      "grad_norm": 9.065935134887695,
      "learning_rate": 3.0656934306569344e-06,
      "loss": 0.9904,
      "step": 128600
    },
    {
      "epoch": 938.7591240875912,
      "grad_norm": 9.135095596313477,
      "learning_rate": 3.0620437956204383e-06,
      "loss": 0.9035,
      "step": 128610
    },
    {
      "epoch": 938.8321167883212,
      "grad_norm": 10.990501403808594,
      "learning_rate": 3.0583941605839418e-06,
      "loss": 0.8977,
      "step": 128620
    },
    {
      "epoch": 938.9051094890511,
      "grad_norm": 11.04013729095459,
      "learning_rate": 3.0547445255474457e-06,
      "loss": 0.9986,
      "step": 128630
    },
    {
      "epoch": 938.978102189781,
      "grad_norm": 7.516759395599365,
      "learning_rate": 3.051094890510949e-06,
      "loss": 1.0826,
      "step": 128640
    },
    {
      "epoch": 939.051094890511,
      "grad_norm": 10.356125831604004,
      "learning_rate": 3.047445255474453e-06,
      "loss": 0.5492,
      "step": 128650
    },
    {
      "epoch": 939.1240875912408,
      "grad_norm": 9.392468452453613,
      "learning_rate": 3.043795620437956e-06,
      "loss": 0.9095,
      "step": 128660
    },
    {
      "epoch": 939.1970802919708,
      "grad_norm": 12.161301612854004,
      "learning_rate": 3.04014598540146e-06,
      "loss": 0.4277,
      "step": 128670
    },
    {
      "epoch": 939.2700729927008,
      "grad_norm": 8.991640090942383,
      "learning_rate": 3.0364963503649634e-06,
      "loss": 0.8874,
      "step": 128680
    },
    {
      "epoch": 939.3430656934306,
      "grad_norm": 0.007913523353636265,
      "learning_rate": 3.0328467153284673e-06,
      "loss": 0.7507,
      "step": 128690
    },
    {
      "epoch": 939.4160583941606,
      "grad_norm": 12.008830070495605,
      "learning_rate": 3.029197080291971e-06,
      "loss": 1.1523,
      "step": 128700
    },
    {
      "epoch": 939.4890510948906,
      "grad_norm": 5.235206604003906,
      "learning_rate": 3.0255474452554746e-06,
      "loss": 1.2216,
      "step": 128710
    },
    {
      "epoch": 939.5620437956204,
      "grad_norm": 8.165046691894531,
      "learning_rate": 3.0218978102189785e-06,
      "loss": 0.9471,
      "step": 128720
    },
    {
      "epoch": 939.6350364963504,
      "grad_norm": 7.686872482299805,
      "learning_rate": 3.018248175182482e-06,
      "loss": 0.6837,
      "step": 128730
    },
    {
      "epoch": 939.7080291970802,
      "grad_norm": 12.33974838256836,
      "learning_rate": 3.014598540145986e-06,
      "loss": 1.2482,
      "step": 128740
    },
    {
      "epoch": 939.7810218978102,
      "grad_norm": 0.05772249400615692,
      "learning_rate": 3.0109489051094893e-06,
      "loss": 0.5725,
      "step": 128750
    },
    {
      "epoch": 939.8540145985402,
      "grad_norm": 9.957601547241211,
      "learning_rate": 3.007299270072993e-06,
      "loss": 1.0642,
      "step": 128760
    },
    {
      "epoch": 939.92700729927,
      "grad_norm": 9.328734397888184,
      "learning_rate": 3.003649635036496e-06,
      "loss": 0.7796,
      "step": 128770
    },
    {
      "epoch": 940.0,
      "grad_norm": 0.045434195548295975,
      "learning_rate": 3e-06,
      "loss": 0.9371,
      "step": 128780
    },
    {
      "epoch": 940.07299270073,
      "grad_norm": 7.133408546447754,
      "learning_rate": 2.9963503649635035e-06,
      "loss": 0.914,
      "step": 128790
    },
    {
      "epoch": 940.1459854014598,
      "grad_norm": 7.894316673278809,
      "learning_rate": 2.9927007299270074e-06,
      "loss": 0.8914,
      "step": 128800
    },
    {
      "epoch": 940.2189781021898,
      "grad_norm": 1.1392143964767456,
      "learning_rate": 2.989051094890511e-06,
      "loss": 0.673,
      "step": 128810
    },
    {
      "epoch": 940.2919708029198,
      "grad_norm": 12.989974021911621,
      "learning_rate": 2.9854014598540148e-06,
      "loss": 0.8617,
      "step": 128820
    },
    {
      "epoch": 940.3649635036496,
      "grad_norm": 6.0046234130859375,
      "learning_rate": 2.9817518248175186e-06,
      "loss": 0.7022,
      "step": 128830
    },
    {
      "epoch": 940.4379562043796,
      "grad_norm": 10.349245071411133,
      "learning_rate": 2.978102189781022e-06,
      "loss": 0.7361,
      "step": 128840
    },
    {
      "epoch": 940.5109489051094,
      "grad_norm": 7.142220497131348,
      "learning_rate": 2.974452554744526e-06,
      "loss": 0.7895,
      "step": 128850
    },
    {
      "epoch": 940.5839416058394,
      "grad_norm": 10.832655906677246,
      "learning_rate": 2.9708029197080294e-06,
      "loss": 1.3201,
      "step": 128860
    },
    {
      "epoch": 940.6569343065694,
      "grad_norm": 5.365034580230713,
      "learning_rate": 2.967153284671533e-06,
      "loss": 0.6841,
      "step": 128870
    },
    {
      "epoch": 940.7299270072992,
      "grad_norm": 0.05224018916487694,
      "learning_rate": 2.9635036496350364e-06,
      "loss": 0.9007,
      "step": 128880
    },
    {
      "epoch": 940.8029197080292,
      "grad_norm": 11.413042068481445,
      "learning_rate": 2.9598540145985402e-06,
      "loss": 0.9249,
      "step": 128890
    },
    {
      "epoch": 940.8759124087592,
      "grad_norm": 12.744742393493652,
      "learning_rate": 2.9562043795620437e-06,
      "loss": 0.7781,
      "step": 128900
    },
    {
      "epoch": 940.948905109489,
      "grad_norm": 12.897539138793945,
      "learning_rate": 2.9525547445255476e-06,
      "loss": 1.0212,
      "step": 128910
    },
    {
      "epoch": 941.021897810219,
      "grad_norm": 9.124658584594727,
      "learning_rate": 2.948905109489051e-06,
      "loss": 0.6941,
      "step": 128920
    },
    {
      "epoch": 941.0948905109489,
      "grad_norm": 0.05110539495944977,
      "learning_rate": 2.945255474452555e-06,
      "loss": 0.7282,
      "step": 128930
    },
    {
      "epoch": 941.1678832116788,
      "grad_norm": 7.565655708312988,
      "learning_rate": 2.9416058394160584e-06,
      "loss": 0.7909,
      "step": 128940
    },
    {
      "epoch": 941.2408759124088,
      "grad_norm": 9.636812210083008,
      "learning_rate": 2.9379562043795623e-06,
      "loss": 0.9547,
      "step": 128950
    },
    {
      "epoch": 941.3138686131387,
      "grad_norm": 10.313652992248535,
      "learning_rate": 2.934306569343066e-06,
      "loss": 1.2274,
      "step": 128960
    },
    {
      "epoch": 941.3868613138686,
      "grad_norm": 16.46762466430664,
      "learning_rate": 2.9306569343065696e-06,
      "loss": 1.2396,
      "step": 128970
    },
    {
      "epoch": 941.4598540145986,
      "grad_norm": 8.686534881591797,
      "learning_rate": 2.927007299270073e-06,
      "loss": 0.3905,
      "step": 128980
    },
    {
      "epoch": 941.5328467153284,
      "grad_norm": 8.557663917541504,
      "learning_rate": 2.9233576642335765e-06,
      "loss": 0.7521,
      "step": 128990
    },
    {
      "epoch": 941.6058394160584,
      "grad_norm": 0.2433154135942459,
      "learning_rate": 2.9197080291970804e-06,
      "loss": 0.7593,
      "step": 129000
    },
    {
      "epoch": 941.6788321167883,
      "grad_norm": 9.490447044372559,
      "learning_rate": 2.916058394160584e-06,
      "loss": 0.914,
      "step": 129010
    },
    {
      "epoch": 941.7518248175182,
      "grad_norm": 13.345309257507324,
      "learning_rate": 2.9124087591240877e-06,
      "loss": 1.0596,
      "step": 129020
    },
    {
      "epoch": 941.8248175182482,
      "grad_norm": 12.333294868469238,
      "learning_rate": 2.908759124087591e-06,
      "loss": 0.8377,
      "step": 129030
    },
    {
      "epoch": 941.8978102189781,
      "grad_norm": 7.125774383544922,
      "learning_rate": 2.905109489051095e-06,
      "loss": 1.3312,
      "step": 129040
    },
    {
      "epoch": 941.970802919708,
      "grad_norm": 16.012664794921875,
      "learning_rate": 2.9014598540145985e-06,
      "loss": 0.9709,
      "step": 129050
    },
    {
      "epoch": 942.043795620438,
      "grad_norm": 11.215971946716309,
      "learning_rate": 2.8978102189781024e-06,
      "loss": 0.4809,
      "step": 129060
    },
    {
      "epoch": 942.1167883211679,
      "grad_norm": 7.801334381103516,
      "learning_rate": 2.8941605839416063e-06,
      "loss": 0.9208,
      "step": 129070
    },
    {
      "epoch": 942.1897810218978,
      "grad_norm": 7.433260917663574,
      "learning_rate": 2.8905109489051098e-06,
      "loss": 1.0007,
      "step": 129080
    },
    {
      "epoch": 942.2627737226277,
      "grad_norm": 17.8660945892334,
      "learning_rate": 2.8868613138686132e-06,
      "loss": 0.711,
      "step": 129090
    },
    {
      "epoch": 942.3357664233577,
      "grad_norm": 0.0550290085375309,
      "learning_rate": 2.8832116788321167e-06,
      "loss": 0.9638,
      "step": 129100
    },
    {
      "epoch": 942.4087591240876,
      "grad_norm": 7.888347148895264,
      "learning_rate": 2.8795620437956206e-06,
      "loss": 0.8937,
      "step": 129110
    },
    {
      "epoch": 942.4817518248175,
      "grad_norm": 8.643400192260742,
      "learning_rate": 2.875912408759124e-06,
      "loss": 0.961,
      "step": 129120
    },
    {
      "epoch": 942.5547445255474,
      "grad_norm": 0.12373918294906616,
      "learning_rate": 2.872262773722628e-06,
      "loss": 0.9459,
      "step": 129130
    },
    {
      "epoch": 942.6277372262774,
      "grad_norm": 12.133013725280762,
      "learning_rate": 2.8686131386861314e-06,
      "loss": 1.0855,
      "step": 129140
    },
    {
      "epoch": 942.7007299270073,
      "grad_norm": 10.708518981933594,
      "learning_rate": 2.8649635036496353e-06,
      "loss": 0.6068,
      "step": 129150
    },
    {
      "epoch": 942.7737226277372,
      "grad_norm": 11.994540214538574,
      "learning_rate": 2.8613138686131387e-06,
      "loss": 1.3115,
      "step": 129160
    },
    {
      "epoch": 942.8467153284671,
      "grad_norm": 7.29644250869751,
      "learning_rate": 2.8576642335766426e-06,
      "loss": 1.0062,
      "step": 129170
    },
    {
      "epoch": 942.9197080291971,
      "grad_norm": 0.027623586356639862,
      "learning_rate": 2.854014598540146e-06,
      "loss": 0.5678,
      "step": 129180
    },
    {
      "epoch": 942.992700729927,
      "grad_norm": 0.02949879691004753,
      "learning_rate": 2.85036496350365e-06,
      "loss": 0.4767,
      "step": 129190
    },
    {
      "epoch": 943.0656934306569,
      "grad_norm": 12.421149253845215,
      "learning_rate": 2.8467153284671534e-06,
      "loss": 1.0086,
      "step": 129200
    },
    {
      "epoch": 943.1386861313869,
      "grad_norm": 17.563947677612305,
      "learning_rate": 2.843065693430657e-06,
      "loss": 0.8672,
      "step": 129210
    },
    {
      "epoch": 943.2116788321168,
      "grad_norm": 12.146625518798828,
      "learning_rate": 2.8394160583941607e-06,
      "loss": 0.4345,
      "step": 129220
    },
    {
      "epoch": 943.2846715328467,
      "grad_norm": 13.035749435424805,
      "learning_rate": 2.835766423357664e-06,
      "loss": 0.8835,
      "step": 129230
    },
    {
      "epoch": 943.3576642335767,
      "grad_norm": 7.58754301071167,
      "learning_rate": 2.832116788321168e-06,
      "loss": 0.8699,
      "step": 129240
    },
    {
      "epoch": 943.4306569343066,
      "grad_norm": 10.631027221679688,
      "learning_rate": 2.8284671532846715e-06,
      "loss": 1.2203,
      "step": 129250
    },
    {
      "epoch": 943.5036496350365,
      "grad_norm": 17.869049072265625,
      "learning_rate": 2.8248175182481754e-06,
      "loss": 0.8284,
      "step": 129260
    },
    {
      "epoch": 943.5766423357665,
      "grad_norm": 7.509507656097412,
      "learning_rate": 2.821167883211679e-06,
      "loss": 0.9124,
      "step": 129270
    },
    {
      "epoch": 943.6496350364963,
      "grad_norm": 0.5990391373634338,
      "learning_rate": 2.8175182481751828e-06,
      "loss": 0.5451,
      "step": 129280
    },
    {
      "epoch": 943.7226277372263,
      "grad_norm": 8.183464050292969,
      "learning_rate": 2.8138686131386862e-06,
      "loss": 0.9306,
      "step": 129290
    },
    {
      "epoch": 943.7956204379562,
      "grad_norm": 0.07276669144630432,
      "learning_rate": 2.8102189781021897e-06,
      "loss": 0.7314,
      "step": 129300
    },
    {
      "epoch": 943.8686131386861,
      "grad_norm": 15.331172943115234,
      "learning_rate": 2.8065693430656936e-06,
      "loss": 0.9685,
      "step": 129310
    },
    {
      "epoch": 943.9416058394161,
      "grad_norm": 12.106902122497559,
      "learning_rate": 2.802919708029197e-06,
      "loss": 1.2422,
      "step": 129320
    },
    {
      "epoch": 944.014598540146,
      "grad_norm": 0.007537812925875187,
      "learning_rate": 2.799270072992701e-06,
      "loss": 0.7455,
      "step": 129330
    },
    {
      "epoch": 944.0875912408759,
      "grad_norm": 0.04294521361589432,
      "learning_rate": 2.7956204379562044e-06,
      "loss": 0.4386,
      "step": 129340
    },
    {
      "epoch": 944.1605839416059,
      "grad_norm": 11.423768997192383,
      "learning_rate": 2.7919708029197082e-06,
      "loss": 0.7536,
      "step": 129350
    },
    {
      "epoch": 944.2335766423357,
      "grad_norm": 0.03694210574030876,
      "learning_rate": 2.7883211678832117e-06,
      "loss": 1.1532,
      "step": 129360
    },
    {
      "epoch": 944.3065693430657,
      "grad_norm": 18.173667907714844,
      "learning_rate": 2.7846715328467156e-06,
      "loss": 1.4434,
      "step": 129370
    },
    {
      "epoch": 944.3795620437957,
      "grad_norm": 10.79079818725586,
      "learning_rate": 2.781021897810219e-06,
      "loss": 0.911,
      "step": 129380
    },
    {
      "epoch": 944.4525547445255,
      "grad_norm": 17.337215423583984,
      "learning_rate": 2.777372262773723e-06,
      "loss": 1.3738,
      "step": 129390
    },
    {
      "epoch": 944.5255474452555,
      "grad_norm": 11.94257640838623,
      "learning_rate": 2.7737226277372264e-06,
      "loss": 0.9538,
      "step": 129400
    },
    {
      "epoch": 944.5985401459855,
      "grad_norm": 5.142261028289795,
      "learning_rate": 2.77007299270073e-06,
      "loss": 0.6979,
      "step": 129410
    },
    {
      "epoch": 944.6715328467153,
      "grad_norm": 7.677383899688721,
      "learning_rate": 2.7664233576642337e-06,
      "loss": 0.6703,
      "step": 129420
    },
    {
      "epoch": 944.7445255474453,
      "grad_norm": 6.747982978820801,
      "learning_rate": 2.762773722627737e-06,
      "loss": 0.946,
      "step": 129430
    },
    {
      "epoch": 944.8175182481751,
      "grad_norm": 10.456277847290039,
      "learning_rate": 2.759124087591241e-06,
      "loss": 0.8473,
      "step": 129440
    },
    {
      "epoch": 944.8905109489051,
      "grad_norm": 8.657872200012207,
      "learning_rate": 2.7554744525547445e-06,
      "loss": 0.9414,
      "step": 129450
    },
    {
      "epoch": 944.9635036496351,
      "grad_norm": 12.522127151489258,
      "learning_rate": 2.7518248175182484e-06,
      "loss": 0.6063,
      "step": 129460
    },
    {
      "epoch": 945.0364963503649,
      "grad_norm": 13.603269577026367,
      "learning_rate": 2.748175182481752e-06,
      "loss": 1.0341,
      "step": 129470
    },
    {
      "epoch": 945.1094890510949,
      "grad_norm": 12.313827514648438,
      "learning_rate": 2.7445255474452558e-06,
      "loss": 1.1922,
      "step": 129480
    },
    {
      "epoch": 945.1824817518249,
      "grad_norm": 12.259228706359863,
      "learning_rate": 2.740875912408759e-06,
      "loss": 0.948,
      "step": 129490
    },
    {
      "epoch": 945.2554744525547,
      "grad_norm": 8.483541488647461,
      "learning_rate": 2.737226277372263e-06,
      "loss": 0.6779,
      "step": 129500
    },
    {
      "epoch": 945.3284671532847,
      "grad_norm": 6.383532524108887,
      "learning_rate": 2.7335766423357666e-06,
      "loss": 0.9994,
      "step": 129510
    },
    {
      "epoch": 945.4014598540145,
      "grad_norm": 10.357627868652344,
      "learning_rate": 2.72992700729927e-06,
      "loss": 0.5971,
      "step": 129520
    },
    {
      "epoch": 945.4744525547445,
      "grad_norm": 8.28077507019043,
      "learning_rate": 2.726277372262774e-06,
      "loss": 0.7347,
      "step": 129530
    },
    {
      "epoch": 945.5474452554745,
      "grad_norm": 18.582778930664062,
      "learning_rate": 2.7226277372262774e-06,
      "loss": 1.2699,
      "step": 129540
    },
    {
      "epoch": 945.6204379562043,
      "grad_norm": 13.897783279418945,
      "learning_rate": 2.7189781021897812e-06,
      "loss": 0.6129,
      "step": 129550
    },
    {
      "epoch": 945.6934306569343,
      "grad_norm": 7.609193801879883,
      "learning_rate": 2.7153284671532847e-06,
      "loss": 0.6576,
      "step": 129560
    },
    {
      "epoch": 945.7664233576643,
      "grad_norm": 2.7689144611358643,
      "learning_rate": 2.7116788321167886e-06,
      "loss": 0.6243,
      "step": 129570
    },
    {
      "epoch": 945.8394160583941,
      "grad_norm": 16.514873504638672,
      "learning_rate": 2.708029197080292e-06,
      "loss": 0.9049,
      "step": 129580
    },
    {
      "epoch": 945.9124087591241,
      "grad_norm": 9.937812805175781,
      "learning_rate": 2.704379562043796e-06,
      "loss": 0.9934,
      "step": 129590
    },
    {
      "epoch": 945.985401459854,
      "grad_norm": 11.683006286621094,
      "learning_rate": 2.7007299270072994e-06,
      "loss": 1.0291,
      "step": 129600
    },
    {
      "epoch": 946.0583941605839,
      "grad_norm": 8.904342651367188,
      "learning_rate": 2.6970802919708033e-06,
      "loss": 0.7362,
      "step": 129610
    },
    {
      "epoch": 946.1313868613139,
      "grad_norm": 7.580275535583496,
      "learning_rate": 2.6934306569343067e-06,
      "loss": 0.8304,
      "step": 129620
    },
    {
      "epoch": 946.2043795620438,
      "grad_norm": 0.11661751568317413,
      "learning_rate": 2.68978102189781e-06,
      "loss": 0.7672,
      "step": 129630
    },
    {
      "epoch": 946.2773722627737,
      "grad_norm": 0.17146922647953033,
      "learning_rate": 2.686131386861314e-06,
      "loss": 0.6039,
      "step": 129640
    },
    {
      "epoch": 946.3503649635037,
      "grad_norm": 6.987618446350098,
      "learning_rate": 2.6824817518248175e-06,
      "loss": 0.9458,
      "step": 129650
    },
    {
      "epoch": 946.4233576642335,
      "grad_norm": 5.987061977386475,
      "learning_rate": 2.6788321167883214e-06,
      "loss": 0.5127,
      "step": 129660
    },
    {
      "epoch": 946.4963503649635,
      "grad_norm": 8.593009948730469,
      "learning_rate": 2.675182481751825e-06,
      "loss": 0.728,
      "step": 129670
    },
    {
      "epoch": 946.5693430656934,
      "grad_norm": 9.385665893554688,
      "learning_rate": 2.6715328467153287e-06,
      "loss": 0.9367,
      "step": 129680
    },
    {
      "epoch": 946.6423357664233,
      "grad_norm": 6.139744758605957,
      "learning_rate": 2.667883211678832e-06,
      "loss": 0.7268,
      "step": 129690
    },
    {
      "epoch": 946.7153284671533,
      "grad_norm": 7.019418239593506,
      "learning_rate": 2.664233576642336e-06,
      "loss": 0.9475,
      "step": 129700
    },
    {
      "epoch": 946.7883211678832,
      "grad_norm": 21.41726303100586,
      "learning_rate": 2.6605839416058395e-06,
      "loss": 0.9313,
      "step": 129710
    },
    {
      "epoch": 946.8613138686131,
      "grad_norm": 11.102503776550293,
      "learning_rate": 2.6569343065693434e-06,
      "loss": 0.9703,
      "step": 129720
    },
    {
      "epoch": 946.9343065693431,
      "grad_norm": 12.703774452209473,
      "learning_rate": 2.6532846715328465e-06,
      "loss": 1.2171,
      "step": 129730
    },
    {
      "epoch": 947.007299270073,
      "grad_norm": 12.563611030578613,
      "learning_rate": 2.6496350364963503e-06,
      "loss": 1.0788,
      "step": 129740
    },
    {
      "epoch": 947.0802919708029,
      "grad_norm": 9.374554634094238,
      "learning_rate": 2.6459854014598542e-06,
      "loss": 0.9454,
      "step": 129750
    },
    {
      "epoch": 947.1532846715329,
      "grad_norm": 9.192207336425781,
      "learning_rate": 2.6423357664233577e-06,
      "loss": 0.6689,
      "step": 129760
    },
    {
      "epoch": 947.2262773722628,
      "grad_norm": 9.213910102844238,
      "learning_rate": 2.6386861313868616e-06,
      "loss": 0.6104,
      "step": 129770
    },
    {
      "epoch": 947.2992700729927,
      "grad_norm": 7.651103973388672,
      "learning_rate": 2.635036496350365e-06,
      "loss": 0.4309,
      "step": 129780
    },
    {
      "epoch": 947.3722627737226,
      "grad_norm": 9.443318367004395,
      "learning_rate": 2.631386861313869e-06,
      "loss": 1.0978,
      "step": 129790
    },
    {
      "epoch": 947.4452554744526,
      "grad_norm": 12.269375801086426,
      "learning_rate": 2.6277372262773724e-06,
      "loss": 1.0157,
      "step": 129800
    },
    {
      "epoch": 947.5182481751825,
      "grad_norm": 6.47593879699707,
      "learning_rate": 2.6240875912408762e-06,
      "loss": 0.6471,
      "step": 129810
    },
    {
      "epoch": 947.5912408759124,
      "grad_norm": 6.297262191772461,
      "learning_rate": 2.6204379562043797e-06,
      "loss": 1.0384,
      "step": 129820
    },
    {
      "epoch": 947.6642335766423,
      "grad_norm": 9.706080436706543,
      "learning_rate": 2.6167883211678836e-06,
      "loss": 1.0255,
      "step": 129830
    },
    {
      "epoch": 947.7372262773723,
      "grad_norm": 12.393396377563477,
      "learning_rate": 2.6131386861313866e-06,
      "loss": 1.1387,
      "step": 129840
    },
    {
      "epoch": 947.8102189781022,
      "grad_norm": 13.77612590789795,
      "learning_rate": 2.6094890510948905e-06,
      "loss": 0.9446,
      "step": 129850
    },
    {
      "epoch": 947.8832116788321,
      "grad_norm": 13.716968536376953,
      "learning_rate": 2.6058394160583944e-06,
      "loss": 0.9697,
      "step": 129860
    },
    {
      "epoch": 947.956204379562,
      "grad_norm": 7.479134559631348,
      "learning_rate": 2.602189781021898e-06,
      "loss": 0.9864,
      "step": 129870
    },
    {
      "epoch": 948.029197080292,
      "grad_norm": 18.8643856048584,
      "learning_rate": 2.5985401459854017e-06,
      "loss": 0.9749,
      "step": 129880
    },
    {
      "epoch": 948.1021897810219,
      "grad_norm": 15.59481143951416,
      "learning_rate": 2.594890510948905e-06,
      "loss": 0.8031,
      "step": 129890
    },
    {
      "epoch": 948.1751824817518,
      "grad_norm": 5.627633571624756,
      "learning_rate": 2.591240875912409e-06,
      "loss": 0.9944,
      "step": 129900
    },
    {
      "epoch": 948.2481751824818,
      "grad_norm": 10.832775115966797,
      "learning_rate": 2.5875912408759125e-06,
      "loss": 1.0303,
      "step": 129910
    },
    {
      "epoch": 948.3211678832117,
      "grad_norm": 6.439515113830566,
      "learning_rate": 2.5839416058394164e-06,
      "loss": 0.777,
      "step": 129920
    },
    {
      "epoch": 948.3941605839416,
      "grad_norm": 13.343927383422852,
      "learning_rate": 2.58029197080292e-06,
      "loss": 0.6281,
      "step": 129930
    },
    {
      "epoch": 948.4671532846716,
      "grad_norm": 10.997844696044922,
      "learning_rate": 2.5766423357664238e-06,
      "loss": 0.7246,
      "step": 129940
    },
    {
      "epoch": 948.5401459854014,
      "grad_norm": 6.581037998199463,
      "learning_rate": 2.572992700729927e-06,
      "loss": 0.978,
      "step": 129950
    },
    {
      "epoch": 948.6131386861314,
      "grad_norm": 8.93380355834961,
      "learning_rate": 2.5693430656934307e-06,
      "loss": 0.4369,
      "step": 129960
    },
    {
      "epoch": 948.6861313868613,
      "grad_norm": 5.298480033874512,
      "learning_rate": 2.565693430656934e-06,
      "loss": 1.0504,
      "step": 129970
    },
    {
      "epoch": 948.7591240875912,
      "grad_norm": 11.580155372619629,
      "learning_rate": 2.562043795620438e-06,
      "loss": 0.8886,
      "step": 129980
    },
    {
      "epoch": 948.8321167883212,
      "grad_norm": 18.083593368530273,
      "learning_rate": 2.558394160583942e-06,
      "loss": 1.0887,
      "step": 129990
    },
    {
      "epoch": 948.9051094890511,
      "grad_norm": 11.488138198852539,
      "learning_rate": 2.5547445255474454e-06,
      "loss": 1.0814,
      "step": 130000
    },
    {
      "epoch": 948.978102189781,
      "grad_norm": 14.767728805541992,
      "learning_rate": 2.5510948905109492e-06,
      "loss": 0.9833,
      "step": 130010
    },
    {
      "epoch": 949.051094890511,
      "grad_norm": 9.412850379943848,
      "learning_rate": 2.5474452554744527e-06,
      "loss": 0.4355,
      "step": 130020
    },
    {
      "epoch": 949.1240875912408,
      "grad_norm": 6.535884380340576,
      "learning_rate": 2.5437956204379566e-06,
      "loss": 0.6948,
      "step": 130030
    },
    {
      "epoch": 949.1970802919708,
      "grad_norm": 21.48638153076172,
      "learning_rate": 2.54014598540146e-06,
      "loss": 1.0423,
      "step": 130040
    },
    {
      "epoch": 949.2700729927008,
      "grad_norm": 7.171560287475586,
      "learning_rate": 2.536496350364964e-06,
      "loss": 0.8532,
      "step": 130050
    },
    {
      "epoch": 949.3430656934306,
      "grad_norm": 16.39520263671875,
      "learning_rate": 2.532846715328467e-06,
      "loss": 1.4665,
      "step": 130060
    },
    {
      "epoch": 949.4160583941606,
      "grad_norm": 13.080992698669434,
      "learning_rate": 2.529197080291971e-06,
      "loss": 0.9574,
      "step": 130070
    },
    {
      "epoch": 949.4890510948906,
      "grad_norm": 17.062944412231445,
      "learning_rate": 2.5255474452554743e-06,
      "loss": 0.796,
      "step": 130080
    },
    {
      "epoch": 949.5620437956204,
      "grad_norm": 7.855672836303711,
      "learning_rate": 2.521897810218978e-06,
      "loss": 1.0069,
      "step": 130090
    },
    {
      "epoch": 949.6350364963504,
      "grad_norm": 0.045636825263500214,
      "learning_rate": 2.5182481751824816e-06,
      "loss": 0.8448,
      "step": 130100
    },
    {
      "epoch": 949.7080291970802,
      "grad_norm": 0.07690699398517609,
      "learning_rate": 2.5145985401459855e-06,
      "loss": 0.8561,
      "step": 130110
    },
    {
      "epoch": 949.7810218978102,
      "grad_norm": 0.18765009939670563,
      "learning_rate": 2.5109489051094894e-06,
      "loss": 0.9317,
      "step": 130120
    },
    {
      "epoch": 949.8540145985402,
      "grad_norm": 10.667131423950195,
      "learning_rate": 2.507299270072993e-06,
      "loss": 1.2847,
      "step": 130130
    },
    {
      "epoch": 949.92700729927,
      "grad_norm": 8.662346839904785,
      "learning_rate": 2.5036496350364967e-06,
      "loss": 0.9737,
      "step": 130140
    },
    {
      "epoch": 950.0,
      "grad_norm": 0.09261234104633331,
      "learning_rate": 2.5e-06,
      "loss": 0.4801,
      "step": 130150
    },
    {
      "epoch": 950.07299270073,
      "grad_norm": 14.825247764587402,
      "learning_rate": 2.496350364963504e-06,
      "loss": 0.9222,
      "step": 130160
    },
    {
      "epoch": 950.1459854014598,
      "grad_norm": 7.743831634521484,
      "learning_rate": 2.492700729927007e-06,
      "loss": 0.9157,
      "step": 130170
    },
    {
      "epoch": 950.2189781021898,
      "grad_norm": 19.8469181060791,
      "learning_rate": 2.489051094890511e-06,
      "loss": 0.6133,
      "step": 130180
    },
    {
      "epoch": 950.2919708029198,
      "grad_norm": 7.445026397705078,
      "learning_rate": 2.4854014598540145e-06,
      "loss": 0.9293,
      "step": 130190
    },
    {
      "epoch": 950.3649635036496,
      "grad_norm": 10.488018035888672,
      "learning_rate": 2.4817518248175183e-06,
      "loss": 0.758,
      "step": 130200
    },
    {
      "epoch": 950.4379562043796,
      "grad_norm": 0.04553546383976936,
      "learning_rate": 2.478102189781022e-06,
      "loss": 1.2287,
      "step": 130210
    },
    {
      "epoch": 950.5109489051094,
      "grad_norm": 11.21343994140625,
      "learning_rate": 2.4744525547445257e-06,
      "loss": 0.6721,
      "step": 130220
    },
    {
      "epoch": 950.5839416058394,
      "grad_norm": 11.068489074707031,
      "learning_rate": 2.4708029197080296e-06,
      "loss": 1.351,
      "step": 130230
    },
    {
      "epoch": 950.6569343065694,
      "grad_norm": 10.619848251342773,
      "learning_rate": 2.467153284671533e-06,
      "loss": 1.0433,
      "step": 130240
    },
    {
      "epoch": 950.7299270072992,
      "grad_norm": 14.29952335357666,
      "learning_rate": 2.463503649635037e-06,
      "loss": 1.138,
      "step": 130250
    },
    {
      "epoch": 950.8029197080292,
      "grad_norm": 5.210423469543457,
      "learning_rate": 2.4598540145985404e-06,
      "loss": 0.7402,
      "step": 130260
    },
    {
      "epoch": 950.8759124087592,
      "grad_norm": 7.5429863929748535,
      "learning_rate": 2.456204379562044e-06,
      "loss": 0.7643,
      "step": 130270
    },
    {
      "epoch": 950.948905109489,
      "grad_norm": 16.862207412719727,
      "learning_rate": 2.4525547445255473e-06,
      "loss": 0.6635,
      "step": 130280
    },
    {
      "epoch": 951.021897810219,
      "grad_norm": 14.859193801879883,
      "learning_rate": 2.448905109489051e-06,
      "loss": 0.706,
      "step": 130290
    },
    {
      "epoch": 951.0948905109489,
      "grad_norm": 14.142724990844727,
      "learning_rate": 2.4452554744525546e-06,
      "loss": 0.9721,
      "step": 130300
    },
    {
      "epoch": 951.1678832116788,
      "grad_norm": 14.224185943603516,
      "learning_rate": 2.4416058394160585e-06,
      "loss": 1.3129,
      "step": 130310
    },
    {
      "epoch": 951.2408759124088,
      "grad_norm": 0.11099235713481903,
      "learning_rate": 2.437956204379562e-06,
      "loss": 0.4211,
      "step": 130320
    },
    {
      "epoch": 951.3138686131387,
      "grad_norm": 8.212098121643066,
      "learning_rate": 2.434306569343066e-06,
      "loss": 0.9933,
      "step": 130330
    },
    {
      "epoch": 951.3868613138686,
      "grad_norm": 16.713380813598633,
      "learning_rate": 2.4306569343065693e-06,
      "loss": 0.8458,
      "step": 130340
    },
    {
      "epoch": 951.4598540145986,
      "grad_norm": 7.123632431030273,
      "learning_rate": 2.427007299270073e-06,
      "loss": 0.8563,
      "step": 130350
    },
    {
      "epoch": 951.5328467153284,
      "grad_norm": 16.364320755004883,
      "learning_rate": 2.423357664233577e-06,
      "loss": 0.965,
      "step": 130360
    },
    {
      "epoch": 951.6058394160584,
      "grad_norm": 8.403644561767578,
      "learning_rate": 2.4197080291970805e-06,
      "loss": 0.4638,
      "step": 130370
    },
    {
      "epoch": 951.6788321167883,
      "grad_norm": 7.893699645996094,
      "learning_rate": 2.416058394160584e-06,
      "loss": 0.8185,
      "step": 130380
    },
    {
      "epoch": 951.7518248175182,
      "grad_norm": 11.089728355407715,
      "learning_rate": 2.4124087591240875e-06,
      "loss": 1.106,
      "step": 130390
    },
    {
      "epoch": 951.8248175182482,
      "grad_norm": 12.286866188049316,
      "learning_rate": 2.4087591240875913e-06,
      "loss": 0.6759,
      "step": 130400
    },
    {
      "epoch": 951.8978102189781,
      "grad_norm": 10.951111793518066,
      "learning_rate": 2.405109489051095e-06,
      "loss": 1.1632,
      "step": 130410
    },
    {
      "epoch": 951.970802919708,
      "grad_norm": 9.028568267822266,
      "learning_rate": 2.4014598540145987e-06,
      "loss": 0.5869,
      "step": 130420
    },
    {
      "epoch": 952.043795620438,
      "grad_norm": 15.667123794555664,
      "learning_rate": 2.397810218978102e-06,
      "loss": 1.1796,
      "step": 130430
    },
    {
      "epoch": 952.1167883211679,
      "grad_norm": 13.402708053588867,
      "learning_rate": 2.394160583941606e-06,
      "loss": 0.9585,
      "step": 130440
    },
    {
      "epoch": 952.1897810218978,
      "grad_norm": 13.017333984375,
      "learning_rate": 2.3905109489051095e-06,
      "loss": 0.8766,
      "step": 130450
    },
    {
      "epoch": 952.2627737226277,
      "grad_norm": 5.166182041168213,
      "learning_rate": 2.3868613138686134e-06,
      "loss": 0.789,
      "step": 130460
    },
    {
      "epoch": 952.3357664233577,
      "grad_norm": 14.685040473937988,
      "learning_rate": 2.3832116788321172e-06,
      "loss": 0.957,
      "step": 130470
    },
    {
      "epoch": 952.4087591240876,
      "grad_norm": 11.462516784667969,
      "learning_rate": 2.3795620437956207e-06,
      "loss": 1.061,
      "step": 130480
    },
    {
      "epoch": 952.4817518248175,
      "grad_norm": 7.82626485824585,
      "learning_rate": 2.375912408759124e-06,
      "loss": 1.2555,
      "step": 130490
    },
    {
      "epoch": 952.5547445255474,
      "grad_norm": 11.485454559326172,
      "learning_rate": 2.3722627737226276e-06,
      "loss": 0.8219,
      "step": 130500
    },
    {
      "epoch": 952.6277372262774,
      "grad_norm": 13.154267311096191,
      "learning_rate": 2.3686131386861315e-06,
      "loss": 0.4956,
      "step": 130510
    },
    {
      "epoch": 952.7007299270073,
      "grad_norm": 13.51440143585205,
      "learning_rate": 2.364963503649635e-06,
      "loss": 0.2994,
      "step": 130520
    },
    {
      "epoch": 952.7737226277372,
      "grad_norm": 9.489494323730469,
      "learning_rate": 2.361313868613139e-06,
      "loss": 0.937,
      "step": 130530
    },
    {
      "epoch": 952.8467153284671,
      "grad_norm": 11.508078575134277,
      "learning_rate": 2.3576642335766423e-06,
      "loss": 1.0426,
      "step": 130540
    },
    {
      "epoch": 952.9197080291971,
      "grad_norm": 12.149742126464844,
      "learning_rate": 2.354014598540146e-06,
      "loss": 1.0413,
      "step": 130550
    },
    {
      "epoch": 952.992700729927,
      "grad_norm": 4.588482856750488,
      "learning_rate": 2.3503649635036496e-06,
      "loss": 0.9344,
      "step": 130560
    },
    {
      "epoch": 953.0656934306569,
      "grad_norm": 0.06598562747240067,
      "learning_rate": 2.3467153284671535e-06,
      "loss": 0.7518,
      "step": 130570
    },
    {
      "epoch": 953.1386861313869,
      "grad_norm": 20.464366912841797,
      "learning_rate": 2.343065693430657e-06,
      "loss": 1.1302,
      "step": 130580
    },
    {
      "epoch": 953.2116788321168,
      "grad_norm": 5.595420837402344,
      "learning_rate": 2.339416058394161e-06,
      "loss": 0.963,
      "step": 130590
    },
    {
      "epoch": 953.2846715328467,
      "grad_norm": 0.15912842750549316,
      "learning_rate": 2.3357664233576643e-06,
      "loss": 0.7743,
      "step": 130600
    },
    {
      "epoch": 953.3576642335767,
      "grad_norm": 19.237117767333984,
      "learning_rate": 2.3321167883211678e-06,
      "loss": 1.0185,
      "step": 130610
    },
    {
      "epoch": 953.4306569343066,
      "grad_norm": 9.873446464538574,
      "learning_rate": 2.3284671532846717e-06,
      "loss": 1.2146,
      "step": 130620
    },
    {
      "epoch": 953.5036496350365,
      "grad_norm": 9.642642974853516,
      "learning_rate": 2.324817518248175e-06,
      "loss": 0.7002,
      "step": 130630
    },
    {
      "epoch": 953.5766423357665,
      "grad_norm": 11.174798011779785,
      "learning_rate": 2.321167883211679e-06,
      "loss": 0.7664,
      "step": 130640
    },
    {
      "epoch": 953.6496350364963,
      "grad_norm": 9.772132873535156,
      "learning_rate": 2.3175182481751825e-06,
      "loss": 0.6054,
      "step": 130650
    },
    {
      "epoch": 953.7226277372263,
      "grad_norm": 7.898491859436035,
      "learning_rate": 2.3138686131386863e-06,
      "loss": 0.6786,
      "step": 130660
    },
    {
      "epoch": 953.7956204379562,
      "grad_norm": 6.333033561706543,
      "learning_rate": 2.31021897810219e-06,
      "loss": 1.0717,
      "step": 130670
    },
    {
      "epoch": 953.8686131386861,
      "grad_norm": 0.32475194334983826,
      "learning_rate": 2.3065693430656937e-06,
      "loss": 0.9132,
      "step": 130680
    },
    {
      "epoch": 953.9416058394161,
      "grad_norm": 12.18379020690918,
      "learning_rate": 2.302919708029197e-06,
      "loss": 1.3107,
      "step": 130690
    },
    {
      "epoch": 954.014598540146,
      "grad_norm": 6.890774250030518,
      "learning_rate": 2.2992700729927006e-06,
      "loss": 0.7136,
      "step": 130700
    },
    {
      "epoch": 954.0875912408759,
      "grad_norm": 8.213033676147461,
      "learning_rate": 2.2956204379562045e-06,
      "loss": 0.637,
      "step": 130710
    },
    {
      "epoch": 954.1605839416059,
      "grad_norm": 10.970311164855957,
      "learning_rate": 2.291970802919708e-06,
      "loss": 0.8434,
      "step": 130720
    },
    {
      "epoch": 954.2335766423357,
      "grad_norm": 10.538664817810059,
      "learning_rate": 2.288321167883212e-06,
      "loss": 0.8909,
      "step": 130730
    },
    {
      "epoch": 954.3065693430657,
      "grad_norm": 8.044371604919434,
      "learning_rate": 2.2846715328467153e-06,
      "loss": 1.1824,
      "step": 130740
    },
    {
      "epoch": 954.3795620437957,
      "grad_norm": 0.03377092257142067,
      "learning_rate": 2.281021897810219e-06,
      "loss": 0.5802,
      "step": 130750
    },
    {
      "epoch": 954.4525547445255,
      "grad_norm": 0.6709591746330261,
      "learning_rate": 2.2773722627737226e-06,
      "loss": 1.0012,
      "step": 130760
    },
    {
      "epoch": 954.5255474452555,
      "grad_norm": 10.099853515625,
      "learning_rate": 2.2737226277372265e-06,
      "loss": 1.0242,
      "step": 130770
    },
    {
      "epoch": 954.5985401459855,
      "grad_norm": 9.897078514099121,
      "learning_rate": 2.27007299270073e-06,
      "loss": 0.4996,
      "step": 130780
    },
    {
      "epoch": 954.6715328467153,
      "grad_norm": 12.008796691894531,
      "learning_rate": 2.266423357664234e-06,
      "loss": 0.962,
      "step": 130790
    },
    {
      "epoch": 954.7445255474453,
      "grad_norm": 0.021745270118117332,
      "learning_rate": 2.2627737226277373e-06,
      "loss": 1.0584,
      "step": 130800
    },
    {
      "epoch": 954.8175182481751,
      "grad_norm": 4.8693623542785645,
      "learning_rate": 2.2591240875912408e-06,
      "loss": 0.835,
      "step": 130810
    },
    {
      "epoch": 954.8905109489051,
      "grad_norm": 10.617618560791016,
      "learning_rate": 2.2554744525547447e-06,
      "loss": 1.1614,
      "step": 130820
    },
    {
      "epoch": 954.9635036496351,
      "grad_norm": 0.09574411064386368,
      "learning_rate": 2.251824817518248e-06,
      "loss": 0.7599,
      "step": 130830
    },
    {
      "epoch": 955.0364963503649,
      "grad_norm": 0.07596225291490555,
      "learning_rate": 2.248175182481752e-06,
      "loss": 1.0393,
      "step": 130840
    },
    {
      "epoch": 955.1094890510949,
      "grad_norm": 7.628974437713623,
      "learning_rate": 2.2445255474452555e-06,
      "loss": 0.9906,
      "step": 130850
    },
    {
      "epoch": 955.1824817518249,
      "grad_norm": 0.044139113277196884,
      "learning_rate": 2.2408759124087593e-06,
      "loss": 0.9715,
      "step": 130860
    },
    {
      "epoch": 955.2554744525547,
      "grad_norm": 7.151954650878906,
      "learning_rate": 2.237226277372263e-06,
      "loss": 0.8161,
      "step": 130870
    },
    {
      "epoch": 955.3284671532847,
      "grad_norm": 5.3600687980651855,
      "learning_rate": 2.2335766423357667e-06,
      "loss": 0.9064,
      "step": 130880
    },
    {
      "epoch": 955.4014598540145,
      "grad_norm": 12.24510669708252,
      "learning_rate": 2.22992700729927e-06,
      "loss": 1.173,
      "step": 130890
    },
    {
      "epoch": 955.4744525547445,
      "grad_norm": 9.553977012634277,
      "learning_rate": 2.226277372262774e-06,
      "loss": 0.721,
      "step": 130900
    },
    {
      "epoch": 955.5474452554745,
      "grad_norm": 13.227293014526367,
      "learning_rate": 2.2226277372262775e-06,
      "loss": 1.5005,
      "step": 130910
    },
    {
      "epoch": 955.6204379562043,
      "grad_norm": 8.469731330871582,
      "learning_rate": 2.218978102189781e-06,
      "loss": 0.6915,
      "step": 130920
    },
    {
      "epoch": 955.6934306569343,
      "grad_norm": 9.017105102539062,
      "learning_rate": 2.215328467153285e-06,
      "loss": 0.7814,
      "step": 130930
    },
    {
      "epoch": 955.7664233576643,
      "grad_norm": 3.373936891555786,
      "learning_rate": 2.2116788321167883e-06,
      "loss": 0.8714,
      "step": 130940
    },
    {
      "epoch": 955.8394160583941,
      "grad_norm": 0.02100645937025547,
      "learning_rate": 2.208029197080292e-06,
      "loss": 0.6299,
      "step": 130950
    },
    {
      "epoch": 955.9124087591241,
      "grad_norm": 7.55331563949585,
      "learning_rate": 2.2043795620437956e-06,
      "loss": 0.7426,
      "step": 130960
    },
    {
      "epoch": 955.985401459854,
      "grad_norm": 0.07995782047510147,
      "learning_rate": 2.2007299270072995e-06,
      "loss": 0.8786,
      "step": 130970
    },
    {
      "epoch": 956.0583941605839,
      "grad_norm": 0.04416731372475624,
      "learning_rate": 2.197080291970803e-06,
      "loss": 0.819,
      "step": 130980
    },
    {
      "epoch": 956.1313868613139,
      "grad_norm": 1.8006651401519775,
      "learning_rate": 2.193430656934307e-06,
      "loss": 0.9588,
      "step": 130990
    },
    {
      "epoch": 956.2043795620438,
      "grad_norm": 9.968887329101562,
      "learning_rate": 2.1897810218978103e-06,
      "loss": 0.9878,
      "step": 131000
    },
    {
      "epoch": 956.2773722627737,
      "grad_norm": 11.594565391540527,
      "learning_rate": 2.186131386861314e-06,
      "loss": 0.8978,
      "step": 131010
    },
    {
      "epoch": 956.3503649635037,
      "grad_norm": 10.41329288482666,
      "learning_rate": 2.1824817518248176e-06,
      "loss": 0.552,
      "step": 131020
    },
    {
      "epoch": 956.4233576642335,
      "grad_norm": 4.977692604064941,
      "learning_rate": 2.178832116788321e-06,
      "loss": 1.284,
      "step": 131030
    },
    {
      "epoch": 956.4963503649635,
      "grad_norm": 11.325642585754395,
      "learning_rate": 2.175182481751825e-06,
      "loss": 1.0373,
      "step": 131040
    },
    {
      "epoch": 956.5693430656934,
      "grad_norm": 7.091619491577148,
      "learning_rate": 2.1715328467153284e-06,
      "loss": 1.0688,
      "step": 131050
    },
    {
      "epoch": 956.6423357664233,
      "grad_norm": 0.04905024915933609,
      "learning_rate": 2.1678832116788323e-06,
      "loss": 0.6272,
      "step": 131060
    },
    {
      "epoch": 956.7153284671533,
      "grad_norm": 10.315142631530762,
      "learning_rate": 2.164233576642336e-06,
      "loss": 0.731,
      "step": 131070
    },
    {
      "epoch": 956.7883211678832,
      "grad_norm": 17.57716178894043,
      "learning_rate": 2.1605839416058397e-06,
      "loss": 0.9691,
      "step": 131080
    },
    {
      "epoch": 956.8613138686131,
      "grad_norm": 11.307272911071777,
      "learning_rate": 2.156934306569343e-06,
      "loss": 0.9348,
      "step": 131090
    },
    {
      "epoch": 956.9343065693431,
      "grad_norm": 0.07765241712331772,
      "learning_rate": 2.153284671532847e-06,
      "loss": 0.5927,
      "step": 131100
    },
    {
      "epoch": 957.007299270073,
      "grad_norm": 6.343796730041504,
      "learning_rate": 2.1496350364963505e-06,
      "loss": 0.7638,
      "step": 131110
    },
    {
      "epoch": 957.0802919708029,
      "grad_norm": 21.620197296142578,
      "learning_rate": 2.1459854014598544e-06,
      "loss": 0.9991,
      "step": 131120
    },
    {
      "epoch": 957.1532846715329,
      "grad_norm": 9.40054702758789,
      "learning_rate": 2.142335766423358e-06,
      "loss": 1.0535,
      "step": 131130
    },
    {
      "epoch": 957.2262773722628,
      "grad_norm": 10.917609214782715,
      "learning_rate": 2.1386861313868613e-06,
      "loss": 0.9195,
      "step": 131140
    },
    {
      "epoch": 957.2992700729927,
      "grad_norm": 14.739598274230957,
      "learning_rate": 2.135036496350365e-06,
      "loss": 1.0033,
      "step": 131150
    },
    {
      "epoch": 957.3722627737226,
      "grad_norm": 13.167003631591797,
      "learning_rate": 2.1313868613138686e-06,
      "loss": 0.5339,
      "step": 131160
    },
    {
      "epoch": 957.4452554744526,
      "grad_norm": 6.681993007659912,
      "learning_rate": 2.1277372262773725e-06,
      "loss": 0.7959,
      "step": 131170
    },
    {
      "epoch": 957.5182481751825,
      "grad_norm": 5.404407024383545,
      "learning_rate": 2.124087591240876e-06,
      "loss": 0.9599,
      "step": 131180
    },
    {
      "epoch": 957.5912408759124,
      "grad_norm": 12.832839965820312,
      "learning_rate": 2.12043795620438e-06,
      "loss": 0.918,
      "step": 131190
    },
    {
      "epoch": 957.6642335766423,
      "grad_norm": 0.014377236366271973,
      "learning_rate": 2.1167883211678833e-06,
      "loss": 1.0388,
      "step": 131200
    },
    {
      "epoch": 957.7372262773723,
      "grad_norm": 6.788807392120361,
      "learning_rate": 2.113138686131387e-06,
      "loss": 0.6431,
      "step": 131210
    },
    {
      "epoch": 957.8102189781022,
      "grad_norm": 0.012090291827917099,
      "learning_rate": 2.1094890510948906e-06,
      "loss": 0.7347,
      "step": 131220
    },
    {
      "epoch": 957.8832116788321,
      "grad_norm": 6.943206787109375,
      "learning_rate": 2.1058394160583945e-06,
      "loss": 0.7273,
      "step": 131230
    },
    {
      "epoch": 957.956204379562,
      "grad_norm": 7.418857574462891,
      "learning_rate": 2.1021897810218976e-06,
      "loss": 1.0361,
      "step": 131240
    },
    {
      "epoch": 958.029197080292,
      "grad_norm": 6.632333755493164,
      "learning_rate": 2.0985401459854014e-06,
      "loss": 0.8355,
      "step": 131250
    },
    {
      "epoch": 958.1021897810219,
      "grad_norm": 14.68670654296875,
      "learning_rate": 2.0948905109489053e-06,
      "loss": 0.9674,
      "step": 131260
    },
    {
      "epoch": 958.1751824817518,
      "grad_norm": 8.277643203735352,
      "learning_rate": 2.0912408759124088e-06,
      "loss": 1.0697,
      "step": 131270
    },
    {
      "epoch": 958.2481751824818,
      "grad_norm": 15.077536582946777,
      "learning_rate": 2.0875912408759127e-06,
      "loss": 0.8889,
      "step": 131280
    },
    {
      "epoch": 958.3211678832117,
      "grad_norm": 8.589675903320312,
      "learning_rate": 2.083941605839416e-06,
      "loss": 0.8966,
      "step": 131290
    },
    {
      "epoch": 958.3941605839416,
      "grad_norm": 0.8020864725112915,
      "learning_rate": 2.08029197080292e-06,
      "loss": 0.3125,
      "step": 131300
    },
    {
      "epoch": 958.4671532846716,
      "grad_norm": 4.495967864990234,
      "learning_rate": 2.0766423357664235e-06,
      "loss": 0.8273,
      "step": 131310
    },
    {
      "epoch": 958.5401459854014,
      "grad_norm": 0.06577223539352417,
      "learning_rate": 2.0729927007299273e-06,
      "loss": 0.5409,
      "step": 131320
    },
    {
      "epoch": 958.6131386861314,
      "grad_norm": 9.432374954223633,
      "learning_rate": 2.069343065693431e-06,
      "loss": 1.0052,
      "step": 131330
    },
    {
      "epoch": 958.6861313868613,
      "grad_norm": 0.07882671803236008,
      "learning_rate": 2.0656934306569347e-06,
      "loss": 1.0249,
      "step": 131340
    },
    {
      "epoch": 958.7591240875912,
      "grad_norm": 0.03098420426249504,
      "learning_rate": 2.0620437956204377e-06,
      "loss": 0.7723,
      "step": 131350
    },
    {
      "epoch": 958.8321167883212,
      "grad_norm": 10.549358367919922,
      "learning_rate": 2.0583941605839416e-06,
      "loss": 1.0602,
      "step": 131360
    },
    {
      "epoch": 958.9051094890511,
      "grad_norm": 8.869454383850098,
      "learning_rate": 2.054744525547445e-06,
      "loss": 0.8242,
      "step": 131370
    },
    {
      "epoch": 958.978102189781,
      "grad_norm": 16.061967849731445,
      "learning_rate": 2.051094890510949e-06,
      "loss": 0.9426,
      "step": 131380
    },
    {
      "epoch": 959.051094890511,
      "grad_norm": 11.274246215820312,
      "learning_rate": 2.047445255474453e-06,
      "loss": 0.5582,
      "step": 131390
    },
    {
      "epoch": 959.1240875912408,
      "grad_norm": 10.939740180969238,
      "learning_rate": 2.0437956204379563e-06,
      "loss": 0.7938,
      "step": 131400
    },
    {
      "epoch": 959.1970802919708,
      "grad_norm": 5.5933451652526855,
      "learning_rate": 2.04014598540146e-06,
      "loss": 0.8111,
      "step": 131410
    },
    {
      "epoch": 959.2700729927008,
      "grad_norm": 17.58760643005371,
      "learning_rate": 2.0364963503649636e-06,
      "loss": 0.7071,
      "step": 131420
    },
    {
      "epoch": 959.3430656934306,
      "grad_norm": 10.049683570861816,
      "learning_rate": 2.0328467153284675e-06,
      "loss": 0.98,
      "step": 131430
    },
    {
      "epoch": 959.4160583941606,
      "grad_norm": 11.283928871154785,
      "learning_rate": 2.029197080291971e-06,
      "loss": 1.0472,
      "step": 131440
    },
    {
      "epoch": 959.4890510948906,
      "grad_norm": 0.03408706188201904,
      "learning_rate": 2.025547445255475e-06,
      "loss": 0.8335,
      "step": 131450
    },
    {
      "epoch": 959.5620437956204,
      "grad_norm": 9.520058631896973,
      "learning_rate": 2.021897810218978e-06,
      "loss": 0.8744,
      "step": 131460
    },
    {
      "epoch": 959.6350364963504,
      "grad_norm": 10.783679008483887,
      "learning_rate": 2.0182481751824818e-06,
      "loss": 0.8456,
      "step": 131470
    },
    {
      "epoch": 959.7080291970802,
      "grad_norm": 15.547038078308105,
      "learning_rate": 2.0145985401459852e-06,
      "loss": 1.1696,
      "step": 131480
    },
    {
      "epoch": 959.7810218978102,
      "grad_norm": 17.844205856323242,
      "learning_rate": 2.010948905109489e-06,
      "loss": 0.8032,
      "step": 131490
    },
    {
      "epoch": 959.8540145985402,
      "grad_norm": 7.070540904998779,
      "learning_rate": 2.0072992700729926e-06,
      "loss": 1.0498,
      "step": 131500
    },
    {
      "epoch": 959.92700729927,
      "grad_norm": 8.136002540588379,
      "learning_rate": 2.0036496350364965e-06,
      "loss": 1.2007,
      "step": 131510
    },
    {
      "epoch": 960.0,
      "grad_norm": 0.061488743871450424,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6751,
      "step": 131520
    },
    {
      "epoch": 960.07299270073,
      "grad_norm": 0.03837905824184418,
      "learning_rate": 1.996350364963504e-06,
      "loss": 0.8398,
      "step": 131530
    },
    {
      "epoch": 960.1459854014598,
      "grad_norm": 7.120492458343506,
      "learning_rate": 1.9927007299270077e-06,
      "loss": 0.7205,
      "step": 131540
    },
    {
      "epoch": 960.2189781021898,
      "grad_norm": 0.05104583501815796,
      "learning_rate": 1.989051094890511e-06,
      "loss": 0.8832,
      "step": 131550
    },
    {
      "epoch": 960.2919708029198,
      "grad_norm": 10.072395324707031,
      "learning_rate": 1.985401459854015e-06,
      "loss": 1.1131,
      "step": 131560
    },
    {
      "epoch": 960.3649635036496,
      "grad_norm": 15.297834396362305,
      "learning_rate": 1.981751824817518e-06,
      "loss": 0.856,
      "step": 131570
    },
    {
      "epoch": 960.4379562043796,
      "grad_norm": 6.665956020355225,
      "learning_rate": 1.978102189781022e-06,
      "loss": 0.973,
      "step": 131580
    },
    {
      "epoch": 960.5109489051094,
      "grad_norm": 8.468979835510254,
      "learning_rate": 1.9744525547445254e-06,
      "loss": 1.1993,
      "step": 131590
    },
    {
      "epoch": 960.5839416058394,
      "grad_norm": 9.181425094604492,
      "learning_rate": 1.9708029197080293e-06,
      "loss": 0.8782,
      "step": 131600
    },
    {
      "epoch": 960.6569343065694,
      "grad_norm": 6.637078285217285,
      "learning_rate": 1.9671532846715327e-06,
      "loss": 0.5449,
      "step": 131610
    },
    {
      "epoch": 960.7299270072992,
      "grad_norm": 14.917548179626465,
      "learning_rate": 1.9635036496350366e-06,
      "loss": 1.0938,
      "step": 131620
    },
    {
      "epoch": 960.8029197080292,
      "grad_norm": 14.682074546813965,
      "learning_rate": 1.9598540145985405e-06,
      "loss": 0.8992,
      "step": 131630
    },
    {
      "epoch": 960.8759124087592,
      "grad_norm": 14.657930374145508,
      "learning_rate": 1.956204379562044e-06,
      "loss": 0.637,
      "step": 131640
    },
    {
      "epoch": 960.948905109489,
      "grad_norm": 0.04552001506090164,
      "learning_rate": 1.952554744525548e-06,
      "loss": 0.9016,
      "step": 131650
    },
    {
      "epoch": 961.021897810219,
      "grad_norm": 0.04584209993481636,
      "learning_rate": 1.9489051094890513e-06,
      "loss": 0.7035,
      "step": 131660
    },
    {
      "epoch": 961.0948905109489,
      "grad_norm": 18.81383514404297,
      "learning_rate": 1.9452554744525548e-06,
      "loss": 0.8258,
      "step": 131670
    },
    {
      "epoch": 961.1678832116788,
      "grad_norm": 6.641512393951416,
      "learning_rate": 1.9416058394160582e-06,
      "loss": 0.69,
      "step": 131680
    },
    {
      "epoch": 961.2408759124088,
      "grad_norm": 7.402923107147217,
      "learning_rate": 1.937956204379562e-06,
      "loss": 1.1343,
      "step": 131690
    },
    {
      "epoch": 961.3138686131387,
      "grad_norm": 6.654089450836182,
      "learning_rate": 1.9343065693430656e-06,
      "loss": 1.1369,
      "step": 131700
    },
    {
      "epoch": 961.3868613138686,
      "grad_norm": 13.37895393371582,
      "learning_rate": 1.9306569343065694e-06,
      "loss": 0.7097,
      "step": 131710
    },
    {
      "epoch": 961.4598540145986,
      "grad_norm": 9.468271255493164,
      "learning_rate": 1.927007299270073e-06,
      "loss": 0.8992,
      "step": 131720
    },
    {
      "epoch": 961.5328467153284,
      "grad_norm": 8.045690536499023,
      "learning_rate": 1.9233576642335768e-06,
      "loss": 0.9015,
      "step": 131730
    },
    {
      "epoch": 961.6058394160584,
      "grad_norm": 13.581472396850586,
      "learning_rate": 1.9197080291970802e-06,
      "loss": 1.1452,
      "step": 131740
    },
    {
      "epoch": 961.6788321167883,
      "grad_norm": 12.738438606262207,
      "learning_rate": 1.916058394160584e-06,
      "loss": 0.7424,
      "step": 131750
    },
    {
      "epoch": 961.7518248175182,
      "grad_norm": 13.690348625183105,
      "learning_rate": 1.912408759124088e-06,
      "loss": 0.9685,
      "step": 131760
    },
    {
      "epoch": 961.8248175182482,
      "grad_norm": 0.01384603139013052,
      "learning_rate": 1.9087591240875915e-06,
      "loss": 0.8175,
      "step": 131770
    },
    {
      "epoch": 961.8978102189781,
      "grad_norm": 9.306123733520508,
      "learning_rate": 1.905109489051095e-06,
      "loss": 0.7885,
      "step": 131780
    },
    {
      "epoch": 961.970802919708,
      "grad_norm": 9.496092796325684,
      "learning_rate": 1.9014598540145986e-06,
      "loss": 0.6029,
      "step": 131790
    },
    {
      "epoch": 962.043795620438,
      "grad_norm": 7.4452619552612305,
      "learning_rate": 1.8978102189781023e-06,
      "loss": 0.5786,
      "step": 131800
    },
    {
      "epoch": 962.1167883211679,
      "grad_norm": 16.7829647064209,
      "learning_rate": 1.894160583941606e-06,
      "loss": 1.0679,
      "step": 131810
    },
    {
      "epoch": 962.1897810218978,
      "grad_norm": 12.378108024597168,
      "learning_rate": 1.8905109489051096e-06,
      "loss": 1.0151,
      "step": 131820
    },
    {
      "epoch": 962.2627737226277,
      "grad_norm": 6.180628299713135,
      "learning_rate": 1.8868613138686133e-06,
      "loss": 0.6951,
      "step": 131830
    },
    {
      "epoch": 962.3357664233577,
      "grad_norm": 5.770936012268066,
      "learning_rate": 1.883211678832117e-06,
      "loss": 0.7291,
      "step": 131840
    },
    {
      "epoch": 962.4087591240876,
      "grad_norm": 15.445712089538574,
      "learning_rate": 1.8795620437956206e-06,
      "loss": 1.0567,
      "step": 131850
    },
    {
      "epoch": 962.4817518248175,
      "grad_norm": 14.084803581237793,
      "learning_rate": 1.8759124087591243e-06,
      "loss": 0.8776,
      "step": 131860
    },
    {
      "epoch": 962.5547445255474,
      "grad_norm": 9.423175811767578,
      "learning_rate": 1.872262773722628e-06,
      "loss": 0.9265,
      "step": 131870
    },
    {
      "epoch": 962.6277372262774,
      "grad_norm": 0.028392424806952477,
      "learning_rate": 1.8686131386861316e-06,
      "loss": 1.2612,
      "step": 131880
    },
    {
      "epoch": 962.7007299270073,
      "grad_norm": 6.256983280181885,
      "learning_rate": 1.8649635036496349e-06,
      "loss": 0.6722,
      "step": 131890
    },
    {
      "epoch": 962.7737226277372,
      "grad_norm": 6.559633731842041,
      "learning_rate": 1.8613138686131385e-06,
      "loss": 0.7866,
      "step": 131900
    },
    {
      "epoch": 962.8467153284671,
      "grad_norm": 9.880023002624512,
      "learning_rate": 1.8576642335766424e-06,
      "loss": 0.8078,
      "step": 131910
    },
    {
      "epoch": 962.9197080291971,
      "grad_norm": 11.39809513092041,
      "learning_rate": 1.854014598540146e-06,
      "loss": 0.8295,
      "step": 131920
    },
    {
      "epoch": 962.992700729927,
      "grad_norm": 7.664713382720947,
      "learning_rate": 1.8503649635036498e-06,
      "loss": 0.9505,
      "step": 131930
    },
    {
      "epoch": 963.0656934306569,
      "grad_norm": 8.269013404846191,
      "learning_rate": 1.8467153284671534e-06,
      "loss": 0.7065,
      "step": 131940
    },
    {
      "epoch": 963.1386861313869,
      "grad_norm": 1.1188567876815796,
      "learning_rate": 1.8430656934306571e-06,
      "loss": 0.7135,
      "step": 131950
    },
    {
      "epoch": 963.2116788321168,
      "grad_norm": 14.673903465270996,
      "learning_rate": 1.8394160583941608e-06,
      "loss": 0.8775,
      "step": 131960
    },
    {
      "epoch": 963.2846715328467,
      "grad_norm": 5.748620986938477,
      "learning_rate": 1.8357664233576645e-06,
      "loss": 0.7943,
      "step": 131970
    },
    {
      "epoch": 963.3576642335767,
      "grad_norm": 0.06139000132679939,
      "learning_rate": 1.8321167883211681e-06,
      "loss": 0.5579,
      "step": 131980
    },
    {
      "epoch": 963.4306569343066,
      "grad_norm": 8.596354484558105,
      "learning_rate": 1.8284671532846718e-06,
      "loss": 1.1267,
      "step": 131990
    },
    {
      "epoch": 963.5036496350365,
      "grad_norm": 7.310730934143066,
      "learning_rate": 1.824817518248175e-06,
      "loss": 1.5165,
      "step": 132000
    },
    {
      "epoch": 963.5766423357665,
      "grad_norm": 6.427978515625,
      "learning_rate": 1.8211678832116787e-06,
      "loss": 1.1296,
      "step": 132010
    },
    {
      "epoch": 963.6496350364963,
      "grad_norm": 6.244306564331055,
      "learning_rate": 1.8175182481751824e-06,
      "loss": 0.9948,
      "step": 132020
    },
    {
      "epoch": 963.7226277372263,
      "grad_norm": 0.04896797984838486,
      "learning_rate": 1.8138686131386863e-06,
      "loss": 0.6378,
      "step": 132030
    },
    {
      "epoch": 963.7956204379562,
      "grad_norm": 9.61375904083252,
      "learning_rate": 1.81021897810219e-06,
      "loss": 0.845,
      "step": 132040
    },
    {
      "epoch": 963.8686131386861,
      "grad_norm": 11.904730796813965,
      "learning_rate": 1.8065693430656936e-06,
      "loss": 0.7623,
      "step": 132050
    },
    {
      "epoch": 963.9416058394161,
      "grad_norm": 24.424257278442383,
      "learning_rate": 1.8029197080291973e-06,
      "loss": 0.7238,
      "step": 132060
    },
    {
      "epoch": 964.014598540146,
      "grad_norm": 11.370247840881348,
      "learning_rate": 1.799270072992701e-06,
      "loss": 0.7942,
      "step": 132070
    },
    {
      "epoch": 964.0875912408759,
      "grad_norm": 11.855734825134277,
      "learning_rate": 1.7956204379562046e-06,
      "loss": 0.9832,
      "step": 132080
    },
    {
      "epoch": 964.1605839416059,
      "grad_norm": 9.597535133361816,
      "learning_rate": 1.7919708029197083e-06,
      "loss": 0.8358,
      "step": 132090
    },
    {
      "epoch": 964.2335766423357,
      "grad_norm": 0.011182545684278011,
      "learning_rate": 1.7883211678832115e-06,
      "loss": 1.0094,
      "step": 132100
    },
    {
      "epoch": 964.3065693430657,
      "grad_norm": 8.159854888916016,
      "learning_rate": 1.7846715328467152e-06,
      "loss": 1.2593,
      "step": 132110
    },
    {
      "epoch": 964.3795620437957,
      "grad_norm": 9.491344451904297,
      "learning_rate": 1.7810218978102189e-06,
      "loss": 0.8561,
      "step": 132120
    },
    {
      "epoch": 964.4525547445255,
      "grad_norm": 0.06588135659694672,
      "learning_rate": 1.7773722627737225e-06,
      "loss": 0.983,
      "step": 132130
    },
    {
      "epoch": 964.5255474452555,
      "grad_norm": 9.222811698913574,
      "learning_rate": 1.7737226277372262e-06,
      "loss": 0.6685,
      "step": 132140
    },
    {
      "epoch": 964.5985401459855,
      "grad_norm": 13.214757919311523,
      "learning_rate": 1.77007299270073e-06,
      "loss": 0.58,
      "step": 132150
    },
    {
      "epoch": 964.6715328467153,
      "grad_norm": 7.044059753417969,
      "learning_rate": 1.7664233576642338e-06,
      "loss": 0.698,
      "step": 132160
    },
    {
      "epoch": 964.7445255474453,
      "grad_norm": 10.95184326171875,
      "learning_rate": 1.7627737226277374e-06,
      "loss": 1.0443,
      "step": 132170
    },
    {
      "epoch": 964.8175182481751,
      "grad_norm": 7.630455493927002,
      "learning_rate": 1.7591240875912411e-06,
      "loss": 1.1215,
      "step": 132180
    },
    {
      "epoch": 964.8905109489051,
      "grad_norm": 7.350781440734863,
      "learning_rate": 1.7554744525547448e-06,
      "loss": 0.8222,
      "step": 132190
    },
    {
      "epoch": 964.9635036496351,
      "grad_norm": 7.746067047119141,
      "learning_rate": 1.7518248175182485e-06,
      "loss": 0.78,
      "step": 132200
    },
    {
      "epoch": 965.0364963503649,
      "grad_norm": 13.365108489990234,
      "learning_rate": 1.7481751824817517e-06,
      "loss": 0.8224,
      "step": 132210
    },
    {
      "epoch": 965.1094890510949,
      "grad_norm": 0.08573313057422638,
      "learning_rate": 1.7445255474452554e-06,
      "loss": 0.9908,
      "step": 132220
    },
    {
      "epoch": 965.1824817518249,
      "grad_norm": 0.051756978034973145,
      "learning_rate": 1.740875912408759e-06,
      "loss": 0.9928,
      "step": 132230
    },
    {
      "epoch": 965.2554744525547,
      "grad_norm": 9.295808792114258,
      "learning_rate": 1.7372262773722627e-06,
      "loss": 0.9913,
      "step": 132240
    },
    {
      "epoch": 965.3284671532847,
      "grad_norm": 7.599080562591553,
      "learning_rate": 1.7335766423357664e-06,
      "loss": 0.9664,
      "step": 132250
    },
    {
      "epoch": 965.4014598540145,
      "grad_norm": 7.9987640380859375,
      "learning_rate": 1.72992700729927e-06,
      "loss": 0.734,
      "step": 132260
    },
    {
      "epoch": 965.4744525547445,
      "grad_norm": 5.772524833679199,
      "learning_rate": 1.726277372262774e-06,
      "loss": 0.5249,
      "step": 132270
    },
    {
      "epoch": 965.5474452554745,
      "grad_norm": 12.156039237976074,
      "learning_rate": 1.7226277372262776e-06,
      "loss": 0.8471,
      "step": 132280
    },
    {
      "epoch": 965.6204379562043,
      "grad_norm": 9.314550399780273,
      "learning_rate": 1.7189781021897813e-06,
      "loss": 0.7936,
      "step": 132290
    },
    {
      "epoch": 965.6934306569343,
      "grad_norm": 0.012728650122880936,
      "learning_rate": 1.715328467153285e-06,
      "loss": 0.8578,
      "step": 132300
    },
    {
      "epoch": 965.7664233576643,
      "grad_norm": 2.312535047531128,
      "learning_rate": 1.7116788321167886e-06,
      "loss": 0.8222,
      "step": 132310
    },
    {
      "epoch": 965.8394160583941,
      "grad_norm": 9.666236877441406,
      "learning_rate": 1.7080291970802919e-06,
      "loss": 0.8742,
      "step": 132320
    },
    {
      "epoch": 965.9124087591241,
      "grad_norm": 10.891058921813965,
      "learning_rate": 1.7043795620437955e-06,
      "loss": 1.0214,
      "step": 132330
    },
    {
      "epoch": 965.985401459854,
      "grad_norm": 7.9235053062438965,
      "learning_rate": 1.7007299270072992e-06,
      "loss": 0.8919,
      "step": 132340
    },
    {
      "epoch": 966.0583941605839,
      "grad_norm": 5.372507572174072,
      "learning_rate": 1.6970802919708029e-06,
      "loss": 0.8864,
      "step": 132350
    },
    {
      "epoch": 966.1313868613139,
      "grad_norm": 11.565214157104492,
      "learning_rate": 1.6934306569343066e-06,
      "loss": 1.0673,
      "step": 132360
    },
    {
      "epoch": 966.2043795620438,
      "grad_norm": 6.951352119445801,
      "learning_rate": 1.6897810218978102e-06,
      "loss": 1.1126,
      "step": 132370
    },
    {
      "epoch": 966.2773722627737,
      "grad_norm": 7.9791436195373535,
      "learning_rate": 1.6861313868613139e-06,
      "loss": 0.8201,
      "step": 132380
    },
    {
      "epoch": 966.3503649635037,
      "grad_norm": 13.150930404663086,
      "learning_rate": 1.6824817518248176e-06,
      "loss": 1.0266,
      "step": 132390
    },
    {
      "epoch": 966.4233576642335,
      "grad_norm": 7.893701076507568,
      "learning_rate": 1.6788321167883214e-06,
      "loss": 1.1513,
      "step": 132400
    },
    {
      "epoch": 966.4963503649635,
      "grad_norm": 12.854286193847656,
      "learning_rate": 1.6751824817518251e-06,
      "loss": 0.6925,
      "step": 132410
    },
    {
      "epoch": 966.5693430656934,
      "grad_norm": 7.9574384689331055,
      "learning_rate": 1.6715328467153288e-06,
      "loss": 0.6917,
      "step": 132420
    },
    {
      "epoch": 966.6423357664233,
      "grad_norm": 9.202008247375488,
      "learning_rate": 1.667883211678832e-06,
      "loss": 0.9882,
      "step": 132430
    },
    {
      "epoch": 966.7153284671533,
      "grad_norm": 7.294680118560791,
      "learning_rate": 1.6642335766423357e-06,
      "loss": 0.9046,
      "step": 132440
    },
    {
      "epoch": 966.7883211678832,
      "grad_norm": 6.4591965675354,
      "learning_rate": 1.6605839416058394e-06,
      "loss": 0.7835,
      "step": 132450
    },
    {
      "epoch": 966.8613138686131,
      "grad_norm": 16.437671661376953,
      "learning_rate": 1.656934306569343e-06,
      "loss": 0.9934,
      "step": 132460
    },
    {
      "epoch": 966.9343065693431,
      "grad_norm": 13.745540618896484,
      "learning_rate": 1.6532846715328467e-06,
      "loss": 0.8895,
      "step": 132470
    },
    {
      "epoch": 967.007299270073,
      "grad_norm": 5.064858913421631,
      "learning_rate": 1.6496350364963504e-06,
      "loss": 0.4862,
      "step": 132480
    },
    {
      "epoch": 967.0802919708029,
      "grad_norm": 0.04452391341328621,
      "learning_rate": 1.645985401459854e-06,
      "loss": 0.5303,
      "step": 132490
    },
    {
      "epoch": 967.1532846715329,
      "grad_norm": 8.396644592285156,
      "learning_rate": 1.6423357664233577e-06,
      "loss": 1.0709,
      "step": 132500
    },
    {
      "epoch": 967.2262773722628,
      "grad_norm": 0.02016822062432766,
      "learning_rate": 1.6386861313868614e-06,
      "loss": 0.8209,
      "step": 132510
    },
    {
      "epoch": 967.2992700729927,
      "grad_norm": 8.511285781860352,
      "learning_rate": 1.6350364963503653e-06,
      "loss": 0.5525,
      "step": 132520
    },
    {
      "epoch": 967.3722627737226,
      "grad_norm": 14.121699333190918,
      "learning_rate": 1.631386861313869e-06,
      "loss": 1.4316,
      "step": 132530
    },
    {
      "epoch": 967.4452554744526,
      "grad_norm": 10.693231582641602,
      "learning_rate": 1.6277372262773722e-06,
      "loss": 1.4563,
      "step": 132540
    },
    {
      "epoch": 967.5182481751825,
      "grad_norm": 3.119974136352539,
      "learning_rate": 1.6240875912408759e-06,
      "loss": 0.4319,
      "step": 132550
    },
    {
      "epoch": 967.5912408759124,
      "grad_norm": 0.01516051683574915,
      "learning_rate": 1.6204379562043795e-06,
      "loss": 0.7269,
      "step": 132560
    },
    {
      "epoch": 967.6642335766423,
      "grad_norm": 0.01331283524632454,
      "learning_rate": 1.6167883211678832e-06,
      "loss": 0.9153,
      "step": 132570
    },
    {
      "epoch": 967.7372262773723,
      "grad_norm": 12.94029712677002,
      "learning_rate": 1.6131386861313869e-06,
      "loss": 0.899,
      "step": 132580
    },
    {
      "epoch": 967.8102189781022,
      "grad_norm": 17.297714233398438,
      "learning_rate": 1.6094890510948906e-06,
      "loss": 0.9287,
      "step": 132590
    },
    {
      "epoch": 967.8832116788321,
      "grad_norm": 7.5120649337768555,
      "learning_rate": 1.6058394160583942e-06,
      "loss": 0.6669,
      "step": 132600
    },
    {
      "epoch": 967.956204379562,
      "grad_norm": 12.145440101623535,
      "learning_rate": 1.602189781021898e-06,
      "loss": 1.0429,
      "step": 132610
    },
    {
      "epoch": 968.029197080292,
      "grad_norm": 9.84205150604248,
      "learning_rate": 1.5985401459854016e-06,
      "loss": 0.9023,
      "step": 132620
    },
    {
      "epoch": 968.1021897810219,
      "grad_norm": 9.719913482666016,
      "learning_rate": 1.5948905109489052e-06,
      "loss": 1.1652,
      "step": 132630
    },
    {
      "epoch": 968.1751824817518,
      "grad_norm": 12.904241561889648,
      "learning_rate": 1.5912408759124087e-06,
      "loss": 1.1561,
      "step": 132640
    },
    {
      "epoch": 968.2481751824818,
      "grad_norm": 9.224535942077637,
      "learning_rate": 1.5875912408759124e-06,
      "loss": 1.0368,
      "step": 132650
    },
    {
      "epoch": 968.3211678832117,
      "grad_norm": 12.57537841796875,
      "learning_rate": 1.583941605839416e-06,
      "loss": 0.9773,
      "step": 132660
    },
    {
      "epoch": 968.3941605839416,
      "grad_norm": 8.084094047546387,
      "learning_rate": 1.5802919708029197e-06,
      "loss": 0.6871,
      "step": 132670
    },
    {
      "epoch": 968.4671532846716,
      "grad_norm": 6.388428211212158,
      "learning_rate": 1.5766423357664234e-06,
      "loss": 0.6939,
      "step": 132680
    },
    {
      "epoch": 968.5401459854014,
      "grad_norm": 0.018251005560159683,
      "learning_rate": 1.572992700729927e-06,
      "loss": 0.6636,
      "step": 132690
    },
    {
      "epoch": 968.6131386861314,
      "grad_norm": 8.7783842086792,
      "learning_rate": 1.5693430656934307e-06,
      "loss": 1.0699,
      "step": 132700
    },
    {
      "epoch": 968.6861313868613,
      "grad_norm": 11.943510055541992,
      "learning_rate": 1.5656934306569344e-06,
      "loss": 0.6962,
      "step": 132710
    },
    {
      "epoch": 968.7591240875912,
      "grad_norm": 18.432497024536133,
      "learning_rate": 1.5620437956204378e-06,
      "loss": 0.6125,
      "step": 132720
    },
    {
      "epoch": 968.8321167883212,
      "grad_norm": 0.14319607615470886,
      "learning_rate": 1.5583941605839417e-06,
      "loss": 0.9003,
      "step": 132730
    },
    {
      "epoch": 968.9051094890511,
      "grad_norm": 7.779223442077637,
      "learning_rate": 1.5547445255474454e-06,
      "loss": 0.6706,
      "step": 132740
    },
    {
      "epoch": 968.978102189781,
      "grad_norm": 13.021624565124512,
      "learning_rate": 1.551094890510949e-06,
      "loss": 1.1522,
      "step": 132750
    },
    {
      "epoch": 969.051094890511,
      "grad_norm": 11.719269752502441,
      "learning_rate": 1.5474452554744527e-06,
      "loss": 0.8844,
      "step": 132760
    },
    {
      "epoch": 969.1240875912408,
      "grad_norm": 15.005906105041504,
      "learning_rate": 1.5437956204379564e-06,
      "loss": 1.1331,
      "step": 132770
    },
    {
      "epoch": 969.1970802919708,
      "grad_norm": 0.06808748096227646,
      "learning_rate": 1.5401459854014599e-06,
      "loss": 0.6966,
      "step": 132780
    },
    {
      "epoch": 969.2700729927008,
      "grad_norm": 6.1361002922058105,
      "learning_rate": 1.5364963503649635e-06,
      "loss": 0.5943,
      "step": 132790
    },
    {
      "epoch": 969.3430656934306,
      "grad_norm": 10.093832969665527,
      "learning_rate": 1.5328467153284672e-06,
      "loss": 0.8413,
      "step": 132800
    },
    {
      "epoch": 969.4160583941606,
      "grad_norm": 12.050984382629395,
      "learning_rate": 1.5291970802919709e-06,
      "loss": 0.8172,
      "step": 132810
    },
    {
      "epoch": 969.4890510948906,
      "grad_norm": 6.412917137145996,
      "learning_rate": 1.5255474452554746e-06,
      "loss": 0.962,
      "step": 132820
    },
    {
      "epoch": 969.5620437956204,
      "grad_norm": 8.191489219665527,
      "learning_rate": 1.521897810218978e-06,
      "loss": 1.0596,
      "step": 132830
    },
    {
      "epoch": 969.6350364963504,
      "grad_norm": 6.474226951599121,
      "learning_rate": 1.5182481751824817e-06,
      "loss": 0.9061,
      "step": 132840
    },
    {
      "epoch": 969.7080291970802,
      "grad_norm": 12.115272521972656,
      "learning_rate": 1.5145985401459856e-06,
      "loss": 0.6704,
      "step": 132850
    },
    {
      "epoch": 969.7810218978102,
      "grad_norm": 8.083313941955566,
      "learning_rate": 1.5109489051094892e-06,
      "loss": 0.925,
      "step": 132860
    },
    {
      "epoch": 969.8540145985402,
      "grad_norm": 0.016792019829154015,
      "learning_rate": 1.507299270072993e-06,
      "loss": 0.6863,
      "step": 132870
    },
    {
      "epoch": 969.92700729927,
      "grad_norm": 19.324766159057617,
      "learning_rate": 1.5036496350364966e-06,
      "loss": 1.1203,
      "step": 132880
    },
    {
      "epoch": 970.0,
      "grad_norm": 0.024544479325413704,
      "learning_rate": 1.5e-06,
      "loss": 0.81,
      "step": 132890
    },
    {
      "epoch": 970.07299270073,
      "grad_norm": 0.027320552617311478,
      "learning_rate": 1.4963503649635037e-06,
      "loss": 0.9076,
      "step": 132900
    },
    {
      "epoch": 970.1459854014598,
      "grad_norm": 5.894969940185547,
      "learning_rate": 1.4927007299270074e-06,
      "loss": 0.927,
      "step": 132910
    },
    {
      "epoch": 970.2189781021898,
      "grad_norm": 7.69197940826416,
      "learning_rate": 1.489051094890511e-06,
      "loss": 0.708,
      "step": 132920
    },
    {
      "epoch": 970.2919708029198,
      "grad_norm": 8.983190536499023,
      "learning_rate": 1.4854014598540147e-06,
      "loss": 1.1465,
      "step": 132930
    },
    {
      "epoch": 970.3649635036496,
      "grad_norm": 6.726107597351074,
      "learning_rate": 1.4817518248175182e-06,
      "loss": 0.5511,
      "step": 132940
    },
    {
      "epoch": 970.4379562043796,
      "grad_norm": 6.518780708312988,
      "learning_rate": 1.4781021897810219e-06,
      "loss": 0.9084,
      "step": 132950
    },
    {
      "epoch": 970.5109489051094,
      "grad_norm": 14.559446334838867,
      "learning_rate": 1.4744525547445255e-06,
      "loss": 1.2857,
      "step": 132960
    },
    {
      "epoch": 970.5839416058394,
      "grad_norm": 7.840410232543945,
      "learning_rate": 1.4708029197080292e-06,
      "loss": 1.2742,
      "step": 132970
    },
    {
      "epoch": 970.6569343065694,
      "grad_norm": 6.857405662536621,
      "learning_rate": 1.467153284671533e-06,
      "loss": 0.7321,
      "step": 132980
    },
    {
      "epoch": 970.7299270072992,
      "grad_norm": 7.209249973297119,
      "learning_rate": 1.4635036496350365e-06,
      "loss": 0.8135,
      "step": 132990
    },
    {
      "epoch": 970.8029197080292,
      "grad_norm": 10.754176139831543,
      "learning_rate": 1.4598540145985402e-06,
      "loss": 0.8304,
      "step": 133000
    },
    {
      "epoch": 970.8759124087592,
      "grad_norm": 14.659095764160156,
      "learning_rate": 1.4562043795620439e-06,
      "loss": 0.5552,
      "step": 133010
    },
    {
      "epoch": 970.948905109489,
      "grad_norm": 8.365800857543945,
      "learning_rate": 1.4525547445255475e-06,
      "loss": 0.8885,
      "step": 133020
    },
    {
      "epoch": 971.021897810219,
      "grad_norm": 12.303053855895996,
      "learning_rate": 1.4489051094890512e-06,
      "loss": 1.0672,
      "step": 133030
    },
    {
      "epoch": 971.0948905109489,
      "grad_norm": 0.009654463268816471,
      "learning_rate": 1.4452554744525549e-06,
      "loss": 0.5701,
      "step": 133040
    },
    {
      "epoch": 971.1678832116788,
      "grad_norm": 1.0860365629196167,
      "learning_rate": 1.4416058394160583e-06,
      "loss": 0.7884,
      "step": 133050
    },
    {
      "epoch": 971.2408759124088,
      "grad_norm": 0.04320870712399483,
      "learning_rate": 1.437956204379562e-06,
      "loss": 0.5909,
      "step": 133060
    },
    {
      "epoch": 971.3138686131387,
      "grad_norm": 20.426029205322266,
      "learning_rate": 1.4343065693430657e-06,
      "loss": 1.4152,
      "step": 133070
    },
    {
      "epoch": 971.3868613138686,
      "grad_norm": 11.111953735351562,
      "learning_rate": 1.4306569343065694e-06,
      "loss": 1.2147,
      "step": 133080
    },
    {
      "epoch": 971.4598540145986,
      "grad_norm": 9.816831588745117,
      "learning_rate": 1.427007299270073e-06,
      "loss": 0.6209,
      "step": 133090
    },
    {
      "epoch": 971.5328467153284,
      "grad_norm": 0.010086518712341785,
      "learning_rate": 1.4233576642335767e-06,
      "loss": 0.8121,
      "step": 133100
    },
    {
      "epoch": 971.6058394160584,
      "grad_norm": 5.171989440917969,
      "learning_rate": 1.4197080291970804e-06,
      "loss": 0.8911,
      "step": 133110
    },
    {
      "epoch": 971.6788321167883,
      "grad_norm": 15.509936332702637,
      "learning_rate": 1.416058394160584e-06,
      "loss": 1.005,
      "step": 133120
    },
    {
      "epoch": 971.7518248175182,
      "grad_norm": 9.6387939453125,
      "learning_rate": 1.4124087591240877e-06,
      "loss": 0.8588,
      "step": 133130
    },
    {
      "epoch": 971.8248175182482,
      "grad_norm": 0.033174023032188416,
      "learning_rate": 1.4087591240875914e-06,
      "loss": 0.645,
      "step": 133140
    },
    {
      "epoch": 971.8978102189781,
      "grad_norm": 0.90118008852005,
      "learning_rate": 1.4051094890510948e-06,
      "loss": 1.0836,
      "step": 133150
    },
    {
      "epoch": 971.970802919708,
      "grad_norm": 8.982726097106934,
      "learning_rate": 1.4014598540145985e-06,
      "loss": 0.8763,
      "step": 133160
    },
    {
      "epoch": 972.043795620438,
      "grad_norm": 11.752674102783203,
      "learning_rate": 1.3978102189781022e-06,
      "loss": 0.6021,
      "step": 133170
    },
    {
      "epoch": 972.1167883211679,
      "grad_norm": 10.842015266418457,
      "learning_rate": 1.3941605839416059e-06,
      "loss": 0.7278,
      "step": 133180
    },
    {
      "epoch": 972.1897810218978,
      "grad_norm": 0.07863908261060715,
      "learning_rate": 1.3905109489051095e-06,
      "loss": 0.7953,
      "step": 133190
    },
    {
      "epoch": 972.2627737226277,
      "grad_norm": 12.167728424072266,
      "learning_rate": 1.3868613138686132e-06,
      "loss": 1.0707,
      "step": 133200
    },
    {
      "epoch": 972.3357664233577,
      "grad_norm": 12.642610549926758,
      "learning_rate": 1.3832116788321169e-06,
      "loss": 1.0972,
      "step": 133210
    },
    {
      "epoch": 972.4087591240876,
      "grad_norm": 13.490509986877441,
      "learning_rate": 1.3795620437956205e-06,
      "loss": 0.6117,
      "step": 133220
    },
    {
      "epoch": 972.4817518248175,
      "grad_norm": 16.268678665161133,
      "learning_rate": 1.3759124087591242e-06,
      "loss": 0.8233,
      "step": 133230
    },
    {
      "epoch": 972.5547445255474,
      "grad_norm": 6.874373435974121,
      "learning_rate": 1.3722627737226279e-06,
      "loss": 0.8094,
      "step": 133240
    },
    {
      "epoch": 972.6277372262774,
      "grad_norm": 15.17065715789795,
      "learning_rate": 1.3686131386861315e-06,
      "loss": 1.2209,
      "step": 133250
    },
    {
      "epoch": 972.7007299270073,
      "grad_norm": 5.7057342529296875,
      "learning_rate": 1.364963503649635e-06,
      "loss": 0.7705,
      "step": 133260
    },
    {
      "epoch": 972.7737226277372,
      "grad_norm": 14.452068328857422,
      "learning_rate": 1.3613138686131387e-06,
      "loss": 1.2803,
      "step": 133270
    },
    {
      "epoch": 972.8467153284671,
      "grad_norm": 5.468502044677734,
      "learning_rate": 1.3576642335766423e-06,
      "loss": 0.6824,
      "step": 133280
    },
    {
      "epoch": 972.9197080291971,
      "grad_norm": 7.90792179107666,
      "learning_rate": 1.354014598540146e-06,
      "loss": 0.8682,
      "step": 133290
    },
    {
      "epoch": 972.992700729927,
      "grad_norm": 10.498738288879395,
      "learning_rate": 1.3503649635036497e-06,
      "loss": 0.6152,
      "step": 133300
    },
    {
      "epoch": 973.0656934306569,
      "grad_norm": 8.246581077575684,
      "learning_rate": 1.3467153284671534e-06,
      "loss": 0.8133,
      "step": 133310
    },
    {
      "epoch": 973.1386861313869,
      "grad_norm": 10.118967056274414,
      "learning_rate": 1.343065693430657e-06,
      "loss": 0.7179,
      "step": 133320
    },
    {
      "epoch": 973.2116788321168,
      "grad_norm": 0.0230683796107769,
      "learning_rate": 1.3394160583941607e-06,
      "loss": 1.015,
      "step": 133330
    },
    {
      "epoch": 973.2846715328467,
      "grad_norm": 7.806389331817627,
      "learning_rate": 1.3357664233576644e-06,
      "loss": 0.9331,
      "step": 133340
    },
    {
      "epoch": 973.3576642335767,
      "grad_norm": 0.5155235528945923,
      "learning_rate": 1.332116788321168e-06,
      "loss": 0.4614,
      "step": 133350
    },
    {
      "epoch": 973.4306569343066,
      "grad_norm": 6.706076622009277,
      "learning_rate": 1.3284671532846717e-06,
      "loss": 0.8716,
      "step": 133360
    },
    {
      "epoch": 973.5036496350365,
      "grad_norm": 6.403722286224365,
      "learning_rate": 1.3248175182481752e-06,
      "loss": 0.8371,
      "step": 133370
    },
    {
      "epoch": 973.5766423357665,
      "grad_norm": 8.147368431091309,
      "learning_rate": 1.3211678832116788e-06,
      "loss": 0.8834,
      "step": 133380
    },
    {
      "epoch": 973.6496350364963,
      "grad_norm": 13.618739128112793,
      "learning_rate": 1.3175182481751825e-06,
      "loss": 1.0962,
      "step": 133390
    },
    {
      "epoch": 973.7226277372263,
      "grad_norm": 12.692302703857422,
      "learning_rate": 1.3138686131386862e-06,
      "loss": 1.0192,
      "step": 133400
    },
    {
      "epoch": 973.7956204379562,
      "grad_norm": 8.855432510375977,
      "learning_rate": 1.3102189781021899e-06,
      "loss": 0.6229,
      "step": 133410
    },
    {
      "epoch": 973.8686131386861,
      "grad_norm": 14.535852432250977,
      "learning_rate": 1.3065693430656933e-06,
      "loss": 1.1331,
      "step": 133420
    },
    {
      "epoch": 973.9416058394161,
      "grad_norm": 14.875736236572266,
      "learning_rate": 1.3029197080291972e-06,
      "loss": 1.1532,
      "step": 133430
    },
    {
      "epoch": 974.014598540146,
      "grad_norm": 6.944092750549316,
      "learning_rate": 1.2992700729927009e-06,
      "loss": 0.7472,
      "step": 133440
    },
    {
      "epoch": 974.0875912408759,
      "grad_norm": 10.535029411315918,
      "learning_rate": 1.2956204379562045e-06,
      "loss": 0.7809,
      "step": 133450
    },
    {
      "epoch": 974.1605839416059,
      "grad_norm": 7.949239730834961,
      "learning_rate": 1.2919708029197082e-06,
      "loss": 0.8446,
      "step": 133460
    },
    {
      "epoch": 974.2335766423357,
      "grad_norm": 13.210970878601074,
      "learning_rate": 1.2883211678832119e-06,
      "loss": 0.7475,
      "step": 133470
    },
    {
      "epoch": 974.3065693430657,
      "grad_norm": 3.188495635986328,
      "learning_rate": 1.2846715328467153e-06,
      "loss": 0.7579,
      "step": 133480
    },
    {
      "epoch": 974.3795620437957,
      "grad_norm": 14.404979705810547,
      "learning_rate": 1.281021897810219e-06,
      "loss": 1.1735,
      "step": 133490
    },
    {
      "epoch": 974.4525547445255,
      "grad_norm": 11.988580703735352,
      "learning_rate": 1.2773722627737227e-06,
      "loss": 0.8208,
      "step": 133500
    },
    {
      "epoch": 974.5255474452555,
      "grad_norm": 11.025644302368164,
      "learning_rate": 1.2737226277372263e-06,
      "loss": 1.1027,
      "step": 133510
    },
    {
      "epoch": 974.5985401459855,
      "grad_norm": 0.050728198140859604,
      "learning_rate": 1.27007299270073e-06,
      "loss": 0.8374,
      "step": 133520
    },
    {
      "epoch": 974.6715328467153,
      "grad_norm": 7.095745086669922,
      "learning_rate": 1.2664233576642335e-06,
      "loss": 0.8761,
      "step": 133530
    },
    {
      "epoch": 974.7445255474453,
      "grad_norm": 3.877805233001709,
      "learning_rate": 1.2627737226277371e-06,
      "loss": 0.9032,
      "step": 133540
    },
    {
      "epoch": 974.8175182481751,
      "grad_norm": 11.29491901397705,
      "learning_rate": 1.2591240875912408e-06,
      "loss": 1.1242,
      "step": 133550
    },
    {
      "epoch": 974.8905109489051,
      "grad_norm": 7.021511077880859,
      "learning_rate": 1.2554744525547447e-06,
      "loss": 0.817,
      "step": 133560
    },
    {
      "epoch": 974.9635036496351,
      "grad_norm": 0.019054705277085304,
      "learning_rate": 1.2518248175182484e-06,
      "loss": 0.957,
      "step": 133570
    },
    {
      "epoch": 975.0364963503649,
      "grad_norm": 16.12808609008789,
      "learning_rate": 1.248175182481752e-06,
      "loss": 1.0115,
      "step": 133580
    },
    {
      "epoch": 975.1094890510949,
      "grad_norm": 5.919936180114746,
      "learning_rate": 1.2445255474452555e-06,
      "loss": 0.6865,
      "step": 133590
    },
    {
      "epoch": 975.1824817518249,
      "grad_norm": 0.029044732451438904,
      "learning_rate": 1.2408759124087592e-06,
      "loss": 0.8245,
      "step": 133600
    },
    {
      "epoch": 975.2554744525547,
      "grad_norm": 9.730912208557129,
      "learning_rate": 1.2372262773722628e-06,
      "loss": 0.419,
      "step": 133610
    },
    {
      "epoch": 975.3284671532847,
      "grad_norm": 6.360228538513184,
      "learning_rate": 1.2335766423357665e-06,
      "loss": 0.8061,
      "step": 133620
    },
    {
      "epoch": 975.4014598540145,
      "grad_norm": 9.95789623260498,
      "learning_rate": 1.2299270072992702e-06,
      "loss": 1.1064,
      "step": 133630
    },
    {
      "epoch": 975.4744525547445,
      "grad_norm": 0.04139212891459465,
      "learning_rate": 1.2262773722627736e-06,
      "loss": 1.3354,
      "step": 133640
    },
    {
      "epoch": 975.5474452554745,
      "grad_norm": 15.435582160949707,
      "learning_rate": 1.2226277372262773e-06,
      "loss": 1.0934,
      "step": 133650
    },
    {
      "epoch": 975.6204379562043,
      "grad_norm": 7.976717472076416,
      "learning_rate": 1.218978102189781e-06,
      "loss": 1.2854,
      "step": 133660
    },
    {
      "epoch": 975.6934306569343,
      "grad_norm": 13.866148948669434,
      "learning_rate": 1.2153284671532847e-06,
      "loss": 1.5193,
      "step": 133670
    },
    {
      "epoch": 975.7664233576643,
      "grad_norm": 10.942768096923828,
      "learning_rate": 1.2116788321167885e-06,
      "loss": 0.7899,
      "step": 133680
    },
    {
      "epoch": 975.8394160583941,
      "grad_norm": 19.612852096557617,
      "learning_rate": 1.208029197080292e-06,
      "loss": 0.8466,
      "step": 133690
    },
    {
      "epoch": 975.9124087591241,
      "grad_norm": 0.10557372868061066,
      "learning_rate": 1.2043795620437957e-06,
      "loss": 0.4988,
      "step": 133700
    },
    {
      "epoch": 975.985401459854,
      "grad_norm": 2.044260263442993,
      "learning_rate": 1.2007299270072993e-06,
      "loss": 0.6131,
      "step": 133710
    },
    {
      "epoch": 976.0583941605839,
      "grad_norm": 8.132240295410156,
      "learning_rate": 1.197080291970803e-06,
      "loss": 1.1003,
      "step": 133720
    },
    {
      "epoch": 976.1313868613139,
      "grad_norm": 1.522094488143921,
      "learning_rate": 1.1934306569343067e-06,
      "loss": 0.4668,
      "step": 133730
    },
    {
      "epoch": 976.2043795620438,
      "grad_norm": 24.297990798950195,
      "learning_rate": 1.1897810218978104e-06,
      "loss": 0.8227,
      "step": 133740
    },
    {
      "epoch": 976.2773722627737,
      "grad_norm": 0.06339807063341141,
      "learning_rate": 1.1861313868613138e-06,
      "loss": 0.7848,
      "step": 133750
    },
    {
      "epoch": 976.3503649635037,
      "grad_norm": 6.912104606628418,
      "learning_rate": 1.1824817518248175e-06,
      "loss": 0.9341,
      "step": 133760
    },
    {
      "epoch": 976.4233576642335,
      "grad_norm": 8.790748596191406,
      "learning_rate": 1.1788321167883212e-06,
      "loss": 0.6562,
      "step": 133770
    },
    {
      "epoch": 976.4963503649635,
      "grad_norm": 9.189910888671875,
      "learning_rate": 1.1751824817518248e-06,
      "loss": 0.5974,
      "step": 133780
    },
    {
      "epoch": 976.5693430656934,
      "grad_norm": 7.048892021179199,
      "learning_rate": 1.1715328467153285e-06,
      "loss": 1.1116,
      "step": 133790
    },
    {
      "epoch": 976.6423357664233,
      "grad_norm": 6.7818922996521,
      "learning_rate": 1.1678832116788322e-06,
      "loss": 1.0367,
      "step": 133800
    },
    {
      "epoch": 976.7153284671533,
      "grad_norm": 21.91817855834961,
      "learning_rate": 1.1642335766423358e-06,
      "loss": 1.051,
      "step": 133810
    },
    {
      "epoch": 976.7883211678832,
      "grad_norm": 9.666608810424805,
      "learning_rate": 1.1605839416058395e-06,
      "loss": 1.1068,
      "step": 133820
    },
    {
      "epoch": 976.8613138686131,
      "grad_norm": 11.490086555480957,
      "learning_rate": 1.1569343065693432e-06,
      "loss": 0.6767,
      "step": 133830
    },
    {
      "epoch": 976.9343065693431,
      "grad_norm": 0.02043457329273224,
      "learning_rate": 1.1532846715328468e-06,
      "loss": 1.1192,
      "step": 133840
    },
    {
      "epoch": 977.007299270073,
      "grad_norm": 17.492422103881836,
      "learning_rate": 1.1496350364963503e-06,
      "loss": 0.6522,
      "step": 133850
    },
    {
      "epoch": 977.0802919708029,
      "grad_norm": 13.571052551269531,
      "learning_rate": 1.145985401459854e-06,
      "loss": 0.5374,
      "step": 133860
    },
    {
      "epoch": 977.1532846715329,
      "grad_norm": 11.228606224060059,
      "learning_rate": 1.1423357664233576e-06,
      "loss": 0.7074,
      "step": 133870
    },
    {
      "epoch": 977.2262773722628,
      "grad_norm": 0.035598915070295334,
      "learning_rate": 1.1386861313868613e-06,
      "loss": 0.609,
      "step": 133880
    },
    {
      "epoch": 977.2992700729927,
      "grad_norm": 0.06968695670366287,
      "learning_rate": 1.135036496350365e-06,
      "loss": 0.7432,
      "step": 133890
    },
    {
      "epoch": 977.3722627737226,
      "grad_norm": 4.218120098114014,
      "learning_rate": 1.1313868613138687e-06,
      "loss": 0.4297,
      "step": 133900
    },
    {
      "epoch": 977.4452554744526,
      "grad_norm": 16.740890502929688,
      "learning_rate": 1.1277372262773723e-06,
      "loss": 1.2124,
      "step": 133910
    },
    {
      "epoch": 977.5182481751825,
      "grad_norm": 17.208890914916992,
      "learning_rate": 1.124087591240876e-06,
      "loss": 1.0998,
      "step": 133920
    },
    {
      "epoch": 977.5912408759124,
      "grad_norm": 14.382762908935547,
      "learning_rate": 1.1204379562043797e-06,
      "loss": 1.0058,
      "step": 133930
    },
    {
      "epoch": 977.6642335766423,
      "grad_norm": 11.317822456359863,
      "learning_rate": 1.1167883211678833e-06,
      "loss": 0.8649,
      "step": 133940
    },
    {
      "epoch": 977.7372262773723,
      "grad_norm": 1.8379725217819214,
      "learning_rate": 1.113138686131387e-06,
      "loss": 1.1664,
      "step": 133950
    },
    {
      "epoch": 977.8102189781022,
      "grad_norm": 0.6628218293190002,
      "learning_rate": 1.1094890510948905e-06,
      "loss": 1.2443,
      "step": 133960
    },
    {
      "epoch": 977.8832116788321,
      "grad_norm": 10.998638153076172,
      "learning_rate": 1.1058394160583941e-06,
      "loss": 0.857,
      "step": 133970
    },
    {
      "epoch": 977.956204379562,
      "grad_norm": 16.789505004882812,
      "learning_rate": 1.1021897810218978e-06,
      "loss": 1.1329,
      "step": 133980
    },
    {
      "epoch": 978.029197080292,
      "grad_norm": 4.519413948059082,
      "learning_rate": 1.0985401459854015e-06,
      "loss": 1.1759,
      "step": 133990
    },
    {
      "epoch": 978.1021897810219,
      "grad_norm": 11.417798042297363,
      "learning_rate": 1.0948905109489052e-06,
      "loss": 0.7654,
      "step": 134000
    },
    {
      "epoch": 978.1751824817518,
      "grad_norm": 17.096418380737305,
      "learning_rate": 1.0912408759124088e-06,
      "loss": 0.7378,
      "step": 134010
    },
    {
      "epoch": 978.2481751824818,
      "grad_norm": 10.40891170501709,
      "learning_rate": 1.0875912408759125e-06,
      "loss": 0.7092,
      "step": 134020
    },
    {
      "epoch": 978.3211678832117,
      "grad_norm": 8.487668991088867,
      "learning_rate": 1.0839416058394162e-06,
      "loss": 1.0969,
      "step": 134030
    },
    {
      "epoch": 978.3941605839416,
      "grad_norm": 12.668734550476074,
      "learning_rate": 1.0802919708029198e-06,
      "loss": 1.143,
      "step": 134040
    },
    {
      "epoch": 978.4671532846716,
      "grad_norm": 7.883432865142822,
      "learning_rate": 1.0766423357664235e-06,
      "loss": 0.8093,
      "step": 134050
    },
    {
      "epoch": 978.5401459854014,
      "grad_norm": 7.616788864135742,
      "learning_rate": 1.0729927007299272e-06,
      "loss": 0.8641,
      "step": 134060
    },
    {
      "epoch": 978.6131386861314,
      "grad_norm": 0.7096871733665466,
      "learning_rate": 1.0693430656934306e-06,
      "loss": 0.8299,
      "step": 134070
    },
    {
      "epoch": 978.6861313868613,
      "grad_norm": 7.9803009033203125,
      "learning_rate": 1.0656934306569343e-06,
      "loss": 0.994,
      "step": 134080
    },
    {
      "epoch": 978.7591240875912,
      "grad_norm": 14.01558780670166,
      "learning_rate": 1.062043795620438e-06,
      "loss": 1.0131,
      "step": 134090
    },
    {
      "epoch": 978.8321167883212,
      "grad_norm": 17.59442710876465,
      "learning_rate": 1.0583941605839416e-06,
      "loss": 0.7867,
      "step": 134100
    },
    {
      "epoch": 978.9051094890511,
      "grad_norm": 14.98000431060791,
      "learning_rate": 1.0547445255474453e-06,
      "loss": 0.794,
      "step": 134110
    },
    {
      "epoch": 978.978102189781,
      "grad_norm": 7.3409037590026855,
      "learning_rate": 1.0510948905109488e-06,
      "loss": 0.9226,
      "step": 134120
    },
    {
      "epoch": 979.051094890511,
      "grad_norm": 10.600119590759277,
      "learning_rate": 1.0474452554744527e-06,
      "loss": 1.1513,
      "step": 134130
    },
    {
      "epoch": 979.1240875912408,
      "grad_norm": 11.894423484802246,
      "learning_rate": 1.0437956204379563e-06,
      "loss": 1.063,
      "step": 134140
    },
    {
      "epoch": 979.1970802919708,
      "grad_norm": 14.761261940002441,
      "learning_rate": 1.04014598540146e-06,
      "loss": 0.8663,
      "step": 134150
    },
    {
      "epoch": 979.2700729927008,
      "grad_norm": 8.466672897338867,
      "learning_rate": 1.0364963503649637e-06,
      "loss": 0.6121,
      "step": 134160
    },
    {
      "epoch": 979.3430656934306,
      "grad_norm": 18.71304702758789,
      "learning_rate": 1.0328467153284673e-06,
      "loss": 0.8224,
      "step": 134170
    },
    {
      "epoch": 979.4160583941606,
      "grad_norm": 10.29464340209961,
      "learning_rate": 1.0291970802919708e-06,
      "loss": 1.0155,
      "step": 134180
    },
    {
      "epoch": 979.4890510948906,
      "grad_norm": 7.7845778465271,
      "learning_rate": 1.0255474452554745e-06,
      "loss": 0.7982,
      "step": 134190
    },
    {
      "epoch": 979.5620437956204,
      "grad_norm": 11.201035499572754,
      "learning_rate": 1.0218978102189781e-06,
      "loss": 1.1701,
      "step": 134200
    },
    {
      "epoch": 979.6350364963504,
      "grad_norm": 6.983160972595215,
      "learning_rate": 1.0182481751824818e-06,
      "loss": 1.0438,
      "step": 134210
    },
    {
      "epoch": 979.7080291970802,
      "grad_norm": 10.501988410949707,
      "learning_rate": 1.0145985401459855e-06,
      "loss": 0.9077,
      "step": 134220
    },
    {
      "epoch": 979.7810218978102,
      "grad_norm": 0.02215658128261566,
      "learning_rate": 1.010948905109489e-06,
      "loss": 0.682,
      "step": 134230
    },
    {
      "epoch": 979.8540145985402,
      "grad_norm": 6.712767124176025,
      "learning_rate": 1.0072992700729926e-06,
      "loss": 0.6446,
      "step": 134240
    },
    {
      "epoch": 979.92700729927,
      "grad_norm": 6.921072959899902,
      "learning_rate": 1.0036496350364963e-06,
      "loss": 0.8413,
      "step": 134250
    },
    {
      "epoch": 980.0,
      "grad_norm": 19.690641403198242,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.6086,
      "step": 134260
    },
    {
      "epoch": 980.07299270073,
      "grad_norm": 12.604455947875977,
      "learning_rate": 9.963503649635038e-07,
      "loss": 1.0787,
      "step": 134270
    },
    {
      "epoch": 980.1459854014598,
      "grad_norm": 16.002885818481445,
      "learning_rate": 9.927007299270075e-07,
      "loss": 0.9948,
      "step": 134280
    },
    {
      "epoch": 980.2189781021898,
      "grad_norm": 6.917233467102051,
      "learning_rate": 9.89051094890511e-07,
      "loss": 0.8513,
      "step": 134290
    },
    {
      "epoch": 980.2919708029198,
      "grad_norm": 8.340149879455566,
      "learning_rate": 9.854014598540146e-07,
      "loss": 1.0453,
      "step": 134300
    },
    {
      "epoch": 980.3649635036496,
      "grad_norm": 7.341466903686523,
      "learning_rate": 9.817518248175183e-07,
      "loss": 0.781,
      "step": 134310
    },
    {
      "epoch": 980.4379562043796,
      "grad_norm": 7.348719120025635,
      "learning_rate": 9.78102189781022e-07,
      "loss": 0.7677,
      "step": 134320
    },
    {
      "epoch": 980.5109489051094,
      "grad_norm": 7.834420680999756,
      "learning_rate": 9.744525547445256e-07,
      "loss": 0.9591,
      "step": 134330
    },
    {
      "epoch": 980.5839416058394,
      "grad_norm": 11.5671968460083,
      "learning_rate": 9.708029197080291e-07,
      "loss": 0.7168,
      "step": 134340
    },
    {
      "epoch": 980.6569343065694,
      "grad_norm": 9.626294136047363,
      "learning_rate": 9.671532846715328e-07,
      "loss": 0.7867,
      "step": 134350
    },
    {
      "epoch": 980.7299270072992,
      "grad_norm": 8.092097282409668,
      "learning_rate": 9.635036496350364e-07,
      "loss": 0.9786,
      "step": 134360
    },
    {
      "epoch": 980.8029197080292,
      "grad_norm": 0.02547435462474823,
      "learning_rate": 9.598540145985401e-07,
      "loss": 0.7351,
      "step": 134370
    },
    {
      "epoch": 980.8759124087592,
      "grad_norm": 0.023277781903743744,
      "learning_rate": 9.56204379562044e-07,
      "loss": 0.9973,
      "step": 134380
    },
    {
      "epoch": 980.948905109489,
      "grad_norm": 13.6021728515625,
      "learning_rate": 9.525547445255475e-07,
      "loss": 0.5865,
      "step": 134390
    },
    {
      "epoch": 981.021897810219,
      "grad_norm": 6.5277605056762695,
      "learning_rate": 9.489051094890511e-07,
      "loss": 0.8543,
      "step": 134400
    },
    {
      "epoch": 981.0948905109489,
      "grad_norm": 7.226444721221924,
      "learning_rate": 9.452554744525548e-07,
      "loss": 1.0122,
      "step": 134410
    },
    {
      "epoch": 981.1678832116788,
      "grad_norm": 5.608438968658447,
      "learning_rate": 9.416058394160585e-07,
      "loss": 0.8065,
      "step": 134420
    },
    {
      "epoch": 981.2408759124088,
      "grad_norm": 10.24271011352539,
      "learning_rate": 9.379562043795621e-07,
      "loss": 0.8262,
      "step": 134430
    },
    {
      "epoch": 981.3138686131387,
      "grad_norm": 12.233993530273438,
      "learning_rate": 9.343065693430658e-07,
      "loss": 0.873,
      "step": 134440
    },
    {
      "epoch": 981.3868613138686,
      "grad_norm": 8.678837776184082,
      "learning_rate": 9.306569343065693e-07,
      "loss": 0.8374,
      "step": 134450
    },
    {
      "epoch": 981.4598540145986,
      "grad_norm": 8.076805114746094,
      "learning_rate": 9.27007299270073e-07,
      "loss": 0.9019,
      "step": 134460
    },
    {
      "epoch": 981.5328467153284,
      "grad_norm": 8.507024765014648,
      "learning_rate": 9.233576642335767e-07,
      "loss": 0.8323,
      "step": 134470
    },
    {
      "epoch": 981.6058394160584,
      "grad_norm": 3.0573270320892334,
      "learning_rate": 9.197080291970804e-07,
      "loss": 0.7058,
      "step": 134480
    },
    {
      "epoch": 981.6788321167883,
      "grad_norm": 1.5389227867126465,
      "learning_rate": 9.160583941605841e-07,
      "loss": 0.9317,
      "step": 134490
    },
    {
      "epoch": 981.7518248175182,
      "grad_norm": 0.5033727288246155,
      "learning_rate": 9.124087591240875e-07,
      "loss": 0.9933,
      "step": 134500
    },
    {
      "epoch": 981.8248175182482,
      "grad_norm": 14.415705680847168,
      "learning_rate": 9.087591240875912e-07,
      "loss": 1.0466,
      "step": 134510
    },
    {
      "epoch": 981.8978102189781,
      "grad_norm": 10.904088020324707,
      "learning_rate": 9.05109489051095e-07,
      "loss": 1.0761,
      "step": 134520
    },
    {
      "epoch": 981.970802919708,
      "grad_norm": 0.06444276124238968,
      "learning_rate": 9.014598540145986e-07,
      "loss": 0.9494,
      "step": 134530
    },
    {
      "epoch": 982.043795620438,
      "grad_norm": 6.439438819885254,
      "learning_rate": 8.978102189781023e-07,
      "loss": 0.6082,
      "step": 134540
    },
    {
      "epoch": 982.1167883211679,
      "grad_norm": 0.18096649646759033,
      "learning_rate": 8.941605839416058e-07,
      "loss": 0.5998,
      "step": 134550
    },
    {
      "epoch": 982.1897810218978,
      "grad_norm": 0.02762463688850403,
      "learning_rate": 8.905109489051094e-07,
      "loss": 0.637,
      "step": 134560
    },
    {
      "epoch": 982.2627737226277,
      "grad_norm": 14.763357162475586,
      "learning_rate": 8.868613138686131e-07,
      "loss": 0.8548,
      "step": 134570
    },
    {
      "epoch": 982.3357664233577,
      "grad_norm": 15.226590156555176,
      "learning_rate": 8.832116788321169e-07,
      "loss": 1.3561,
      "step": 134580
    },
    {
      "epoch": 982.4087591240876,
      "grad_norm": 12.70237922668457,
      "learning_rate": 8.795620437956206e-07,
      "loss": 1.0238,
      "step": 134590
    },
    {
      "epoch": 982.4817518248175,
      "grad_norm": 20.298583984375,
      "learning_rate": 8.759124087591242e-07,
      "loss": 0.678,
      "step": 134600
    },
    {
      "epoch": 982.5547445255474,
      "grad_norm": 7.70018196105957,
      "learning_rate": 8.722627737226277e-07,
      "loss": 0.9939,
      "step": 134610
    },
    {
      "epoch": 982.6277372262774,
      "grad_norm": 17.54655647277832,
      "learning_rate": 8.686131386861314e-07,
      "loss": 1.1086,
      "step": 134620
    },
    {
      "epoch": 982.7007299270073,
      "grad_norm": 17.61493492126465,
      "learning_rate": 8.64963503649635e-07,
      "loss": 1.1092,
      "step": 134630
    },
    {
      "epoch": 982.7737226277372,
      "grad_norm": 6.629315376281738,
      "learning_rate": 8.613138686131388e-07,
      "loss": 1.3307,
      "step": 134640
    },
    {
      "epoch": 982.8467153284671,
      "grad_norm": 4.992305278778076,
      "learning_rate": 8.576642335766425e-07,
      "loss": 0.8592,
      "step": 134650
    },
    {
      "epoch": 982.9197080291971,
      "grad_norm": 0.057248152792453766,
      "learning_rate": 8.540145985401459e-07,
      "loss": 0.4985,
      "step": 134660
    },
    {
      "epoch": 982.992700729927,
      "grad_norm": 8.554936408996582,
      "learning_rate": 8.503649635036496e-07,
      "loss": 0.8235,
      "step": 134670
    },
    {
      "epoch": 983.0656934306569,
      "grad_norm": 19.95144271850586,
      "learning_rate": 8.467153284671533e-07,
      "loss": 0.9402,
      "step": 134680
    },
    {
      "epoch": 983.1386861313869,
      "grad_norm": 0.8048163652420044,
      "learning_rate": 8.430656934306569e-07,
      "loss": 1.2042,
      "step": 134690
    },
    {
      "epoch": 983.2116788321168,
      "grad_norm": 9.61133861541748,
      "learning_rate": 8.394160583941607e-07,
      "loss": 1.0799,
      "step": 134700
    },
    {
      "epoch": 983.2846715328467,
      "grad_norm": 0.022604338824748993,
      "learning_rate": 8.357664233576644e-07,
      "loss": 0.5681,
      "step": 134710
    },
    {
      "epoch": 983.3576642335767,
      "grad_norm": 7.831677436828613,
      "learning_rate": 8.321167883211679e-07,
      "loss": 0.5182,
      "step": 134720
    },
    {
      "epoch": 983.4306569343066,
      "grad_norm": 8.02419376373291,
      "learning_rate": 8.284671532846715e-07,
      "loss": 1.2209,
      "step": 134730
    },
    {
      "epoch": 983.5036496350365,
      "grad_norm": 0.01681067794561386,
      "learning_rate": 8.248175182481752e-07,
      "loss": 0.982,
      "step": 134740
    },
    {
      "epoch": 983.5766423357665,
      "grad_norm": 12.723241806030273,
      "learning_rate": 8.211678832116789e-07,
      "loss": 1.2054,
      "step": 134750
    },
    {
      "epoch": 983.6496350364963,
      "grad_norm": 7.322258949279785,
      "learning_rate": 8.175182481751826e-07,
      "loss": 0.7554,
      "step": 134760
    },
    {
      "epoch": 983.7226277372263,
      "grad_norm": 0.009866542182862759,
      "learning_rate": 8.138686131386861e-07,
      "loss": 0.9767,
      "step": 134770
    },
    {
      "epoch": 983.7956204379562,
      "grad_norm": 17.928068161010742,
      "learning_rate": 8.102189781021898e-07,
      "loss": 0.9488,
      "step": 134780
    },
    {
      "epoch": 983.8686131386861,
      "grad_norm": 17.155439376831055,
      "learning_rate": 8.065693430656934e-07,
      "loss": 0.8025,
      "step": 134790
    },
    {
      "epoch": 983.9416058394161,
      "grad_norm": 17.830909729003906,
      "learning_rate": 8.029197080291971e-07,
      "loss": 0.892,
      "step": 134800
    },
    {
      "epoch": 984.014598540146,
      "grad_norm": 0.014275970868766308,
      "learning_rate": 7.992700729927008e-07,
      "loss": 0.4508,
      "step": 134810
    },
    {
      "epoch": 984.0875912408759,
      "grad_norm": 11.669845581054688,
      "learning_rate": 7.956204379562043e-07,
      "loss": 0.7522,
      "step": 134820
    },
    {
      "epoch": 984.1605839416059,
      "grad_norm": 12.803975105285645,
      "learning_rate": 7.91970802919708e-07,
      "loss": 0.7072,
      "step": 134830
    },
    {
      "epoch": 984.2335766423357,
      "grad_norm": 8.76146411895752,
      "learning_rate": 7.883211678832117e-07,
      "loss": 1.034,
      "step": 134840
    },
    {
      "epoch": 984.3065693430657,
      "grad_norm": 8.306456565856934,
      "learning_rate": 7.846715328467154e-07,
      "loss": 0.7962,
      "step": 134850
    },
    {
      "epoch": 984.3795620437957,
      "grad_norm": 12.083850860595703,
      "learning_rate": 7.810218978102189e-07,
      "loss": 0.6969,
      "step": 134860
    },
    {
      "epoch": 984.4525547445255,
      "grad_norm": 15.166666984558105,
      "learning_rate": 7.773722627737227e-07,
      "loss": 1.04,
      "step": 134870
    },
    {
      "epoch": 984.5255474452555,
      "grad_norm": 0.05293683335185051,
      "learning_rate": 7.737226277372264e-07,
      "loss": 1.3686,
      "step": 134880
    },
    {
      "epoch": 984.5985401459855,
      "grad_norm": 3.474461793899536,
      "learning_rate": 7.700729927007299e-07,
      "loss": 0.568,
      "step": 134890
    },
    {
      "epoch": 984.6715328467153,
      "grad_norm": 9.571319580078125,
      "learning_rate": 7.664233576642336e-07,
      "loss": 0.8588,
      "step": 134900
    },
    {
      "epoch": 984.7445255474453,
      "grad_norm": 10.38591480255127,
      "learning_rate": 7.627737226277373e-07,
      "loss": 0.9965,
      "step": 134910
    },
    {
      "epoch": 984.8175182481751,
      "grad_norm": 9.965446472167969,
      "learning_rate": 7.591240875912408e-07,
      "loss": 0.9042,
      "step": 134920
    },
    {
      "epoch": 984.8905109489051,
      "grad_norm": 12.682323455810547,
      "learning_rate": 7.554744525547446e-07,
      "loss": 0.9392,
      "step": 134930
    },
    {
      "epoch": 984.9635036496351,
      "grad_norm": 0.09448017179965973,
      "learning_rate": 7.518248175182483e-07,
      "loss": 1.0357,
      "step": 134940
    },
    {
      "epoch": 985.0364963503649,
      "grad_norm": 7.15928840637207,
      "learning_rate": 7.481751824817519e-07,
      "loss": 0.8501,
      "step": 134950
    },
    {
      "epoch": 985.1094890510949,
      "grad_norm": 13.842442512512207,
      "learning_rate": 7.445255474452555e-07,
      "loss": 1.0318,
      "step": 134960
    },
    {
      "epoch": 985.1824817518249,
      "grad_norm": 5.70934534072876,
      "learning_rate": 7.408759124087591e-07,
      "loss": 0.5846,
      "step": 134970
    },
    {
      "epoch": 985.2554744525547,
      "grad_norm": 7.690746784210205,
      "learning_rate": 7.372262773722628e-07,
      "loss": 0.7769,
      "step": 134980
    },
    {
      "epoch": 985.3284671532847,
      "grad_norm": 5.140170097351074,
      "learning_rate": 7.335766423357665e-07,
      "loss": 0.9893,
      "step": 134990
    },
    {
      "epoch": 985.4014598540145,
      "grad_norm": 9.606157302856445,
      "learning_rate": 7.299270072992701e-07,
      "loss": 0.6391,
      "step": 135000
    },
    {
      "epoch": 985.4744525547445,
      "grad_norm": 9.715871810913086,
      "learning_rate": 7.262773722627738e-07,
      "loss": 0.7865,
      "step": 135010
    },
    {
      "epoch": 985.5474452554745,
      "grad_norm": 11.430502891540527,
      "learning_rate": 7.226277372262774e-07,
      "loss": 1.0337,
      "step": 135020
    },
    {
      "epoch": 985.6204379562043,
      "grad_norm": 14.33515453338623,
      "learning_rate": 7.18978102189781e-07,
      "loss": 0.5986,
      "step": 135030
    },
    {
      "epoch": 985.6934306569343,
      "grad_norm": 7.554815769195557,
      "learning_rate": 7.153284671532847e-07,
      "loss": 0.5263,
      "step": 135040
    },
    {
      "epoch": 985.7664233576643,
      "grad_norm": 13.802291870117188,
      "learning_rate": 7.116788321167883e-07,
      "loss": 1.1137,
      "step": 135050
    },
    {
      "epoch": 985.8394160583941,
      "grad_norm": 10.104124069213867,
      "learning_rate": 7.08029197080292e-07,
      "loss": 1.3623,
      "step": 135060
    },
    {
      "epoch": 985.9124087591241,
      "grad_norm": 4.494970321655273,
      "learning_rate": 7.043795620437957e-07,
      "loss": 1.0248,
      "step": 135070
    },
    {
      "epoch": 985.985401459854,
      "grad_norm": 8.790841102600098,
      "learning_rate": 7.007299270072993e-07,
      "loss": 0.7127,
      "step": 135080
    },
    {
      "epoch": 986.0583941605839,
      "grad_norm": 7.791629314422607,
      "learning_rate": 6.970802919708029e-07,
      "loss": 0.7758,
      "step": 135090
    },
    {
      "epoch": 986.1313868613139,
      "grad_norm": 8.962450981140137,
      "learning_rate": 6.934306569343066e-07,
      "loss": 1.0504,
      "step": 135100
    },
    {
      "epoch": 986.2043795620438,
      "grad_norm": 9.404264450073242,
      "learning_rate": 6.897810218978103e-07,
      "loss": 0.8484,
      "step": 135110
    },
    {
      "epoch": 986.2773722627737,
      "grad_norm": 0.02939670905470848,
      "learning_rate": 6.861313868613139e-07,
      "loss": 0.6764,
      "step": 135120
    },
    {
      "epoch": 986.3503649635037,
      "grad_norm": 11.311629295349121,
      "learning_rate": 6.824817518248175e-07,
      "loss": 0.9097,
      "step": 135130
    },
    {
      "epoch": 986.4233576642335,
      "grad_norm": 5.082568168640137,
      "learning_rate": 6.788321167883212e-07,
      "loss": 0.592,
      "step": 135140
    },
    {
      "epoch": 986.4963503649635,
      "grad_norm": 8.019625663757324,
      "learning_rate": 6.751824817518248e-07,
      "loss": 0.6785,
      "step": 135150
    },
    {
      "epoch": 986.5693430656934,
      "grad_norm": 11.834510803222656,
      "learning_rate": 6.715328467153285e-07,
      "loss": 1.6164,
      "step": 135160
    },
    {
      "epoch": 986.6423357664233,
      "grad_norm": 14.85368824005127,
      "learning_rate": 6.678832116788322e-07,
      "loss": 0.8817,
      "step": 135170
    },
    {
      "epoch": 986.7153284671533,
      "grad_norm": 11.655754089355469,
      "learning_rate": 6.642335766423359e-07,
      "loss": 1.0544,
      "step": 135180
    },
    {
      "epoch": 986.7883211678832,
      "grad_norm": 5.250353813171387,
      "learning_rate": 6.605839416058394e-07,
      "loss": 0.6693,
      "step": 135190
    },
    {
      "epoch": 986.8613138686131,
      "grad_norm": 0.011872492730617523,
      "learning_rate": 6.569343065693431e-07,
      "loss": 0.6501,
      "step": 135200
    },
    {
      "epoch": 986.9343065693431,
      "grad_norm": 8.622873306274414,
      "learning_rate": 6.532846715328467e-07,
      "loss": 1.1192,
      "step": 135210
    },
    {
      "epoch": 987.007299270073,
      "grad_norm": 7.698653697967529,
      "learning_rate": 6.496350364963504e-07,
      "loss": 1.0232,
      "step": 135220
    },
    {
      "epoch": 987.0802919708029,
      "grad_norm": 8.123069763183594,
      "learning_rate": 6.459854014598541e-07,
      "loss": 0.7213,
      "step": 135230
    },
    {
      "epoch": 987.1532846715329,
      "grad_norm": 12.258895874023438,
      "learning_rate": 6.423357664233577e-07,
      "loss": 0.6141,
      "step": 135240
    },
    {
      "epoch": 987.2262773722628,
      "grad_norm": 6.471399307250977,
      "learning_rate": 6.386861313868613e-07,
      "loss": 0.7287,
      "step": 135250
    },
    {
      "epoch": 987.2992700729927,
      "grad_norm": 13.765758514404297,
      "learning_rate": 6.35036496350365e-07,
      "loss": 1.4224,
      "step": 135260
    },
    {
      "epoch": 987.3722627737226,
      "grad_norm": 12.832935333251953,
      "learning_rate": 6.313868613138686e-07,
      "loss": 0.9357,
      "step": 135270
    },
    {
      "epoch": 987.4452554744526,
      "grad_norm": 7.886949062347412,
      "learning_rate": 6.277372262773724e-07,
      "loss": 1.0257,
      "step": 135280
    },
    {
      "epoch": 987.5182481751825,
      "grad_norm": 16.816457748413086,
      "learning_rate": 6.24087591240876e-07,
      "loss": 0.706,
      "step": 135290
    },
    {
      "epoch": 987.5912408759124,
      "grad_norm": 9.055760383605957,
      "learning_rate": 6.204379562043796e-07,
      "loss": 1.0518,
      "step": 135300
    },
    {
      "epoch": 987.6642335766423,
      "grad_norm": 8.08986759185791,
      "learning_rate": 6.167883211678833e-07,
      "loss": 0.815,
      "step": 135310
    },
    {
      "epoch": 987.7372262773723,
      "grad_norm": 0.043818071484565735,
      "learning_rate": 6.131386861313868e-07,
      "loss": 0.7426,
      "step": 135320
    },
    {
      "epoch": 987.8102189781022,
      "grad_norm": 6.6993327140808105,
      "learning_rate": 6.094890510948905e-07,
      "loss": 0.7326,
      "step": 135330
    },
    {
      "epoch": 987.8832116788321,
      "grad_norm": 8.4712495803833,
      "learning_rate": 6.058394160583943e-07,
      "loss": 0.9629,
      "step": 135340
    },
    {
      "epoch": 987.956204379562,
      "grad_norm": 0.07726509124040604,
      "learning_rate": 6.021897810218978e-07,
      "loss": 0.705,
      "step": 135350
    },
    {
      "epoch": 988.029197080292,
      "grad_norm": 0.03285384923219681,
      "learning_rate": 5.985401459854015e-07,
      "loss": 0.7894,
      "step": 135360
    },
    {
      "epoch": 988.1021897810219,
      "grad_norm": 7.399845123291016,
      "learning_rate": 5.948905109489052e-07,
      "loss": 0.3116,
      "step": 135370
    },
    {
      "epoch": 988.1751824817518,
      "grad_norm": 1.4998879432678223,
      "learning_rate": 5.912408759124087e-07,
      "loss": 1.093,
      "step": 135380
    },
    {
      "epoch": 988.2481751824818,
      "grad_norm": 0.014874642714858055,
      "learning_rate": 5.875912408759124e-07,
      "loss": 0.8641,
      "step": 135390
    },
    {
      "epoch": 988.3211678832117,
      "grad_norm": 10.86375617980957,
      "learning_rate": 5.839416058394161e-07,
      "loss": 1.045,
      "step": 135400
    },
    {
      "epoch": 988.3941605839416,
      "grad_norm": 8.348004341125488,
      "learning_rate": 5.802919708029198e-07,
      "loss": 1.1464,
      "step": 135410
    },
    {
      "epoch": 988.4671532846716,
      "grad_norm": 0.09770036488771439,
      "learning_rate": 5.766423357664234e-07,
      "loss": 0.9689,
      "step": 135420
    },
    {
      "epoch": 988.5401459854014,
      "grad_norm": 14.192669868469238,
      "learning_rate": 5.72992700729927e-07,
      "loss": 0.9996,
      "step": 135430
    },
    {
      "epoch": 988.6131386861314,
      "grad_norm": 5.9496870040893555,
      "learning_rate": 5.693430656934307e-07,
      "loss": 0.8025,
      "step": 135440
    },
    {
      "epoch": 988.6861313868613,
      "grad_norm": 9.168587684631348,
      "learning_rate": 5.656934306569343e-07,
      "loss": 0.8397,
      "step": 135450
    },
    {
      "epoch": 988.7591240875912,
      "grad_norm": 7.518235206604004,
      "learning_rate": 5.62043795620438e-07,
      "loss": 0.878,
      "step": 135460
    },
    {
      "epoch": 988.8321167883212,
      "grad_norm": 16.570951461791992,
      "learning_rate": 5.583941605839417e-07,
      "loss": 0.9337,
      "step": 135470
    },
    {
      "epoch": 988.9051094890511,
      "grad_norm": 15.956790924072266,
      "learning_rate": 5.547445255474452e-07,
      "loss": 1.3244,
      "step": 135480
    },
    {
      "epoch": 988.978102189781,
      "grad_norm": 0.029303619638085365,
      "learning_rate": 5.510948905109489e-07,
      "loss": 0.829,
      "step": 135490
    },
    {
      "epoch": 989.051094890511,
      "grad_norm": 5.479558944702148,
      "learning_rate": 5.474452554744526e-07,
      "loss": 0.469,
      "step": 135500
    },
    {
      "epoch": 989.1240875912408,
      "grad_norm": 14.693406105041504,
      "learning_rate": 5.437956204379562e-07,
      "loss": 1.6017,
      "step": 135510
    },
    {
      "epoch": 989.1970802919708,
      "grad_norm": 13.676132202148438,
      "learning_rate": 5.401459854014599e-07,
      "loss": 0.767,
      "step": 135520
    },
    {
      "epoch": 989.2700729927008,
      "grad_norm": 9.078880310058594,
      "learning_rate": 5.364963503649636e-07,
      "loss": 1.2182,
      "step": 135530
    },
    {
      "epoch": 989.3430656934306,
      "grad_norm": 8.084504127502441,
      "learning_rate": 5.328467153284672e-07,
      "loss": 0.9066,
      "step": 135540
    },
    {
      "epoch": 989.4160583941606,
      "grad_norm": 0.08201615512371063,
      "learning_rate": 5.291970802919708e-07,
      "loss": 0.7497,
      "step": 135550
    },
    {
      "epoch": 989.4890510948906,
      "grad_norm": 2.297593355178833,
      "learning_rate": 5.255474452554744e-07,
      "loss": 0.5157,
      "step": 135560
    },
    {
      "epoch": 989.5620437956204,
      "grad_norm": 12.405064582824707,
      "learning_rate": 5.218978102189782e-07,
      "loss": 0.726,
      "step": 135570
    },
    {
      "epoch": 989.6350364963504,
      "grad_norm": 9.586408615112305,
      "learning_rate": 5.182481751824818e-07,
      "loss": 0.8957,
      "step": 135580
    },
    {
      "epoch": 989.7080291970802,
      "grad_norm": 7.253145217895508,
      "learning_rate": 5.145985401459854e-07,
      "loss": 0.5886,
      "step": 135590
    },
    {
      "epoch": 989.7810218978102,
      "grad_norm": 8.240349769592285,
      "learning_rate": 5.109489051094891e-07,
      "loss": 1.1218,
      "step": 135600
    },
    {
      "epoch": 989.8540145985402,
      "grad_norm": 13.425204277038574,
      "learning_rate": 5.072992700729927e-07,
      "loss": 0.9943,
      "step": 135610
    },
    {
      "epoch": 989.92700729927,
      "grad_norm": 10.057165145874023,
      "learning_rate": 5.036496350364963e-07,
      "loss": 0.7992,
      "step": 135620
    },
    {
      "epoch": 990.0,
      "grad_norm": 19.789093017578125,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.7477,
      "step": 135630
    },
    {
      "epoch": 990.07299270073,
      "grad_norm": 11.797828674316406,
      "learning_rate": 4.963503649635038e-07,
      "loss": 1.0417,
      "step": 135640
    },
    {
      "epoch": 990.1459854014598,
      "grad_norm": 5.6364545822143555,
      "learning_rate": 4.927007299270073e-07,
      "loss": 1.0957,
      "step": 135650
    },
    {
      "epoch": 990.2189781021898,
      "grad_norm": 5.554722309112549,
      "learning_rate": 4.89051094890511e-07,
      "loss": 0.6544,
      "step": 135660
    },
    {
      "epoch": 990.2919708029198,
      "grad_norm": 0.9652275443077087,
      "learning_rate": 4.854014598540146e-07,
      "loss": 0.6244,
      "step": 135670
    },
    {
      "epoch": 990.3649635036496,
      "grad_norm": 0.041008759289979935,
      "learning_rate": 4.817518248175182e-07,
      "loss": 0.6933,
      "step": 135680
    },
    {
      "epoch": 990.4379562043796,
      "grad_norm": 8.207049369812012,
      "learning_rate": 4.78102189781022e-07,
      "loss": 0.6307,
      "step": 135690
    },
    {
      "epoch": 990.5109489051094,
      "grad_norm": 14.644950866699219,
      "learning_rate": 4.7445255474452557e-07,
      "loss": 1.1684,
      "step": 135700
    },
    {
      "epoch": 990.5839416058394,
      "grad_norm": 0.04836634173989296,
      "learning_rate": 4.7080291970802924e-07,
      "loss": 0.9452,
      "step": 135710
    },
    {
      "epoch": 990.6569343065694,
      "grad_norm": 10.622503280639648,
      "learning_rate": 4.671532846715329e-07,
      "loss": 0.906,
      "step": 135720
    },
    {
      "epoch": 990.7299270072992,
      "grad_norm": 11.889496803283691,
      "learning_rate": 4.635036496350365e-07,
      "loss": 0.7232,
      "step": 135730
    },
    {
      "epoch": 990.8029197080292,
      "grad_norm": 13.60328483581543,
      "learning_rate": 4.598540145985402e-07,
      "loss": 0.6753,
      "step": 135740
    },
    {
      "epoch": 990.8759124087592,
      "grad_norm": 0.028946341946721077,
      "learning_rate": 4.5620437956204376e-07,
      "loss": 1.188,
      "step": 135750
    },
    {
      "epoch": 990.948905109489,
      "grad_norm": 6.829273700714111,
      "learning_rate": 4.525547445255475e-07,
      "loss": 1.2135,
      "step": 135760
    },
    {
      "epoch": 991.021897810219,
      "grad_norm": 14.2467622756958,
      "learning_rate": 4.4890510948905115e-07,
      "loss": 1.0614,
      "step": 135770
    },
    {
      "epoch": 991.0948905109489,
      "grad_norm": 11.22042465209961,
      "learning_rate": 4.452554744525547e-07,
      "loss": 0.9898,
      "step": 135780
    },
    {
      "epoch": 991.1678832116788,
      "grad_norm": 0.028980286791920662,
      "learning_rate": 4.4160583941605844e-07,
      "loss": 0.9046,
      "step": 135790
    },
    {
      "epoch": 991.2408759124088,
      "grad_norm": 11.34528636932373,
      "learning_rate": 4.379562043795621e-07,
      "loss": 1.1465,
      "step": 135800
    },
    {
      "epoch": 991.3138686131387,
      "grad_norm": 0.012783036567270756,
      "learning_rate": 4.343065693430657e-07,
      "loss": 0.5821,
      "step": 135810
    },
    {
      "epoch": 991.3868613138686,
      "grad_norm": 6.377719879150391,
      "learning_rate": 4.306569343065694e-07,
      "loss": 0.9136,
      "step": 135820
    },
    {
      "epoch": 991.4598540145986,
      "grad_norm": 9.95975399017334,
      "learning_rate": 4.2700729927007297e-07,
      "loss": 0.8182,
      "step": 135830
    },
    {
      "epoch": 991.5328467153284,
      "grad_norm": 5.632920742034912,
      "learning_rate": 4.2335766423357664e-07,
      "loss": 0.9011,
      "step": 135840
    },
    {
      "epoch": 991.6058394160584,
      "grad_norm": 14.40340518951416,
      "learning_rate": 4.1970802919708036e-07,
      "loss": 0.7533,
      "step": 135850
    },
    {
      "epoch": 991.6788321167883,
      "grad_norm": 7.776165962219238,
      "learning_rate": 4.160583941605839e-07,
      "loss": 0.6932,
      "step": 135860
    },
    {
      "epoch": 991.7518248175182,
      "grad_norm": 18.340417861938477,
      "learning_rate": 4.124087591240876e-07,
      "loss": 0.805,
      "step": 135870
    },
    {
      "epoch": 991.8248175182482,
      "grad_norm": 11.982067108154297,
      "learning_rate": 4.087591240875913e-07,
      "loss": 1.1353,
      "step": 135880
    },
    {
      "epoch": 991.8978102189781,
      "grad_norm": 11.869797706604004,
      "learning_rate": 4.051094890510949e-07,
      "loss": 1.1257,
      "step": 135890
    },
    {
      "epoch": 991.970802919708,
      "grad_norm": 15.59540843963623,
      "learning_rate": 4.0145985401459856e-07,
      "loss": 0.9275,
      "step": 135900
    },
    {
      "epoch": 992.043795620438,
      "grad_norm": 6.775579452514648,
      "learning_rate": 3.978102189781022e-07,
      "loss": 0.7466,
      "step": 135910
    },
    {
      "epoch": 992.1167883211679,
      "grad_norm": 9.227781295776367,
      "learning_rate": 3.9416058394160584e-07,
      "loss": 1.1524,
      "step": 135920
    },
    {
      "epoch": 992.1897810218978,
      "grad_norm": 10.11252212524414,
      "learning_rate": 3.9051094890510946e-07,
      "loss": 0.9045,
      "step": 135930
    },
    {
      "epoch": 992.2627737226277,
      "grad_norm": 5.669538974761963,
      "learning_rate": 3.868613138686132e-07,
      "loss": 0.5721,
      "step": 135940
    },
    {
      "epoch": 992.3357664233577,
      "grad_norm": 7.128130912780762,
      "learning_rate": 3.832116788321168e-07,
      "loss": 0.3446,
      "step": 135950
    },
    {
      "epoch": 992.4087591240876,
      "grad_norm": 0.02062222920358181,
      "learning_rate": 3.795620437956204e-07,
      "loss": 0.8445,
      "step": 135960
    },
    {
      "epoch": 992.4817518248175,
      "grad_norm": 13.198503494262695,
      "learning_rate": 3.7591240875912414e-07,
      "loss": 0.9851,
      "step": 135970
    },
    {
      "epoch": 992.5547445255474,
      "grad_norm": 4.293448448181152,
      "learning_rate": 3.7226277372262776e-07,
      "loss": 0.6216,
      "step": 135980
    },
    {
      "epoch": 992.6277372262774,
      "grad_norm": 13.321056365966797,
      "learning_rate": 3.686131386861314e-07,
      "loss": 0.9079,
      "step": 135990
    },
    {
      "epoch": 992.7007299270073,
      "grad_norm": 16.94917106628418,
      "learning_rate": 3.6496350364963505e-07,
      "loss": 1.4861,
      "step": 136000
    }
  ],
  "logging_steps": 10,
  "max_steps": 137000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 624069092160.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
